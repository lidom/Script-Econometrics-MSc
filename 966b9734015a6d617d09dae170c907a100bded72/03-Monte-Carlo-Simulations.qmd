# Estimation Theory and Monte Carlo Simulations

In the following parts of the lecture, we will use Monte Carlo simulations in oder to check whether a certain estimator is able to estimate its (usually unknown) target parameter. In this chapter, we will learn what Monte Carlo simulations are and how they can be implemented. 


## Estimator vs. Estimate 

Let's assume that we have an iid random sample $\{X_1,\dots,X_n\}$ with 
$$
X_i\overset{iid}{\sim} F_X
$$ 
for all $i=1,\dots,n$, and let $\theta\in\mathbb{R}$ denote some parameter (e.g. the mean or the variance) of the distribution $F_X$. 

An **estimator** $\hat\theta_n$ of $\theta$ is a function of the random sample $X_1,\dots,X_n$,
$$
\hat\theta_n:=\hat\theta(X_1,\dots,X_n).
$$

Since $\hat\theta_n$ is a function of the random variables $X_1,\dots,X_n$, the estimator $\hat\theta_n$ is itself a **random variable**. 

The observed data $X_{1,obs},\dots,X_{n,obs}$ is assumed to be a certain realization of the random sample $X_1,\dots,X_n$. The corresponding **realization** of the estimator is called an **estimate** of $\theta$
$$
\hat\theta_{n,obs}=\hat\theta(X_{1,obs},\dots,X_{n,obs}).
$$

::: {.callout-note}
Often we do not use a distinguishing notation, but denote both the estimator and its realization as $\hat\theta_{n}$. This ambiguity is often convenient since both points of views can make sense in a given context. 
:::

**Examples:**

* The sample mean as an estimator of the population mean $E(X_i) =\theta$:
$$
\hat\theta_n=\bar{X}_n=\frac{1}{n}\sum_{i=1}^nX_i
$$ 

* The sample variance as an estimator of the population variance $Var(X_i) =\theta$:
$$
\hat\theta_n=s_{UB}^2=\frac{1}{n-1}\sum_{i=1}^n\left(X_i - \bar{X}_n\right)^2
$$ 


## Deriving the Distribution of Estimators

Usually, we do not know the distribution $F_X$ of the random sample $X_1,\dots,X_n$ and thus do not know the distribution of the estimator 
$$
\hat\theta_n=\hat\theta(X_1,\dots,X_n).
$$ 
This is a fundamental statistical problem and we need to overcome this problem in order to do statistical inference (hypothesis testing, etc.).  


There are different possibilities to derive/approximate the distribution of an estimator $\hat\theta_n$.  In times when when computers were expensive, statisticians mainly used mathematical derivations:  

* **Mathematical Derivation using Distributional Assumptions.** Assuming a certain distribution $F_X$ for the random sample $X_1,\dots,X_n$ allows us to derive the *exact* distribution of $\hat\theta_n$ mathematically. (We consider this option in @sec-ssinf.)
  * Pro: If the distributional assumption is correct, one has *exact* inference for each sample size $n$. 
  * Con: This option can fail miserably if the distributional assumption on $F_X$ is wrong. 
  * Con: Such mathematical derivations are often only possible for particular distributions $F_X$ like the normal distribution. 

* **Mathematical Derivation using Asymptotic Statistics.** Large sample $(n\to\infty)$ approximations (i.e. laws of large numbers and central limit theorems) allow us to derive the *approximate* distribution of $\hat\theta_n$. This option uses mathematical limit considerations by letting the sample size $n$ diverge to infinity. (We consider this option in @sec-lsinf.) 
  * Pro: Only a few qualitative distributional assumptions are needed. 
  * Con: The derived asymptotic ($n\to\infty$) distribution is only exact for the practically impossible case where $n=\infty$ and thus can fail to approximate the exact distribution of $\hat\theta_n$ for given (finite) sample sizes $n$; particularly if $n$ is small. 

With computers, we have further options to approximate the exact distribution of estimators using (pseudo-)random number generators. One example are Monte Carlo simulations:

* **Monte Carlo Simulations.** Let's say the distribution $F_X$ of the random sample were known. Then, using (pseudo-)random number generators, we can draw `B` many realizations 
\begin{align*}
&(X_{1,1,obs},\dots,X_{n,1,obs})\\
&(X_{1,2,obs},\dots,X_{n,2,obs})\\
& \hspace{2cm}\vdots \\
&(X_{1,B,obs},\dots,X_{n,B,obs})
\end{align*}
of the random sample $X_{1},\dots,X_{n}$ from $F_X$ and thus can compute `B` many realizations 
\begin{align*}
\hat\theta_{n,1,obs} &= \hat\theta(X_{1,1,obs},\dots,X_{n,1,obs})\\
\hat\theta_{n,2,obs} &= \hat\theta(X_{1,2,obs},\dots,X_{n,2,obs})\\
&\vdots\\
\hat\theta_{n,B,obs} &= \hat\theta(X_{1,B,obs},\dots,X_{n,B,obs})
\end{align*} 
of the estimator $\hat\theta_n$. <br>
This set of realizations $\hat\theta_{n,1,obs},\dots,\hat\theta_{n,B,obs}$ allows us then to approximate the exact distribution of $\hat\theta_n$ for given sample sizes $n$ and given distributions $F_X$, since the empirical distribution function 
$$
\hat{F}_{\hat{\theta}_n}(x)=\frac{1}{B}\sum_{j=1}^BI_{(\hat\theta_{n,1,obs} \leq x)}
$$
approximates the true (unknown) distribution function of $\hat{\theta}_n$
$$
F_{\hat{\theta}_n}(x)=P(\hat\theta_{n} \leq x)
$$
arbitrarily well as $B\to\infty,$ i.e.
$$
\sup_x\left| \hat{F}_{\hat{\theta}_n}(x) - F_{\hat{\theta}_n}(x)\right|\to 0\quad\text{as}\quad B\to\infty.
$$
almost surely as $B\to\infty$; see the famous [Glivenko‚ÄìCantelli theorem](https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem). (We use this option to *validate* theoretical statements in @sec-ssinf and @sec-lsinf.)

  * Pro: Works for basically every distributional assumption. 
  * Con: This option can fail miserably if the distributional assumption on $F_X$ is wrong. 


<!-- Monte Carlo Simulations rely on distributional assumptions and thus may fail just as the first option (Mathematical Derivation using Distributional Assumptions). However, Monte Carlo Simulations are much wider applicable than since we can use computers to sample data from basically every distribution.   -->

### Example: Sample Mean $\bar{X}_n$ {#sec-ExampleSampleMean}

Let $\{X_1,\dots,X_n\}$ be an iid random sample  with 
$$
X_i\overset{iid}{\sim} F_X,
$$ 
where 

* $F_X$ is a normal distribution $\mathcal{N}(\mu, \sigma^2)$ with 
  * mean $(\theta=)\mu=10$ and 
  * variance $\sigma^2=5$. 

To estimate the (usually unknown) mean value $\mu=10,$ we use the sample mean estimator
$$
\bar{X}_n =  \frac{1}{n}\sum_{i=1}^n X_i
$$


We consider two sample sizes $n=5$ and $n=50.$

::: {.callout-note}

## Mathematical Derivation using the Distributional Assumptions

Here we have specified the distribution $F_X$ completely by setting $F_X=\mathcal{N}(\mu=10,\sigma^2=5).$ This is such a simple case, that we can actually use mathematical derivations to derive the distribution of $\bar{X}_n.$ (Often, this is not possible.)

Observe that since $X_1\sim\mathcal{N}(\mu,\sigma^2)$ and $X_2\sim\mathcal{N}(\mu,\sigma^2)$
$$
X_1+X_2\sim\mathcal{N}(\mu + \mu, \sigma^2 + \sigma^2).
$$
Thus 
$$
\sum_{i=1}^nX_i\sim\mathcal{N}(n \mu, n \sigma^2).
$$
Multiplying by $\frac{1}{n}$ yields
\begin{align*}
\frac{1}{n}\sum_{i=1}^nX_i = \bar{X}_n 
&\sim\mathcal{N}\left(\frac{1}{n}n \mu, \frac{1}{n^2}n \sigma^2\right)\\[2ex]
\bar{X}_n &\sim\mathcal{N}\left( \mu, \frac{1}{n} \sigma^2\right).
\end{align*}

**Summing up:** If $X_i\overset{iid}{\sim}\mathcal{N}(\mu,\sigma^2),$ 
then the exact (exact for each $n$) distribution of $\bar{X}_n$ is given by 
$$
\bar{X}_n\sim\mathcal{N}\left( \mu, \frac{1}{n} \sigma^2\right).
$$

* For $\mu=10,$ $\sigma=5,$ $n=5$:
$$
\bar{X}_n\sim\mathcal{N}\left(10, 1\right).
$$
* For $\mu=10,$ $\sigma=5,$ $n=50$:
$$
\bar{X}_n\sim\mathcal{N}\left(10, 0.1\right).
$$

‚ö†Ô∏è Unfortunately, such a mathematical derivation works only for very simple estimators and only for simple (and completely specified) distributions $F_X.$

ü§ì But for this special case, we can now check, whether a Monte Carlo simulation is able to approximate the distribution of $\bar{X}_n\sim\mathcal{N}\left( \mu, \frac{1}{n} \sigma^2\right).$
:::



Next, we use a Monte Carlo simulation to approximate the distribution of the estimator 
$$
\bar{X}_n =  \frac{1}{n}\sum_{i=1}^n X_i
$$
of $\mu=10.$ 


The following `R` code generates `B` many realizations of the random sample $X_i\overset{iid}{\sim}\mathcal{N}(\mu,\sigma^2)$ with $\mu=10$ and $\sigma^2=5.$

\begin{align*}
&(X_{1,1,obs},\dots,X_{n,1,obs})\\
&(X_{1,2,obs},\dots,X_{n,2,obs})\\
& \hspace{2cm}\vdots \\
&(X_{1,B,obs},\dots,X_{n,B,obs})
\end{align*}
leading to `B` many realizations of the estimator $\bar{X}_n$ 
\begin{align*}
\bar{X}_{n,1,obs} &= \hat\theta(X_{1,1,obs},\dots,X_{n,1,obs})\\
\bar{X}_{n,2,obs} &= \hat\theta(X_{1,2,obs},\dots,X_{n,2,obs})\\
&\vdots\\
\bar{X}_{n,B,obs} &= \hat\theta(X_{1,B,obs},\dots,X_{n,B,obs})
\end{align*} 
These realizations are then used to approximate the true distribution of $\bar{X}_n.$

```{r}
## True parameter value 
mu            <- 10
## Number of Monte Carlo repetitions:
B             <- 10000
## Sequence of different sample sizes:
n_seq         <- c(5, 50)


## ######################################
## 1st Possibility: Using a for() loop ##
## ######################################

## Set seed for the random number generator to get reproducible results
set.seed(3)

# Container for the generated estimates:
estimates_mat <- matrix(NA, nrow = B, ncol = length(n_seq))

for(j in 1:length(n_seq)){
  ## select the sample size
  n <- n_seq[j]
  for(b in 1:B){
    ## generate realization of the random sample 
    X_sample <- rnorm(n = n, mean = mu, sd = sqrt(5))
    ## compute the sample mean and safe it
    estimates_mat[b,j] <- mean(X_sample)
  }
}

## #####################################
## 2nd Possibility: Using replicate() ##
## #####################################

## Set seed for the random number generator to get reproducible results
set.seed(3)

## Function that generates estimator realizations 
my_estimates_generator <- function(n){
  X_sample <- rnorm(n = n, mean = mu, sd = sqrt(5))
  ## compute the sample mean realization
  return(mean(X_sample))
}

estimates_mat <- cbind(
  replicate(B, my_estimates_generator(n = n_seq[1])),
  replicate(B, my_estimates_generator(n = n_seq[2]))
)
```

Based on the `B`$=10,000$ realizations of the estimator $\bar{X}_n$, we can compute the empirical density functions $\hat{F}_{X_n,B}$ (see @fig-ecdf) and histograms (see @fig-histdensplots) to get an idea about the true distribution of $\bar{X}_n.$ 

In this simple case, we also know the theoretical distribution function $F_{X_n}$ and density function which allows us to check the simulation results (see @fig-ecdf and @fig-histdensplots). 
```{r, fig.align="center", fig.cap=""}
#| label: fig-ecdf
#| fig-cap: "Empirical distribution functions $\\hat{F}_{X_{n},B}$ computed from the $(B=10000)$ simulated realizations $\\bar{X}_{n,1,obs},\\dots,\\bar{X}_{n,B,obs}$ and the theoretical distribution functions for $n=5,50.$ The empirical and the theoretical distribution functions match perfectly." 
library(scales)
par(mfrow=c(1,2))
plot(ecdf(estimates_mat[,1]), main="n=5", ylab="", xlab="", col = "black", xlim = range(estimates_mat[,1]), ylim=c(0,1.25))
mtext(expression(mu==10), side = 1, at = 10, line = 2.5)
curve(pnorm(x, mean=10, sd=sqrt(5/5)), add=T, col=alpha("blue", 0.25), lty = 3, lwd=4)
legend("topleft", legend = c("True Distr.-Function", "Empir. Distr.-Function"), 
      col = c("black", alpha("blue", 0.5)), lty = c(1,2), lwd = c(1.3, 2), bty = "n")
##      
plot(ecdf(estimates_mat[,2]), main="n=50", ylab="", xlab="", col = "black",  xlim = range(estimates_mat[,1]), ylim=c(0,1.25))
mtext(expression(mu==10), side = 1, at = 10, line = 2.5)
curve(pnorm(x, mean=10, sd=sqrt(5/50)), add=T, col=alpha("blue", 0.25), lty = 3, lwd=4)
legend("topleft", legend = c("True Distr.-Function", "Empir. Distr.-Function"), 
      col = c("black", alpha("blue", 0.5)), lty = c(1,2), lwd = c(1.3, 2), bty = "n")
```


```{r, fig.align="center", fig.cap=""}
#| label: fig-histdensplots
#| fig-cap: "Histrograms of $(B=10000)$ simulated realizations $\\bar{X}_{n,1,obs},\\dots,\\bar{X}_{n,B,obs}$ and true density functions for $n=5,50.$ The empirical (simulation based) histrograms and the theoretical density functions match perfectly." 
library(scales)
par(mfrow=c(1,2))
hist(estimates_mat[,1], main="n=5", xlab="",  xlim = range(estimates_mat[,1]), prob = TRUE, ylim = c(0, 1.5))
mtext(expression(mu==10), side = 1, at = 10, line = 2.5)
curve(dnorm(x, mean=10, sd=sqrt(5/5)), add=T, col=alpha("blue", 0.25), lty = 3, lwd=4)
legend("topleft", legend = c("True Density", "Histogram"), 
      col = c("black", alpha("blue", 0.5)), lty = c(1,2), lwd = c(1.3, 2), bty = "n")
hist(estimates_mat[,2], main="n=50", xlab="", xlim = range(estimates_mat[,1]), prob = TRUE, ylim = c(0, 1.5))
mtext(expression(mu==10), side = 1, at = 10, line = 2.5)
curve(dnorm(x, mean=10, sd=sqrt(5/50)), add=T, col=alpha("blue", 0.25), lty = 3, lwd=4)
legend("topleft", legend = c("True Density", "Histogram"), 
      col = c("black", alpha("blue", 0.5)), lty = c(1,2), lwd = c(1.3, 2), bty = "n")
```

Observations in @fig-ecdf and @fig-histdensplots: The empirical distribution functions and the histograms based on the simulated realizations 
$$
\bar{X}_{n,1,obs},\dots,\bar{X}_{n,B,obs}
$$ 
mach their theoretical counterparts almost perfectly since we chose a sufficient large number of $B=10000$ simulations. 

::: {.callout-tip}

## Take away message

We can use Monte Carlo simulations to approximate the *exact* distribution of an estimator $\hat{\theta}_n$ for given distributions $F_X$ of the underlying random sample. These approximations become arbitrarily precise as $B\to\infty$. 
:::

<!-- * At least on average, the estimates $\bar{X}_n$ are close to the target parameter $\mu=10$ for each sample size $n\in\{5,15,50\}$. This feature of the estimator's distribution is summarized by the **bias** (see next section) of an estimator.

* As the sample size increases, the distributions of the estimators $\bar{X}_n$ concentrate around the target parameter $\mu=10$. This feature of the estimator's distribution is summarized by the **mean squared error** (see next section) of an estimator.

Thus here the quality of the estimator $\bar{X}_n$ gets better as $n$ gets large. To describe the quality of estimators more compactly, statisticians/econometricians use specific metrics like bias, variance and the mean squared error of the distribution of a estimator $\hat\theta$. -->

## Assessing the Quality of Estimators 

<!-- But how good is a given estimator $\hat\theta_n$ for a given sample size?  -->

Any reasonable estimator $\hat\theta_n$ should be able to approximate the (usually unknown) parameter value $\theta$,
$$
\left(\text{random quantity}\right)\quad\hat\theta_n\approx\theta\quad\left(\text{deterministic parameter}\right),
$$
and the approximation should get better as the sample size increases, i.e. as $n\to\infty$. 


The simulation results shown in @fig-ecdf and @fig-histdensplots show this desired behavior for the case of $\hat{\theta}_n=\bar{X}_n.$ 

To check (via MC-Simulations) the quality of an estimator, one can look at the total distribution or density function of $\hat{\theta}_n$; as done in @fig-ecdf and @fig-histdensplots. However, it is often more convenient to consider only the most relevant features of the distribution of an estimator. 


Statisticians/econometricians use different metrics to assess the quality of an estimator $\hat\theta_n$. The most prominent metrics are:

* bias of an estimator $\hat{\theta}_n$
* variance and standard error of an estimator $\hat{\theta}_n$
* mean squared error (mse) of an estimator $\hat{\theta}_n$


::: {.callout-note icon=false}
##
::: {#def-bias}
## Bias of $\theta$

The **bias** of an estimator $\hat\theta_n$ is defined as

$$
\operatorname{Bias}\left(\hat\theta_n\right) = E\left(\hat\theta_n\right) - \theta.
$$
:::
:::

If an estimator $\hat\theta_n$ has no bias 
$$
\operatorname{Bias}\left(\hat\theta_n\right)=0
$$ 
for all $\theta$ and all sample sizes $n,$ we call it an **unbiased estimator**. 

Many modern estimators are *not* unbiased. However, every estimator should be at least **asymptotically unbiased**, i.e.
$$
\lim_{n\to\infty}\operatorname{Bias}\left(\hat\theta_n\right)=0
$$ 
for all $\theta.$


We would like to have estimators with a small (or zero) bias. 

If the bias of an estimator is small (or zero), we know that the distribution of the estimator is roughly (or exactly) centered around the true (usually unknown) parameter $\theta.$ 


However, also unbiased estimators $\hat{\theta}_n$ may still vary a lot around the parameter $\theta$ to be estimated. Therefore, is is also important to assess the variance of the estimator. 


::: {.callout-note icon=false}
##
::: {#def-var}

## Variance and Standard Error of $\theta$

The **variance** of an estimator $\hat\theta_n$ is defined equivalently to the variance of any other random variable

$$
Var\left(\hat\theta_n\right) = E\left[\left(\hat\theta_n - E(\hat\theta_n)\right)^2\right].
$$
The square root of the variance of an estimator is called **standard error** (not standard deviation) of $\hat\theta_n$, 
$$
\operatorname{SE}\left(\hat\theta_n\right) = \sqrt{Var\left(\hat\theta_n\right)}.
$$
:::
:::

We would like to have estimators with a small as possible variance, and the variance should decline as the sample size increases, such that $\lim_{n\to\infty}Var\left(\hat\theta_n\right)=0$.


::: {.callout-note icon=false}
##
::: {#def-mse}

## Mean Squared Error of $\theta$

The **mean squared error** of an estimator $\hat\theta_n$ is defined as

$$
\operatorname{MSE}\left(\hat\theta_n\right) =  E\left[\left(\hat\theta_n - \theta\right)^2\right].
$$
:::
:::

We would like to have estimators with a small as possible mean squared error, and the mean squared error should decline as the sample size increases, such that $\lim_{n\to\infty}\operatorname{MSE}\left(\hat\theta_n\right)=0$.

The following holds true:

* The mean squared error equals the sum of the squared bias and the variance: 

$$
\operatorname{MSE}\left(\hat\theta_n\right) = \left(\operatorname{Bias}\left(\hat\theta_n\right)\right)^2 +  Var\left(\hat\theta_n\right) 
$$

* For unbiased estimators (i.e. $E(\hat\theta_n)=\theta$) the mean squared error equals the variance, i.e.

$$
\underbrace{E\left[\left(\hat\theta_n - \theta\right)^2\right]}_{\operatorname{MSE}\left(\hat\theta_n\right)} = \underbrace{E\left[\left(\hat\theta_n - E\left(\hat\theta_n\right)\right)^2\right]}_{ Var\left(\hat\theta_n\right)} 
$$


Unfortunately, it is often difficult to derive the above assessment metrics for given sample sizes $n$ and given data distributions $F_X$. Monte Carlo simulations allow us to solve this issue.

### Approximating Bias, Variance, and MSE using MC Simulations 

We can use Monte Carlo simulations to approximate the assessment metrics $\operatorname{Bias}\left(\hat\theta_n\right),$ $Var\left(\hat\theta_n\right),$ and  $\operatorname{MSE}\left(\hat\theta_n\right)$ for given sample sizes $n$ and given data distributions $F_X$ with arbitrary precision. 


Any of the the above assessment metrics require us to compute means of random variables: 

* For the $\operatorname{Bias}\left(\hat\theta_n\right)$ we need to compute $E\left(\hat\theta_n\right)-\theta$

* For the $Var\left(\hat\theta_n\right)$ we need to compute $E\left[\left(\hat\theta_n - E(\hat\theta_n)\right)^2\right]$.

* For the $\operatorname{MSE}\left(\hat\theta_n\right)$ we need to compute $E\left[\left(\hat\theta_n - \theta\right)^2\right]$.


A Monte Carlo simulation can approximate these means by using the **law of large numbers** which states that a sample mean over iid random variables is able to approximate the population mean of these random variables as the number of random variables to average over get large.[^1]

[^1]: See @thm-SLLN1 in @sec-lsinf.


Thus, to compute a very precise approximation to $E\left(\hat\theta_n\right)-\theta$, we can use a computer to execute the following algorithm:

**Step 1.** Generate $B$ many (e.g. $B=10,000$) realizations of the iid random sample 

$$
(X_{1,1},\dots,X_{n,1}),\; (X_{1,2},\dots,X_{n,2}),\dots, (X_{1,B},\dots,X_{n,B})
$$

**Step 2.** Compute the corresponding $B$ many realizations of the estimator 

$$
\underbrace{\hat\theta(X_{1,1},\dots,X_{n,1})}_{=\hat\theta_{n,1}},\;\underbrace{\hat\theta(X_{1,2},\dots,X_{n,2})}_{=\hat\theta_{n,2}},\dots,\underbrace{\hat\theta(X_{1,B},\dots,X_{n,B})}_{=\hat\theta_{n,B}}
$$ 
**Step 3.** Use the simulated realizations $\hat{\theta}_{n,1},\dots,\hat{\theta}_{n,B}$ to approximate the bias, variance, and the mean squared error of the estimator $\hat{\theta}_n$:

* The bias of $\operatorname{Bias}\left(\hat\theta_n\right)=E\left(\hat\theta_n\right)-\theta$ can be approximated by 

$$
\widehat{\operatorname{Bias}}_{MC}\left(\hat\theta_n\right) = \left(\frac{1}{B}\sum_{b=1}^B \hat\theta_{n,b}\right) - \theta 
$$

* The variance $Var\left(\hat\theta_n\right)=E\left[\left(\hat\theta_n - E(\hat\theta_n)\right)^2\right]$ can be approximated by
$$
\widehat{Var}_{MC}\left(\hat\theta_n\right) = \frac{1}{B}\sum_{b=1}^B \left(\hat\theta_{n,b} - \left(\frac{1}{B}\sum_{b=1}^B \hat\theta_{n,b}\right)\right)^2 
$$

* The mean squared error $\operatorname{MSE}\left(\hat\theta_n\right)=E\left[\left(\hat\theta_n - \theta\right)^2\right]$ can be approximated by 
$$
\widehat{\operatorname{MSE}}_{MC}\left(\hat\theta_n\right) = \frac{1}{B}\sum_{b=1}^B \left(\hat\theta_{n,b} - \theta\right)^2 
$$


By law of large numbers these approximations get arbitrarily precise as $B \to \infty$. 


### Example (revisited): Sample Mean {-}

The following `R` code contains a Monte Carlo simulation ( `B = 10000` replications) computing the bias, variance, and means squared error for the sample mean $(\hat\theta_n=)\bar{X}_n=\sum_{i=1}^nX_i$ as the estimator of the population mean $(\theta=)\mu$. The random sample $X_i\overset{iid}{\sim}F_X$, $i=1,\dots,n$, is drawn from a normal distribution $F_X=\mathcal{N}(\mu,\sigma^2)$ with mean $\mu=10$ and variance $\sigma^2=5$. We investigate the accuracy of the estimator for different sample sizes $n\in\{5,15,50\}$. 
<!-- The following `R` codes generated `B = 10000` realizations of the estimator $\bar{X}_n$ for each sample size $n\in\{5,15,50\}$ and stores all these realizations in a $10000\times 3$ matrix `estimates_mat`: -->
```{r}
## Set seed for the random number generator to get reproducible results
set.seed(3)
## True parameter value ('theta' here 'mu')
mu            <- 10
## Number of Monte Carlo repetitions:
B             <- 10000
## Sequence of different sample sizes:
n_seq         <- c(5, 15, 50)

## Function that generates estimator realizations 
my_estimates_generator <- function(n){
  X_sample <- rnorm(n = n, mean = mu, sd = sqrt(5))
  ## compute the sample mean realization
  return(mean(X_sample))
}

estimates_mat <- cbind(
  replicate(B, my_estimates_generator(n = n_seq[1])),
  replicate(B, my_estimates_generator(n = n_seq[2])),
  replicate(B, my_estimates_generator(n = n_seq[3]))
)
```



The following `R` code computes the Monte Carlo (MC) approximations for the bias, variance, and mean squared error of $\bar{X}_n$. The results should capture our observations. 
```{r}
## Bias of the sample mean for different sample sizes n
MC_Bias_n_seq <- apply(estimates_mat, 2, mean) - mu

## Variance of the sample mean for different sample sizes n
MC_Var_n_seq  <- apply(estimates_mat, 2, var)

## Mean squared error of the sample mean for different sample sizes n
MC_MSE_n_seq  <- apply(estimates_mat, 2, function(x){mean((x - mu)^2)})
```



The table shows the numerical values of the Monte Carlo *approximations* for the true bias, true variance, and true mean squared error of $\bar{X}_n$:
```{r, echo=FALSE}
#| label: tbl-mcbvmse
#| tbl-cap: Monte Carlo approximations for the true bias, true variance, and true mean squared error of sample mean.
suppressPackageStartupMessages(library("kableExtra"))
suppressPackageStartupMessages(library("tidyverse"))

MCResults <- tibble(
  "n"                  = n_seq,
  "Bias (MC-Sim) "     = round(MC_Bias_n_seq, 3),
  "Variance (MC-Sim)"  = round(MC_Var_n_seq,  2),
  "MSE (MC-Sim) "      = round(MC_MSE_n_seq,  2))
           
MCResults %>% kbl() %>%  kable_styling()
```


These Monte Carlo approximations (@tbl-mcbvmse) indicate that:  

- The true bias $\operatorname{Bias}(\bar{X}_n)$ is very likely zero for all sample sizes $n\in\{5,15,50\}$

<!-- , and thus $Var(\bar{X}_n)\approx \operatorname{MSE}(\bar{X}_n)$ for all sample sizes $n\in\{5,15,50\}$. -->

- The true mean squared error $\operatorname{MSE}(\bar{X}_n)$ is very likely decreasing as the sample size $n$ get larger. 


<!-- The sample mean $\bar{X}_n$ is known to be a very good estimator of the population mean $\mu$ and the above simulation results conform this.  -->


In @sec-ExampleSampleMean, we have already derived the exact theoretical distribution of $\bar{X}_n$ under the assumed random sample with sampling distribution $F_X=\mathcal{N}(\mu,\sigma^2):$
$$
\bar{X}_n\sim\mathcal{N}\left(\mu,\frac{1}{n}\sigma^2\right).
$$ 

Using this, we can simply compute the *true* bias, variance and mean squared error of $\bar{X}_n$ for $n\in\{5,15,50\}$ and compare them with their Monte Carlo approximations: 

* True bias of $\bar{X}_n$:
\begin{align*}
\operatorname{Bias}\left(\bar{X}_n\right)
&=E\left(\bar{X}_n\right) - \mu \\[2ex]
&=E\left(\frac{1}{n}\sum_{i=1}^n X_i\right) - \mu \\[2ex]
&= \left(\frac{1}{n}\sum_{i=1}^nE(X_i)\right) -\mu\\[2ex] 
&= \frac{n}{n}\mu-\mu \\[2ex]
&=0,
\end{align*}
thus $\bar{X}_n$ is unbiased for all $\mu.$

* True variance of $\bar{X}_n$:
\begin{align*}
Var\left(\bar{X}_n\right)
&=Var\left(\frac{1}{n}\sum_{i=1}^n X_i\right)\\[2ex] 
&= \frac{1}{n^2} \sum_{i=1}^nVar\left(X_i\right)\\[2ex]
&= \frac{n}{n^2}\sigma^2 \\[2ex]
&= \frac{1}{n}\sigma^2 
\end{align*}

* True MSE of $\bar{X}_n$:
\begin{align*}
\operatorname{MSE}\left(\bar{X}_n\right)
&=\left(\operatorname{Bias}\left(\bar{X}_n\right)\right)^2 +
Var\left(\frac{1}{n}\sum_{i=1}^nX_i\right)\\[2ex]
&= 0+\frac{1}{n}\sigma^2 
\end{align*}

The following table shows the true bias, true variance and true mean squared error values for $\sigma^2=5$ and $n\in\{5,15,50\}$:

```{r, echo=FALSE}
#| label: tbl-truebvmse
#| tbl-cap: True bias, true variance, and true mean squared error of sample mean. (Only computable in simple special cases.)
suppressPackageStartupMessages(library("kableExtra"))
suppressPackageStartupMessages(library("tidyverse"))

MCResults <- tibble(
  "n"        = n_seq,
  "Bias (true) "     = rep(0, 3),
  "Variance (true)"  = round(5/n_seq, 2),
  "MSE (true) "      = round(5/n_seq, 2))
           
MCResults %>% kbl() %>%  kable_styling()
```

Obviously, the Monte Carlo approximations (@tbl-mcbvmse) for these true values (@tbl-truebvmse) are very good. 

If we would further increase the number of Monte Carlo repetitions `B`, the Monte Carlo approximations in @tbl-mcbvmse would get even more precise since we can make them arbitrarily precise by letting `B`$\to\infty$ using the law of large numbers. 


::: {.callout-tip}

## Take away message (continued)

We can use Monte Carlo simulations to approximate the *exact* distribution and its features (bias, variance, mse) of an estimator $\hat{\theta}_n$ for given distributions $F_X$ of the underlying random sample. These approximations become arbitrarily precise as $B\to\infty$. 

We Monte Carlo simulations can be used to check estimation procedures when applied to specific data generating processes and sample sizes. 

MC simulations are an important tool, since modern estimation procedures are usually only justified based on asymptotic statistics $(n\to\infty)$ and sometimes only under relative restrictive assumptions. 

In such cases, MC simulations can be used to investigate the behavior of the estimation procedure in practical, finite sample sizes $n$ and under different data generating processes. 

MC simulations can also help to get an idea about how restrictive theoretical assumptions are in practice and how sensitive the an estimation procedure is with respect to violations of the theoretical assumptions.  
:::


<!-- ## Exercises -->

<!-- * [Exercises for Chapter 4](Exercises/Ch4_Exercises.pdf) -->

<!-- * [Exercises of Chapter 4 with Solutions](Exercises/Ch4_Exercises_with_Solutions.pdf) -->








<!-- ```{r, fig.align="center"} -->
<!-- par(mfrow=c(1,3)) -->
<!-- plot(x = n_seq, y = Bias_n_seq, type = "b", ylim = c(-0.2, 0.2),  -->
<!--      main="Bias", xlab = "n", ylab = "") -->
<!-- abline(h = 0) -->
<!-- plot(x = n_seq, y = Var_n_seq, type = "b", ylim = c(0, 1.5), -->
<!--      main="Variance", xlab = "n", ylab = "") -->
<!-- abline(h = 0) -->
<!-- plot(x = n_seq, y = MSE_n_seq, type = "b", ylim = c(0, 1.5), -->
<!--      main="MSE", xlab = "n", ylab = "") -->
<!-- abline(h = 0) -->
<!-- ``` -->




<!-- like, for instance, the arithmetic mean   -->
<!-- $$ -->
<!-- \bar{X}_n=\frac{1}{n}\sum_{i=1}^nX_i -->
<!-- $$ -->
<!-- as an estimator of the population mean value $\mu$.  -->

