<!-- LTeX: language=en-US -->
# Small Sample Inference {#sec-ssinf}

::: {.hidden}
$$
\require{color}
%% Colorbox within equation-environments:
\newcommand{\highlight}[2][yellow]{\mathchoice%
  {\colorbox{#1}{$\displaystyle#2$}}%
  {\colorbox{#1}{$\displaystyle#2$}}%
  {\colorbox{#1}{$\displaystyle#2$}}%
  }%
$$
:::

> The original version of this chapter was inspired by Chapter 1 of @Hayashi2000. The current version, however, deviates in many aspects from @Hayashi2000. 


In this chapter, we focus on inference with small sample sizes. It's is very hard to say when a sample size $n$ is small. A very rough rule of thumb could be, for instance, the following one: 

* $n/K<20$ means small samples and $n/K\geq 20$ large samples

The core issue with small sample sizes is that we cannot do inference using the law of large numbers and the central limit theorem---both requiring $n\to\infty.$ Thus we need rather strict assumptions on the distribution of the error term, in order to do inference in finite samples. If these assumption are fulfilled, however, then we do **exact inference**. 

**Exact inference:**  By *exact inference* we mean correct inference for each sample size $n$. That is, no approximate results based on asymptotic $(n\to\infty)$ arguments will be used.  


## Normality of the OLS-Estimator


**Assumptions:** In @sec-MLR, we did not impose a complete distributional assumption on $\varepsilon$ (see Assumption 4). For instance, the i.i.d. normal case in Assumption 4 was only one possible *option*.  However, to do inference in small samples, the normality Assumption on the error terms is not a mere option, but a *necessity*. 

Therefore, in this chapter we assume that Assumptions 1-3 from @sec-MLR hold and that additionally the following assumption holds: 

**Assumption 4$^\boldsymbol{\ast}$: Conditional Gaussian error distribution:** The error terms are 
Gaussian and homoskedastik, i.e., 
$$
\varepsilon_i|X_i\sim\mathcal{N}(0,\sigma^2)
$$ 
for all $i=1,\dots,n.$ 


Assumption 4$^\boldsymbol{\ast}$ together with the random sample assumption of Assumption 1, part (b), leads to Gaussian spherical errors,
$$
\varepsilon|X\sim\mathcal{N}_n\left(0,\sigma^2I_n\right),
$$
where $\varepsilon=(\varepsilon_1,\dots,\varepsilon_n)',$ and where $\mathcal{N}_n\left(0,\sigma^2I_n\right)$ denotes the $n$-dimensional normal distribution with $(n\times 1)$-dimensional mean zero vector $0$ and $(n\times n)$-dimensional covariance matrix $\sigma^2I_n.$ 

<!-- I.e. the distribution of $\varepsilon$ is here independent of $X.$ -->

\bigskip

::: {.callout icon=false}
##
::: {#thm-normalbeta}
# Normality of $\hat\beta$ 

Under Assumptions 1-4$^\ast$ we have that 
$$
\hat\beta_n|X \sim \mathcal{N}_K\left(\beta,Var(\hat\beta_n|X)\right),
$${#eq-ssnorm}
where 
$$
Var(\hat\beta_n|X)=\underset{(K\times K)}{\sigma^2(X'X)^{-1}}.
$$
:::
::: 

::: {.proof}
This result follows from noting that 
$$
\begin{align*}
\hat\beta_n
&=(X'X)^{-1}X'Y\\[2ex]
&=\beta+(X'X)^{-1}X'\varepsilon
\end{align*} 
$$
and because $(X'X)^{-1}X'\varepsilon$ is just a linear combination of the normally distributed error terms $\varepsilon$ which, therefore, is again normally distributed, conditionally on $X$. Note that the specific normal distribution depends on the observed realization of $X$. 
:::

**Remark:** The subscript $n$ in $\hat\beta_n$ is here only to emphasize that the distribution of $\hat\beta_n$ depends on $n$; we will, however, often simply write $\hat\beta$.


## $F$-Tests: Hypothesis Tests about Multiple Parameters {#sec-testmultp}

Let us consider the following system of $q$-many null hypotheses:
$$
\begin{align*}
H_0: \underset{(q\times K)}{R}\underset{(K\times 1)}{\beta}  = \underset{(q\times 1)}{r^{(0)}},
\end{align*}
$$
where 

* the $(q \times K)$ matrix $R,$ which describes the considered linear combinations of the unknown $\beta=(\beta_1,\dots,\beta_K)'$ values, and 
* the $(q\times 1)$ vector $r^{(0)}=(r^{(0)}_{1},\dots,r^{(0)}_{q})'$ of null hypothetical values

**are chosen by the statistician** to specify the null hypothesis about the unknown true parameter vector $\beta$. 

To make sure that there are no redundant equations, it is required that $\operatorname{rank}(R)=q$.

We must also specify the alternative against which we are testing the null hypothesis, for instance
$$
\begin{equation*}
H_1: R\beta  \neq r^{(0)}
\end{equation*}
$$

::: {.callout-note}
The above multiple parameter hypotheses cover also the special case of single parameter hypothesis; for instance, by setting 

* $R=(0,1,0\dots,0)$ and 
* $r^{(0)}=0$ 

we get the classic single parameter $H_0$ and $H_1$ that allows us to test the hypothesis that "$X_{i2}$ has no effect"  
$$
\begin{equation*}
\begin{array}{ll}
&H_0:  \beta_{2}=0 \\
\text{versus}\quad &H_1:  \beta_{2} \ne 0 \\
\end{array}
\end{equation*}
$$
We come back to this in @sec-testingsinglep. 
:::

Under our assumptions (Assumptions 1 to 4$^\ast$), we have that 
$$
\begin{align*}
(R\hat\beta_n-r^{(0)})|X&\sim\mathcal{N}_q\left(R\beta -r^{(0)}, RVar(\hat\beta_n|X)R'\right)%\\[2ex]
%\Leftrightarrow\quad \left(RVar(\hat\beta_n|X)R'\right)^{-1/2}(R\hat\beta_n-r^{(0)})|X&\sim\\[2ex]
%\hspace{2cm}\sim\mathcal{N}_q\left(\left(RVar(\hat\beta_n|X)R'\right)^{-1/2}(R\beta -r^{(0)}),I_q\right)
\end{align*}
$$ 


Thus, under the scenario that the null-hypothesis is true, we have that
$$
\begin{align*}
(R\hat\beta_n-r^{(0)})|X&\overset{{\color{red}{H_0}}}{\sim}\mathcal{N}_q\left(\,{\color{red}0}\,,RVar(\hat\beta_n|X)R'\right)\\[2ex]
\Leftrightarrow\quad \left(RVar(\hat\beta_n|X)R'\right)^{-1/2}(R\hat\beta_n-r^{(0)})\highlight{|X}&\overset{{\color{red}{H_0}}}{\sim}\underbrace{\mathcal{N}_q\left(\,{\color{red}0}\,,I_q\right)}_{\highlight{\text{doesn't depend on $X$}}}\\[2ex]
\highlight{\Rightarrow}\quad \left(RVar(\hat\beta_n|X)R'\right)^{-1/2}(R\hat\beta_n-r^{(0)})&\overset{{\color{red}{H_0}}}{\sim}\mathcal{N}_q\left(\,{\color{red}0}\,,I_q\right),
\end{align*}
$$ 
where the last implication follows, since the standardized normal distribution does not depend on $X.$

That is, if $H_0$ is correct (i.e., if $R\beta-r^{(0)}=0$), the realizations of 
$$
\left(RVar(\hat\beta_n|X)R'\right)^{-1/2}(R\hat\beta_n-r^{(0)})
$$ 
will scatter around the $(q\times 1)$ vector $0$ in a Gaussian fashion.  

We use a test statistic to detect a systematic location shift away from the $(q\times 1)$ vector $0.$  

<!-- * if the alternative hypothesis is correct (i.e., $(R\beta-r)=a\neq 0$), there will be a systematic location-shift in $(R\hat\beta_n-r)|X$ which we try to detect using statistical hypothesis testing.  -->

<!-- the realizations of $R\hat\beta_n-r|X$ scatter around the $q$-vector $a\neq 0$.  So, under the alternative hypothesis,  -->


### The Test Statistic and its Null Distribution

The fact that 
$$
(R\hat\beta_n-r^{(0)})\in\mathbb{R}^q
$$ 
is a $q$-dimensional random variable makes it a little bothersome to use as a test-statistic.  Fortunately, we can turn $(R\hat\beta_n-r^{(0)})$ into a scalar-valued test statistic using the following quadratic form:
$$
\begin{align*}
W 
& = \overbrace{\left(\left(RVar(\hat\beta_n|X)R'\right)^{-1/2}(R\hat\beta_n-r^{(0)})\right)'\;\;}^{(1\times q)}\\
&\;\;\;\;\;\;\underbrace{\left(\left(RVar(\hat\beta_n|X)R'\right)^{-1/2}(R\hat\beta_n-r^{(0)})\right)}_{(q \times 1)}\\[2ex]
& =\underbrace{(R\hat\beta_n -r^{(0)})'}_{(1\times q)}\underbrace{[RVar(\hat\beta_n|X)R']^{-1}}_{(q\times q)}\underbrace{(R\hat\beta_n -r^{(0)})}_{(q\times 1)}
\end{align*}
$${#eq-TestStatW}

::: {.callout-note}
Note that the test statistic $W$ is simply measuring the distance (it's a weighted, squared Euclidean distance) between the $(q\times 1)$ vectors $R\hat\beta_n$ and $r^{(0)}.$ 
:::

Under the null hypothesis (i.e., if $H_0$ is true), $W$ is a sum of $q$-many independent, squared standard normal $\mathcal{N}(0,1)$ random variables. Therefore, under the null hypothesis, $W$ is chi-square distributed with $q$ degrees of freedom (see definition of the chi-square distribution in @sec-chisqdist), 
$$
\begin{align*}
W&\overset{H_0}{\sim} \chi^2_{(q)}
\end{align*}
$$
Note that the distribution of $W$ does not depend on $X.$ I.e. $W$ follows a $\chi^2_{(q)}$-distribution no matter the realization of $X.$ 


Usually, however, we do not know $Var(\hat\beta_n|X)=\sigma^2 (X'X)^{-1},$ since we usually do not know the value of $\sigma^2.$ Thus, we need to substitute $\sigma^2$ by an estimator. 

For exact finite sample inference, we need a variance estimator of $\sigma^2$ for which we can derive its exact small sample distribution. Therefore, we require Assumption 4$^*$ of spherical errors, i.e. $Var(\varepsilon|X)=\sigma^2I_n,$ which implies that $Var(\hat\beta_n|X)=\sigma^2(X'X)^{-1}$, and where $\sigma^2$ can be estimated by the unbiased ($UB$) variance estimator  
$$
s_{UB}^2=(n-K)^{-1}\sum_{i=1}^n\hat\varepsilon_i^2.
$$  
From the normality assumption in Assumption 4$^*$, it follows then that 
$$
\frac{(n-K)}{\sigma^{2}}s_{UB}^2\sim\chi^2_{(n-K)}.
$${#eq-distsquared} 

Substituting the unknown 
$$
Var(\hat\beta_n|X)=\sigma^2 (X'X)^{-1}
$$ 
in @eq-TestStatW by its estimator 
$$
\widehat{Var}(\hat\beta_n|X)=s_{UB}^2 (X'X)^{-1}
$$ 
leads to the $F$-test statistic
$$
F=(R\hat\beta_n -r^{(0)})'[R(s_{UB}^2(X'X)^{-1})R']^{-1}(R\hat\beta_n -r^{(0)})/q
$$
and takes into account the additional randomness (estimation errors) due to estimating $\sigma^2$ by $s_{UB}^2.$ 

Under the null-hypothesis, one can show that
$$
F\overset{H_0}{\sim} F_{(q,n-K)},
$${#eq-Ftest}
where $F_{(q,n-K)}$ denotes the $F$-distribution with $q$ numerator and $n-K$ denominator degrees of freedom. 


As in the case of $W$, the distribution of $F$ does not depend on $X.$

> Note: The distributional statements in @eq-distsquared and @eq-Ftest are a little cumbersome to derive and we do not go into details here, but in case you're interested you can find some more details, for instance, in Chapter 1 of @Hayashi2000. 

By contrast to $W,$ $F$ is now a practically useful test statistic, and we can use the observed value $F_{\text{obs}}$ to measure the distance of our observed estimate $R\hat\beta_n$ from its null-hypothetical value $r^{(0)}.$  

Observed values, $F_{\text{obs}}$, that are "unusually large" under the null hypothesis, lead to a rejection of the null hypothesis. The null distribution $F_{(q,n-K)}$ of $F$ is used to judge what's "unusually large" under the null hypothesis. 


**The F distribution.** The F distribution is a ratio of two $\chi^2$ distributions. It has two parameters: the numerator degrees of freedom, and the denominator degrees of freedom. Each combination of the parameters yields a different F distribution (@fig-FDistribution). See @sec-Fdist for more information on the $F$ distribution. 
```{r, fig.align="center", echo=FALSE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
#| label: fig-FDistribution
#| fig-cap: Graphs of the density function of the $F$-distribution for different numerator degrees of freedom and different denominator degrees of freedom.
 
# When fixing rate (lambda) and changing shape (r) for Gamma Distribution,
# When the shape (r) increases, based on the formula, 
# the mean increases (shift to the right),
# the variance increases
# the skewness decreases
# the excess kurtosis decreases

### F Distribution
# Plot 1: Fix df2 and changing df1
par(mfrow=c(1,2))
curve(expr = df(x = x, df1 = 3, df2 = 5),
      xlab = "", ylab = "", main = "",
      lwd = 2, col = 1, xlim = c(0, 4),
      ylim = c(0, 1))

for (i in 1:2) {
  curve(expr = df(x = x, df1  = 3, 
                  df2=c(15,500)[i]),
        lwd = 2, col = (2:3)[i], add = TRUE)
}  
legend(x = "topright", legend = c("F(3,5)", "F(3,15)", "F(3,500)"),
       lwd = 2, col = 1:3)
# Plot 2: Changing df1 and fix df2       
curve(expr = df(x = x, df1 = 1, df2 = 30),
      xlab = "", ylab = "", main = "",
      lwd = 2, col = 1, xlim = c(0, 4),
      ylim = c(0, 1))

for (i in 1:2) {
  curve(expr = df(x = x, df1  = c(3,15)[i], 
                  df2=30),
        lwd = 2, col = (4:5)[i], add = TRUE)
}  
legend(x = "topright", legend = c("F(1,30)", "F(3,30)", "F(15,30)"),
       lwd = 2, col = c(1,4,5))
```


### Test Decision using the Rejection Region 

Let $0<\alpha<1$ denote the significance level and let $c_{1-\alpha}$ denote the $(1-\alpha)$ quantile of the $F$-distribution with $(q,n-K)$ degrees of freedom. 

This quantile $c_{1-\alpha}$ is the **critical value** that defines the **rejection region**: 
$$
\mathcal{R}=\; ]c_{1-\alpha},\infty[
$$
<!-- , and 
- non-rejection region, $]0,c_{1-\alpha}]$  -->

* We can rejection $H_0$ if
$$
F_{obs}\in \mathcal{R}=\; ]c_{1-\alpha},\infty[
$$
* We cannot rejection $H_0$ if
$$
F_{obs}\not \in \mathcal{R}=\; ]c_{1-\alpha},\infty[
$$


```{r, fig.align="center", echo=FALSE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
#| label: fig-FDistributionRejection
#| fig-cap: Graph of the density function of the $F$-distribution with $9$ numerator degrees of freedom and $120$ denominator degrees of freedom. The rejection region $\mathcal{R}$ is shown in red.  
# plot the standard normal density
library("scales")
curve(expr = df(x = x, df1 = 9, df2 = 120),
      xlab = "", ylab = "", main = "",
      lwd = 2, col = 1, xlim = c(0, 4),
      ylim = c(0, 1), xaxs="i",yaxs="i")

alpha <- 0.05
q     <- qf(p = 1-alpha, df1 = 9, df2 = 120)
xx    <- seq(0,q,len=25)
yy    <- df(x = xx, df1 = 9, df2 = 120)

polygon(x = c(xx,rev(xx)), y=c(yy, rep(0,length(yy))), border = NA, col = alpha("green", 0.25))
q     <- qf(p = 1-alpha, df1 = 9, df2 = 120)
xx    <- seq(q,4,len=25)
yy    <- df(x = xx, df1 = 9, df2 = 120)

polygon(x = c(xx,rev(xx)), y=c(yy, rep(0,length(yy))), border = NA, col = alpha("red", 0.25))

lines(x=c(0,q-0.02),y=c(0,0), col="darkgreen", lwd=10)
lines(x=c(q+0.02,4),y=c(0,0), col="red", lwd=10)

legend("topright", pch=c(22,NA, 22, NA), lty=c(NA,1,NA,1), lwd=c(NA,4,NA,4), cex=1, 
       col = c(alpha("green", 0.25),"darkgreen",alpha("red", 0.25),"red"), 
       legend=c(expression(1-alpha==~"95% of"~F['9,120']),"Non-Rejection Region",
                expression(alpha==~"5% of"~F['9,120']),"Rejection Region"), 
       bty="n", pt.bg = c(alpha("green", 0.25),alpha("green", 0.25),alpha("red", 0.25),alpha("red", 0.25)))
curve(expr = df(x = x, df1 = 9, df2 = 120), lwd = 2, col = 1, add=TRUE, from = 0, to = 4)
lines(x=c(q,q), y=c(0,.6),lwd=2,lty=2)
text(x = q, y = .65, labels = expression(c[1-alpha]==1.9588))
```

**The rejection region:** The rejection region describes a range of values of the test statistic $F$ which we rarely see if the null hypothesis is true (only in at most $\alpha \cdot 100\%$ cases). If the observed value of the test statistic, $F_{\text{obs}}$, falls in this region, we will reject the null  hypothesis and acknowledge a type-I-error rate of at most $\alpha$. 

**The non-rejection region:** The non-rejection region describes a range of values of the test statistic $F$ which we expect to see (in $(1-\alpha) \cdot 100\%$ cases) if the null hypothesis is true. If the observed value of the test statistic, $F_{\text{obs}}$ falls in this region, we cannot reject the null hypothesis. 


To find the critical value $c_{1-\alpha}$ we can use `R` as following: 
```{r}
alpha <- 0.05 # chosen significance level
df1   <- 9    # numerator df
df2   <- 120  # denominator df

## Critical value:
crit_value <- qf(p = 1-alpha, df1 = df1, df2 = df2)
crit_value
```
Changing the significance level from $\alpha=0.05$ to $\alpha=0.01$ makes the critical value $c_{1-\alpha}$ larger and, therefore, the  rejection region smaller (smaller probability of type-I-errors).
```{r}
alpha <- 0.01 # chosen significance level
## Critical value:
crit_value <- qf(p = 1-alpha, df1 = df1, df2 = df2)
crit_value
```


### Test Decision using the $p$-Value

Remember that (under Assumption 4$^\boldsymbol{\ast}$)
$$
F\overset{H_0}{\sim}F_{q,n-K}.
$$

The $p$-value of the $F$-test is the probability of seeing realizations of $F$ that are equal to or larger than the observed value $F_{\text{obs}}$ given that the null hypothesis is true
$$
p_{\text{obs}}=P(F\geq F_{\text{obs}}\;|\;H_0 \text{ is true})
$$

* We reject the null hypothesis $H_0$ if
$$
p_{\text{obs}} < \alpha
$$

* We cannot reject the null hypothesis $H_0$ if
$$
p_{\text{obs}} \geq \alpha,
$$
where $0<\alpha<1$ denotes the chosen significance level. Typical choices are
  * $\alpha = 0.05$
  * $\alpha = 0.01$




## $t$-Tests: Hypothesis Tests about One Parameter {#sec-testingsinglep}

A hypothesis about only **one parameter**
$$
\begin{equation*}
\begin{array}{ll}
& H_0: \beta_k=\beta_k^{(0)}\\
\text{versus}\quad & H_1: \beta_k\ne \beta_k^{(0)}\\
\end{array}
\end{equation*}
$$
is simply a special case of the general null hypothesis $H_0:R\beta =r^{(0)},$ where 

* $R$ is a $(1\times K)$ row-vector of zeros, but with a one as the $k$th element, and where 
* we write $r^{(0)}=\beta_k^{(0)}$ since we make a hypothesis only about $\beta_k.$

Thus the $F$-test statistic simplifies to
$$
F=\frac{\left(\hat{\beta}_k-\beta_k^{(0)}\right)^2}{\widehat{Var}(\hat{\beta}_k|X)}\overset{H_0}{\sim}F_{(1,n-K)},
$$
where 
$$
\widehat{Var}(\hat{\beta}_k|X)=s^2_{UB}[(X'X)^{-1}]_{(k,k)},
$$ 
and where $[(X'X)^{-1}]_{(k,k)}$ denotes the element in the $k$th row and $k$th column of the $(K\times K)$ matrix $(X'X)^{-1}.$


### The Test Statistic and its Null Distribution {#sec-tTestTestStat}

Taking square roots yields the $t$ test statistic 
$$
T=\frac{\hat{\beta}_k-\beta_k^{(0)}}{\widehat{\operatorname{SE}}(\hat{\beta}_k|X)}\overset{H_0}{\sim}t_{(n-K)},
$$
where 
$$
\begin{align*}
\widehat{\operatorname{SE}}(\hat{\beta}_k|X)
&=\sqrt{\widehat{Var}(\hat{\beta}_k|X)}\\[2ex]
&=s_{UB}[(X'X)^{-1/2}]_{(k,k)},
\end{align*}
$$ 
and where $t_{(n-K)}$ denotes the $t$-distribution with $n-K$ degrees of freedom. 

Thus the $t$-distribution with $n-K$ degrees of freedom is the appropriate distribution to judge whether an observed value $T_{\text{obs}}$ of the test statistic is "unusually large" under the null hypothesis. 

::: {.callout-tip}
All commonly used statistical software packages report in their regression output tables $t$-tests testing the "no (linear) effect" null hypothesis 
$$
H_0:\beta_k=0
$$
for each $k=1,\dots,K.$

This means to test the null hypothesis that $X_k$ has on average no (linear) effect on the outcome variable $Y.$ 
:::

**The $t$ distribution.** @fig-tDistribution illustrates that as the degrees of freedom increase, the shape of the $t$ distribution comes closer to that of a standard normal bell curve. Already for $25$ degrees of freedom we find little difference to the standard normal density. In case of small degrees of freedom values, we find the distribution to have heavier tails than a standard normal. See @sec-tdist for more information about the $t$-distribution.

```{r, fig.align="center", echo=FALSE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"} 
#| label: fig-tDistribution
#| fig-cap: Graphs of the density function of the $t$-distribution with $2,\,4,$ and $25$ degrees of freedom and of the density of the standard normal distribution.
# plot the standard normal density
curve(dnorm(x), 
      xlim = c(-4, 4), 
      xlab = "", 
      lty = 2, 
      ylab = "", 
      main = "")

# plot the t density for M=2
curve(dt(x, df = 2), 
      xlim = c(-4, 4), 
      col = 2, 
      add = T)

# plot the t density for M=4
curve(dt(x, df = 4), 
      xlim = c(-4, 4), 
      col = 3, 
      add = T)

# plot the t density for M=25
curve(dt(x, df = 25), 
      xlim = c(-4, 4), 
      col = 4, 
      add = T)

# add a legend
legend("topright", bty="n", 
       c("N(0, 1)", expression(t[2]), expression(t[4]), expression(t[25])), 
       col = 1:4, 
       lty = c(2, 1, 1, 1))
```



### Test Decision using the Rejection Region  (One-Sided 1/2) 

The right-sided version of the one-sided hypothesis is given by
$$
\begin{align*}
&H_0: \beta_k \leq \beta_k^{(0)}\\
\text{versus}\quad 
& H_1: \beta_k > \beta_k^{(0)},
\end{align*}
$$
where $\beta_k$ denotes the true (unknown) parameter value and $\beta_k^{(0)}$ the null hypothetical value specified by the statistician (e.g. $\beta_k^{(0)}=0$). 

The rejection region is
$$
\mathcal{R}=\;]c_{1-\alpha}, \infty[,
$$
where $0<\alpha<1$ denotes the significance level and $c_{1-\alpha}$ denotes the $(1-\alpha)$ quantile of the $t$-distribution with $(n-K)$ degrees of freedom. 

* We can reject $H_0$ if
$$
\begin{align*}
T_{obs} 
& \in \mathcal{R} = \;]c_{1-\alpha}, \infty[
\end{align*}
$$
* We cannot reject $H_0$ if
$$
\begin{align*}
T_{obs} 
&\not \in \mathcal{R} = \;]c_{1-\alpha}, \infty[
\end{align*}
$$

@fig-oneSidedRight shows an example of the rejection region for the case of a significance level $\alpha=0.05$ and $n-K=12$ degrees of freedom. 

```{r, fig.align="center", echo=FALSE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
#| label: fig-oneSidedRight
#| fig-cap: $t$-distribution with $n-K=12$ degrees of freedom and critical value $c_{1-\alpha}=1.78$ for the significance level $\alpha=0.05$.
library("scales")
alpha <- 0.05
q      <- qt(p = 1-alpha, df=12)

xx1    <- seq(-5,-q,len=25)
yy1    <- dt(x = xx1, df = 12)
xx2    <- seq(-q,5,len=25)
yy2    <- dt(x = xx2, df = 12)
##
xx3    <- seq(q,5,len=25)
yy3    <- dt(x = xx3, df = 12)
xx4    <- seq(-5,q,len=25)
yy4    <- dt(x = xx4, df = 12)

par(mfrow=c(1,1))
curve(expr = dt(x = x, df = 12),
      xlab = "", ylab = "", main = "",
      lwd = 2, col = 1, xlim = c(-5, 5),
      ylim = c(0, .6), xaxs="i",yaxs="i")
polygon(x = c(xx3,rev(xx3)), y=c(yy3, rep(0,length(yy3))), border = NA, col = alpha("red", 0.25))
polygon(x = c(xx4,rev(xx4)), y=c(yy4, rep(0,length(yy4))), border = NA, col = alpha("green", 0.25))
lines(x=c(q,q), y=c(0,.35),lwd=2,lty=2)
text(x =  q, y = .45, labels = expression(c[1-alpha]==1.78))
legend("topright", pch=c(22,22), lty=c(NA,NA), lwd=c(NA,NA), cex=1, 
       col = c(alpha("green", 0.25),alpha("red", 0.25)), 
       legend=c(expression("95% of"~t['12']),
                expression("5% of"~t['12'])), 
       bty="n", pt.bg = c(alpha("green", 0.25),alpha("red", 0.25)))
par(mfrow=c(1,1))
```

To find the $c_{1-\alpha}$ critical value we can use `R` as following:
```{r}
alpha <- 0.05 # chosen significance level 
df    <- 12   # degrees of freedom 

## One-sided critical value (1-alpha) quantile:
c_oneSided <- qt(p = 1-alpha, df = df)
c_oneSided
```


### Test Decision using the Rejection Region  (One-Sided 2/2)

The left-sided version of the one-sided hypothesis is given by
$$
\begin{align*}
&H_0:  \beta_k \geq \beta_k^{(0)}\\
\text{versus}\quad 
&H_1:  \beta_k <  \beta_k^{(0)},
\end{align*}
$$
where $\beta_k$ denotes the true (unknown) parameter value and $\beta_k^{(0)}$ the null hypothetical value specified by the statistician (e.g. $\beta_k^{(0)}=0$). 

The rejection region is 
$$
\mathcal{R}=\;]-\infty,c_{\alpha}[,
$$
where $0<\alpha<1$ denotes the significance level and $c_{1-\alpha}$ denotes the $(1-\alpha)$ quantile of the $t$-distribution with $(n-K)$ degrees of freedom. 


* We can reject $H_0$ if
$$
\begin{align*}
T_{obs} 
& \in \mathcal{R} = \;]-\infty,c_{\alpha}[
\end{align*}
$$
* We cannot reject $H_0$ if
$$
\begin{align*}
T_{obs} 
&\not \in \mathcal{R} = \;]-\infty,c_{\alpha}[
\end{align*}
$$

@fig-oneSidedLeft shows an example of the rejection region for the case of a significance level $\alpha=0.05$ and $n-K=12$ degrees of freedom. 

```{r, fig.align="center", echo=FALSE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
#| label: fig-oneSidedLeft
#| fig-cap: $t$-distribution with $n-K=12$ degrees of freedom and critical value $c_{\alpha}=-1.78$ for the significance level $\alpha=0.05$.

library("scales")
alpha <- 0.05
q      <- qt(p = 1-alpha, df=12)

xx1    <- seq(-5,-q,len=25)
yy1    <- dt(x = xx1, df = 12)
xx2    <- seq(-q,5,len=25)
yy2    <- dt(x = xx2, df = 12)
##
xx3    <- seq(q,5,len=25)
yy3    <- dt(x = xx3, df = 12)
xx4    <- seq(-5,q,len=25)
yy4    <- dt(x = xx4, df = 12)

par(mfrow=c(1,1))
curve(expr = dt(x = x, df = 12),
      xlab = "", ylab = "", main = "",
      lwd = 2, col = 1, xlim = c(-5, 5),
      ylim = c(0, .6), xaxs="i",yaxs="i")
polygon(x = c(xx1,rev(xx1)), y=c(yy1, rep(0,length(yy1))), border = NA, col = alpha("red", 0.25))
polygon(x = c(xx2,rev(xx2)), y=c(yy2, rep(0,length(yy2))), border = NA, col = alpha("green", 0.25))
lines(x=c(-q,-q), y=c(0,.35),lwd=2,lty=2)
text(x = -q, y = .45, labels = expression(c[alpha]==-1.78))
legend("topright", pch=c(22,22), lty=c(NA,NA), lwd=c(NA,NA), cex=1, 
       col = c(alpha("green", 0.25),alpha("red", 0.25)), 
       legend=c(expression("95% of"~t['12']),
                expression("5% of"~t['12'])), 
       bty="n", pt.bg = c(alpha("green", 0.25),alpha("red", 0.25)))
par(mfrow=c(1,1))
```


To find the $c_{\alpha}$ critical value we can use `R` as following:
```{r}
alpha <- 0.05 # chosen significance level 
df    <- 12   # degrees of freedom 

## One-sided critical value (alpha) quantile:
c_oneSided <- qt(p = alpha, df = df)
c_oneSided
```



### Test Decision using the Rejection Region (Two-Sided)

The two-sided $t$-test allows us to test
$$
\begin{align*}
& H_0: \beta_k=\beta_k^{(0)}\\
\text{versus}\quad 
& H_1: \beta_k\ne \beta_k^{(0)},
\end{align*}
$$
where $\beta_k$ denotes the true (unknown) parameter value and $\beta_k^{(0)}$ the null hypothetical value specified by the statistician (e.g. $\beta_k^{(0)}=0$). 


The rejection region is 
$$
\mathcal{R}=\;]-\infty,c_{\alpha/2}[\;\;\cup\;\;]c_{1-\alpha/2}, \infty[,
$$
where $0<\alpha<1$ denotes the significance level and $c_{\alpha/2}$ and $c_{1-\alpha/2}$ denote the $\alpha/2$ and the $(1-\alpha/2)$ quantiles of the $t$-distribution with $(n-K)$ degrees of freedom. 


* We can reject $H_0$ if
$$
\begin{align*}
T_{obs} 
& \in \mathcal{R}%\\[2ex]
%& \in \;]-\infty,c_{\alpha/2}[\;\;\cup\;\;]c_{1-\alpha/2}, \infty[
\end{align*}
$$
* We cannot reject $H_0$ if
$$
\begin{align*}
T_{obs} 
&\not \in \mathcal{R}%\\[2ex]
%&\not \in \;]-\infty,c_{\alpha/2}[\;\;\cup\;\;]c_{1-\alpha/2}, \infty[
\end{align*}
$$

@fig-twoSided shows an example of the rejection region for the case of a significance level $\alpha=0.05$ and $n-K=12$ degrees of freedom. 

```{r, fig.align="center", echo=FALSE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
#| label: fig-twoSided
#| fig-cap: $t$-distribution with $n-K=12$ degrees of freedom and critical values $c_{\alpha/2}=-2.18$ and $c_{1-\alpha/2}=2.18$ for the significance level $\alpha=0.05$.
library("scales")#| 
curve(expr = dt(x = x, df = 12),
      xlab = "", ylab = "", main = "",
      lwd = 2, col = 1, xlim = c(-5, 5),
      ylim = c(0, .6), xaxs="i",yaxs="i")

alpha <- 0.05/2
q      <- qt(p = 1-alpha, df=12)
xx1    <- seq(-5,-q,len=25)
yy1    <- dt(x = xx1, df = 12)
xx2    <- seq(q,5,len=25)
yy2    <- dt(x = xx2, df = 12)
xx3    <- seq(-q,q,len=25)
yy3    <- dt(x = xx3, df = 12)

polygon(x = c(xx1,rev(xx1)), y=c(yy1, rep(0,length(yy1))), border = NA, col = alpha("red", 0.25))

polygon(x = c(xx2,rev(xx2)), y=c(yy2, rep(0,length(yy2))), border = NA, col = alpha("red", 0.25))

polygon(x = c(xx3,rev(xx3)), y=c(yy3, rep(0,length(yy3))), border = NA, col = alpha("green", 0.25))

legend("topright", pch=c(22,22), lty=c(NA,NA), lwd=c(NA,NA), cex=1, 
       col = c(alpha("green", 0.25),alpha("red", 0.25)), 
       legend=c(expression("95% of"~t['12']),
                expression("5% of"~t['12'])), 
       bty="n", pt.bg = c(alpha("green", 0.25),alpha("red", 0.25)))
curve(expr = dt(x = x, df= 12), lwd = 2, col = 1, add=TRUE)
lines(x=c(q,q), y=c(0,.35),lwd=2,lty=2)
lines(x=c(-q,-q), y=c(0,.35),lwd=2,lty=2)
text(x =  q, y = .45, labels = expression(c[1-alpha/2]==2.18))
text(x = -q, y = .45, labels = expression(c[alpha/2]==-2.18))
```


To find the $c_{\alpha/2}$ and $c_{1-\alpha/2}$ critical values we can use `R` as following:
```{r}
alpha <- 0.05 # chosen signficance level 
df    <- 12   # degrees of freedom 

## Two-sided critical value (= (1-alpha/2) quantile):
c_twoSided <- qt(p = 1-alpha/2, df = df)

## lower critical value
-c_twoSided
## upper critical value
c_twoSided
```



### Test Decision using the $p$-Value (One-Sided 1/2) {#sec-TDecRS}

Right-sided version of the one-sided hypothesis:
$$
\begin{align*}
&H_0: \beta_k \leq \beta_k^{(0)}\\
\text{versus}\quad & H_1:\beta_k >    \beta_k^{(0)}\\
\end{align*}
$$

<!-- In case of a one-sided $t$-test, we will reject the null if $T_{\text{obs}}$ is sufficiently "far away" from zero in the relevant direction of $H_1$.  -->

We know that
$$
T\overset{H_0}{\sim}t_{n-K}.
$$


The $p$-value is the probability of seeing realizations of $T$ that are equal to or larger than the observed value $T_{\text{obs}}$ given that the null hypothesis is true
$$
p_{\text{obs}}=P(T\geq T_{\text{obs}}\;|\;H_0 \text{ is true}).
$$
* We reject the null hypothesis $H_0$ if
$$
p_{\text{obs}} < \alpha
$$

* We cannot reject the null hypothesis $H_0$ if
$$
p_{\text{obs}} \geq \alpha,
$$
where $0<\alpha<1$ denotes the chosen significance level. Typical choices are
   * $\alpha = 0.05$
   * $\alpha = 0.01$


### Test Decision using the $p$-Value (One-Sided 2/2) 

Left-sided version of the one-sided hypothesis:
$$
\begin{align*}
&H_0: \beta_k \geq \beta_k^{(0)}\\
\text{versus}\quad & H_1: \beta_k <  \beta_k^{(0)}\\
\end{align*}
$$

The $p$-value is the probability of seeing realizations of $T$ that are equal to or smaller than the observed value $T_{\text{obs}}$ given that the null hypothesis is true
$$
p_{\text{obs}}=P(T\leq T_{\text{obs}}\;|\;H_0 \text{ is true}).
$$


* We reject the null hypothesis $H_0$ if
$$
p_{\text{obs}} < \alpha
$$

* We cannot reject the null hypothesis $H_0$ if
$$
p_{\text{obs}} \geq \alpha,
$$
where $0<\alpha<1$ denotes the chosen significance level. Typical choices are
   * $\alpha = 0.05$
   * $\alpha = 0.01$



### Test Decision using the $p$-Value (Two-Sided)

The two-sided $t$-test allows us to test
$$
\begin{align*}
& H_0: \beta_k=\beta_k^{(0)}\\
\text{versus}\quad 
& H_1: \beta_k\ne \beta_k^{(0)}
\end{align*}
$$
where $\beta_k$ denotes the true (unknown) parameter value and $\beta_k^{(0)}$ the null hypothetical value specified by the statisticaian (e.g. $\beta_k^{(0)}=0$). 

We know that
$$
T\overset{H_0}{\sim}t_{n-K}.
$$


The $p$-value of the two-sided $t$-test is the probability of seeing realizations of $T$ that are equal to or more extreme than the observed value $T_{\text{obs}}$ given that the null hypothesis is true
$$
\begin{align*}
p_{\text{obs}}
&=P(|T|\geq |T_{\text{obs}}|\;|\;H_0 \text{ is true})\\[2ex]
&=2\cdot\min\{P(T\leq T_{\text{obs}}\;|\;H_0 \text{ is true}), 
            P(T\geq T_{\text{obs}}\;|\;H_0 \text{ is true})\}
\end{align*}
$$

* We reject the null hypothesis $H_0$ if
$$
p_{\text{obs}} < \alpha
$$

* We cannot reject the null hypothesis $H_0$ if
$$
p_{\text{obs}} \geq \alpha,
$$
where $0<\alpha<1$ denotes the chosen significance level. Typical choices are
   * $\alpha = 0.05$
   * $\alpha = 0.01$



## Confidence Intervals {#sec-CIsmallsample}

We define a two-sided $(1-\alpha)\cdot 100\%$ percent confidence interval for the *deterministic* (unknown) true $\beta_k$ as the **random interval** $\operatorname{CI}_{k,n,1-\alpha}$ for which 
$$
P\Big(\beta_k\in\operatorname{CI}_{k,n,1-\alpha}\Big)\geq 1-\alpha.
$$

#### **Derivation of the random interval $\operatorname{CI}_{k,n,1-\alpha}$** {-} 

Observe that (under Ass 1-4$^\ast$)
$$
\frac{\hat\beta_{n,k}-\beta_k}{\widehat{\operatorname{SE}}(\hat\beta_{n,k}|X)}\sim t_{(n-K)}
$${#eq-CIDistr}
Therefore,
$$
\begin{align*}
P\left(-c_{n,1-\alpha/2}\leq\frac{\hat\beta_{n,k}-\beta_k}{\widehat{\operatorname{SE}}(\hat\beta_{n,k}|X)}\leq c_{n,1-\alpha/2}\right)=1-\alpha,
\end{align*}
$$
where $c_{n,1-\alpha/2}$ denotes the $(1-\alpha/2)$ quantile of the $t$-distribution with $(n-K)$ degrees of freedom. 

Next, we can do the following equivalent transformations
$$
\begin{align*}
P\left(-c_{n,1-\alpha/2}\leq\frac{\hat\beta_{n,k}-\beta_k}{\widehat{\operatorname{SE}}(\hat\beta_{n,k}|X)}\leq c_{n,1-\alpha/2}\right)&=1-\alpha\\[2ex]
\Leftrightarrow P\left(\hat\beta_{n,k}-c_{n,1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_{n,k}|X)\leq \beta_k\leq\hat\beta_{n,k} +c_{n,1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_{n,k}|X)\right)&=1-\alpha\\[2ex]
\Leftrightarrow P\left(\beta_k\in\underbrace{\left[\hat\beta_{n,k}-c_{n,1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_{n,k}|X),\;\hat\beta_{n,k} +c_{n,1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_{n,k}|X)\right]}_{=:\operatorname{CI}_{k,n,1-\alpha}}\right)&=1-\alpha
\end{align*}
$$
That is, the random interval 
$$
\begin{align*}
\operatorname{CI}_{k,n,1-\alpha}
&=\left[\hat\beta_{n,k}-c_{n,1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_{n,k}|X),\;\hat\beta_{n,k} + c_{n,1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_{n,k}|X)\right]\\[2ex]
&=\left[\hat\beta_{n,k}\pm c_{n,1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_{n,k}|X)\right]
\end{align*}
$$
is our $(1-\alpha)\cdot 100\%$ confidence interval for $\beta_k$. 


Since the confidence interval is based on the exact distribution (under Assumptions 1-4$^\ast$) in @eq-CIDistr, the confidence interval has an **exact** coverage probability
$$
\begin{align*}
P\left(\beta_k\in\operatorname{CI}_{k,n,1-\alpha}\right)&=1-\alpha
\end{align*}
$$
provided the Assumptions 1-4$^\ast$ are true. 

::: {.callout-tip}
# Interpretation of Confidence Intervals 

The random interval  $\operatorname{CI}_{k,n,1-\alpha}$ for $\beta_k$ contains the true parameter value $\beta_k$ with probability $1-\alpha;$ i.e. we expect that $\operatorname{CI}_{k,n,1-\alpha}$ covers $\beta_k$ in $(1-\alpha)\cdot 100\%$ of resamplings from the random sample. 

It's best to take a look at dynamic visualizations like this one: 

<center>
[https://rpsychologist.com/d3/ci/](https://rpsychologist.com/d3/ci/)
</center>


Unfortunately, this "frequentist" interpretation is not a statement about a single given $\operatorname{CI}_{k,n,1-\alpha,\text{obs}}$ realization computed for an observed realization of the random sample. A given, realized $\operatorname{CI}_{k,n,1-\alpha,\text{obs}}$ will either contain the true parameter $\beta_k$ or not, and usually we do not know the answer. So, confidence intervals are quite hard to interpret. However, they are very well suited as a tool to visualize estimation uncertainties in $\hat\beta_{n,k}$, $k=1,\dots,K$.   

<center>
![](images/Meme_CI_2.jpg)
</center>
:::


#### **Point Estimator versus Interval Estimator** {-}

Often, $\hat\beta_{n,k}$ is called a **point estimator** of $\beta_k$ and the confidence interval
$$
\begin{align*}
\operatorname{CI}_{k,n,1-\alpha}
&=\left[\hat\beta_{n,k}\pm c_{n,1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_{n,k}|X)\right]
\end{align*}
$$
is called an **interval estimator** of $\beta_k,$ where the width of $\operatorname{CI}_{k,n,1-\alpha}$ quantifies the estimation uncertainties.  



### Confidence Intervals for Statistical Hypothesis Testing 

We can use the $(1-\alpha)\cdot 100\%$ confidence interval $\operatorname{CI}_{k,n,1-\alpha}$ to do statistical hypothesis testing at the significance level $0<\alpha<1.$ Typical significance levels: 


Let us consider the following two-sided statistical hypotheses
$$
\begin{align*}
H_0:&\;\beta_k=\beta^{(0)}_{k}\\ 
H_1:&\;\beta_k\neq \beta^{(0)}_{k}
\end{align*}
$$



**Testing-Procedure:**

* If the observed (obs) realization of the confidence interval, $\operatorname{CI}_{k,n,1-\alpha,\text{obs}},$ **contains** the null-hypothetical value $\beta^{(0)}_{k},$ i.e.
$$
\begin{align*}
\beta^{(0)}_{k}&\in\operatorname{CI}_{k,n,1-\alpha,\text{obs}}\\[2ex]
\Leftrightarrow\beta^{(0)}_{k}&\in
\left[\hat\beta_{n,k,\text{obs}}\pm c_{n,1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_{n,k}|X)_{\text{obs}}\right],
\end{align*}
$$
then we **cannot reject the null hypothesis.**

* If, however, the observed (obs) realization of the confidence interval, $\operatorname{CI}_{k,n,1-\alpha,\text{obs}},$ **does *not* contain** the null-hypothetical value $\beta^{(0)}_{k},$ i.e.
$$
\begin{align*}
\beta^{(0)}_{k}&\not\in\operatorname{CI}_{k,n,1-\alpha,\text{obs}}\\[2ex]
\Leftrightarrow\beta^{(0)}_{k}&\not\in
\left[\hat\beta_{n,k,\text{obs}}\pm c_{n,1-\alpha/2}\widehat{\operatorname{SE}}(\hat\beta_{n,k}|X)_{\text{obs}}\right],
\end{align*}
$$
then we **reject the null hypothesis.** 


The test decisions are then perfectly equivalent to those based on the two-sided $t$-test. 





## Testtheory

Every statistical test statistic is a function of the random sample, i.e.
$$
T_n\equiv T((X_1,Y_1),\dots,(X_n,Y_n))
$$
and is thus a random variable. 

**Caution:** In this section, $T_n$ denotes *any* test statistic. Specific examples for $T_n$ are, for instance, 

* the $F$-test statistic (@sec-testmultp) and 
* the $t$-test statistic (@sec-testingsinglep).

Generally, we can only derive the distribution of $T_n$ under $H_0;$ i.e. under the scenario that $H_0$ is true, 
$$
\begin{equation*}
\begin{array}{ll}
T_n & \overset{{H_0}}\sim f_{T_n}\quad ✅
\end{array}
\end{equation*}
$$
For instance, in case of the $F$-test, $f_{T_n}$ denotes the $F$-distribution with $q$ and $n-K$ degrees of freedom and in case of the $t$-test, $f_{T_n}$ denotes the $t$-distribution with $n-K$ degrees of freedom. 

However, the distribution of $T_n$ under $H_1$ is generally unknown. 
$$
\begin{equation*}
\begin{array}{ll}
T_n & \overset{{H_1}}\sim ❌ 
\end{array}
\end{equation*}
$$

<!-- * in @sec-testmultp we discussed that $F\overset{H_0}{\sim} F_{(q,n-K)}.$

* in @sec-testingsinglep we discussed (for simple null hypothesis) that $T\overset{H_0}{\sim} t_{(n-K)}$ -->

### Simple versus Composite Hypotheses

We use the observed realization
$$
T_{n,\text{obs}}\equiv T((X_{1,\text{obs}},Y_{1,\text{obs}}),\dots,(X_{n,\text{obs}},Y_{n,\text{obs}}))
$$
of the random test statistic $T_n$ to decide: 

* whether we cannot reject a null hypothesis $H_0$ about some parameter $\theta$ 
$$
H_0: \theta\in\Theta_0,
$$ 
* or wether we can reject $H_0$ in favor of the alternative hypothesis $H_1$
$$
H_1: \theta\in\Theta_1.
$$ 
The test decision is made using the **rejection region**, the **$p$-value**, or a confidence interval. 

Notation:

* $\Theta_0$ denotes the set of parameter values $\theta$ under $H_0$.
* $\Theta_1$ denotes the set of parameter values $\theta$ under $H_1$.

It is required that
$$
\Theta_0 \cap \Theta_1 = \emptyset.
$$
 

* If $\Theta_j,$ $j=1,2,$ contains only **one value** 
$$
\Theta_j=\theta^{(j)},
$$ 
it is called a **simple hypothesis**. 
* If $\Theta_j,$ $j=1,2,$ contains **multiple values**, it is called a **composite hypothesis**.



The idea of a **composite null hypothesis** 
$$
H_0:\theta\in\Theta_0
$$ 
is to collect all hypotheses which we do not care to detect by the statistical test. This way, the set of alternative hypotheses $H_1:\theta\in\Theta_1$ becomes smaller which leads to more powerful tests.  


::: {.callout-tip icon="false"}
# Example: Two-Sided Test with $\theta\in\mathbb{R}$
$$
\begin{equation*}
\begin{array}{ll}
& H_0: \theta = \theta^{(0)}\\
\text{versus}\quad 
& H_1: \theta\neq \theta^{(0)}\\
\end{array}
\end{equation*}
$$

Here we have a **simple** null hypothesis 
$$
\Theta_0=\theta^{(0)}
$$ 
and a **composite** alternative hypothesis 
$$
\Theta_1=\mathbb{R}\setminus\theta^{(0)}.
$$
:::

::: {.callout-tip icon="false"}
# Example: One-Sided Test with $\theta\in\mathbb{R}$
$$
\begin{equation*}
\begin{array}{ll}
& H_0: \theta \geq  \theta^{(0)}\\
\text{versus}\quad 
& H_1: \theta  < \theta^{(0)}\\
\end{array}
\end{equation*}
$$

Here we have a **composite** null hypothesis
$$
\Theta_0=\;[\theta^{(0)},\infty[
$$ 
and a **composite** alternative hypothesis
$$
\Theta_1=\;]-\infty,\theta^{(0)}[.
$$

(Likewise for the other direction of the one-sided test.) 
::: 

### Decisions Errors

Hypothesis testing is like a legal trial. We assume someone is innocent
unless the evidence strongly suggests that he is guilty. Similarly, we retain $H_0$ unless there is strong evidence to reject $H_0.$ 


We differentiate two decision errors events: 

* **type-I-error (or false positive):**</br> 
Rejecting $H_0$ even though $H_0$ is true. 
* **type-II-error:**</br>
Not rejecting $H_0$ even though $H_1$ is true. 




::: {.callout-caution}
Not being able to reject $H_0,$ does not mean a validation/confirmation of $H_0,$ but only reflects that we do not have sufficient evidence against a possibly false $H_0.$

Indeed, a given violation of $H_0$ may only be too small to stand out from the estimation errors; i.e. we may do a type-II-error. Problem is, we generally cannot control the probability of type-II-errors since we do not know the distribution of the test statistic under $H_1.$ We can only control the probability of type-I-errors since we know the distribution of the test statistic under $H_0.$

Therefore, if you are not able to reject $H_0,$ **never ever** state something like: "I conclude $H_0$ is true."

::: {.callout-tip icon="false" title=""}
For the special case of a "no-effect" null hypothesis 
$$
H_0:\beta_k=0,
$$ 
there's a famous sentence which goes back to @Altman_Bland_1995 :</br> 
<center>
"Absence of evidence is not evidence of absence." 
</center>
:::
::: 



### Size 

The probability of a type-I-error event is called **size** of $T_n.$ 

::: {.callout icon=false}
#
::: {#def-size}
# Size 

The size of a test statistic $T_n$ is the largest probability of rejecting $H_0$ over all possible null-hypothetical parameter values $\theta\in\Theta_0$
$$
\begin{align*}
\text{Size}_{n,\alpha}
=& \sup_{\theta\in\Theta_0} P(T_n \in \mathcal{R}_{n,\alpha} | \theta\in \Theta_0 ).
\end{align*}
$$
:::
:::

A statistical hypothesis test procedure is called **valid test** if its size can be bounded from above by the chosen **significance level (nominal size)** $\alpha$, i.e. if  
$$
\begin{align*}
\text{Size}_{n,\alpha}\; \leq\; \alpha = \text{Nominal Size}
\end{align*}
$$

Since we want to keep the probability of falsely rejecting $H_0$ small, we choose small singificance levels such as 

* $\alpha=0.05$, 
* $\alpha=0.01$, or 
* $\alpha=0.001.$



::: {.callout-note}
# Exact vs conservative vs invalid

A test statistic $T_n$ is called 

* **exact** if 
$$
\text{Size}_{n,\alpha}=\sup_{\theta\in\Theta_0} P(T_n \in \mathcal{R}_{n,\alpha}| \theta\in \Theta_0 ) =\alpha,
$$
* **conservative** if
$$
\text{Size}_{n,\alpha}= \sup_{\theta\in\Theta_0} P(T_n \in \mathcal{R}_{n,\alpha}| \theta\in \Theta_0 ) <\alpha,
$$
* **invalid** if
$$
\text{Size}_{n,\alpha}=\sup_{\theta\in\Theta_0} P(T_n \in \mathcal{R}_{n,\alpha}| \theta\in \Theta_0 ) > \alpha.
$$
:::

::: {.callout-tip}
Under Assumptions 1-4$^\ast,$ the $F$-test and the $t$-test are **exact** test statistics. 
:::


#### **Showing Exactness of the $F$-Test** {-}

$$
\begin{equation*}
\begin{array}{lll}
& H_0: R\beta = r^{(0)}   & (\text{simple hypothesis})\\
\text{versus}\quad 
& H_1: R\beta \neq r^{(0)}& (\text{composite hypothesis})\\
\end{array}
\end{equation*}
$$

Under Assumptions 1-4$^\ast$ (see @sec-testmultp), we have that 
$$
F_n\overset{H_0}{\sim}F_{q,n-K}.
$${#eq-FNullDistr}

@eq-FNullDistr allows us to show that the $F_n$-test is an exact test. 
Since the null hypothesis is a **simple** hypothesis we do not need to consider the supremum $(\sup_{\theta\in\Theta_0}),$ but simply have
$$
\begin{align*}
\text{Size}_{n,\alpha}
= & P(F_n \in \mathcal{R}_{n,\alpha} \;|\; R\beta = r^{(0)}).
\end{align*}
$$
Thus,
$$
\begin{align*}
\text{Size}_{n,\alpha}
= & P(F_n \in \mathcal{R}_{n,\alpha} \;|\; R\beta = r^{(0)})\\[2ex]
= & P\Big(F_n > c_{n,1-\alpha}\;|\;R\beta = r^{(0)}\Big)\\[2ex]
= & 1 - P\Big(F \leq c_{n,1-\alpha}\;|\;R\beta = r^{(0)}\Big)=\alpha,
\end{align*}
$$
for all sample sizes $n>K.$


#### **Showing Exactness of the Two-Sided $t$-Test:** {-}

$$
\begin{equation*}
\begin{array}{lll}
& H_0: \beta_k =  \beta_k^{(0)}   & (\text{simple hypothesis})\\
\text{versus}\quad 
& H_1: \beta_k \neq \beta_k^{(0)}& (\text{composite hypothesis})\\
\end{array}
\end{equation*}
$$

Under Assumptions 1-4$^\ast$ (see @sec-testingsinglep), we have that 
$$
T_n\overset{H_0}{\sim}t_{n-K}.
$${#eq-T2SNullDistr}


@eq-T2SNullDistr allows us to show that the $t$-test is an exact test. Since the null hypothesis is a **simple** hypothesis we do not need to consider the supremum $(\sup_{\theta\in\Theta_0}),$ but simply have
$$
\begin{align*}
\text{Size}_{n,\alpha}
=&P(T_n \in \mathcal{R}_{n,\alpha} \;|\; \beta_k = \beta_k^{(0)}).
\end{align*}
$$
Thus, 
$$
\begin{align*}
\text{Size}_{n,\alpha}
=&P(T_n \in \mathcal{R}_{n,\alpha} \;|\; \beta_k = \beta_k^{(0)})\\[2ex]
=&P\Big(T_n < c_{n,\alpha/2}\quad\text{or}\quad T_n>c_{n,1-\alpha/2} \;\Big|\; \beta_k = \beta_k^{(0)}\Big)\\[2ex]
=&P\Big(T_n < c_{n,\alpha/2} \;\Big|\; \beta_k = \beta_k^{(0)}\Big) + 
  P\Big(T_n > c_{n,1-\alpha/2} \;\Big|\; \beta_k = \beta_k^{(0)}\Big)\\[2ex]
&=\frac{\alpha}{2}+\frac{\alpha}{2}=\alpha.
\end{align*}
$$
for all sample sizes $n>K.$



#### **Showing Exactness of the One-Sided $t$-Test:** {-}

Consider, without loss of generality, the following right-side version of the one-sided hypothesis:

$$
\begin{equation*}
\begin{array}{lll}
& H_0: \beta_k \leq \beta_k^{(0)}  & (\text{composite hypothesis})\\
\text{versus}\quad 
& H_1: \beta_k > \beta_k^{(0)}     & (\text{composite hypothesis})\\
\end{array}
\end{equation*}
$$

> Considering the right-side version is without loss of generality, since effectively the same arguments apply to the case of the left-side version.


The case of a one-sided hypothesis is slightly more involved since the null hypothesis is a **composite hypothesis**; here: 
$$
H_0: \beta_k \in \;\big]-\infty,\beta_k^{(0)}\big].
$$

To conduct the $t$-test we need to take **one** null hypothetical value 
$$
\tilde\beta_k^{(0)}\in ]-\infty,\beta_k^{(0)}]
$$ 
which then leads to a $\tilde\beta_k^{(0)}$ specific $t$-test
$$
\begin{align*}
T_n(\tilde\beta_k^{(0)}) := 
\frac{\hat\beta_{n,k} - \tilde\beta_k^{(0)}}{\widehat{SE}(\hat\beta_{n,k}|X)} 
\end{align*}
$$


Under Assumptions 1-4$^\ast,$ we have that 
$$
\begin{align*}
T_n(\tilde\beta_{n,k}^{(0)})
&=
\frac{\hat\beta_{n,k} \overbrace{- \beta_k + \beta_k}^{=0} - \tilde\beta_k^{(0)}}{\widehat{SE}(\hat\beta_{n,k}|X)}\\[2ex]
&=
\underbrace{\frac{\hat\beta_{n,k} - \beta_k}{\widehat{SE}(\hat\beta_{n,k}|X)}}_{\sim t_{(n-K)}} + 
\underbrace{\frac{\beta_k - \tilde\beta_k^{(0)}}{\widehat{SE}(\hat\beta_{n,k}|X)}}_{❓},
\end{align*}
$${#eq-NullDistrOneSided}
where under $H_0:\beta_k \in ]-\infty,\beta_k^{(0)}].$

The second term in @eq-NullDistrOneSided is challenging, since generally, we do not know its value and thus also not its sign. 

**Solution:** 

The solution of this problem is to set (as done in @sec-tTestTestStat)
$$
\tilde\beta_k^{(0)} = \beta_k^{(0)}
$$ 
and to use the $t$-test statistic
$$
T_n(\beta_k^{(0)})\equiv T_n
=\frac{\hat\beta_{n,k} - \beta_k^{(0)}}{\widehat{SE}(\hat\beta_{n,k}|X)}.
$$
Under $H_0,$  
$$
\beta_k \leq \beta_k^{(0)} \Leftrightarrow \beta_k - \beta_k^{(0)}\highlight{\leq 0}, 
$$
which implies that 
$$
\begin{align*}
T_n
&=
\frac{\hat\beta_{n,k} - \beta_k^{(0)}}{\widehat{SE}(\hat\beta_{n,k}|X)}\\[2ex]
&=
\underbrace{\frac{\hat\beta_{n,k} - \beta_k}{\widehat{SE}(\hat\beta_{n,k}|X)}}_{\sim t_{(n-K)}} + 
\underbrace{\frac{\beta_k - \beta_k^{(0)}}{\widehat{SE}(\hat\beta_{n,k}|X)}}_{\highlight{\leq 0}}\\[2ex]
&\highlight{\leq} 
\frac{\hat\beta_{n,k} - \beta_k}{\widehat{SE}(\hat\beta_{n,k}|X)} \overset{H_0}{\sim} t_{(n-K)},
\end{align*}
$${#eq-NullDistrOneSidedIneq}
where the inequality holds with probability one (i.e. for any possible realization). 



The inequality in @eq-NullDistrOneSidedIneq implies that 
$$
\begin{align*}
P(T_n\in\mathcal{R}_{n,\alpha}|\beta_k \leq \beta_k^{(0)}) \highlight{\leq}  P(T_n\in\mathcal{R}_{n,\alpha} \;|\;\beta_k = \beta_k^{(0)}) & = \alpha\\[2ex]
\Leftrightarrow\quad  
\text{Size}_{n,\alpha} = \sup_{\beta_k \in ]-\infty,\beta_k^{(0)}]} P(T_n\in\mathcal{R}_{n,\alpha}\;|\;\beta_k \in ]-\infty,\beta_k^{(0)}]) & = \alpha,
\end{align*}
$$
for all sample sizes $n>K,$
where 
$$
\mathcal{R}_{n,\alpha} = ]c_{n,1-\alpha},\infty[,
$$
with $c_{n,1-\alpha}$ denoting the $(1-\alpha)$-quantile of the $t$-distribution with $(n-K)$ degrees of freedom.


<!-- 
::: {.callout-tip icon="false"}
#
Let $F_{T(\beta_k^{(0)})}$ and $F_{t_{(n-K)}}$ denote cumulative distribution functions of $T(\beta_k^{(0)})$ and of the $t$-distribution with $(n-K)$ degrees of freedom. 

The inequality in @eq-NullDistrOneSidedIneq implies that:

1. If $\beta_k = \beta_k^{(0)},$ the distribution of $T(\beta_k^{(0)})$ equals the $t$-distribution with $(n-K)$ degrees of freedom, i.e.
$$
\begin{align*}
T(\beta_k^{(0)}) &\sim t_{(n-K)}\\[2ex] 
\Leftrightarrow \; F_{T(\beta_k^{(0)})} (x) &= F_{t_{(n-K)}}(x) \quad\text{for all}\quad x\in\mathbb{R} 
\end{align*}
$$
2. If $\beta_k < \beta_k^{(0)},$ the distribution of $T(\beta_k^{(0)})$ is **strictly dominated** by the $t$-distribution with $(n-K)$ degrees of freedom, i.e.
$$
F_{T(\beta_k^{(0)})} (x) > F_{t_{(n-K)}}(x) \quad\text{for all}\quad x\in\mathbb{R}. 
$$
::: -->



<!-- ::: {.callout-tip icon="false"}
#
The inequality in @eq-NullDistrOneSidedIneq implies that
$$
P(T(\beta_k^{(0)}) \in\mathcal{R}) = \alpha
\quad\text{if}\quad
\beta_k = \beta_k^{(0)}
$$
and that 
$$
P(T(\beta_k^{(0)}) \in\mathcal{R}) < \alpha
\quad\text{if}\quad
\beta_k < \beta_k^{(0)}.
$$
::: 
-->



<!-- 
### Rejection Regions of the $F$- and $t$-Test

To decide whether we can reject $H_0$, we need to compare the observed value $T_{\text{obs}}$ with the distribution of $T$ under $H_0.$ This can be done using 

* **rejection regions/critical values,** 
* **$p$-values** (@sec-pValue) or  
* **confidence intervals** (@sec-CIsmallsample)

All of these options lead to equivalent test decisions. 

The **rejection region** for a statistical test statistic $T$ is defined using **critical values** which are certain quantiles of the distribution of $T$ under $H_0.$  

#### Rejection Regions of the $F$-Test 

The $F$-test allows us to test
$$
\begin{align*}
&H_0: R\beta = r^{(0)}\\[2ex]
\text{versus}\quad 
&H_1: R\beta\neq r^{(0)},
\end{align*}
$$
where $\beta$ denotes the true (unknown) parameter vector and $r^{(0)}$ the null-hypothetical value specified by the statistician (often, e.g. $r^{(0)}$).  
-->

<!-- 
Let $0<\alpha<1$ denote the significance level and let $c_{1-\alpha}$ denote the $(1-\alpha)$ quantile of the $F$-distribution with $(q,n-K)$ degrees of freedom. 

This quantile $c_{1-\alpha}$ is the **critical value** that defines the **rejection region**: 
$$
\mathcal{R}=\; ]c_{1-\alpha},\infty[
$$
<!-- , and 
- non-rejection region, $]0,c_{1-\alpha}]$  
-->

<!-- * We can rejection $H_0$ if
$$
F_{obs}\in \mathcal{R}=\; ]c_{1-\alpha},\infty[
$$
* We cannot rejection $H_0$ if
$$
F_{obs}\not \in \mathcal{R}=\; ]c_{1-\alpha},\infty[
$$ --> 







<!-- To handle this issue, it is useful to state the **composite null hypothesis** as a family of *infinitely many* specific null hypotheses:
\begin{align*}
&H_0: \beta_k =\tilde\beta_k^{(0)}\quad\text{with}\quad \tilde\beta_k^{(0)}\in ]-\infty,\beta_k^{(0)}]\\
\text{versus}\quad 
& H_1: \beta_k > \beta_k^{(0)}
\end{align*} -->





<!-- 
Thus, for testing 
$$
\begin{align*}
&H_0: \beta_k \leq \beta_k^{(0)}\\
\text{versus}\quad 
& H_1: \beta_k >    \beta_k^{(0)}\\
\end{align*}
$$ -->










### Power  



::: {.callout icon=false}
#
::: {#def-power}
# Power
The power of a test statistic $T_n$ is the probability of correctly rejecting $H_0$ for a given $\theta\in \Theta_1$
$$
\begin{align*}
\text{Power}_{n,\theta,\alpha} 
& = P(T_n \in \mathcal{R}_{n,\alpha} | \theta\in \Theta_1)\\[2ex]
& = 1-P(\text{type-II-error}| \theta\in \Theta_1)
\end{align*}
$$
:::
:::

Since we want to detect a given violations of $H_0,$ we want test statistics with a large power.



::: {.callout-note} 

# Power is a function of $\alpha,$ $\theta,$ and $n$

* Large violations of $H_0$:
$$
\begin{align*}
&\text{Power}_{n,\theta,\alpha}
\to 1 \quad \text{as}\quad |\theta - \theta^{(0)}|\to\infty,
\end{align*}
$$
while keeping $0<\alpha<1$ and $n$ fix. 

* Large sample sizes $n$:
$$
\begin{align*}
&\text{Power}_{n,\theta,\alpha}
\to 1 \quad \text{as}\quad n\to\infty,
\end{align*}
$$
while keeping $0<\alpha<1$ and $|\theta - \theta^{(0)}|>0$ fix. 

* Small significance levels $\alpha$:
$$
\begin{align*}
&\text{Power}_{n,\theta,\alpha}
\to 0 \quad \text{as}\quad \alpha\to 0,
\end{align*}
$$
while keeping $n$ and $|\theta - \theta^{(0)}|>0$ fix. 
:::






::: {.callout icon=false}
#
::: {#def-power}
# Consistency of a Statistical Test
A statistical test is called **consistent** if its power increases as the sample size gets large
$$
\begin{align*}
&\text{Power}_{n,\theta,\alpha}
\to 1 \quad \text{as}\quad n\to\infty\\[2ex]
\end{align*}
$$
for any fixed $0<\alpha<1$ and any fixed $|\theta - \theta^{(0)}|>0.$ 
:::
:::


**Caution:** Consistency of a statistical test is an important proporty, however, this property implies that even small, practically irrelevant violations of $H_0$ will be detected if the sample size is sufficiently large. 


#### **Power of the One-Sided $t$-Test** {-}

<!-- A type-II-error is the mistake of not rejecting the null hypothesis when in fact it should have been rejected. The probability of making a type-II-error equals one minus the probability of correctly rejecting the null hypothesis ("Power").  -->

<!-- For instance, in the case of using the $t$-test to test the null hypothesis $H_0: \beta_k=0$ versus the one-sided alternative hypothesis $H_1:\beta_k>0$) we have that 
\begin{align*}
P(\text{type-II-error})
&=P_{H_1}\Big(t\;\in\;\overbrace{]-\infty,c_{1-\alpha}]}^{\text{non-rejection region}}\Big)\\
&=1-\underbrace{P_{H_1}\Big(t\;\in\;\overbrace{]c_{1-\alpha},\infty[}^{\text{rejection region}}\Big)}_{\text{"Power"}},
\end{align*}
where $P_{H_1}$ means that we compute the probability under the assumption that $H_1$ is true.  -->

<!-- 
::: {.callout-note}
There is a trade off between the probability of making a type-I-error and the probability of making a type-II-error: a lower significance level $\alpha$ decreases $P(\text{type-I-error})$, but necessarily increases $P(\text{type-II-error})$ and vice versa.  
Ideally, we would have some sense of the costs of making each of these errors, and would choose our significance level to minimize these total costs. However, the costs are often difficult to know.  
:::  
-->


Unfortunately, computing the power of a statistical test is usually impossible, since this requires knowing the distribution of the test statistic under the alternative hypothesis $H_1.$ The distribution of a test statistic under $H_1$ can only be derived under quite restrictive setups. 

In the following, we consider such a restrictive setup for the $t$-test statistic: 

* Let's consider the right-sided version of the one-sided hypothesis
$$
\begin{align*}
&H_0: \beta_k\leq \beta_{k}^{(0)}\\
&H_1: \beta_k > \beta_{k}^{(0)}
\end{align*}
$$

* Let $X$ be deterministic. 
* Let the true standard error of $\hat\beta_{n,k}$ be  
$$
\operatorname{SE}(\hat\beta_{n,k}|X)=\frac{1}{\sqrt{n}}4.5.
$$

::: {.callout-tip}

# Standard error of $\hat\beta_{nk}$ is proportional to $1/\sqrt{n}$ 

Of course, usually we do not know the standard error of the estimator, but have to estimate it. However, it is true that the standard error of the OLS estimator $\hat{\beta}_{n,k}$ is **proportional to** $\boldsymbol{1/\sqrt{n}},$
$$
\operatorname{SE}(\hat\beta_{n,k}|X) = C \cdot \frac{1}{\sqrt{n}},
$$
where $C>0$ denotes a constant that does not depend on $n.$
:::

Under this simplified setup and under Assumptions 1-4$^\ast,$ the $t$-test statistic is **normally distributed**. 

If $H_0$ is true with $\beta_k=\beta_k^{(0)},$ then
$$
\begin{align*}
T_n
&=\frac{\hat\beta_{n,k}-\beta_k^{(0)}}{\frac{1}{\sqrt{n}}4.5}\\[2ex]
&\overset{H_0}{=}\frac{\sqrt{n}(\hat\beta_{n,k}-\beta_k)}{4.5} \overset{H_0}{\sim} \mathcal{N}(0,1).
\end{align*}
$$

<!-- 
> **Note:** It suffices to look at this specific null hypothesis $\beta_k=\beta_k^{(0)},$ since the distribution of $T$ is dominated by $\mathcal{N}(0,1)$ for all other null hypotheses $\beta_k<\beta_k^{(0)};$ see our discussions around @eq-NullDistrOneSidedIneq. 
-->

If $H_1: \beta_k-\beta_k^{(0)}>0$ is true, then 
$$
\begin{align*}
T_n&=\frac{\hat\beta_{n,k}-\beta_k^{(0)}}{\frac{1}{\sqrt{n}}4.5}
=\frac{\hat\beta_{n,k}\overbrace{-\beta_k+\beta_k}^{=0}-\beta_k^{(0)}}{\frac{1}{\sqrt{n}}4.5}\\[2ex]
&=\underbrace{\frac{\sqrt{n}(\hat\beta_{n,k}-\beta_k)}{4.5}}_{\sim \mathcal{N}(0,1)}+\underbrace{\frac{\sqrt{n}(\beta_k-\beta_k^{(0)})}{4.5}}_{=\text{ mean shift }>0}\\[2ex] 
&\overset{H_1}{\sim} \mathcal{N}\Bigg(\;\underbrace{\frac{\sqrt{n}(\beta_k-\beta_k^{(0)})}{4.5}}_{=\text{ mean shift }>0}\;,\;1\Bigg)
\end{align*}
$$

Thus
$$ 
\begin{align*}
\text{Power}_{n,\theta,\alpha} 
& = P\Big(\;\underbrace{T_n > z_{1-\alpha}}_{T_n\in\mathcal{R}_{n,\alpha}}\;|\; \underbrace{\beta_k > \beta_k^{(0)}}_{H_1\text{ is true}}\;\Big),
\end{align*}
$$
where $z_{1-\alpha}$ denotes the $(1-\alpha)$ quantile of the standard normal distribution $\mathcal{N}(0,1),$ and where 
$$
T_n\sim \mathcal{N}\left(\frac{\sqrt{n}(\beta_k-\beta_k^{(0)})}{4.5},1\right).
$$

This allows us to compute the power as following:
$$
\begin{align*}
& \text{Power}_{n,\theta,\alpha}=\\[2ex]  
& = P(T_n > z_{1-\alpha}\;|\; \beta_k > \beta_k^{(0)}),
\\[2ex]
& = P\Bigg(\;\overbrace{T_n - \frac{\sqrt{n}(\beta_k-\beta_k^{(0)})}{4.5}}^{=Z\sim\mathcal{N}(0,1)} > z_{1-\alpha} - \frac{\sqrt{n}(\beta_k-\beta_k^{(0)})}{4.5}\;\Bigg|\;\beta_k > \beta_k^{(0)}\Bigg)\\[2ex]
& = P\left(Z > z_{1-\alpha} - \frac{\sqrt{n}(\beta_k-\beta_k^{(0)})}{4.5}\Bigg|\;\beta_k > \beta_k^{(0)}\right)\\[2ex]
&=1-P\left(Z \leq z_{1-\alpha} - \frac{\sqrt{n}(\beta_k-\beta_k^{(0)})}{4.5}\Bigg|\;\beta_k > \beta_k^{(0)}\right)\\[2ex]
&=1-\Phi\left(z_{1-\alpha} - \frac{\sqrt{n}(\beta_k-\beta_k^{(0)})}{4.5}\right)\quad\text{with}\quad \beta_k > \beta_k^{(0)},
\end{align*}
$$
where $\Phi$ denotes the cumulative distribution function of the standard normal distribution $\mathcal{N}(0,1).$
 

@fig-Power illustrates the probability of a type-II-error and the power for the case where 

* $\alpha = 0.05$
* $n=9$ 
* $\beta_k - \beta_k^{(0)}=3$

such that 
$$
\begin{align*}
\text{Power} 
&=1-\Phi\Bigg(z_{1-\alpha} - \overbrace{\frac{\sqrt{n}(\beta_k-\beta_k^{(0)})}{4.5}}^{=\frac{3\cdot 3}{4.5} = 2}\Bigg)\\[2ex]
&=1-\Phi\left(1.64  - 2 \right)\\[2ex]
&=1-0.359=0.641
\end{align*}
$$
That is, we expect to detect the  violation of the null hypothesis $(\beta_k - \beta_k^{(0)}=3)$ in $64\%$ of resamplings from the random sample (data generating process). 

```{r, fig.align="center", echo=FALSE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
#| label: fig-Power
#| fig-cap: Probability of a type-II-error and the power for a one-sided $t$-test at significance level $\alpha = 0.05,$ sample size $n=9,$ and violation of the null hypothesis $\beta_k - \beta_k^{(0)}=3.$

library("scales") # transperent color
mean.alt <- 2

x  <- seq(-4, 4, length=1000)
hx <- dnorm(x)
alpha <- 0.05

plot(x, hx, type="n", xlim=c(-4, 7), ylim=c(0, 0.65), ylab = "", xlab = "", axes=T)
#axis(1)

xfit2 <- x + mean.alt
yfit2 <- dnorm(x)

## Print null hypothesis area
polygon(c(min(x), x,  max(x)), 
        c(0,      hx, 0), 
        col   =alpha("grey", 0.5), 
        border=alpha("grey", 0.9))

ub <- max(x)
lb <- round(qnorm(1-alpha),2)

## The green area: Power
i <- xfit2 >= lb
polygon(c(min(xfit2[i]), xfit2[i], max(xfit2[i])), 
        c(0,  yfit2[i], 0), 
        col=alpha("green", 0.25),
        border=alpha("green", 0.25))

## The blue area: P(type-II-error)
lb <- min(xfit2)
ub <- round(qnorm(1-alpha),2)

i <- xfit2 >= lb & xfit2 <= ub
polygon(c(lb,xfit2[i],ub), c(0,yfit2[i],0), col=alpha("darkblue", 0.25), border=alpha("darkblue", 0.25))

lines(x=c(ub,ub), y=c(0,.47),lwd=2,lty=2)
text(x = ub, y = .57, labels = expression(z[1-alpha]==1.64))

text(x=0+.25,y=.425, "N(0,1)", pos=2)
text(x=2+.5,y=.425, "N(2,1)", pos=4)
legend(x=-4.5,y=.65, title=NULL, bty="n", 
   c(expression("Null Distribution"~"N(0,1)"),"P(type-II-error)","P(type-I-error)", expression(paste("Power")))[-3], 
    fill=c(alpha("grey", 0.5), alpha("darkblue", 0.25), alpha("red", 0.25), alpha("green", 0.5))[-3], horiz=FALSE)
```






### $p$-Value {#sec-pValue}


The $p$-value of a test-statistic $T_n$ is defined as the probability of observing realizations of $T_n$ that are equal to or more extreme (in direction of the alternative) than the observed value $T_{n,\text{obs}}$ **given that the null hypothesis is true.**


$p$-value for a left-sided version of a one-sided hypothesis test:
$$
p_{\text{left},\,\text{obs}} = P(T_n\leq T_{n,\text{obs}}\;|\;H_0 \text{ is true})
$$

$p$-value for a right-sided version of a one-sided hypothesis test:
$$
p_{\text{right},\,\text{obs}} = P(T_n\geq T_{n,\text{obs}}\;|\;H_0 \text{ is true})
$$

$p$-value for a two-sided hypothesis test:
$$
\begin{align*}
p_{\text{obs}}
&=P(|T_n|\geq |T_{n,\text{obs}}|\;|\;H_0 \text{ is true})\\[2ex]
&=2\cdot\min\{p_{\text{left},\,\text{obs}}, 
              p_{\text{right},\,\text{obs}}\}
\end{align*}
$$


::: {.callout-tip}
# Marginal Significance Value
The $p$-value equals the significance level $\alpha$ for which we just fail to reject the null. Therefore, the $p$-value is sometimes also called "marginal significance value". 
:::



Since the $p$-value is defined under the hypothetical **scenario that the null-hypothesis is true**, ... 

<center>
❗... the $p$-value **cannot** be the probability that the null-hypothesis is true. (A common misinterpretation of the $p$-value.)❗
</center>

</br>

![](images/terminator.jpeg)



#### **The $p$-value is a Random Variable** {-} 


Let's consider the two-sided $t$-test for testing the hypotheses
$$
H_0: \beta_k   =  \beta_k^{(0)}\\
H_1: \beta_k \neq \beta_k^{(0)}.
$$
Given an observed value of the test statistik $T_{n,\operatorname{obs}},$ the observed $p$-value is
$$
\begin{align*}
p_{\text{obs}} 
&= P(|T_n|\geq |T_{n,\text{obs}}|\;|\;H_0 \text{ is true})\\[2ex]
&= 1-P(|T_n|\leq |T_{n,\text{obs}}|\;|\;H_0 \text{ is true})\\[2ex]
&= 1-F_{H_0,|T_n|}(|T_{n,\text{obs}}|),
\end{align*}
$$
where $F_{H_0,|T_n|}$ denotes the cumulative distribution function of $|T_n|$ under $H_0.$ 

Generally, the $p$-value is a *random variable*, since it depends on the data. For the random $p$-value, we have that 
$$
\begin{align*}
p
&= P(|T_n|\geq |T_{n}|\;|\;H_0 \text{ is true})\\[2ex]
&= 1-P(|T_n|\leq |T_{n}|\;|\;H_0 \text{ is true})\\[2ex]
&= 1-F_{H_0,|T_n|}(|T_{n}|).
\end{align*}
$$

Under $H_0,$ the $p$-value of the two-sided $t$-test is a **uniformly distributed random variable** 
$$
p \overset{H_0}{\sim}\mathcal{U}[0,1].
$${#eq-pValueUnif}

**Derivation of the above result:**

<!-- 
The $p$-value, 
$$
p= 1 - F_{H_0,|T_n|}(|T_{n}|),
$$ 
is defined by transforming the random variable $|T_n|$ by $1 - F_{H_0,|T_n|}(|T_{n}|),$ where, under $H_0,$ the cdf $F_{H_0,|T_n|}$ is just the cdf of $|H_0|.$  
-->


Let 
$$
F_{H_0,p}(x) = P(p \leq x\;|\;H_0 \text{ is true})
$$ 
denote the cdf of the random $p$-value under $H_0.$ If the claim in @eq-pValueUnif is correct, then the random $p$-value has, under $H_0,$ the cumulative distribution function of $\mathcal{U}[0,1],$ that is
$$
F_{H_0,p}(x) = x\quad\text{for}\quad x\in[0,1].
$$

This is indeed true since, under $H_0,$
$$
\begin{align*}
F_{H_0,p}(x) 
&=   P(p                       \leq                      x \;|\;H_0 \text{ is true}) \\[2ex]
&[\text{using that }\;p= 1-F_{H_0,|T_n|}(|T_{n}|)]\\[2ex]
&=   P(1-F_{H_0,|T_n|}(|T_{n}|)\leq                      x \;|\;H_0 \text{ is true}) \\[2ex]
&=   P(  F_{H_0,|T_n|}(|T_{n}|)\geq                    1-x \;|\;H_0 \text{ is true}) \\[2ex]
&= 1-P(  F_{H_0,|T_n|}(|T_{n}|)\leq                    1-x \;|\;H_0 \text{ is true}) \\[2ex]
&[\text{using that }\;F_{H_0,|T_n|}\;\text{ is invertible}]\\[2ex]
&= 1-P(                |T_{n}| \leq F^{-1}_{H_0,|T_n|}(1-x)\;|\;H_0 \text{ is true}) \\[2ex]
&= 1-F_{H_0,|T_n|}( F^{-1}_{H_0,|T_n|}(1-x) ) \\[2ex]
&= 1-(1-x)\\[2ex]
&= x,
\end{align*}
$$
where $F_{H_0,|T_n|}$ is invertible, since $|T_n|$ is a continuously distribued random variable. 





## Monte Carlo Simulations {#sec-PSSI}

Let's check the above exact inference results using Monte Carlo simulations. The check, of course, applies only to the considered special case and generally does not generalize to other data generating processes.  

First, we program a function `myDataGenerator()` which allows us to generate data from the following model, i.e., from the following fully specified data generating process:
$$
\begin{align*}
Y_i &=\beta_1+\beta_2X_{i2}+\beta_3X_{i3}+\varepsilon_i,\qquad i=1,\dots,n\\
\beta &=(\beta_1,\beta_2,\beta_3)'=(2,3,4)'\\
X_{i2}&\sim U[2,10]\\
X_{i3}&\sim U[12,22]\\
\varepsilon_i|X&\sim\mathcal{N}(0,3^2),
\end{align*}
$$
where $(Y_i,X_i)$ is i.i.d. across $i=1,\dots,n$. 

Let us consider a small sample size of $n=7$. 


The below function `myDataGenerator()` allows to sample new realizations of the random sample 
$$
((X_1,Y_1),\dots,(X_n,Y_n))
$$ 
You can provide your own values for the sample size $n$ and for the parameter vector $\beta=(\beta_1,\beta_2,\beta_3)'$. 
```{r}
## Function to generate artificial data
## If the user provides 'X_cond' data, 
## the sampling of new Y variables is 
## conditionally on the given X_cond variables.
## If X_cond = NULL, sampling is done unconditionally. 

myDataGenerator <- function(n, beta){

## sampling predictors X:
X   <- cbind(rep(1, n), 
            runif(n, 2, 10), 
            runif(n,12, 22))
## sampling error terms: 
eps  <- rnorm(n, sd = 3)
## generate realizations of Y:
Y    <- X %*% beta + eps
## safe X and Y as a data frame:
data <- data.frame("Y"   = Y, 
                   "X_1" = X[,1], 
                   "X_2" = X[,2], 
                   "X_3" = X[,3])
## return the data frame
return(data)
}

## Small sample size
n             <- 7        

## Define the true beta vector
beta_true     <- c(2,3,4)

## Generate Y and X data 
test_data     <- myDataGenerator(n = n, beta=beta_true)

## Look at the first six lines of the data frame
round(head(test_data,     3), 2) 
```

### Check: Testing Multiple Parameters 

In the following, we do inference about multiple parameters. We test 
\begin{align*}
H_0:\;&\beta_2=3\quad\text{and}\quad\beta_3=4\\
\text{versus}\quad H_1:\;&\beta_2\neq 3\quad\text{and/or}\quad\beta_3\neq 4.
\end{align*}
Or equivalently
\begin{align*}
H_0:\;&R\beta  = r^{(0)} \\
H_1:\;&R\beta  \neq r^{(0)},
\end{align*}
where 
$$
R=\left(
\begin{matrix}
0&1&0\\
0&0&1\\
\end{matrix}\right)\quad\text{ and }\quad 
r^{(0)}=\left(\begin{matrix}3\\4\\\end{matrix}\right).
$$
The following `R` code can be used to test this hypothesis:
```{r}
## Library containing the function 'linearHyothesis()' 
## for testing multiple parameters 
suppressMessages(library("car")) 
## See ?linearHypothesis

## Generate one Monte Carlo sample (under H0)
data   <- myDataGenerator(n = n, beta = beta_true)

## Estimate the linear regression model parameters
lm_obj <- lm(Y ~ X_2 + X_3, data = data)

## Option 1:
test_result <- car::linearHypothesis(
      model = lm_obj, 
      hypothesis.matrix = c("X_2=3", "X_3=4"))   
test_result        
```
The $p$-value 
<center>
$p_{\text{obs}}=$ `r round(test_result[[6]][2],4) ` $\;>\alpha=0.05$
</center>
is larger than the chosen significance level $\alpha=0.05.$ Thus we **cannot reject** the null hypothesis 
$$
H_0:\;\beta_2=3\quad\text{and}\quad \beta_3=4.
$$ 

The following codes gives an alternative, equivalent way to compute the test result:
```{r, echo=TRUE, eval=FALSE}
## Option 2:
R <- rbind(c(0,1,0),
           c(0,0,1))
car::linearHypothesis(model = lm_obj, 
                      hypothesis.matrix = R, 
                      rhs = c(3,4))
```


We simulated data under $H_0$ and thus it is not surprising that we cannot reject $H_0.$ 

However, in repeated samples we should nevertheless observe $\alpha\cdot 100\%$ type I errors (false rejections of $H_0$) under $H_0.$ Let's check the type-I-error rate using the following Monte Carlo simulation:
```{r}
## Let's generate 5000 F-test decisions and check 
## whether the empirical rate of type I errors is 
## close to the theoretical significance level. 
B               <- 5000 # MC replications
F_test_pvalues  <- rep(NA, times=B)
##
for(r in 1:B){
  ## generate new data (under H0)
  MC_data <- myDataGenerator(n = n, beta = beta_true)
  ## estimate 
  lm_obj  <- lm(Y ~ X_2 + X_3, data = MC_data)
  ## compute test and p-value
  p       <- linearHypothesis(lm_obj, c("X_2=3", "X_3=4"))$`Pr(>F)`[2]
  ## save the p-value
  F_test_pvalues[r] <- p
}
```

Using the collection of $p$-value realizations (under $H_0$) we can check whether the size equals the nominal size (significance level) $\alpha.$

For $\alpha = 0.05:$
```{r}
alpha        <-  0.05          # signif level
rejections   <- F_test_pvalues[F_test_pvalues < alpha]
round(length(rejections)/B, 4) # actual type-I-error rate 
```
For $\alpha = 0.01:$
```{r}
alpha        <-  0.01  # signif level
rejections   <- F_test_pvalues[F_test_pvalues < alpha]
round(length(rejections)/B, 4) # actual type-I-error rate 
```

Observations:

1. We correctly control for the type-I-error rate since the empirical type-I-error rate is not larger than the chosen significance level $\alpha.$ 
2. The $F$ test is not conservative since the empirical type-I-error rates essentially matches the chosen significance levels $\alpha.$ </br>
In fact, if we would increase the number of Monte Carlo repetitions, the empirical type-I-error rate would converge to the nominal type-I-error rate $\alpha$ due to the law of large numbers.
3. Last but not least: All this works **unconditionally** on $X$ since the distribution of the $F$ statistic (@eq-Ftest) does not depend on $X$. 


Next, we check how well the $F$ test detects certain **violations of the null hypothesis**. We do this by using the same data generating process, but by testing the following <span style="color:#FF0000">incorrect</span> null hypothesis:
\begin{align*}
H_0:\;&{\color{red}\beta_2=4}\quad\text{and}\quad\beta_3=4\\
H_1:\;&\beta_2\neq 4\quad\text{and/or}\quad\beta_3\neq 4
\end{align*}
```{r}
B               <- 5000 # MC replications
F_test_pvalues  <- rep(NA, times=B)
##
for(r in 1:B){
  ## generate new data 
  MC_data <- myDataGenerator(n    = n, beta = beta_true)
  ## estimate 
  lm_obj  <- lm(Y ~ X_2 + X_3, data = MC_data)
  ## compute test and p-value (for a false H0)
  p       <- linearHypothesis(lm_obj, c("X_2=4", "X_3=4"))$`Pr(>F)`[2]
  ## save the p-value
  F_test_pvalues[r] <- p
}

## Checking the power of the F test 

alpha       <-  0.05  # signif_level
rejections  <- F_test_pvalues[F_test_pvalues < alpha]
length(rejections)/B  # power 
```

We can now correctly reject the false null hypothesis in approximately `r length(rejections)/B *100` % of all Monte Carlo replications. 

**Caution:** This means that we are not able to detect the violation of the null hypothesis in `r 100 - length(rejections)/B *100` % of cases. Therefore, we can never use an insignificant test result ($p$-value $\geq\alpha$) as a confirmation of the null hypothesis. Obviously, there are type-II-error events (not rejecting a false $H_0$), but since we typically do not know the distribution of the test statistic under the alternative hypothesis, we cannot control the type-II-error rate. We can only control the type-I-error rate by using a small significance level $\alpha$. 

Moreover, note that the $F$ test is not informative about which part of the null hypothesis ($\beta_2=4$ and/or $\beta_3=4$) is violated. We only get the information that at least one of the multiple parameter hypotheses is violated. Test statistics with this property are called **omnibus tests**.


### Check: Dualty of Confidence Intervals and Hypothesis Tests

Confidence intervals can be computed using `R` as following:
```{r}
## Significance level
alpha        <- 0.05
## Confidence level
conf_level   <- 1 - alpha

## 95% CI for beta_2
confint(lm_obj, parm = "X_2", level = conf_level)
## 95% CI for beta_3 
confint(lm_obj, parm = "X_3", level = conf_level)
```
We can use these two-sided confidence intervals to conduct hypotheses tests. This property of confidence intervals is called the **duality of confidence intervals and hypothesis tests**. 

For instance, when testing the null hypothesis
\begin{align*}
H_0:&\;\beta_2=3\\
\text{versus}\quad H_1: &\;\beta_2\neq 3
\end{align*}
we can either use a $t$-test or equivalently check whether the confidence interval $\operatorname{CI}_{2,1-\alpha}$ for $\beta_2$ contains the hypothetical value $4$ or not. 

* In case of $3    \in\operatorname{CI}_{2,1-\alpha}$, we cannot reject the null hypothesis $H_0$: $\beta_2=3.$ 
* In case of $3\not\in\operatorname{CI}_{2,1-\alpha}$, we can reject the null hypothesis $H_0$: $\beta_2=3.$

If the Assumptions 1-4$^\ast$ hold true, then $\operatorname{CI}_{2,1-\alpha}$ is an exact confidence interval. That is, under the null hypothesis, it falsely rejects the null hypothesis in only $\alpha\cdot 100\%$ of resamplings. Let's check this in the following Monte Carlo simulation:
```{r, fig.align="center", echo=TRUE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
## Significance level
alpha        <- 0.05

## beta_2 safed separately
beta_true_2  <- beta_true[2]

## Container to save all CI realizations
confint_m  <- matrix(NA, nrow=2, ncol=B)
##
for(r in 1:B){
  ## generate new data 
  MC_data <- myDataGenerator(n = n, beta = beta_true)
  ## estimate
  lm_obj  <- lm(Y ~ X_2 + X_3, data = MC_data)
  ## compute confidence interval 
  CI <- confint(lm_obj, parm="X_2", level = 1 - alpha)
  ## save confidence interval
  confint_m[,r] <- CI
}
## check whether true parameter is inside the CI
inside_CI  <- confint_m[1,] <= beta_true_2 & 
                beta_true_2 <= confint_m[2,]

## CI-lower, CI-upper, beta_true_2 inside?
head(cbind(t(confint_m), inside_CI))
```

The following code computes the relative frequency of confidence intervals **not containing** the true parameter value $(\beta_2=3)$:
```{r, fig.align="center", echo=TRUE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
round(length(inside_CI[inside_CI == FALSE])/B, 4)
```
That's good! The relative frequency is basically equal to the chosen $\alpha=0.05$ value. 


Next, we visualize a subsample of `100` confidence intervals from the total sample of `5000` generated confidence interval realizations: 
```{r, fig.align="center", echo=TRUE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
nCIs <- 100
plot(x=0, y=0,type="n", xlim=c(0,nCIs), ylim=range(confint_m[,1:nCIs]),
     ylab="", xlab="Resamplings", main="Confidence Intervals")
for(r in 1:nCIs){
  if(inside_CI[r]==TRUE){
      lines(x=c(r,r), y=c(confint_m[1,r], confint_m[2,r]), lwd=2, col=gray(.5,.5))
  }else{
      lines(x=c(r,r), y=c(confint_m[1,r], confint_m[2,r]), lwd=2, col="darkred")
    }
}
axis(4, at=beta_true_2, labels = expression(beta[2]))
abline(h=beta_true_2)
```

As expected, only about $\alpha\cdot 100\%=5\%$ of all confidence intervals do not contain the true parameter value $\beta_2=3$, but about $(1-\alpha)\cdot 100\%=95\%$ of all confidence intervals contain the true parameter value $\beta_2=3$. 


## Real Data Example {#sec-RDSSInf}

```{r}
## The AER package contains a lot of datasets 
suppressPackageStartupMessages(library(AER))

## Attach the DoctorVisits data to make it usable
data("DoctorVisits")

lm_obj <- lm(visits ~ gender + age + income, data = DoctorVisits)
```


The above `R` codes estimate the following regression model
$$
Y_i = \beta_1 + \beta_{gender} X_{gender,i} 
              + \beta_{age} X_{age,i}
              + \beta_{income} X_{income,i} + \varepsilon_i,
$$
where $i=1,\dots,n$ and

* $X_{gender,i}=1$ if the $i$th subject is a woman and $X_{gender,i}=0$ if the $i$th subject is a man
* $X_{age,i}$ is the age of subject $i$ measured in years divided by $100$
* $X_{income,i}$ is the annual income of subject $i$ in tens of thousands of dollars


The following `R` codes produces the classic regression output table (simular tables are produced by all statistical/econometric software packages):
```{r, echo=TRUE, eval=TRUE}
lm_obj_summary <- summary(lm_obj)
lm_obj_summary
```

The above regression output table contains the following information:

* **Estimate:** The column "Estimate" containes the estimates 
$$
\hat\beta_{j},\quad j\in\{1,gender, age, income\}
$$
You can extract them using `coef(lm_obj)`.


* **Std. Error:** The column "Std. Error" containes the estimates 
$$
\widehat{\operatorname{SE}}(\hat\beta_{j}|X),\quad j\in\{1,gender, age, income\}
$$ 
  * You can extract the total $(K\times K)=(4\times 4)$ variance-covariance matrix estimate $\widehat{Var}(\hat\beta|X)$ using `vcov(lm_obj)`. 
  * The diagonal `diag(vcov(lm_obj))` contains the variance estimates $\widehat{Var}(\hat\beta_j|X)$, $j\in\{1,gender, age, income\}$. 
  * The square root of the diagonal `sqrt(diag(vcov(lm_obj)))` allows you to compute the estimated standard errors shown in the regression table.

* **t value:** The column "t value" contains the observed $t$ test statistics 
$$
T_{obs,j}=\frac{\hat\beta_{j}-0}{\widehat{\operatorname{SE}}(\hat\beta_{j}|X)},\quad j\in\{1,gender, age, income\}
$$
You can extract the values using `lm_obj_summary$coefficients[,3]`.


* **Pr(>|t|):** The column "Pr(>|t|)" contains the $p$ values
$$
P_{H_0}(|t|>t_{obs,j}),\quad j\in\{1,gender, age, income\}
$$
You can extract the values using `lm_obj_summary$coefficients[,4]`.

* **Residual standard error**  $\sqrt{\frac{1}{n-K}\sum_{i=1}^n\hat\varepsilon^2_i}=$ `sqrt(sum(resid(lm_obj)^2)/(n-4))` $=$ `r round(sqrt(sum(resid(lm_obj)^2)/(nrow(DoctorVisits)-4)), 4)`

* **Multiple R-squared:** $R^2=$ `lm_obj_summary$r.squared` $=$ `r round(lm_obj_summary$r.squared, 5)`

* **Adjusted R-squared:** $\bar{R}^2=$ `lm_obj_summary$adj.r.squared` $=$ `r round(lm_obj_summary$adj.r.squared, 5)`

* **F-statistic:** This is a standard $F$ test that tests the null hypothesis that all parameters except the intercept are zero; i.e.<br> 
$H_0$: $\beta_{gender}=\beta_{age}=\beta_{income}=0$<br>
versus<br> 
$H_1$: At least one parameter is not zero. `R`'s `summary()` functions reports an observed $F$ statistic value of $33.22$ which needs to be evaluated for an $F$ distribution with $3$ and $5186$ degrees of freedom leading to a $p$-value $p< 0.00001.$
<br><br>
You can replicate this $F$-test result using the following `R` code:
```{r, eval=TRUE, echo=TRUE}
car::linearHypothesis(
      model = lm_obj, 
      hypothesis.matrix = c("genderfemale=0", "age=0", "income=0"))  
```

<!-- #### Interpretation of $\hat\beta_{gender}$ {-} -->

<!-- Since $X_{gender,i}$ is a dummy variable,  -->

#### `R` Package Stargazer {-}

Beautiful and "publication ready" regression outputs can be produced using the `R` package `stargazer` and its function `stargazer()`:

```{r, echo=FALSE, eval=TRUE}
suppressPackageStartupMessages(library(stargazer))
```

<center>
```{r, echo=TRUE, eval=TRUE, results='asis'}
## Hint: use type = "latex" 
## to produce a latex table
stargazer(lm_obj, type="html")
```
</center>


#### Critical Discussion of the Regression above Results {-}

The above real data analysis does **not** fit into the small sample inference framework we introduced in this chapter. 

1. The dependent variable $Y_i$ `visits` is a *categorial* variable taking finitely many discrete values, indeed 
<center>
`unique(DoctorVisits$visits)` = `r unique(DoctorVisits$visits)`. 
</center>
Consequently, the error term $\varepsilon_i$ **cannot be normal distributed**. 
2. The diagnostic plot ("Residuals versus Fitted") indicates a possible issue **violation of the homoskedasticity assumption**. In case of homokedastic variances, the data points $(\hat\varepsilon_i,\hat{Y}_i)$, $i=1,\dots,n$ should roughly show a homogenous scattering across the fitted values $\hat{Y}_i=X\hat\beta$. This seems not to be the case here.
```{r}
## Diagonstic Plot 
## Residuals versus fitted values
plot(lm_obj, which = 1)
```


Lukily, the data set `DoctorVisits` actually has a **large sample size** of $n=$ `r nrow(DoctorVisits)` and thus there is a way out of this problem: The large sample inference framework introduced in the next chapter. 

<!-- 
## Exercises

* [Exercises for Chapter 6](https://www.dropbox.com/scl/fi/zga37r4btryd61uc1rb0b/Ch6_Exercises1.pdf?rlkey=ine2f41nivqteps58sj7ju6nj&dl=0)

* [Exercises of Chapter 6 with Solutions](https://www.dropbox.com/scl/fi/o1k0b69ll10vyc8qbw850/Ch6_Exercises_with_Solutions1.pdf?rlkey=oue9pbl5jatyc9c9d1rmffni2&dl=0) 
-->


## References {-}

