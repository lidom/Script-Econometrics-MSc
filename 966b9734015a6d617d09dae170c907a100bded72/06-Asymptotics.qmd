# Large Sample Inference {#sec-lsinf}

The content of this chapter is very much inspired by Chapter 2 of the textbook of @Hayashi2000. 

## Tools for Asymptotic Statistics

Basically every modern econometric method is justified using the toolbox of **asymptotic statistics**. The following core concepts from asymptotic statistics will allow us to drop the restrictive normality assumption of @sec-ssinf and to introduce robust standard errors:   

* Concepts on stochastic convergence
* Continuous mapping theorem
* Slutsky's theorem
* Law of large numbers
* Central limit theorems 
* Cramér-Wold device


### Modes of Convergence

In the following we will discuss the four most important convergence concepts for sequences of random variables 
$$
\{z_n\}:=(z_1,z_2,\dots,z_n).
$$ 
Non-random scalars (or vectors or matrices) will be denoted by Greek letters such as $\alpha$. 


<!-- Sequences of random vectors (or matrices) will be denoted by $\{\mathbf{z}_n\}$.  -->

<!-- % Vector-Convergence if and only if element-wise converegence:  -->
<!-- %https://www.statlect.com/asymptotic-theory/mean-square-convergence#:~:text=The%20concept%20of%20mean%2Dsquare,difference%20is%20on%20average%20small. -->

### Four Important Modes of Convergence {-}

Probably the most often used mode of convergence is **convergence in probability**. It states that a stochastic sequence $\{z_n\}$ concentrates around its limit $\alpha$ such that deviations $|z_n-\epsilon|$ larger than some small $\epsilon >0$ occur eventually (as $n\to\infty$) with probability zero. In order to show that some stochastic sequence converges in probability to its limit, one typically uses a *"weak law of large numbers"*.

::: {#def-conv_prop}

## Convergence in Probability 

A sequence of random scalars $\{z_n\}$ **converges in probability** to a constant (non-random) $\alpha$ if, for any (arbitrarily small) $\varepsilon>0$,
\begin{eqnarray*}
  \lim_{n\to\infty} P\left(|z_n-\alpha|>\epsilon\right)=0.
\end{eqnarray*}
We write: 
<center>
$\operatorname{plim}_{n\to\infty}z_n=\alpha\quad$ or shortly 
$\quad z_n\to_{p}\alpha,\quad$ as $\quad n\to\infty.$
</center> 
Convergence in probability of a sequence of random vectors or matrices $\{z_n\}$ to a constant vector or matrix $\alpha$ requires *element-wise* convergence in probability.
:::

A stricter mode of convergence is **almost sure convergence**. Almost sure convergence is (usually) rather hard to derive, since the probability is about an event concerning an infinite sequence. Fortunately, however, there are established *"strong laws of large numbers"* that we can use to argue that some stochastic sequence converges almost surely to its limit. 

::: {#def-conv_as}

## Almost Sure Convergence 

A sequence of random scalars $\{z_n\}$ **converges almost surely** to a constant (non-random) $\alpha$ if
\begin{eqnarray*}
P\left(\lim_{n\to\infty}z_n=\alpha\right)=1.
\end{eqnarray*}
We write:
<center>
$z_n\to_{as}\alpha,\quad$ as $\quad n\to\infty.$
</center> 
Almost sure convergence of a sequence of random vectors (or matrices) $\{z_n\}$ to a constant vector (or matrix) $\alpha$ requires *element-wise* almost sure convergence. 
:::


**Convergence in mean square** is typically the most intuitive mode of convergence. If a stochastic sequence converges to a certain limit, then the mean of the stochastic sequence $E(z_n)$ must converge to this limit and the variance of the stochastic sequence must converge to zero. Convergence in mean square is typically easy to show. Moreover, every sequence that converges in mean square, also converges in probability. 

::: {#def-conv_ms}

## Convergence in Mean Square 

A sequence of random scalars $\{z_n\}$ **converges in mean square** (or **in quadratic mean**) to a constant (non-random) $\alpha$ if
$$
\begin{align*}
  \lim_{n\to\infty}E\left((z_n-\alpha)^2\right)&=0.
\end{align*}
$$
<!-- where
$$
\operatorname{MSE}(z_n,\alpha) = E\left((z_n-\alpha)^2\right)=\left(E(z_n)-\alpha\right)^2 + Var(z_n). 
$$ 
-->
We write: 
<center>
$z_n\to_{ms}\alpha,\quad$ as $\quad n\to\infty.$
</center> 
Mean square convergence of a sequence of random vectors (or matrices) $\{z_n\}$ to a deterministic vector (or matrix) $\alpha$ requires *element-wise* mean square convergence. 
:::


Note: If $z_n$ is an estimator (e.g., $z_n=\hat\beta_{k,n}$), then $E\left((z_n-\alpha)^2\right)$ is called the **mean squared error (MSE)** 
$$
\begin{align*}
\operatorname{MSE}(z_n,\alpha)
&=E\left((z_n-\alpha)^2\right)\\
&=\left(E(z_n)-\alpha\right)^2 + Var(z_n)\\
&=\left(\operatorname{Bias}(z_n,\alpha)\right)^2 + Var(z_n).
\end{align*}
$$ 


<!-- **Convergence to a Random Variable:**The above presented definitions of convergence can be also applied to limits that are random variables. We say that a sequence of random vectors $\{\mathbf{z}_n\}$ converges to a random vector $\mathbf{z}$ and write $\mathbf{z}_n\to_{p}\mathbf{z}$ if the sequence $\{\mathbf{z}_n-\mathbf{z}\}$ converges to $\mathbf{0}$. Similarly, for $\mathbf{z}_n\to_{as}\mathbf{z}$ and $\mathbf{z}_n\to_{ms}\mathbf{z}$.  -->


**Convergence in distribution** is the weakest and at the same time most important mode of convergence. 



::: {#def-conv_distr}

## Convergence in Distribution 

Let $F_n$ be the cumulative distribution function (cdf) of $z_n$ and $F$ the cdf of $z$. A sequence of real valued random variables $\{z_n\}$ **converges in distribution** to a real valued random variable $z$ if 
\begin{eqnarray*}
  \lim_{n\to\infty}F_n(t)=F(t)
\end{eqnarray*}
for all $t$ at which $F(t)$ is continuous. We write: 
<center>
$z_n\to_{d}z,\quad$ as $\quad n\to\infty,$
</center> 
and we call $F$ the **asymptotic (or limit) distribution** of $z_n$. 
:::

Remarks on @def-conv_distr:

1. Often you will see statements like $z_n\to_{d} N(0,1)$ or $z_n\overset{a}{\sim}N(0,1)$, which should be read as $\lim_{n\to\infty}F_n(t) = \Phi(t)$ for all $t$, where $\Phi$ denotes the  distribution function of the standard normal distribution. 
2. A stochastic sequence $\{z_n\}$ can also convergence in distribution to a **constant** $\alpha$. In this case $\alpha$ is treated as a degenerated random variable with cdf 
$$
F_\alpha(t)=\left\{\begin{matrix}0&\text{if}\;\;t<\alpha\\ 1&\text{if}\;\;t\geq\alpha.\end{matrix}\right.
$$


By contrast to all other modes of convergence, @def-conv_distr only addresses the univariate case. The reason for this is that @def-conv_distr cannot simply applied element-wise since this would ignore all interdependencies between the univariate random variables of a radom vector. To handle multivariate convergence in distribution, we need the following theorem known as the Cramér-Wold device. 


::: {#thm-CramerWold}

## Cramér-Wold

Let $z_n\in\mathbb{R}^K$ and $z\in\mathbb{R}^K$ be $K$-dimensional random variables ($K\geq 1$), then
$$
z_n\to_{d} z\text{\quad if and only if \quad}\lambda'z_n\to_{d}\lambda'z
$$ 
for any $\lambda\in\mathbb{R}^K$. 

:::

A proof of @thm-CramerWold can be found, e.g., in @Billingsley2008 (p. 383). 

The Cramér-Wold theorem (@thm-CramerWold) is needed since element-wise convergence in distribution does generally not imply convergence of the *joint* distribution of $z_n$ to the *joint* distribution of $z$; except, if all elements are independent from each other.


<!-- **Remark:**Note that convergence in distribution, as the name suggests, only involves the distributions of the random variables. Thus, the random variables need not even be defined on the same probability space (that is, they need not be defined for the same random experiment), and indeed we don't even need the random variables at all. (But all this is just thought provoking \dots typically, we'll only consider cases with a common probability space.) -->

### Relations among Modes of Convergence {-}

::: {#lem-Relations}

## Relationship among the four modes of convergence

The following relationships hold:

(i) Mean square convergence implies convergence in probability:
$$
 z_n\to_{ms}\alpha\quad \Rightarrow\quad  z_n\to_{p}\alpha
$$
(ii) Almost sure convergence implies convergence in probability:
$$
z_n\to_{as}\alpha\quad \Rightarrow\quad  z_n\to_{p}\alpha
$$ 
(iii) Convergence in distribution to a constant is equivalent to convergence in probability to the same constant:
$$
z_n\to_{d}\alpha\quad \Leftrightarrow\quad z_n\to_{p}\alpha
$$ 
:::

Proofs of the result in @lem-Relations can be found, e.g., here: [https://www.statlect.com/asymptotic-theory/relations-among-modes-of-convergence](https://www.statlect.com/asymptotic-theory/relations-among-modes-of-convergence)


### Continuous Mapping Theorem (CMT)


::: {#thm-Preserv}

## Preservation of convergence for continuous transformations (or "continuous mapping theorem (CMT)")

Suppose $\{z_n\}$ is a stochastic sequence of random scalars, vectors, or matrices and that $f(\cdot)$ is a *continuous* function that does not depended on $n$. Then

(i)   $z_n\to_{p}  \alpha\quad\Rightarrow\quad f(z_n)\to_{p} f(\alpha)$
(ii)  $z_n\to_{as} \alpha\quad\Rightarrow\quad f(z_n)\to_{as} f(\alpha)$
(iii) $z_n\to_{d}  \alpha\quad\Rightarrow\quad f(z_n)\to_{d} f(\alpha)$

:::

Proofs of @thm-Preserv can be found, e.g., in @Vaart2000 (see Theorem 2.3)  or here: [https://www.statlect.com/asymptotic-theory/continuous-mapping-theorem](https://www.statlect.com/asymptotic-theory/continuous-mapping-theorem)

**Note:** The CMT does *not* hold for m.s.-convergence except for the case where $f(.)$ is a linear function.  
<!-- %This is easily seen using the expression %$\E[(z_n-\alpha)^2]=Var(z_n)+(\E(z_n)-\alpha)^2$ and the  -->
<!-- %application of Jensen's inequality (see below). -->

**Examples:** As a consequence of the CMT (@thm-Preserv) we have that the usual arithmetic operations preserve convergence in probability (and equivalently for almost sure convergence and convergence in distribution): 

- If $x_n\to_{p} \beta$ and $y_n\to_{p} \gamma$ then $x_n+y_n\to_{p} \beta+\gamma$
- If $x_n\to_{p} \beta$ and $y_n\to_{p} \gamma$ then $x_n\cdot y_n\to_{p} \beta\cdot\gamma$
- If $x_n\to_{p} \beta$ and $y_n\to_{p} \gamma$ then $x_n/y_n\to_{p} \beta/\gamma$, provided that $\gamma\neq 0$
- If $X_n'X_n\to_{p} \Sigma_{X'X}$ then $(X_n'X_n)^{-1}\to_{p} \Sigma_{X'X}^{-1}$, provided $\Sigma_{X'X}$ is a nonsingular matrix.

<!-- The first statement above is immediately seen by setting $\mathbf{z}_n=\left(x_n, y_n\right)'$, $\boldsymbol\alpha=\left(\beta, \gamma\right)'$, and $\mathbf{a}(\boldsymbol\alpha)=(1,1)\boldsymbol\alpha$, similarly for all others.  -->


### Slutsky's Theorem

The following results are concerned with combinations of convergence in probability and convergence in distribution. These are particularly important for the derivation of the asymptotic distribution of estimators.  


::: {#thm-Slutsky}

## Slutsky's Theorem

Let $x_n$ and $y_n$ denote sequences of random scalars or vectors and let $A_n$ denote a sequences of random matrices. Moreover, $\alpha$ and $A$ are deterministic limits of appropriate dimensions and $x$ is a random limit of appropriate dimension. 

(i) If $x_n\to_{d} x\quad$ and  $\quad y_n\to_{p} \alpha,\quad$  then $\quad x_n+y_n\to_{d} x+\alpha$
(ii) If $x_n\to_{d} x\quad$ and $\quad y_n\to_{p} 0,\quad$ then $\quad x_n'y_n\to_{p} 0$
(iii) If $x_n\to_{d} x\quad$ and $\quad A_n\to_{p} A,\quad$ then $\quad A_nx_n\to_{d} Ax$, where it is assumed that $A_n$ and $x_n$ are "conformable" (i.e., the matrix- and vector-dimensions fit to each other). 
:::

Proofs of @thm-Slutsky can be found, e.g., in @Vaart2000 (see Theorem 2.8) or here: [https://www.statlect.com/asymptotic-theory/Slutsky-theorem](https://www.statlect.com/asymptotic-theory/Slutsky-theorem)


**Remark:** Sometimes, only parts i. and ii. of @thm-Slutsky are called "Slutsky's theorem."


Important special case of @thm-Slutsky: 

If 
$$
x_n\to_{d} N(0,\Sigma)\quad\text{and}\quad A_n\to_{p} A
$$ 
then
$$
A_nx_n\to_{d} N(0,A\Sigma A').
$$






### Law of Large Numbers and Central Limit Theorems

So far, we discussed the definitions of the four most important convergence modes, their relations among each other, and basic theorems about functionals of stochastic sequences (CMT and Slutsky). Though, we still lack of tools that allow us to actually show that a stochastic sequence convergences (in some of the discussed modes) to some limit.

In the following we consider the stochastic sequences 
$$
\bar{z}_1,\bar{z}_2,\dots,\bar{z}_n,\quad\text{as}\quad n \to\infty, 
$$ 
of sample means 
$$
\bar{z}_n:=n^{-1}\sum_{i=1}^nz_i,
$$ 
where $z_i$, $i=1,\dots,n$, are (scalar, vector, or matrix-valued) *random variables*. 

**Remember:** The sample mean $\bar{z}_n$ is an estimator of the deterministic population mean $\mu$. 

Weak Law of Large Numbers (WLLNs), Strong LLNs (SLLNs), and Central Limit Theorems (CLTs) tell us conditions under which arithmetic means $\bar{z}_n=n^{-1}\sum_{i=1}^nz_i$ converge in probability, almost surely, and in distribution, respectively: 

* Weak LLN: $\bar{z}_n \to_{p}\mu$
* Strong LLN: $\bar{z}_n\to_{as}\mu$
* CLT: $\sqrt{n}(\bar{z}_n-\mu)\to_{d}N(0,\sigma^2)$

In the following we introduce the most well-known versions of a WLLN, SLLN, and a CLT.


::: {#thm-WLLN1}

## Weak LLN (by Chebychev)

If $\lim_{n\to\infty} E(\bar{z}_n)=\mu$ and $\lim_{n\to\infty}Var(\bar{z}_n)=0,$ then $\bar{z}_n\to_{p}\mu.$

:::

A proof of @thm-WLLN1 can be found, for instance, here: [https://www.statlect.com/asymptotic-theory/law-of-large-numbers](https://www.statlect.com/asymptotic-theory/law-of-large-numbers)


::: {#thm-SLLN1}

## Strong LLN (by Kolmogorov)

If $\{z_i\}$ is an iid sequence with $E(z_i)=\mu,$ then $\bar{z}_n\to_{as}\mu.$

:::

A proof of @thm-SLLN1 can be found, e.g., in *Linear Statistical Inference and Its Applications*, Rao (1973), pp. 112-114. 


> **Interactive visualization of the Law of Large Numbers:** [http://shiny.webpopix.org/sia/LLN/](http://shiny.webpopix.org/sia/LLN/)


**Note:** The WLLN and the SLLN for **random vectors** follow from applying the the theorem separately for each element of the random vectors.

::: {#thm-CLT1}

## CLT (Lindeberg-Levy)
If $\{z_i\}$ is an iid sequence and $E(z_i)=\mu$ and $Var(z_i)=\sigma^2$ then
$$
\sqrt{n}(\bar{z}_n-\mu)\to_{d} N(0,\sigma^2),\quad\text{as}\quad n\to\infty
$$

:::

A proof of @thm-CLT1 can be found, e.g., in @Vaart2000 (see Theorem 2.17).


> **Interactive visualization of the CLT:** [https://tuomonieminen.shinyapps.io/CLTdemo/](https://tuomonieminen.shinyapps.io/CLTdemo/)


Using the Cramér-Wold device (@thm-CramerWold), the Lindeberg-Levy CLT (@thm-CLT1) can also be applied to $K$-dimensional random vectors: <br>
To show that $\sqrt{n}(\bar{z}_n-\mu)$ converges to a multivariate ($K>1$ dimensional) normal distribution as $n\to\infty$, we need to check whether the **uni**variate stochastic sequence $\{\lambda'z_i\}$ is i.i.d. with $E(\lambda'z_i)=\lambda'\mu$ and $Var(\lambda'z_i)=\lambda'\Sigma\lambda$ for any  $\lambda\in\mathbb{R}^K$. This is the case if the multivariate stochastic sequence $\{z_i\}$ is an i.i.d. sequence with $E(_i)=\mu$ and $Var(z_i)=\Sigma$.


**Note:** The LLNs and the CLT are stated with respect to sequences of sample means $\{\bar{z}_n\}$; i.e., the simplest estimators you probably can think of.  We will see, however, that this is all we need in order to analyze also more complicated estimators such as the OLS estimator.



### Estimators: Sequences of Random Variables

Our concepts above readily apply to univariate or multivariate ($K$-dimensional) estimators $\hat\theta_n\in\mathbb{R}^K$ computed from random samples with sample size $n$:<br>
An increasing sample size $n\to\infty$ makes an estimator $\hat\theta_n$ nothing but a sequence of random variables converging (hopefully) to the correct limit $\theta$. 

If an estimator $\hat\theta_n$ converges in probability to its limit $\theta$, we call the estimator **weakly consistent** or simply **consistent**. If it converges almost surely to $\theta,$ we call the estimator **strongly consistent**. 

<!-- **Note:**Under the asymptotic perspective ($n\to\infty$) it's not necessary anymore to condition on $X$ since as $n\to\infty$ the stochastic influence of $X$ on  vanishes  -->

::: {#def-WConsist}

## (Weak) Consistency 

We say that an estimator $\hat\theta_n$ is *(weakly) consistent for $\theta$* if 
$$
\hat\theta_n\to_{p}\theta,\quad\text{as}\quad n\to\infty.
$$
:::

::: {#def-SConsist}

## Strong Consistency 

We say that an estimator $\hat\theta_n$ is *strongly consistent for $\theta$* if 
$$
\hat\theta_n\to_{as}\theta,\quad\text{as}\quad n\to\infty.
$$
:::

A necessary requirement for weak and strong consistency is that the estimator is asymptotically unbiased. 

::: {#def-ABias}

## Asymptotic Bias

The asymptotic bias of an estimator $\hat\theta_n$ of some  parameter $\theta$ is defined as
$$
\begin{align*}
\operatorname{ABias}(\hat\theta_n,\theta)
&=\lim_{n\to\infty}\operatorname{Bias}(\hat\theta_n,\theta)\\
&=\lim_{n\to\infty}E(\hat\theta_n)-\theta.
\end{align*}
$$
If $\text{ABias}(\hat\theta_n,\theta)=0$, then $\hat\theta_n$ is called an **asymptotically unbiased**.
:::



::: {#def-ANorm}

## Asymptotic Normality

An estimator $\hat\theta_n$ is asymptotically normal distributed if
$$
\sqrt{n}(\hat\theta_n-\theta)\to_{d} \mathcal{N}(0,\Sigma),\quad\text{as}\quad n\to\infty,
$$
where 
$$
\Sigma=\lim_{n\to\infty}Var(\sqrt{n}(\hat\theta_n-\theta))=\lim_{n\to\infty}Var(\sqrt{n}\hat\theta_n)
$$ 
is called the asymptotic variance of $\sqrt{n}(\hat\theta_n-\theta)$.
:::


::: {#def-RootnConsistent}

## $\sqrt{n}$-Consistent 

Consistent estimators are called $\sqrt{n}$-consistent if
$$
\sqrt{n}(\hat\theta_n-\theta)\to_{d} z,\quad\text{as}\quad n\to\infty,
$$
where $z\sim F.$ 
:::




## Asymptotics under the Classic Regression Model


Given the above introduced machinery on asymptotic concepts and results, we can now proof that the OLS estimators 
<center>
$\hat\beta_n\equiv\hat\beta\quad$ and $\quad s_{UB,n}\equiv s_{UB}^2$ 
</center>
applied to the classic regression model (defined by Assumptions 1-4 in @sec-MLR are both consistent, and that $\hat\beta_n$ is asymptotically normal distributed. That is, we can drop the unrealistic normality and spherical errors assumption (Assumption 4$^\ast$) of @sec-ssinf, but still use our inference tools ($t$-tests, $F$-tests); as long as the sample size $n$ is "large."

For the below results, we need to introduce an asymptotic version of the full rank assumption: 

<!-- However, before we can formally state the asymptotic properties, we, first, need to adjust our "data generating process"assumption (Assumption 1) such that we can apply Kolmogorov's strong LLN and Lindeberg-Levy's CLT. Second, we need to adjust the rank assumption (Assumption 3), such that the full column rank of $X$ is guaranteed for the limiting case as $n\to\infty$, too. Assumptions 2 and 4 from @sec-MLR are assumed to hold (and do not need to be changed).  -->

<!-- **Assumption 1$^\ast$: Data Generating Process (for Asymptotics)** Assumption 1 of @sec-MLR applies, but *additionally* we assume that $(\varepsilon_i, X_i)\in\mathbb{R}^{K+1}$ (or equivalently $(Y_i,X_i)\in\mathbb{R}^{K+1}$) is jointly i.i.d. for all $i=1,\dots,n$, with existing and finite second moments for $X_i$ and fourth moments for $\varepsilon_i$.


**Note 1:** The fourth moment of $\varepsilon_i$ is actually only needed for @thm-Consistency_s1; for the rest two moments are sufficient.


**Note 2:** The above adjustment of Assumption 1 is far less restrictive than assuming that the error-terms $\varepsilon_i$ are i.i.d. normally distributed and independent from $X_i$ (as it's necessary for small sample inference in @sec-ssinf).  -->


**Assumption 3$^\ast$: Rank Condition (for Asymptotics)** The $(K\times K)$ matrix 
$$
\Sigma_{X'X}:=E(S_{X'X})=E(n^{-1}X'X)=E(X_iX_i')
$$
<!-- $$ -->
<!-- n^{-1}X'X=S_{X'X}\to_{p}\Sigma_{X'X}\quad\text{as}\quad n\to\infty -->
<!-- $$ -->
has full rank $K$. I.e., $\Sigma_{X'X}$ is nonsingular and invertible. 
<!-- (Note, this assumption does not assume that $S_{X'X}\to_{p}\Sigma_{X'X}$, but if this convergence hold, it assumes that the limiting matrix is of full rank $K$.) -->

<!-- **Note:**The crucial part of Assumption 3$^\ast$ is that the limit matrix has full rank $K$. The convergence in probability statement ($S_{X'X}\to_{p}\Sigma_{X'X}$) follows already from Assumption 1$^\ast$.   -->

<!-- **Note:** Assumption 3$^\ast$ implies the existence and finiteness of the first two moments of $X_i$ (even without Assumption 1$^\ast$). -->



<!-- % Under the Assumptions 1$^\ast$, 2, 3$^\ast$, and 4, we can show the following results. -->


 <!-- 1$^\ast$ and 3$^\ast$  -->

::: {#thm-Sxx1}

## Consistency of $S_{X'X}^{-1}$ 

Under Assumptions 1, 2, 3$^\ast$, and 4, and under the assumption that $X_i$ has finite second moments, 
we have that
$$
\left(\frac{1}{n}X'X\right)^{-1}=:S_{X'X}^{-1}\quad\to_{p}\quad\Sigma_{X'X}^{-1},\quad\text{as}\quad n\to\infty.
$$
:::

The proof of @thm-Sxx1 is done in the lecture. 


<!-- Under Assumption 1$^\ast$, 2 and 3$^\ast$ we have that -->

::: {#thm-bconsistent1}

## Consistency of $\hat\beta$

Under Assumptions 1, 2, 3$^\ast$, and 4, and under the assumption that $X_i$ has a finite second moment, and $\varepsilon_i$ has a finite first moment, we have that
$$
\hat\beta_n\to_{p}\beta\quad\text{as}\quad n\to\infty.
$$
:::

The proof of @thm-bconsistent1 is done in the lecture.


<!-- **Note:** In @thm-bconsistent1 it would suffice to assume that $\varepsilon_i$ has only a finite *first* moment, but since we need also a finite second moment for the error term in the following theorems, we may also require two moments in @thm-bconsistent1.   -->


Moreover, we can show that the appropriately scaled OLS estimator is asymptotically normal distributed. The following theorem is stated for the simpler homoskedastic case, the heteroskedastic case is presented in @sec-CaseHetero.


::: {#thm-OLSnormality1}

## Asymptotic Normality of $\hat{\beta}$ (Homoskedastic Case)

Under Assumption 1, 2, 3$^\ast$, and 4, and under the assumption that $X_i$ and $\varepsilon_i$ have finite second moments, and under the simplifying assumption of spherical errors $(Var(\varepsilon_i|X)=\sigma^2I_n)$, we have that
$$
\sqrt{n}(\hat\beta_n-\beta)\to_{d} \mathcal{N}\left(0,\sigma^2 \Sigma^{-1}_{X'X}\right),\quad\text{as}\quad n\to\infty.
$$
:::


The proof of @thm-OLSnormality1 is done in the lecture. 

In principle, we can derive the usual test statistics from the latter result. Though, as long as we do not know (we usually don't) $\sigma^2$ and $\Sigma_{X'X}$ we need to plug-in the (consistent!) estimators $S_{X'X}^{-1}$ and $s_{UB}^2$, where the consistency of the former estimator is provided by @thm-Sxx1 and the consistency of $s_{UB}^2$ is provided by the following result.


::: {#thm-Consistency_s1}

## Consistency of $s^2_{UB}$

Under the assumptions of @thm-OLSnormality1, but with the additional requirement that $\varepsilon_i$ has finite fourth moments, we have that
$$
s_{UB}^2\to_{p}\sigma^2,\quad\text{as}\quad n\to\infty.
$$
:::

The proof of @thm-Consistency_s1 is skipped, but a detailed proof can be found here: [https://www.statlect.com/fundamentals-of-statistics/OLS-estimator-properties](https://www.statlect.com/fundamentals-of-statistics/OLS-estimator-properties)


### The Case of Heteroskedasticity {#sec-CaseHetero}

@thm-OLSnormality1 can also be stated and proofed for conditionally heteroscedastic error terms. In this case one gets
$$
\sqrt{n}(\hat\beta_n-\beta)\to_{d} \mathcal{N}\left(0,\Sigma_{X'X}^{-1}E(\varepsilon^2_iX_iX_i')\Sigma_{X'X}^{-1}\right)
$${#eq-OLSnormality1Rob}
as $n\to\infty.$ 

The asymptotic variance 
$$
\lim_{n\to\infty}Var(\sqrt{n}(\hat\beta_n-\beta))=\Sigma_{X'X}^{-1}E(\varepsilon_i^2X_iX_i')\Sigma_{X'X}^{-1}
$$ 
is, of course, usually unknown and needs to be estimated from the data by some consistent estimator such that 
$$
S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\to_{p} \Sigma_{X'X}^{-1}E(\varepsilon^2_iX_iX_i')\Sigma_{X'X}^{-1}
$$
as $n\to\infty.$ 

The $\widehat{E}(\varepsilon^2_iX_iX_i')$ is here a placeholder for one of the existing Heteroskedasticity Consistent (HC) estimators of $E(\varepsilon^2X_iX_i')$:

HC-Type  | Formular
------|------------
HC0   |$\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\hat\varepsilon_i^2X_iX_i'$
HC1   | $\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\frac{n}{n-K}\hat\varepsilon_i^2X_iX_i'$
HC2   | $\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{1-h_{i}}X_iX_i'$
HC3   | $\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{2}}X_iX_i'$
HC4    | $\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{\delta_{i}}}X_iX_i'$

HC3 is the most often used HC-estimator. 


\paragraph*{Side Note.} The statistic $h_i:=[P_X]_{ii}$ is called the **leverage statistic** of $X_i,$ where 

* $1/n\leq h_i\leq 1$ and 
* $\bar{h}=n^{-1}\sum_{i=1}^nh_i=K/n$. 

Observations $X_i$ with leverage statistics $h_i$ that greatly exceed the average leverage value $K/n$ are referred to as "high leverage" observations. High leverage observations $X_i$ are observations that are far away from all other observations $X_j$, $i\neq j=1,\dots,n.$ 

High leverage observations $X_i$ have the potential to distort the estimation results, $\hat\beta_n$. Indeed, a high leverage observation $X_i$ will have an distorting effect on the estimation results if the absolute value of the corresponding residual $|\hat{\varepsilon}_i|$ is unusually large---such observations are called **influencial outliers**. Such observations increase the estimation uncertainty. 


The estimator HC0 was suggested in the econometrics literature by @White1980 and is justified by asymptotic ($n\to\infty$) arguments. The estimators HC1, HC2 and HC3 were suggested by @MacKinnon_White_1985 to improve the finite sample performance of HC0. Using an extensive Monte Carlo simulation study comparing HC0-HC3, @Long_Ervin_2000 concludes that HC3 provides the best overall performance in finite samples. @Cribari_2004 suggested the estimator HC4 to further improve the performance in finite sample behavior, especially in the presence of influential observations (large $h_i$ values).

General idea of the HC1-HC4 estimators is to increase the estimated variance in oder to account for the effects of influencial outliers. The residuals $\hat\varepsilon_i$ belonging to $X_i$ values that have a large leverate $h_i$ are multiplied by a factor $\gg 1$ leading to increased $\widehat{E}(\varepsilon^2_iX_iX_i')$. This strategy takes into account increased estimation uncertainties due to single influencial outliers. <!-- (Too small variance estimates lead to false inference, too large variance estimates lead to conservative inference.) -->

### Robust Inference 

<!-- From our asymptotic results under the classic regression model (Assumptions 1$^\ast$, 2, 3$^\ast$, and 4) we get the following results important for testing statistical hypothesis.  -->

#### Robust Hypothesis Testing: Multiple Parameters

Let us reconsider the following system of $q$-many null hypotheses:
\begin{align*}
H_0: \underset{(q\times K)}{R}\underset{(K\times 1)}{\beta} - \underset{(q\times 1)}{r} = \underset{(q\times 1)}{0},
\end{align*}
where the $(q \times K)$ matrix $R$ and the $q$-vector $r=(r_{1},\dots,r_{q})'$ are chosen by the statistician to specify her/his null hypothesis about the unknown true parameter vector $\beta$. To make sure that there are no redundant equations, it is required that $\operatorname{rank}(R)=q$.

By contrast to the multiple parameter tests for small samples (see @sec-testmultp), we can work here with a heterosedasticity robust test statistic which is applicable for heteroscedastic error terms:
$$
\begin{align*}
W&=n(R\hat\beta_n -r)'[R\,S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\,R']^{-1}(R\hat\beta_n-r)\\ 
W&\overset{H_0}{\to}_d\chi^2(q), \quad\text{as}\quad n\to\infty. 
\end{align*}
$${#eq-Ftestasymp}
The price to pay is that the distribution of the test statistic under the null hypothesis is only valid asymptotically; i.e. for large $n$. That is, the critical values taken from the asymptotic distribution will be useful only for "largish" samples sizes. 

In case of homoscedastic error terms, one can substitute $S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}$ by $s_{UB}^2S_{X'X}^{-1}$. 

**Finite-sample correction:** In order to improve the finite-sample performance of this test, one usually uses the $F_{q,n-K}$ distribution with $q$ and $n-K$ degrees of freedoms instead of the $\chi^2(q)$ distribution. Asymptotically ($n\to\infty$), $F_{q,n-K}$ is equivalent to $\chi^2(q)$. However, for finite sample sizes $n$ (i.e., the practically relevant case) $F_{q,n-K}$ leads to larger critical values which helps to account for the estimation errors in $S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}$ (or in $s_{UB}^2S_{X'X}^{-1}$) which are otherwise neglected by the pure asymptotic perspective. 


#### Robust Hypothesis Testing: Single Parameters

Let us reconsider the case of hypotheses about only one parameter $\beta_k,$ with $k=1,\dots,K$
\begin{equation*}
\begin{array}{ll}
H_0: & \beta_k=r\\
H_A: & \beta_k\ne r\\
\end{array}
\end{equation*}
We can selecting the $k$th diagonal element of the test-statistic in @eq-Ftestasymp and taking the square root yields 
$$
\begin{align*}
t&=\frac{\sqrt{n}\left(\hat{\beta}_k-r\right)}{\sqrt{\left[S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\right]_{kk}}}\\[1.5ex] 
\text{where}\qquad t&\overset{H_0}{\to}_d\mathcal{N}(0,1),\quad\text{as}\quad n\to\infty. 
\end{align*}
$$
This $t$ test statistic allows for heteroscedastic error terms. In case of homoscedastic error terms, one can substitute $[S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{kk}$ by $s_{UB}^2[S_{X'X}^{-1}]_{kk}$. 

**Finite-sample correction:** In order to improve the finite-sample performance of this $t$ test, one usually uses the $t_{(n-K)}$ distribution with $n-K$ degrees of freedoms instead of the $\mathcal{N}(0,1)$ distribution. Asymptotically ($n\to\infty$), $t_{(n-K)}$ is equivalent to $\mathcal{N}(0,1)$. However, for finite sample sizes $n$ (i.e., the practically relevant case) $t_{n-K}$ leads to larger critical values which helps to account for the estimation errors in $[S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{kk}$ (or in $s_{UB}^2[S_{X'X}^{-1}]_{kk}$) which are otherwise neglected by the pure asymptotic perspective. 



#### Robust Confidence Intervals

Following the derivations in Chapter @sec-CIsmallsample, but using the expression for the robust standard errors, we get the following heteroscedasticity robust (random) $(1-\alpha)\cdot 100\%$ confidence interval 
$$
\operatorname{CI}_{1-\alpha}=
\left[\hat\beta_k\pm t_{1-\alpha/2,n-K}\sqrt{n^{-1}[S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{kk}}\right].
$$
Here, the coverage probability is an asymptotic coverage probability with $P(\beta_k\in\operatorname{CI}_{1-\alpha})\to \gamma$ as $n\to\infty$, where $\gamma\geq 1-\alpha.$ 



## Monte Carlo Simulations {#sec-MCLS}

Let's apply the above asymptotic inference methods using `R`. As in Chapter @sec-PSSI we, first, program a function `myDataGenerator()` which allows us to generate data from the following model, i.e., from the following fully specified data generating process:
\begin{align*}
Y_i &=\beta_1+\beta_2X_{i2}+\beta_3X_{i3}+\varepsilon_i,\qquad i=1,\dots,n\\
\beta &=(\beta_1,\beta_2,\beta_3)'=(2,3,4)'\\
X_{i2}&\sim U[-4,4]\\
X_{i3}&\sim U[-5,5]\\
\varepsilon_i|X_i&\sim U[-0.5 |X_{i2}|, 0.5 |X_{i2}|],
\end{align*}
where $(Y_i,X_i)$ is assumed i.i.d. across $i=1,\dots,n$ with $X_{i2}$ and $X_{i3}$ being independent of each other. 

By contrast to Chapter @sec-PSSI, the error terms are **heteroskedastic** 
$$
Var(\varepsilon_i|X_i)=\frac{1}{12}X_{i2}^2
$$ 
and **not Gaussian**. 

> As a side note: The unconditional variance follows by the law of total variance and is given by 
> \begin{align*}
> Var(\varepsilon_i)
> &=E(Var(\varepsilon_i|X_i))+Var(E(\varepsilon_i|X_i))\\
> &=E\left(\frac{1}{12}X_{i2}^2\right)+0\\
> &=\frac{1}{12}\left(\frac{1}{12}(4-(-4))^2\right)=\frac{4}{9}.
> \end{align*}

Moreover, the large sample normality result in @eq-OLSnormality1Rob does not need a conditioning on $X$. Consequently, the option to condition on $X$ is removed in the following `R`-function `myDataGenerator()`.
```{r}
## Function to generate artificial data
myDataGenerator <- function(n, beta){
  ##
  X   <- cbind(rep(1, n), 
                 runif(n, -4, 4), 
                 runif(n, -5, 5))
  ##
  eps  <- runif(n, - 0.5 * abs(X[,2]), + 0.5 * abs(X[,2]))
  Y    <- X %*% beta + eps
  data <- data.frame("Y"=Y, "X_1"=X[,1], "X_2"=X[,2], "X_3"=X[,3])
  ##
  return(data)
}
```


### Check: Distribution of $\hat\beta_n$ 

The above data generating process fulfills our regulatory assumptions of this chapter. So, by theory, the estimators $\hat\beta_k$ should be normal distributed for sufficiently large sample sizes $n$. 
$$
\sqrt{n}\left(\hat\beta_{n,k}-\beta_k\right)\to_d\mathcal{N}\left(0,\left[\Sigma_{X'X}^{-1}E(\varepsilon^2_iX_iX_i')\Sigma_{X'X}^{-1}\right]_{kk}\right)
$$
or (slight abuse of notation):
$$
\hat\beta_{n,k}\to_d\mathcal{N}\left(\beta_k, \;n^{-1}\;\left[\Sigma_{X'X}^{-1}E(\varepsilon^2_iX_iX_i')\Sigma_{X'X}^{-1}\right]_{kk}\right)
$$
as $n\to\infty.$ 

Mathematically, the latter is a bit sloppy since the right hand side of $\to_d$ depends on $n$, i.e., is not the stable limit object for $n\to\infty$. However, this sloppiness is nevertheless instructive since it gives us the approximative distribution for given largish sample sizes like $n=100$.
```{r, eval=FALSE, echo=FALSE}
library("fractional")
moment_fun <- function(m=3, a, b){fractional((b^(m+1)-a^(m+1))/((m+1)*(b-a)))}
moment_fun(m=4, a=-4,b=4)
fractional((1/12)*(256/5))
```

For our above specified data generating process, we can derive all (usually unknown) population quantities. 

- From the assumed distributions of $X_{i2}$ and $X_{i3}$ we have that:
$$
\Sigma_{X'X}=E(S_{X'X})=E(X_iX_i')
=\left(\begin{matrix}1&0&0\\0&E(X_{i2}^2)&0\\0&0&E(X_{i3}^2)\end{matrix}\right)
=\left(\begin{matrix}1&0&0\\0&\frac{16}{3}&0\\0&0&\frac{25}{3}\end{matrix}\right)
$$
The above result follows from observing that $E(X^2)=Var(X)$ if $X$ has mean zero, and that the variance of uniform $U[a,b]$ distributed random variables is given by $\frac{1}{12}(b-a)^2$.

- Moreover, $E(\varepsilon^2_iX_iX_i')=E(X_iX_i'E(\varepsilon^2_i|X_i))=E\left(X_iX_i'\left(\frac{1}{12}X_{i2}^2\right)\right)$ such that
\begin{align*}
E(\varepsilon^2_iX_iX_i')
&=\left(\begin{matrix}E\left(\frac{1}{12}X_{i2}^2\right)&0&0\\
                   0&E\left(X_{i2}^2\cdot\frac{1}{12}X_{i2}^2\right)&0\\0&0&E\left(X_{i3}^2\cdot\frac{1}{12}X_{i2}^2\right)
      \end{matrix}\right)\\
&=\left(\begin{matrix}\frac{1}{12}E\left(X_{i2}^2\right)&0&0\\
       0&\frac{1}{12}E\left(X_{i2}^4\right)&0\\0&0&\frac{1}{12}E\left(X_{i2}^2\right)\,E\left(X_{i3}^2\right)
\end{matrix}\right)\\      
&=\left(\begin{matrix}\frac{1}{12}\frac{16}{3}&0&0\\
                      0&\frac{1}{12}\frac{256}{5}&0\\
                      0&0&\frac{1}{12}\frac{16}{3}\frac{25}{3}\end{matrix}\right)\\
&=\left(\begin{matrix}\frac{4}{9}&0&0\\0&\frac{64}{15}&0\\0&0&\frac{100}{27}\end{matrix}\right)
\end{align*}
The above result follow from observing that for $X\sim U[a,b]$ one has $E(X^k)=\frac{b^{k+1}-a^{k+1}}{(k+1)(b-a)}$, $k=1,2,\dots$; see, for instance, [Wikipedia](https://en.wikipedia.org/wiki/Continuous_uniform_distribution).

So, for instance, for $\hat{\beta}_2$ we have the following theoretical large sample distribution:
$$
\begin{align}
\hat\beta_{n,2}\to_d&\mathcal{N}\left(\beta_2, \;\frac{1}{n}\;\left[\left(\begin{matrix}1&0&0\\0&\frac{16}{3}&0\\0&0&\frac{25}{3}\end{matrix}\right)^{-1}\left(\begin{matrix}\frac{4}{9}&0&0\\0&\frac{64}{15}&0\\0&0&\frac{100}{27}\end{matrix}\right)\left(\begin{matrix}1&0&0\\0&\frac{16}{3}&0\\0&0&\frac{25}{3}\end{matrix}\right)^{-1}\right]_{22}\right)\\
\hat\beta_{n,2}\to_d&\mathcal{N}\left(\beta_2, \;\frac{1}{n}\;\left[
  \left(
  \begin{matrix}
  0.444 & 0 &0\\
  0 & 0.15 &0\\
  0&0&0.053
  \end{matrix}\right)\right]_{22}\right)\\
\hat\beta_{n,2}\to_d&\mathcal{N}\left(\beta_2, \;\frac{1}{n}\;0.15\right)
\end{align}
$$
Let's use a Monte Carlo simulation to check how well the theoretical large sample ($n\to\infty$) distribution of $\hat\beta_2$ works as an approximative distribution for a practical largish sample size of $n=100$.
```{r}
set.seed(123)
n           <- 100      # a largish sample size
beta_true   <- c(2,3,4) # true data vector

## Mean and variance of the true asymptotic 
## normal distribution of beta_hat_2:
# true mean
beta_true_2     <- beta_true[2] 
# true variance
var_true_beta_2 <- (solve(diag(c(1, 16/3, 25/3)))    %*% 
                          diag(c(4/9, 64/15, 100/27))%*% 
                    solve(diag(c(1, 16/3, 25/3))))[2,2]/n

## Let's generate 5000 realizations from beta_hat_2, and check 
## whether their distribution is close to the true normal 
## distribution.
## (We don't condition on X since the theoretical limit 
## distribution is unconditional on X)
rep        <- 5000 # MC replications
beta_hat_2 <- rep(NA, times=rep)
##
for(r in 1:rep){
    MC_data <- myDataGenerator(n    = n, 
                               beta = beta_true)
    lm_obj        <- lm(Y ~ X_2 + X_3, data = MC_data)
    beta_hat_2[r] <- coef(lm_obj)[2]
}

## Compare:
## True beta_2 versus average of beta_hat_2 estimates
c(beta_true_2, round(mean(beta_hat_2), 4))
```
Good! As expected, the average of the `5000` simulated realizations of $\hat\beta_2$ is basically equal to the theoretical true mean $E(\hat\beta_2)=\beta_2=3$ which indicates a bias of zero. 

```{r}
## True variance of beta_hat_2 versus 
## empirical variance of beta_hat_2 estimates
c(round(var_true_beta_2, 5), round(var(beta_hat_2), 5))
```
Great! The variance of the `5000` simulated realizations of $\hat\beta_2$ is basically equal to the theoretical true variance $Var(\hat\beta_{n,2})=0.15/n=0.0015$. 


```{r, fig.align="center", echo=TRUE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
## True normal distribution of beta_hat_2 versus 
## empirical density of beta_hat_2 estimates
library("scales")
curve(expr = dnorm(x, mean = beta_true_2, 
                   sd=sqrt(var_true_beta_2)), 
      xlab="",ylab="", col=gray(.2), lwd=3, lty=1, 
xlim=range(beta_hat_2),ylim=c(0,14.1),main=paste0("n=",n))
lines(density(beta_hat_2, bw = bw.SJ(beta_hat_2)), 
      col=alpha("blue",.5), lwd=3)
legend("topleft", lty=c(1,1), lwd=c(3,3), 
     col=c(gray(.2), alpha("blue",.5)), bty="n", legend= 
c(expression(
  "Theoretical (Asymptotic) Gaussian Density of"~hat(beta)[2]), 
  expression(
  "Empirical Density Estimation based on MC realizations from"~
  hat(beta)[2])))
```
Great! The nonparametric density estimation (estimated via `density()`) computed from the `5000` simulated realizations of $\hat\beta_2$ is indicating that $\hat\beta_2$ is really normally distributed as described by our theoretical results. 

 <!-- in @thm-OLSnormality1 (homoscedastic case) and in Equation @eq-OLSnormality1Rob (heterosceda# car::linearHypothesis(model = lm_obj, 
#                       hypothesis.matrix = c("X_2=3", "X_3=4"), 
#                       vcov = vcovHC3_mat)
stic case).  -->


However, is the asymptotic distribution of $\hat\beta_2$ also usable for (very) small samples like $n=5$? Let's check that:
```{r}
set.seed(123)
n           <- 5       # a small sample size
beta_true   <- c(2,3,4) # true data vector

## Mean and variance of the true asymptotic 
## normal distribution of beta_hat_2:
# true mean
beta_true_2     <- beta_true[2] 
# true variance
var_true_beta_2 <- (solve(diag(c(1, 16/3, 25/3)))%*% 
                          diag(c(4/9, 64/15, 100/27))%*% 
                    solve(diag(c(1, 16/3, 25/3))))[2,2]/n

## Let's generate 5000 realizations from beta_hat_2, and check 
## whether their distribution is close to the true normal 
## distribution.
## (We don't condition on X since the theoretical limit 
## distribution is unconditional on X)
rep        <- 5000 # MC replications
beta_hat_2 <- rep(NA, times=rep)
##
for(r in 1:rep){
    MC_data <- myDataGenerator(n    = n, 
                               beta = beta_true)
    lm_obj        <- lm(Y ~ X_2 + X_3, data = MC_data)
    beta_hat_2[r] <- coef(lm_obj)[2]
}

## Compare:
## True beta_2 versus average of beta_hat_2 estimates
c(beta_true_2, round(mean(beta_hat_2), 4))
```
OK, at least on average the `5000` simulated realizations of $\hat\beta_2$ are basically equal to the true mean $E(\hat\beta_2)=\beta_2=3$. 

```{r}
## True variance of beta_hat_2 versus 
## empirical variance of beta_hat_2 estimates
c(round(var_true_beta_2, 4), round(var(beta_hat_2), 4))
```
Ouch! The theoretical large sample variance $Var(\hat\beta_2)=0.15/n=0.03$ is about 50% smaller than the actual (small sample) variance of $\hat\beta_2$ approximated by the empirical variance of the `5000` simulated realizations of $\hat\beta_2$. That is, **we cannot simply use a large sample result in small samples**. 

This issue can also seen when comparing the theoretical large sample distribution of $\hat\beta_2$ with an estimate of the actual finite-sample distribution of $\hat\beta_2.$


```{r, fig.align="center", echo=TRUE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
## True normal distribution of beta_hat_2 versus 
## empirical density of beta_hat_2 estimates
library("scales")
curve(expr = dnorm(x, 
                   mean = beta_true_2, 
                   sd   = sqrt(var_true_beta_2)), 
      xlab = "", ylab = "", col=gray(.2), lwd=3, lty=1, 
xlim=c(2,4), ylim=c(0,3), main=paste0("n=",n))
lines(density(beta_hat_2, bw = bw.SJ(beta_hat_2)), 
      col=alpha("blue",.5), lwd=3)
## Legend      
legend("topleft", lty=c(1,1), lwd=c(3,3), 
       col=c(gray(.2), alpha("blue",.5)), bty="n", 
       legend= 
c(expression(
  "Theoretical (Asymptotic) Gaussian Density of"~hat(beta)[2]), 
  expression(
  "Empirical Density Estimation based on MC realizations from"~
  hat(beta)[2])))      
```
Not good. The actual finite-sample distribution has substantially fatter tails. That is, if we would use the quantiles of the asymptotic distribution, we would falsely reject the null-hypothesis too often (probability of type I errors would be larger than the significance level).  

Fortunately, asymptotics are usually kicking in relatively fast; here,  things become much more reliable already for $n\geq 15$. 



### Check: Testing Multiple Parameters 

In the following, we do inference about multiple parameters. We test the (here correct) null hypothesis 
\begin{align*}
H_0:\;&\beta_2=3\quad\text{and}\quad\beta_3=4\\
\text{versus}\quad H_A:\;&\beta_2\neq 3\quad\text{and/or}\quad\beta_3\neq 4.
\end{align*}
Or equivalently
\begin{align*}
H_0:\;&R\beta -r = 0 \\
H_A:\;&R\beta -r \neq 0,
\end{align*}
where 
$$
R=\left(
\begin{matrix}
0&1&0\\
0&0&1\\
\end{matrix}\right)\quad\text{ and }\quad 
r=\left(\begin{matrix}3\\5\\\end{matrix}\right).
$$
The following `R` code can be used to test this hypothesis. Note that we use HC3 robust variance estimation `sandwich::vcovHC(lm_obj, type="HC3")` to take into account that the error terms are heteroscedastic. 
```{r}
suppressMessages(library("car")) # for linearHyothesis()
# ?linearHypothesis
library("sandwich") # for vcovHC(), robust variance estimations

set.seed(1009)

## Generate data
MC_data <- myDataGenerator(n    = 100, 
                           beta = beta_true)

## Estimate the linear regression model parameters
lm_obj <- lm(Y ~ X_2 + X_3, data = MC_data)

vcovHC3_mat <- sandwich::vcovHC(lm_obj, type="HC3")

## Option 1:
# car::linearHypothesis(model = lm_obj, 
#                       hypothesis.matrix = c("X_2=3", "X_3=4"), 
#                       vcov = vcovHC3_mat)

## Option 2:
R <- rbind(c(0,1,0),
           c(0,0,1))
car::linearHypothesis(model = lm_obj, 
                      hypothesis.matrix = R, 
                      rhs  = c(3,4),
                      vcov = vcovHC3_mat)
```
The large $p$-value does not allow us to reject the null-hypothesis at any of the usual significance levels. This is good, since we test here a correct null-hypothesis and rejecting it would mean to do a false rejection (type I error, false positive). 

On average, however, there will be some false rejections of the null-hypothesis, but this type I error rate needs to be samler or equal to $\alpha$. 

```{r}
set.seed(1110)

B        <- 10000 
p_values <- numeric(B)

for(r in 1:B){ 
  MC_data     <- myDataGenerator(n    = 100, beta = beta_true)
  lm_obj      <- lm(Y ~ X_2 + X_3, data = MC_data)
  vcovHC3_mat <- sandwich::vcovHC(lm_obj, type="HC3")
  Ftest       <- car::linearHypothesis(model = lm_obj, 
                        hypothesis.matrix = c("X_2=3", "X_3=4"), 
                        vcov = vcovHC3_mat)
  p_values[r] <- Ftest$'Pr(>F)'[2]                      
}
## Nominal type I error rate 
alpha <- 0.05 
## Empirical type I error rate
length(p_values[p_values < alpha])/B
```

This is more or less ok, but you see that the upper bound is not so strict here as it was in the small sample inference case in @sec-PSSI. A larger sample size $n\geq 100$ would improve this simulation result. 

### Check: Testing Single Parameters

Next, we do inference about a single parameter. We test 
\begin{align*}
H_0:&\beta_3=5\\
\text{versus}\quad H_A:&\beta_3\neq 5.
\end{align*}
```{r}
# Load libraries
suppressMessages(library("lmtest"))  # for coeftest()

## Generate data
n <- 100
MC_data <- myDataGenerator(n    = n, 
                           beta = beta_true)

## Estimate the linear regression model parameters
lm_obj <- lm(Y ~ X_2 + X_3, data = MC_data)

## Robust t test

## Robust standard error for \hat{\beta}_3:
SE_rob <- sqrt(vcovHC(lm_obj, type = "HC3")[3,3])
## hypothetical (H0) value of \beta_3:
beta_3_H0 <- 4
## estimate for beta_3:
beta_3_hat <- coef(lm_obj)[3]
## robust t-test statistic
t_test_stat <- (beta_3_hat - beta_3_H0)/SE_rob
## p-value
K <- length(coef(lm_obj))
##
p_value <- 2 * min(   pt(q = t_test_stat, df = n - K), 
                   1- pt(q = t_test_stat, df = n - K))
p_value
```
Agian, the large $p$-value does not allow us to reject the (here true) null-hypothesis at any of the usual significance levels. Let us check the empirical type I error rate. 


```{r}
set.seed(1009)
n        <- 100
B        <- 10000 
p_values <- numeric(B)

for(r in 1:B){ 
  MC_data     <- myDataGenerator(n = n, beta = beta_true)
  lm_obj      <- lm(Y ~ X_2 + X_3, data = MC_data)
  SE_rob      <- sqrt(vcovHC(lm_obj, type = "HC3")[3,3])
  beta_3_hat  <- coef(lm_obj)[3]
  ## robust t-test statistic
  t_test_stat <- (beta_3_hat - beta_3_H0)/SE_rob
  p_values[r] <- 2 * min(   pt(q = t_test_stat, df = n - length(beta_true)), 
                         1- pt(q = t_test_stat, df = n - length(beta_true)))
}
## Nominal type I error rate 
alpha <- 0.05 
## Empirical type I error rate
length(p_values[p_values < alpha])/B
```

Good! The empirical type I error rate is bounded from above by the nominal type I error rate $\alpha$. 


## Real Data Example {#sec-RDLSInf}

In the following, we revisit the read data study from @sec-RDSSInf. Now, we have the tools to allow for heteroskedastic and non-Gaussian errors.  

```{r}
## The AER package contains a lot of datasets 
suppressPackageStartupMessages(library(AER))

## Attach the DoctorVisits data to make it usable
data("DoctorVisits")

lm_obj <- lm(visits ~ gender + age + income, data = DoctorVisits)
```

The above `R` codes re-estimate the following regression model
$$
Y_i = \beta_1 + \beta_{gender} X_{gender,i} 
              + \beta_{age} X_{age,i}
              + \beta_{income} X_{income,i} + \varepsilon_i,
$$
where $i=1,\dots,n$ and

* $X_{gender,i}=1$ if the $i$th subject is a woman and $X_{gender,i}=0$ if the $i$th subject is a man
* $X_{age,i}$ is the age of subject $i$ measured in years divided by $100$
* $X_{income,i}$ is the annual income of subject $i$ in tens of thousands of dollars

Now, to do heteroskedasticity consistent robust inference, one can use the `vcocHV()` frunction from the `R` package `sandwich` together with the `coeftest()` function from the `R` package `lmtest`. 

```{r, echo = TRUE, eval = TRUE}
## No robust standard errors:
coeftest(lm_obj)
## HC3 robust standard errors:
coeftest(lm_obj, vcov = vcovHC(lm_obj, type = "HC3"))
```

You can do this also using the `R` package `stargazer` which gives you (almost) puplication ready regression output tables. The following `R` code produced two regression outputs---one without and one with robust standard errors:
```{r, echo=TRUE, eval=TRUE, results='asis'}
suppressPackageStartupMessages(library("stargazer"))

# Adjust standard errors
cov_beta_HC3   <- vcovHC(lm_obj, type = "HC3")
robust_HC3_se  <- sqrt(diag(cov_beta_HC3))

# Stargazer output (with and without RSE)
stargazer(lm_obj, lm_obj, 
          se   = list(NULL, robust_HC3_se),
          column.labels = c("", "Robust SE"),
          type = "html") # alternatively: type = "text" or type = "latex"
```


<!-- ## Exercises -->

<!-- * [Exercises for Chapter 6](Exercises/Ch6_Exercises.pdf) -->

<!-- * [Exercises of Chapter 6 with Solutions](Exercises/Ch6_Exercises_with_Solutions.pdf) -->

<!-- 
### Variance Inflation

https://online.stat.psu.edu/stat462/node/180/
 -->


# References {-}
