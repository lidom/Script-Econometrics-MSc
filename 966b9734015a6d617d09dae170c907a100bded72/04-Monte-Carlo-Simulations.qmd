<!-- LTeX: language=en-US -->
# Estimation Theory and Monte Carlo Simulations


Learning outcomes of this chapter: 

* You know the basic concepts of estimation theory and you can apply them to specific estimators.
* You know how to use a Monte Carlo simulation to check the accuracy (bias, variance, and standard error) of a given estimator 


## Estimator vs. Estimate 

Let's assume that we have an iid random sample $\{X_1,\dots,X_n\}$ with 
$$
X_i\overset{iid}{\sim} F_X
$$ 
for all $i=1,\dots,n$, and let 
$$
\theta\in\mathbb{R}
$$ 
denote some parameter (e.g. the mean or the variance) of the distribution $F_X$. 

An **estimator** $\hat\theta_n$ of $\theta$ is a function of the random sample $X_1,\dots,X_n$,
$$
\hat\theta_n:=\hat\theta(X_1,\dots,X_n).
$$

Since $\hat\theta_n$ is a function of the random variables $X_1,\dots,X_n$, the estimator $\hat\theta_n$ is itself a **random variable**. 

The observed data 
$$
X_{1,obs},\dots,X_{n,obs}
$$ 
is assumed to be a certain realization of the random sample 
$$
X_1,\dots,X_n.
$$ 
The corresponding **realization** of the estimator is called an **estimate** of $\theta$
$$
\hat\theta_{n,obs}=\hat\theta(X_{1,obs},\dots,X_{n,obs}).
$$

**Examples:**

* The sample mean as an estimator of the population mean $E(X_i) =\theta:$
$$
\hat\theta_n=\bar{X}_n=\frac{1}{n}\sum_{i=1}^nX_i
$$ 
For given data, we observe the realization
$$
\hat\theta_{n,obs}=\bar{X}_{n,obs}=\frac{1}{n}\sum_{i=1}^nX_{i,obs}
$$ 

* The sample variance as an estimator of the population variance $Var(X_i) =\theta:$
$$
\hat\theta_n=s_{UB}^2=\frac{1}{n-1}\sum_{i=1}^n\left(X_i - \bar{X}_n\right)^2
$$ 
For given data, we observe the realization
$$
\hat\theta_{n,obs}=s_{UB,obs}^2=\frac{1}{n-1}\sum_{i=1}^n\left(X_{i,obs} - \bar{X}_{n,obs}\right)^2
$$ 


::: {.callout-note}
Often we do not use a distinguishing notation, but denote both the estimator $\hat\theta_{n}$ and its realization $\hat\theta_{n,obs}$ as $\hat\theta_{n}$. This ambiguity is often convenient since both points of views can make sense in a given context. Sometimes, also the subscript $n$ in $\hat\theta_{n}$ is dropped and one simply writes $\hat\theta.$
:::



## Assessing the Quality of Estimators 

Any reasonable estimator $\hat\theta_n$ should be able to approximate the (usually unknown) parameter value $\theta$,
$$
\left(\text{random quantity}\right)\quad\hat\theta_n\approx\theta\quad\left(\text{deterministic parameter}\right),
$$
and the approximation should get better as the sample size increases, i.e. as $n\to\infty$. 

<!-- 
The simulation results shown in @fig-ecdf and @fig-histdensplots show this desired behavior for the case of $\hat{\theta}_n=\bar{X}_n.$ 

To check (via MC-Simulations) the quality of an estimator, one can look at the total distribution or density function of $\hat{\theta}_n$; as done in @fig-ecdf and @fig-histdensplots. However, it is often more convenient to consider only the most relevant features of the distribution of an estimator.  
-->


Statisticians/econometricians use different metrics to assess the quality of an estimator $\hat\theta_n$. The most prominent metrics are:

* bias of an estimator $\hat{\theta}_n$
* variance and standard error of an estimator $\hat{\theta}_n$
* mean squared error (mse) of an estimator $\hat{\theta}_n$


::: {.callout icon=false}
##
::: {#def-bias}
## Bias of $\hat\theta_n$

The **bias** of an estimator $\hat\theta_n$ is defined as

$$
\operatorname{Bias}\left(\hat\theta_n\right) = E\left(\hat\theta_n\right) - \theta.
$$
:::
:::

If an estimator $\hat\theta_n$ has no bias, i.e. if 
$$
\operatorname{Bias}\left(\hat\theta_n\right)=E\left(\hat\theta_n\right) - \theta=0
$$ 
**for all** $\theta\in\mathbb{R}$ and all sample sizes $n,$ we call it an **unbiased estimator**. 

Many modern estimators are *not* unbiased. However, every estimator should be at least **asymptotically unbiased**, i.e.
$$
\lim_{n\to\infty}\operatorname{Bias}\left(\hat\theta_n\right)=0
$$ 
**for all** $\theta\in\mathbb{R}.$


We would like to have estimators $\hat\theta_n$ with a small (or zero) bias. 

If the bias of an estimator $\hat\theta_n$ is small (or zero), we know that the distribution of the estimator $\hat\theta_n$ is roughly (or exactly) centered around the true (usually unknown) parameter $\theta.$ 


However, also unbiased estimators $\hat{\theta}_n$ may still vary a lot around the parameter $\theta$ to be estimated. Therefore, is is also important to assess the variance (or the standard deviation) of the estimator. 


::: {.callout icon=false}
##
::: {#def-var}

## Variance and Standard Error of $\hat\theta_n$

The **variance** of an estimator $\hat\theta_n$ is defined equivalently to the variance of any other random variable

$$
Var\left(\hat\theta_n\right) = E\left[\left(\hat\theta_n - E(\hat\theta_n)\right)^2\right].
$$
The square root of the variance (i.e. the standard deviation) of an estimator $\hat\theta_n$ is called **standard error** of $\hat\theta_n$, 
$$
\operatorname{SE}\left(\hat\theta_n\right) = \sqrt{Var\left(\hat\theta_n\right)}.
$$
:::
:::

We would like to have estimators with a small as possible variance/standard error. 

The variance (and thus also standard error) should decline as the sample size increases, such that 
$$
\lim_{n\to\infty}Var\left(\hat\theta_n\right)=0.
$$


To **combine bias and variance** into one metric, one typically uses the **Mean Squared Error (MSE)** of an estimator $\hat\theta_n.$


::: {.callout icon=false}
##
::: {#def-mse}

## Mean Squared Error of $\hat\theta_n$

The **mean squared error** of an estimator $\hat\theta_n$ is defined as

$$
\operatorname{MSE}\left(\hat\theta_n\right) =  E\left[\left(\hat\theta_n - \theta\right)^2\right].
$$
:::
:::

We would like to have estimators with a small as possible mean squared error, and the mean squared error should decline as the sample size increases, such that 
$$
\lim_{n\to\infty}\operatorname{MSE}\left(\hat\theta_n\right)=0.
$$

The following holds true:

* The mean squared error equals the sum of the squared bias and the variance: 

$$
\operatorname{MSE}\left(\hat\theta_n\right) = \left(\operatorname{Bias}\left(\hat\theta_n\right)\right)^2 +  Var\left(\hat\theta_n\right) 
$$

* For unbiased estimators (i.e. $E(\hat\theta_n)=\theta$) the mean squared error equals the variance, i.e.

$$
\underbrace{E\left[\left(\hat\theta_n - \theta\right)^2\right]}_{\operatorname{MSE}\left(\hat\theta_n\right)} = \underbrace{E\left[\left(\hat\theta_n - E\left(\hat\theta_n\right)\right)^2\right]}_{ Var\left(\hat\theta_n\right)} 
$$


::: {.callout}

# Monte Carlo (MC) Simulations 

</br>

Unfortunately, it is often difficult to derive the above assessment metrics for given sample sizes $n$ and given data distributions $F_X,$ since for given $n$ we typically do not know the distribution of the estimator $\hat\theta_n$ and thus also not its bias and variance. Monte Carlo simulations allow us to solve this issue.
::: 


## Approximating Bias, Variance, and MSE using MC Simulations 

Any of the the above assessment metrics requires us to compute **means**, i.e. $E(\cdot)$, of random variables: 

* For the $\operatorname{Bias}\left(\hat\theta_n\right)$ we need to compute $E\left(\hat\theta_n\right)-\theta$

* For the $Var\left(\hat\theta_n\right)$ we need to compute $E\left[\left(\hat\theta_n - E(\hat\theta_n)\right)^2\right]$.

* For the $\operatorname{MSE}\left(\hat\theta_n\right)$ we need to compute $E\left[\left(\hat\theta_n - \theta\right)^2\right]$.


We can use a Monte Carlo (MC) simulation to approximate these means by **sample means** using the **law of large numbers** which states that a sample mean over iid random variables is able to approximate the population mean of these random variables as the number of random variables to average over gets large (see @thm-SLLN1 in @sec-lsinf).


The core part of a MC-simulation generates a large number of $B$ (e.g. $B=10,000$) many realizations of $\hat{\theta}_n$ for 

* a given sample size $n$ and 
* a given data distribution $F_X.$


::: {.callout}
## Core Part of the MC-Simulation Algorithm:

</br>

**1. Step:** Use a (pseudo-)random number generator to draw a realization of the random sample $\{X_1,\dots,X_n\}$ for a given distribution $F_X$ and a given sample size $n:$ 
\begin{align*}
&(X_{1,j,obs},\dots,X_{n,j,obs})\\
%&(X_{1,2,obs},\dots,X_{n,2,obs})\\
%& \hspace{2cm}\vdots \\
%&(X_{1,B,obs},\dots,X_{n,B,obs})
\end{align*}

**2. Step:** Compute the corresponding realization of $\hat{\theta}_n:$ 
\begin{align*}
\hat\theta_{n,j,obs} &= \hat\theta(X_{1,j,obs},\dots,X_{n,j,obs})\\
%\hat\theta_{n,2,obs} &= \hat\theta(X_{1,2,obs},\dots,X_{n,2,obs})\\
%&\vdots\\
%\hat\theta_{n,B,obs} &= \hat\theta(X_{1,B,obs},\dots,X_{n,B,obs})
\end{align*} 

**Repeat** the two steps above for $j=1,\dots,B$ with $B$ being a large number (e.g. $B=10,000$) to generate $B$ independent realizations of $\hat\theta_n$
\begin{align*}
\hat{\theta}_{n,1,obs} &= \hat\theta(X_{1,1,obs},\dots,X_{n,1,obs})\\
\hat{\theta}_{n,2,obs} &= \hat\theta(X_{1,2,obs},\dots,X_{n,2,obs})\\
&\;\vdots\\
\hat{\theta}_{n,B,obs} &= \hat\theta(X_{1,B,obs},\dots,X_{n,B,obs})
\end{align*} 
:::


We can use the above core part of the MC-Simulation algorithm to generate $B$ many realizations of the estimator $\hat{\theta}_n$ 
$$
\hat{\theta}_{n,1,obs},\dots,\hat{\theta}_{n,B,obs}
$$ 
which allow us to compute the following MC-approximations based on sample means over the generated $B$ realizations of $\hat\theta_n$:

* The bias of $\operatorname{Bias}\left(\hat\theta_n\right)=E\left(\hat\theta_n\right)-\theta$ can be approximated by 

$$
\widehat{\operatorname{Bias}}_{MC}\left(\hat\theta_n\right) = \left(\frac{1}{B}\sum_{b=1}^B \hat\theta_{n,b,obs}\right) - \theta 
$$

* The variance $Var\left(\hat\theta_n\right)=E\left[\left(\hat\theta_n - E(\hat\theta_n)\right)^2\right]$ can be approximated by
$$
\widehat{Var}_{MC}\left(\hat\theta_n\right) = \frac{1}{B}\sum_{b=1}^B \left(\hat\theta_{n,b,obs} - \left(\frac{1}{B}\sum_{b=1}^B \hat\theta_{n,b,obs}\right)\right)^2 
$$

* The mean squared error $\operatorname{MSE}\left(\hat\theta_n\right)=E\left[\left(\hat\theta_n - \theta\right)^2\right]$ can be approximated by 
$$
\widehat{\operatorname{MSE}}_{MC}\left(\hat\theta_n\right) = \frac{1}{B}\sum_{b=1}^B \left(\hat\theta_{n,b,obs} - \theta\right)^2 
$$

::: {.callout-note}

By the law of large numbers (see @thm-SLLN1 in @sec-lsinf) these approximations get arbitrarily precise as $B \to \infty,$ i.e.
$$
\begin{align*}
\widehat{\operatorname{Bias}}_{MC}\left(\hat\theta_n\right)&\to_p
\operatorname{Bias}\left(\hat\theta_n\right)\quad\text{as}\quad B\to\infty\\[2ex]
\widehat{Var}_{MC}\left(\hat\theta_n\right)&\to_p
Var\left(\hat\theta_n\right)\quad\text{as}\quad B\to\infty\\[2ex]
\widehat{\operatorname{MSE}}_{MC}\left(\hat\theta_n\right)&\to_p
\operatorname{MSE}\left(\hat\theta_n\right)\quad\text{as}\quad B\to\infty
\end{align*} 
$$

So for large $B$ (e.g. $B=10,000$) we can consider 
$$
\widehat{\operatorname{Bias}}_{MC}(\hat\theta_n), 
\widehat{Var}_{MC}(\hat\theta_n),\text{ and }\;\;  
\widehat{\operatorname{MSE}}_{MC}(\hat\theta_n)
$$ 
as roughly equal to 
$$
\operatorname{Bias}(\hat\theta_n), 
Var(\hat\theta_n),\text{ and } 
\operatorname{MSE}(\hat\theta_n).
$$
:::




## Example: Sample Mean 

The following `R` code contains a Monte Carlo simulation with $B = 10000$ replications to approximate the bias, the variance, and the mean squared error for the sample mean 
$$
(\hat\theta_n=)\bar{X}_n=\frac{1}{n}\sum_{i=1}^nX_i
$$ 
Setup: 

* $X_i\overset{iid}{\sim}F_X$, $i=1,\dots,n$, with $F_X=\mathcal{N}(\mu,\sigma^2)$ 
* Mean $\mu=10$ 
* Variance $\sigma^2=5$ 
* Sample sizes $n\in\{5,15,50\}$ 
<!-- The following `R` codes generated `B = 10000` realizations of the estimator $\bar{X}_n$ for each sample size $n\in\{5,15,50\}$ and stores all these realizations in a $10000\times 3$ matrix `estimates_mat`: -->
```{r}
## Set seed for the random number generator to get reproducible results
set.seed(3)
## True parameter value ('theta' here 'mu')
mu            <- 10
## Number of Monte Carlo repetitions:
B             <- 10000
## Sequence of different sample sizes:
n_seq         <- c(5, 15, 50)

## Function that generates estimator realizations 
my_estimates_generator <- function(n){
  X_sample <- rnorm(n = n, mean = mu, sd = sqrt(5))
  ## compute the sample mean realization
  return(mean(X_sample))
}

estimates_mat <- cbind(
  replicate(B, my_estimates_generator(n = n_seq[1])),
  replicate(B, my_estimates_generator(n = n_seq[2])),
  replicate(B, my_estimates_generator(n = n_seq[3]))
)

## Bias of the sample mean for different sample sizes n
MC_Bias_n_seq <- apply(estimates_mat, 2, mean) - mu

## Variance of the sample mean for different sample sizes n
MC_Var_n_seq  <- apply(estimates_mat, 2, var)

## Mean squared error of the sample mean for different sample sizes n
MC_MSE_n_seq  <- apply(estimates_mat, 2, function(x){mean((x - mu)^2)})
```



@tbl-mcbvmse shows the Monte Carlo approximations for the bias, the variance, and the mean squared error of $\bar{X}_n.$

```{r, echo=FALSE}
#| label: tbl-mcbvmse
#| tbl-cap: Monte Carlo approximations for the true bias, true variance, and true mean squared error of sample mean.
suppressPackageStartupMessages(library("kableExtra"))
suppressPackageStartupMessages(library("tidyverse"))

MCResults <- tibble(
  "n"                  = n_seq,
  "Bias (MC-Sim) "     = round(MC_Bias_n_seq, 3),
  "Variance (MC-Sim)"  = round(MC_Var_n_seq,  2),
  "MSE (MC-Sim) "      = round(MC_MSE_n_seq,  2))
           
MCResults %>% kbl() %>%  kable_styling()
```


The Monte Carlo approximations in @tbl-mcbvmse indicate that:  

- The bias $\operatorname{Bias}(\bar{X}_n)$ is effectively zero for all sample sizes $n\in\{5,15,50\}$

- The mean squared error $\operatorname{MSE}(\bar{X}_n)$ is decreasing as the sample size $n$ gets larger. 

- The log-log plot in @fig-loglog suggests a convergence rate of $-1$ for $\operatorname{MSE}(\bar{X}_n)$ i.e.
$$
\begin{align*}
\operatorname{MSE}(\bar{X}_n) &= \texttt{Constant} \cdot n^{-1}\\[2ex]
\Leftrightarrow\log\left(\operatorname{MSE}(\bar{X}_n)\right) &= \log\left(\texttt{Constant}\right) -1 \cdot \log\left(n\right),
\end{align*}
$$
where $\texttt{Constant}$ depends on the data generating process and the estimator, but not on the sample size $n$. 

```{r}
#| label: fig-loglog
#| fig-cap: "Plotting the logarithm of the MSE-values against the logarithm of the sample size values $n$ allows to approximate the convergence rate of the MSE-values as the sample size increases by the slope of the graph." 
plot(y    = MC_MSE_n_seq,  
     x    = n_seq, 
     type = "o", 
     log  = "xy", 
     ylab = "MSE (log scale)",
     xlab = "n (log scale)")

## Slope: 
(log(0.1) - log(1)) / (log(50) - log(5))
```



<!-- 
## Deriving the Distribution of Estimators 

Usually, we do not know the distribution $F_X$ of the random sample $X_1,\dots,X_n$ and thus we neither know the value of $\theta$ nor the distribution of the estimator 
$$
\hat\theta_n=\hat\theta(X_1,\dots,X_n).
$$ 
This is the fundamental statistical problem that we need to overcome in statistical inference (estimating $\theta$, hypothesis testing about $\theta$, etc.).  


There are (roughly) three different possibilities to derive/approximate the distribution of an estimator $\hat\theta_n:$  

* **Option 1: Mathematical derivation using a *complete* distributional assumption on $F_X$.** Assuming a certain distribution $F_X$ for the random sample $X_1,\dots,X_n$ may allow us to derive mathematically the *exact* distribution of $\hat\theta_n$ for given sample sizes $n.$<br>
‚û°Ô∏è We consider this option in @sec-ssinf.
  * Pro: If the distributional assumption is correct, one has *exact* inference for each sample size $n$. 
  * Con: This option can fail miserably if the distributional assumption on $F_X$ is wrong. 
  * Con: This option is often only possible for rather simple distributions $F_X$ like the normal distribution. 



* **Option 2: Mathematical derivation using only an *incomplete* distributional assumption on $F_X$ and asymptotic statistics.** Large sample $(n\to\infty)$ approximations (i.e. laws of large numbers and central limit theorems) often allows us to derive the *approximate* distribution of $\hat\theta_n$ for large sample sizes $n.$<br>
‚û°Ô∏è We consider this option in @sec-lsinf. 
  * Pro: Only a few qualitative distributional assumptions are needed. (Typically: A random sample with finite variances.)  
  * Con: The derived asymptotic ($n\to\infty$) distribution is only exact for the practically impossible case where $n=\infty$ and thus can fail to approximate the exact distribution of $\hat\theta_n$ for given (finite) sample sizes $n$; particularly if $n$ is small. 


* **Option 3: Monte Carlo (MC) Simulations using a *complete* distributional assumption on $F_X$.** Assuming a certain distribution $F_X$ for the random sample $X_1,\dots,X_n$ we can approximate (with arbitrary precision) the *exact* distribution of $\hat\theta_n$ for given sample sizes $n;$ see the Algorithm "MC-Simulation". <br> 
‚û°Ô∏è We use this option to check the behavior of estimators under different scenarios for $F_X$ and $n$ throughout the rest of this script.<br>
  * Pro: Works for a basically every distribution $F_X$ and sample size $n.$  
  * Con: This option can fail miserably if the distributional assumption on $F_X$ is wrong.

**Algorithm "MC-Simulation":**<br>
**1. Step: Generate realizations of $\hat{\theta}_n$.** Use a (pseudo-)random number generator to draw a large number of $B$ (e.g. $B=10,000$) many realizations of the random sample $\{X_1,\dots,X_n\}$ for a given distribution $F_X$ and a given sample size $n:$ 
\begin{align*}
&(X_{1,1,obs},\dots,X_{n,1,obs})\\
&(X_{1,2,obs},\dots,X_{n,2,obs})\\
& \hspace{2cm}\vdots \\
&(X_{1,B,obs},\dots,X_{n,B,obs})
\end{align*}
Compute for each realization of the random sample a realization of $\hat{\theta}_n:$ 
\begin{align*}
\hat\theta_{n,1,obs} &= \hat\theta(X_{1,1,obs},\dots,X_{n,1,obs})\\
\hat\theta_{n,2,obs} &= \hat\theta(X_{1,2,obs},\dots,X_{n,2,obs})\\
&\vdots\\
\hat\theta_{n,B,obs} &= \hat\theta(X_{1,B,obs},\dots,X_{n,B,obs})
\end{align*} 
**2. Step: Approximate the distribution of $\hat{\theta}_n$ (or a features of it).**
Use the realizations $\hat\theta_{n,1,obs},\dots,\hat\theta_{n,B,obs}$ to approximate the exact distribution of $\hat\theta_n$ for a given $F_X$ and a give sample size $n.$<br>




::: {.callout-note}
Step 2 of the above algorithm works, since the empirical distribution function 
$$
\hat{F}_{\hat{\theta}_n,B}(x)=\frac{1}{B}\sum_{j=1}^BI_{(\hat\theta_{n,j} \leq x)}
$$
approximates the true (unknown) distribution function of $\hat{\theta}_n$
$$
F_{\hat{\theta}_n}(x)=P(\hat\theta_{n} \leq x)
$$
arbitrarily well as $B\to\infty.$ 

This hold true, since by the famous [Glivenko‚ÄìCantelli theorem](https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem)  
$$
\sup_x\left| \hat{F}_{\hat{\theta}_n,B}(x) - F_{\hat{\theta}_n}(x)\right|\to 0\quad\text{as}\quad B\to\infty.
$$
almost surely as $B\to\infty.$


Instead of approximating the whole distribution of $\hat{\theta}_n,$ we may only be interested in approximating specific features of the this distribution, such as: 

* the bias of $\hat{\theta}_n$
* the variance of $\hat{\theta}_n$
* the standard error of $\hat{\theta}_n$
* the mean squared error of $\hat{\theta}_n$
* etc.

**Note:** These features are simple functionals of $\hat{F}_{\hat{\theta}_n,B},$ and thus can also be approximated arbitrarily well as $B\to\infty.$
:::


### Example: Sample Mean $\bar{X}_n$ {#sec-ExampleSampleMean}

Let $\{X_1,\dots,X_n\}$ be an iid random sample  with 
$$
X_i\overset{iid}{\sim} F_X,
$$ 
where 

* $F_X$ is a normal distribution $\mathcal{N}(\mu, \sigma^2)$ with 
  * mean $(\theta=)\mu=10$ and 
  * variance $\sigma^2=5$. 

To estimate the (usually unknown) mean value $\mu=10,$ we use the sample mean estimator
$$
\bar{X}_n =  \frac{1}{n}\sum_{i=1}^n X_i
$$


We consider two sample sizes $n=5$ and $n=50.$

::: {.callout-note}

## Mathematical Derivation using the Distributional Assumptions

Here we have specified the distribution $F_X$ completely by setting $F_X=\mathcal{N}(\mu=10,\sigma^2=5).$ This is such a simple case, that we can actually use mathematical derivations to derive the distribution of $\bar{X}_n.$ (Often, this is not possible.)

Observe that since $X_i\overset{\text{iid}}{\sim}\mathcal{N}(\mu,\sigma^2),$ 
$$
\sum_{i=1}^nX_i\sim\mathcal{N}(n \mu, n \sigma^2).
$$
Multiplying by $\frac{1}{n}$ yields
\begin{align*}
\frac{1}{n}\sum_{i=1}^nX_i = \bar{X}_n 
&\sim\mathcal{N}\left(\frac{1}{n}n \mu, \frac{1}{n^2}n \sigma^2\right)\\[2ex]
\bar{X}_n &\sim\mathcal{N}\left( \mu, \frac{1}{n} \sigma^2\right).
\end{align*}

**Summing up:** If $X_i\overset{iid}{\sim}\mathcal{N}(\mu,\sigma^2),$ 
then the exact (exact for each $n$) distribution of $\bar{X}_n$ is given by 
$$
\bar{X}_n\sim\mathcal{N}\left( \mu, \frac{1}{n} \sigma^2\right).
$$

* For $\mu=10,$ $\sigma=5,$ $n=5$:
$$
\bar{X}_n\sim\mathcal{N}\left(10, 1\right).
$$
* For $\mu=10,$ $\sigma=5,$ $n=50$:
$$
\bar{X}_n\sim\mathcal{N}\left(10, 0.1\right).
$$

‚ö†Ô∏è Unfortunately, such a mathematical derivation works only for very simple estimators and only for simple (and completely specified) distributions $F_X.$

ü§ì But for this special case, we can now check, whether a Monte Carlo simulation is able to approximate the distribution of $\bar{X}_n\sim\mathcal{N}\left( \mu, \frac{1}{n} \sigma^2\right).$
:::



Next, we use a Monte Carlo simulation to approximate the distribution of the estimator 
$$
\bar{X}_n =  \frac{1}{n}\sum_{i=1}^n X_i.
$$


The following `R` code generates $B=10,000$ many realizations of the random sample $X_i\overset{iid}{\sim}\mathcal{N}(\mu,\sigma^2)$ with $\mu=10$ and $\sigma^2=5.$

\begin{align*}
&(X_{1,1,obs},\dots,X_{n,1,obs})\\
&(X_{1,2,obs},\dots,X_{n,2,obs})\\
& \hspace{2cm}\vdots \\
&(X_{1,B,obs},\dots,X_{n,B,obs})
\end{align*}
leading to $B$ many realizations of the estimator $\bar{X}_n$ 
\begin{align*}
\bar{X}_{n,1,obs} &= \hat\theta(X_{1,1,obs},\dots,X_{n,1,obs})\\
\bar{X}_{n,2,obs} &= \hat\theta(X_{1,2,obs},\dots,X_{n,2,obs})\\
&\vdots\\
\bar{X}_{n,B,obs} &= \hat\theta(X_{1,B,obs},\dots,X_{n,B,obs})
\end{align*} 
These realizations are then used to approximate the true distribution of $\bar{X}_n.$

```{r}
## True parameter value 
mu            <- 10
## Number of Monte Carlo repetitions:
B             <- 10000
## Sequence of different sample sizes:
n_seq         <- c(5, 50)


## #############################################
## 1st Coding-Possibility: Using a for() loop ##
## #############################################

## Set seed for the random number generator to get reproducible results
set.seed(3)

# Container for the generated estimates:
estimates_mat <- matrix(NA, nrow = B, ncol = length(n_seq))

for(j in 1:length(n_seq)){
  ## select the sample size
  n <- n_seq[j]
  for(b in 1:B){
    ## generate realization of the random sample 
    X_sample <- rnorm(n = n, mean = mu, sd = sqrt(5))
    ## compute the sample mean and safe it
    estimates_mat[b,j] <- mean(X_sample)
  }
}

## ############################################
## 2nd Coding-Possibility: Using replicate() ##
## ############################################

## Set seed for the random number generator to get reproducible results
set.seed(3)

## Function that generates estimator realizations 
my_estimates_generator <- function(n){
  X_sample <- rnorm(n = n, mean = mu, sd = sqrt(5))
  ## compute the sample mean realization
  return(mean(X_sample))
}

estimates_mat <- cbind(
  replicate(B, my_estimates_generator(n = n_seq[1])),
  replicate(B, my_estimates_generator(n = n_seq[2]))
)
```

Based on the $B=10,000$ realizations of the estimator $\bar{X}_n$, we can compute the empirical density functions $\hat{F}_{X_n,B}$ (see @fig-ecdf) and histograms (see @fig-histdensplots) to get an idea about the true distribution of $\bar{X}_n.$ 

In this simple case, we also know the theoretical distribution function $F_{X_n}$ and density function which allows us to check the simulation results (see @fig-ecdf and @fig-histdensplots). 
```{r, fig.align="center", fig.cap=""}
#| label: fig-ecdf
#| fig-cap: "Empirical distribution functions $\\hat{F}_{X_{n},B}$ computed from the $(B=10000)$ simulated realizations $\\bar{X}_{n,1,obs},\\dots,\\bar{X}_{n,B,obs}$ and the theoretical distribution functions for $n=5,50.$ The empirical and the theoretical distribution functions match perfectly." 
library(scales)
par(mfrow=c(1,2))
plot(ecdf(estimates_mat[,1]), main="n=5", ylab="", xlab="", col = "black", xlim = range(estimates_mat[,1]), ylim=c(0,1.25))
mtext(expression(mu==10), side = 1, at = 10, line = 2.5)
curve(pnorm(x, mean=10, sd=sqrt(5/5)), add=TRUE, col="red", lty = 3, lwd=4)
legend("topleft", legend = c("Empir. Distr.-Function", "True Distr.-Function"), col = c("black", "red"), lty = c(1, 3), lwd = c(1.3, 2), bty = "n")
##      
plot(ecdf(estimates_mat[,2]), main="n=50", ylab="", xlab="", col = "black", xlim = range(estimates_mat[,1]), ylim=c(0,1.25))
mtext(expression(mu==10), side = 1, at = 10, line = 2.5)
curve(pnorm(x, mean=10, sd=sqrt(5/50)), add=TRUE, col="red", lty = 3, lwd=4)
legend("topleft", legend = c("Empir. Distr.-Function", "True Distr.-Function"), col = c("black", "red"), lty = c(1, 3), lwd = c(1.3, 2), bty = "n")
```


```{r, fig.align="center", fig.cap=""}
#| label: fig-histdensplots
#| fig-cap: "Histrograms of $(B=10000)$ simulated realizations $\\bar{X}_{n,1,obs},\\dots,\\bar{X}_{n,B,obs}$ and true density functions for $n=5,50.$ The empirical (simulation based) histrograms and the theoretical density functions match perfectly." 
library(scales)
par(mfrow=c(1,2))
hist(estimates_mat[,1], main="n=5", xlab="",  xlim = range(estimates_mat[,1]), prob = TRUE, ylim = c(0, 1.5))
mtext(expression(mu==10), side = 1, at = 10, line = 2.5)
curve(dnorm(x, mean=10, sd=sqrt(5/5)), add=TRUE, lty = 3, lwd=4, col="red")
legend("topleft", legend = c("Histogram","True Density"), 
      col = c("black", "red"), lty = c(1,3), lwd = c(1.3, 4), bty = "n")
##
hist(estimates_mat[,2], main="n=50", xlab="",  xlim = range(estimates_mat[,1]), prob = TRUE, ylim = c(0, 1.5))
mtext(expression(mu==10), side = 1, at = 10, line = 2.5)
curve(dnorm(x, mean=10, sd=sqrt(5/50)), add=TRUE, lty = 3, lwd=4, col="red")
legend("topleft", legend = c("Histogram","True Density"), 
      col = c("black", "red"), lty = c(1,3), lwd = c(1.3, 4), bty = "n")

```

Observations in @fig-ecdf and @fig-histdensplots: The empirical distribution functions and the histograms based on the simulated realizations 
$$
\bar{X}_{n,1,obs},\dots,\bar{X}_{n,B,obs}
$$ 
mach their theoretical counterparts almost perfectly since we chose a sufficient large number of $B=10000$ simulations. 

::: {.callout-tip}

## Take away message

We can use Monte Carlo simulations to approximate the *exact* distribution of an estimator $\hat{\theta}_n$ for given distributions $F_X$ of the underlying random sample. These approximations become arbitrarily precise as $B\to\infty$. 
:::


-->


<!-- * At least on average, the estimates $\bar{X}_n$ are close to the target parameter $\mu=10$ for each sample size $n\in\{5,15,50\}$. This feature of the estimator's distribution is summarized by the **bias** (see next section) of an estimator.

* As the sample size increases, the distributions of the estimators $\bar{X}_n$ concentrate around the target parameter $\mu=10$. This feature of the estimator's distribution is summarized by the **mean squared error** (see next section) of an estimator.

Thus here the quality of the estimator $\bar{X}_n$ gets better as $n$ gets large. To describe the quality of estimators more compactly, statisticians/econometricians use specific metrics like bias, variance and the mean squared error of the distribution of a estimator $\hat\theta$. -->






## Exercises


* [Exercises for Chapter 4](https://www.dropbox.com/scl/fi/w1nj9wnpj15r53opudhns/Ch4_Exercises1.pdf?rlkey=4bk9h4rpg4mkizi46s2r11okv&st=0dawfy6m&dl=0)

<!-- 
* [Exercises for Chapter 4 with Solutions](https://www.dropbox.com/scl/fi/jssefvtifo6wczxaxr250/Ch4_Exercises_with_Solutions1.pdf?rlkey=wcineb1vy4vpqxy8989qplexr&st=fp5sf04v&dl=0)
 -->





<!-- ```{r, fig.align="center"} -->
<!-- par(mfrow=c(1,3)) -->
<!-- plot(x = n_seq, y = Bias_n_seq, type = "b", ylim = c(-0.2, 0.2),  -->
<!--      main="Bias", xlab = "n", ylab = "") -->
<!-- abline(h = 0) -->
<!-- plot(x = n_seq, y = Var_n_seq, type = "b", ylim = c(0, 1.5), -->
<!--      main="Variance", xlab = "n", ylab = "") -->
<!-- abline(h = 0) -->
<!-- plot(x = n_seq, y = MSE_n_seq, type = "b", ylim = c(0, 1.5), -->
<!--      main="MSE", xlab = "n", ylab = "") -->
<!-- abline(h = 0) -->
<!-- ``` -->




<!-- like, for instance, the arithmetic mean   -->
<!-- $$ -->
<!-- \bar{X}_n=\frac{1}{n}\sum_{i=1}^nX_i -->
<!-- $$ -->
<!-- as an estimator of the population mean value $\mu$.  -->

