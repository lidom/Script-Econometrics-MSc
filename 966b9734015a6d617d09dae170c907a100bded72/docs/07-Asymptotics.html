<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>7&nbsp; Large Sample Inference – Basis Module Econometrics (M.Sc.)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./06-Small-Sample-Inference.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./07-Asymptotics.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Large Sample Inference</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none"></a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Basis Module Econometrics (M.Sc.)</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Organization of the Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-Introduction-to-R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to R</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-Probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-Matrix-Algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-Monte-Carlo-Simulations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Estimation Theory and Monte Carlo Simulations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-Multiple-Linear-Regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-Small-Sample-Inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Small Sample Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-Asymptotics.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Large Sample Inference</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#tools-for-asymptotic-statistics" id="toc-tools-for-asymptotic-statistics" class="nav-link active" data-scroll-target="#tools-for-asymptotic-statistics"><span class="header-section-number">7.1</span> Tools for Asymptotic Statistics</a>
  <ul class="collapse">
<li><a href="#modes-of-convergence" id="toc-modes-of-convergence" class="nav-link" data-scroll-target="#modes-of-convergence"><span class="header-section-number">7.1.1</span> Modes of Convergence</a></li>
  <li><a href="#four-important-modes-of-convergence" id="toc-four-important-modes-of-convergence" class="nav-link" data-scroll-target="#four-important-modes-of-convergence">Four Important Modes of Convergence</a></li>
  <li><a href="#relations-among-modes-of-convergence" id="toc-relations-among-modes-of-convergence" class="nav-link" data-scroll-target="#relations-among-modes-of-convergence">Relations among Modes of Convergence</a></li>
  <li><a href="#continuous-mapping-theorem-cmt" id="toc-continuous-mapping-theorem-cmt" class="nav-link" data-scroll-target="#continuous-mapping-theorem-cmt"><span class="header-section-number">7.1.2</span> Continuous Mapping Theorem (CMT)</a></li>
  <li><a href="#slutskys-theorem" id="toc-slutskys-theorem" class="nav-link" data-scroll-target="#slutskys-theorem"><span class="header-section-number">7.1.3</span> Slutsky’s Theorem</a></li>
  <li><a href="#law-of-large-numbers-and-central-limit-theorems" id="toc-law-of-large-numbers-and-central-limit-theorems" class="nav-link" data-scroll-target="#law-of-large-numbers-and-central-limit-theorems"><span class="header-section-number">7.1.4</span> Law of Large Numbers and Central Limit Theorems</a></li>
  <li><a href="#estimators-sequences-of-random-variables" id="toc-estimators-sequences-of-random-variables" class="nav-link" data-scroll-target="#estimators-sequences-of-random-variables"><span class="header-section-number">7.1.5</span> Estimators: Sequences of Random Variables</a></li>
  </ul>
</li>
  <li>
<a href="#asymptotics-under-the-classic-regression-model" id="toc-asymptotics-under-the-classic-regression-model" class="nav-link" data-scroll-target="#asymptotics-under-the-classic-regression-model"><span class="header-section-number">7.2</span> Asymptotics under the Classic Regression Model</a>
  <ul class="collapse">
<li><a href="#sec-CaseHetero" id="toc-sec-CaseHetero" class="nav-link" data-scroll-target="#sec-CaseHetero"><span class="header-section-number">7.2.1</span> The Case of Heteroskedasticity</a></li>
  <li><a href="#robust-inference" id="toc-robust-inference" class="nav-link" data-scroll-target="#robust-inference"><span class="header-section-number">7.2.2</span> Robust Inference</a></li>
  </ul>
</li>
  <li>
<a href="#sec-MCLS" id="toc-sec-MCLS" class="nav-link" data-scroll-target="#sec-MCLS"><span class="header-section-number">7.3</span> Monte Carlo Simulations</a>
  <ul class="collapse">
<li><a href="#check-distribution-of-hatbeta_n" id="toc-check-distribution-of-hatbeta_n" class="nav-link" data-scroll-target="#check-distribution-of-hatbeta_n"><span class="header-section-number">7.3.1</span> Check: Distribution of <span class="math inline">\(\hat\beta_n\)</span></a></li>
  <li><a href="#check-testing-multiple-parameters" id="toc-check-testing-multiple-parameters" class="nav-link" data-scroll-target="#check-testing-multiple-parameters"><span class="header-section-number">7.3.2</span> Check: Testing Multiple Parameters</a></li>
  <li><a href="#check-testing-single-parameters" id="toc-check-testing-single-parameters" class="nav-link" data-scroll-target="#check-testing-single-parameters"><span class="header-section-number">7.3.3</span> Check: Testing Single Parameters</a></li>
  </ul>
</li>
  <li><a href="#sec-RDLSInf" id="toc-sec-RDLSInf" class="nav-link" data-scroll-target="#sec-RDLSInf"><span class="header-section-number">7.4</span> Real Data Example</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">7.5</span> Exercises</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-lsinf" class="quarto-section-identifier"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Large Sample Inference</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p>The content of this chapter is very much inspired by Chapter 2 of the textbook of <span class="citation" data-cites="Hayashi2000">Hayashi (<a href="#ref-Hayashi2000" role="doc-biblioref">2000</a>)</span>.</p>
<section id="tools-for-asymptotic-statistics" class="level2" data-number="7.1"><h2 data-number="7.1" class="anchored" data-anchor-id="tools-for-asymptotic-statistics">
<span class="header-section-number">7.1</span> Tools for Asymptotic Statistics</h2>
<p>Basically every modern econometric method is justified using the toolbox of <strong>asymptotic statistics</strong>. The following core concepts from asymptotic statistics will allow us to drop the restrictive normality assumption of <a href="06-Small-Sample-Inference.html" class="quarto-xref"><span>Chapter 6</span></a> and to introduce robust standard errors:</p>
<ul>
<li>Concepts on stochastic convergence</li>
<li>Continuous mapping theorem</li>
<li>Slutsky’s theorem</li>
<li>Law of large numbers</li>
<li>Central limit theorems</li>
<li>Cramér-Wold device</li>
</ul>
<section id="modes-of-convergence" class="level3" data-number="7.1.1"><h3 data-number="7.1.1" class="anchored" data-anchor-id="modes-of-convergence">
<span class="header-section-number">7.1.1</span> Modes of Convergence</h3>
<p>In the following we will discuss the four most important convergence concepts for sequences of random variables <span class="math display">\[
\{Z_n\}:=(Z_1,Z_2,\dots,Z_n),
\]</span> where <span class="math display">\[
Z_i,\quad i=1,\dots,n,
\]</span> is a uni-variate or multivariate random variable.</p>
<p>Non-random quantities (scalars, vectors or matrices) will be denoted by Greek letters such as <span class="math inline">\(\alpha\)</span>.</p>
<!-- Sequences of random vectors (or matrices) will be denoted by $\{\mathbf{z}_n\}$.  -->
<!-- % Vector-Convergence if and only if element-wise converegence:  -->
<!-- %https://www.statlect.com/asymptotic-theory/mean-square-convergence#:~:text=The%20concept%20of%20mean%2Dsquare,difference%20is%20on%20average%20small. -->
</section><section id="four-important-modes-of-convergence" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="four-important-modes-of-convergence">Four Important Modes of Convergence</h3>
<p>Probably the most often used mode of convergence is <strong>convergence in probability</strong>. It states that a stochastic sequence <span class="math inline">\(\{Z_n\}\)</span> concentrates around its limit <span class="math inline">\(\alpha\)</span> such that deviations <span class="math inline">\(|Z_n-\epsilon|\)</span> larger than some small <span class="math inline">\(\epsilon &gt;0\)</span> occur eventually (as <span class="math inline">\(n\to\infty\)</span>) with probability zero.</p>
<p>In order to show that some stochastic sequence converges in probability to its limit, one typically uses a <em>(weak) law of large numbers</em>.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-conv_prop" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.1 (Convergence in Probability)</strong></span> </p>
<p>A sequence of univariate real valued random variables <span class="math inline">\(\{Z_n\}\)</span> <strong>converges in probability</strong> to a deterministic <span class="math inline">\(\alpha\in\mathbb{R}\)</span> if for any (arbitrarily small) <span class="math inline">\(\varepsilon&gt;0\)</span> <span class="math display">\[
\begin{align*}
  \lim_{n\to\infty} P\left(|Z_n-\alpha|&lt;\epsilon\right)&amp;=1\\
  \Leftrightarrow\quad \lim_{n\to\infty} P\left(|Z_n-\alpha|&gt;\epsilon\right)&amp;=0.
\end{align*}
\]</span> We write: <span class="math display">\[
\operatorname{plim}_{n\to\infty}Z_n=\alpha
\]</span> or more shortly <span class="math display">\[
Z_n\to_{p}\alpha,\quad\text{as}\quad n\to\infty.
\]</span></p>
<p><strong>Note:</strong></p>
<ol type="1">
<li>Convergence in probability of a sequence of random vectors or matrices <span class="math inline">\(\{Z_n\}\)</span> to a deterministic vector or matrix <span class="math inline">\(\alpha\)</span> requires <strong>element-wise</strong> convergence in probability.</li>
<li>The above definition can also be stated for random limits; i.e., for <span class="math inline">\(\alpha\)</span> being a random variable.</li>
</ol>
</div>
</div>
</div>
</div>
<p>A stricter mode of convergence is <strong>almost sure convergence</strong>. Almost sure convergence is (usually) rather hard to derive, since the probability is about an event concerning an infinite sequence. Fortunately, however, there are established <em>strong laws of large numbers</em> that we can use to argue that some stochastic sequence converges almost surely to its limit.</p>
<p>Every sequence that converges almost surely, also converges in probability.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-conv_as" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.2 (Almost Sure Convergence)</strong></span> </p>
<p>A sequence of univariate real valued random variables <span class="math inline">\(\{Z_n\}\)</span> <strong>converges almost surely</strong> to a deterministic <span class="math inline">\(\alpha\in\mathbb{R}\)</span> if <span class="math display">\[\begin{eqnarray*}
P\left(\lim_{n\to\infty}Z_n=\alpha\right)=1.
\end{eqnarray*}\]</span> We write: <span class="math display">\[
Z_n\to_{as}\alpha,\quad\text{as}\quad n\to\infty.
\]</span></p>
<p><strong>Note:</strong></p>
<ol type="1">
<li>Almost sure convergence of a sequence of random vectors (or matrices) <span class="math inline">\(\{Z_n\}\)</span> to a deterministic vector (or matrix) <span class="math inline">\(\alpha\)</span> requires <strong>element-wise</strong> almost sure convergence.</li>
<li>The above definition can also be stated for random limits; i.e., for <span class="math inline">\(\alpha\)</span> being a random variable.</li>
</ol>
</div>
</div>
</div>
</div>
<p><strong>Convergence in mean square</strong> is typically the most intuitive mode of convergence. If a stochastic sequence <span class="math inline">\(\{Z_n\}\)</span> converges to a certain limit <span class="math inline">\(\alpha,\)</span> then</p>
<ol type="1">
<li>the <strong>mean</strong> of the stochastic sequence <span class="math inline">\(E(Z_n)\)</span> must converge to the limit <span class="math inline">\(\alpha,\)</span> and</li>
<li>the <strong>variance</strong> of the stochastic sequence <span class="math inline">\(E(Z_n)\)</span> must converge to zero.</li>
</ol>
<p>Convergence in mean square is typically easy to show.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-conv_ms" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.3 (Convergence in Mean Square)</strong></span> </p>
<p>A sequence of univariate real valued random variables <span class="math inline">\(\{z_n\}\)</span> <strong>converges in mean square</strong> (or <strong>in quadratic mean</strong>) to a deterministic <span class="math inline">\(\alpha\in\mathbb{R}\)</span> if <span class="math display">\[
\begin{align*}
  \lim_{n\to\infty}E\left((Z_n-\alpha)^2\right)&amp;=0.
\end{align*}
\]</span> We write: <span class="math display">\[
Z_n\to_{ms}\alpha,\quad\text{as}\quad n\to\infty.
\]</span></p>
<p><strong>Note:</strong></p>
<ol type="1">
<li>Mean square convergence of a sequence of random vectors (or matrices) <span class="math inline">\(\{Z_n\}\)</span> to a deterministic vector (or matrix) <span class="math inline">\(\alpha\)</span> requires <em>element-wise</em> mean square convergence.</li>
<li>The above definition can also be stated for random limits; i.e., for <span class="math inline">\(\alpha\)</span> being a random variable.</li>
</ol>
</div>
</div>
</div>
</div>
<!-- **Convergence to a Random Variable:**The above presented definitions of convergence can be also applied to limits that are random variables. We say that a sequence of random vectors $\{\mathbf{z}_n\}$ converges to a random vector $\mathbf{z}$ and write $\mathbf{z}_n\to_{p}\mathbf{z}$ if the sequence $\{\mathbf{z}_n-\mathbf{z}\}$ converges to $\mathbf{0}$. Similarly, for $\mathbf{z}_n\to_{as}\mathbf{z}$ and $\mathbf{z}_n\to_{ms}\mathbf{z}$.  -->
<p><strong>Convergence in distribution</strong> is the weakest and at the same time most important mode of convergence.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-conv_distr" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.4 (Convergence in Distribution)</strong></span> &nbsp;</p>
<ul>
<li>Let <span class="math inline">\(F_n\)</span> denote the cumulative distribution function (cdf) of the univariate real valued random variable <span class="math inline">\(Z_n\in\mathbb{R},\)</span> and</li>
<li>let <span class="math inline">\(F\)</span> denote the cdf of the univariate real valued random variable <span class="math inline">\(Z\in\mathbb{R}.\)</span>
</li>
</ul>
<p>A sequence of univariate real valued random variables <span class="math inline">\(\{Z_n\}\)</span> <strong>converges in distribution</strong> to a the univariate real valued random variable <span class="math inline">\(Z\)</span> if <span class="math display">\[
\begin{align*}
  \lim_{n\to\infty}F_n(x)=F(x)
\end{align*}
\]</span> for all <span class="math inline">\(x\in\mathbb{R}\)</span> at which <span class="math inline">\(F(x)\)</span> is continuous.</p>
<p>We write: <span class="math display">\[
Z_n\to_{d}Z,\quad\text{as}\quad n\to\infty
\]</span> We call <span class="math inline">\(F\)</span> the <strong>asymptotic (or limit) distribution</strong> of <span class="math inline">\(Z_n\)</span>.</p>
</div>
</div>
</div>
</div>
<p>Remarks on <a href="#def-conv_distr" class="quarto-xref">Definition&nbsp;<span>7.4</span></a>:</p>
<ol type="1">
<li>Often you will see statements like <span class="math display">\[
Z_n\to_{d} \mathcal{N}(0,1)
\]</span> or <span class="math display">\[
Z_n\overset{a}{\sim}\mathcal{N}(0,1),
\]</span> which should be read as <span class="math display">\[
\lim_{n\to\infty}F_n(x) = \Phi(x)\quad\text{for all}\quad x\in\mathbb{R},
\]</span> where <span class="math inline">\(\Phi\)</span> denotes the distribution function of the standard normal distribution.</li>
<li>A stochastic sequence <span class="math inline">\(\{Z_n\}\)</span> can also convergence in distribution to a <strong>constant</strong> <span class="math inline">\(\alpha\)</span>. In this case <span class="math inline">\(\alpha\)</span> is treated as a <strong>degenerated random variable</strong> with cdf <span class="math display">\[
F_\alpha(x)=\left\{
  \begin{matrix}
  0&amp;\text{if}\;\;x   &lt;\alpha\\
  1&amp;\text{if}\;\;x\geq\alpha.
  \end{matrix}\right.
\]</span>
</li>
</ol>
<p>By contrast to all other modes of convergence, <a href="#def-conv_distr" class="quarto-xref">Definition&nbsp;<span>7.4</span></a> only addresses the univariate case <span class="math inline">\((Z_n\in\mathbb{R}).\)</span> The reason for this is that <a href="#def-conv_distr" class="quarto-xref">Definition&nbsp;<span>7.4</span></a> cannot simply applied element-wise since this would ignore the possible dependencies between the different uni-variate random variables elements of a random vector.</p>
<p>To handle <strong>multivariate convergence in distribution</strong>, we need the following theorem known as the <strong>Cramér-Wold device</strong>.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-CramerWold" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.1 (Cramér-Wold)</strong></span> </p>
<p>Let <span class="math inline">\(Z_n\in\mathbb{R}^K\)</span> and <span class="math inline">\(Z\in\mathbb{R}^K\)</span> be <span class="math inline">\((K\times 1)\)</span>-dimensional random vectors, then <span class="math display">\[
Z_n\to_{d} Z\quad\text{if and only if}\quad\lambda'Z_n\to_{d}\lambda'Z
\]</span> for any <span class="math inline">\(\lambda\in\mathbb{R}^K\)</span>.</p>
</div>
</div>
</div>
</div>
<p>A proof of <a href="#thm-CramerWold" class="quarto-xref">Theorem&nbsp;<span>7.1</span></a> can be found, e.g., in <span class="citation" data-cites="Billingsley2008">Billingsley (<a href="#ref-Billingsley2008" role="doc-biblioref">2008</a>)</span> (p.&nbsp;383).</p>
<p>The Cramér-Wold theorem (<a href="#thm-CramerWold" class="quarto-xref">Theorem&nbsp;<span>7.1</span></a>) is needed since element-wise convergence in distribution generally does not imply convergence of the <em>joint</em> distribution of <span class="math inline">\(Z_n\)</span> to the <em>joint</em> distribution of <span class="math inline">\(Z\)</span>; except, if all elements in the random vectors <span class="math inline">\(Z_n\)</span> and <span class="math inline">\(Z\)</span> are independent from each other.</p>
<!-- **Remark:**Note that convergence in distribution, as the name suggests, only involves the distributions of the random variables. Thus, the random variables need not even be defined on the same probability space (that is, they need not be defined for the same random experiment), and indeed we don't even need the random variables at all. (But all this is just thought provoking \dots typically, we'll only consider cases with a common probability space.) -->
</section><section id="relations-among-modes-of-convergence" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="relations-among-modes-of-convergence">Relations among Modes of Convergence</h3>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="lem-Relations" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 7.1 (Relations among Modes of Convergence)</strong></span> </p>
<p>The following relationships hold:</p>
<ol type="i">
<li>Mean square convergence implies convergence in probability: <span class="math display">\[
Z_n\to_{ms}\alpha\quad \Rightarrow\quad Z_n\to_{p}\alpha
\]</span>
</li>
<li>Almost sure convergence implies convergence in probability: <span class="math display">\[
Z_n\to_{as}\alpha\quad \Rightarrow\quad Z_n\to_{p}\alpha
\]</span>
</li>
<li>Convergence in distribution to a constant is equivalent to convergence in probability to the same constant: <span class="math display">\[
Z_n\to_{d}\alpha\quad \Leftrightarrow\quad Z_n\to_{p}\alpha
\]</span>
</li>
</ol>
</div>
</div>
</div>
</div>
<p>Proofs of the result in <a href="#lem-Relations" class="quarto-xref">Lemma&nbsp;<span>7.1</span></a> can be found, e.g., here: <a href="https://www.statlect.com/asymptotic-theory/relations-among-modes-of-convergence">https://www.statlect.com/asymptotic-theory/relations-among-modes-of-convergence</a></p>
</section><section id="continuous-mapping-theorem-cmt" class="level3" data-number="7.1.2"><h3 data-number="7.1.2" class="anchored" data-anchor-id="continuous-mapping-theorem-cmt">
<span class="header-section-number">7.1.2</span> Continuous Mapping Theorem (CMT)</h3>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-Preserv" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.2 (Preservation of Convergence for Continuous Transformations (or “Continuous Mapping Theorem (CMT)”))</strong></span> </p>
<p>Let <span class="math inline">\(\{Z_n\}\)</span> denote a stochastic sequence of univariate (or multivariate) random variables and let <span class="math inline">\(f\)</span> denote a <em>continuous</em> function that does not depended on <span class="math inline">\(n\)</span>. Then <span class="math display">\[
\begin{align*}
Z_n\to_{p}  \alpha \quad &amp; \Rightarrow\quad f(Z_n)\to_{p} f(\alpha)\\[2ex]
Z_n\to_{as} \alpha \quad &amp; \Rightarrow\quad f(Z_n)\to_{as} f(\alpha)\\[2ex]
Z_n\to_{d}  \alpha \quad &amp; \Rightarrow\quad f(Z_n)\to_{d} f(\alpha)
\end{align*}
\]</span></p>
</div>
</div>
</div>
</div>
<p>Proofs of <a href="#thm-Preserv" class="quarto-xref">Theorem&nbsp;<span>7.2</span></a> can be found, e.g., in <span class="citation" data-cites="Vaart2000">Van der Vaart (<a href="#ref-Vaart2000" role="doc-biblioref">2000</a>)</span> (see Theorem 2.3) or here: <a href="https://www.statlect.com/asymptotic-theory/continuous-mapping-theorem">https://www.statlect.com/asymptotic-theory/continuous-mapping-theorem</a></p>
<p><strong>Note:</strong> The CMT does <em>not</em> hold for m.s.-convergence except for the case where <span class="math inline">\(f\)</span> is a linear function.</p>
<p><strong>Examples:</strong> As a consequence of the CMT (<a href="#thm-Preserv" class="quarto-xref">Theorem&nbsp;<span>7.2</span></a>) we have that the usual arithmetic operations preserve convergence in probability (and equivalently for almost sure convergence and convergence in distribution):</p>
<ul>
<li><p>If <span class="math inline">\(X_n\to_{p} \beta\)</span> and <span class="math inline">\(Y_n\to_{p} \gamma,\)</span> then <span class="math display">\[
X_n+Y_n\to_{p} \beta+\gamma
\]</span></p></li>
<li><p>If <span class="math inline">\(X_n\to_{p} \beta\)</span> and <span class="math inline">\(Y_n\to_{p} \gamma,\)</span> then <span class="math display">\[
X_n\cdot Y_n\to_{p} \beta\cdot\gamma
\]</span></p></li>
<li><p>If <span class="math inline">\(X_n\to_{p} \beta\)</span> and <span class="math inline">\(Y_n\to_{p} \gamma,\)</span> then <span class="math display">\[
X_n/Y_n\to_{p} \beta/\gamma,
\]</span> provided that <span class="math inline">\(\gamma\neq 0\)</span></p></li>
<li><p>If <span class="math inline">\(\frac{1}{n}\sum_{i=1}^nX_iX_i'=\frac{1}{n}X_n'X_n\to_{p} \Sigma_{X'X},\)</span> then <span class="math display">\[
\left(\frac{1}{n}X_n'X_n\right)^{-1}\to_{p} \Sigma_{X'X}^{-1},
\]</span> provided <span class="math inline">\(\Sigma_{X'X}\)</span> is a nonsingular/invertible matrix.</p></li>
</ul>
<!-- 
The first statement above is immediately seen by setting $\mathbf{z}_n=\left(x_n, y_n\right)'$, $\boldsymbol\alpha=\left(\beta, \gamma\right)'$, and $\mathbf{a}(\boldsymbol\alpha)=(1,1)\boldsymbol\alpha$, similarly for all others.  
--></section><section id="slutskys-theorem" class="level3" data-number="7.1.3"><h3 data-number="7.1.3" class="anchored" data-anchor-id="slutskys-theorem">
<span class="header-section-number">7.1.3</span> Slutsky’s Theorem</h3>
<p><strong>Slutsky’s Theorem</strong> is a collection of results concerned with combinations of</p>
<ul>
<li>convergence in probability and</li>
<li>convergence in distribution.</li>
</ul>
<p>These results are particularly important for the derivation of the asymptotic distribution of estimators.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-Slutsky" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.3 (Slutsky’s Theorem)</strong></span> </p>
<p>Let <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> denote sequences of random scalars or vectors and let <span class="math inline">\(A_n\)</span> denote a sequences of random matrices. Moreover, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(A\)</span> are deterministic limits of appropriate dimensions and <span class="math inline">\(X\)</span> is a <em>random</em> limit of appropriate dimension.</p>
<ul>
<li><p>If <span class="math inline">\(X_n\to_{d} X\quad\)</span> and <span class="math inline">\(\quad Y_n\to_{p} \alpha,\quad\)</span> then <span class="math display">\[
X_n+Y_n\to_{d} X + \alpha.
\]</span></p></li>
<li><p>If <span class="math inline">\(X_n\to_{d} X\quad\)</span> and <span class="math inline">\(\quad Y_n\to_{p} 0,\quad\)</span> then <span class="math display">\[
X_n'Y_n\to_{p} 0.
\]</span></p></li>
<li><p>If <span class="math inline">\(X_n\to_{d} X\quad\)</span> and <span class="math inline">\(\quad A_n\to_{p} A,\quad\)</span> then <span class="math display">\[
A_n X_n\to_{d} AX.
\]</span></p></li>
</ul>
<p>Above, we assume that <span class="math inline">\(X_n,\)</span> <span class="math inline">\(Y_n,\)</span> and <span class="math inline">\(A_n\)</span> are “conformable” (i.e., the matrix- and vector-dimensions fit to each other).</p>
</div>
</div>
</div>
</div>
<p>Proofs of <a href="#thm-Slutsky" class="quarto-xref">Theorem&nbsp;<span>7.3</span></a> can be found, e.g., in <span class="citation" data-cites="Vaart2000">Van der Vaart (<a href="#ref-Vaart2000" role="doc-biblioref">2000</a>)</span> (see Theorem 2.8) or here: <a href="https://www.statlect.com/asymptotic-theory/Slutsky-theorem">https://www.statlect.com/asymptotic-theory/Slutsky-theorem</a></p>
<p><strong>Remark:</strong> Sometimes, only the first two points in <a href="#thm-Slutsky" class="quarto-xref">Theorem&nbsp;<span>7.3</span></a> are called “Slutsky’s theorem.”</p>
<p>Important special case of <a href="#thm-Slutsky" class="quarto-xref">Theorem&nbsp;<span>7.3</span></a>:</p>
<p>Let <span class="math inline">\(\{X_n\}\)</span> and <span class="math inline">\(\{X_n\}\)</span> be sequences of real valued <span class="math inline">\((K\times 1)\)</span>-dimensional random vectors. If <span class="math display">\[
X_n\to_{d}\mathcal{N}_K(0,\Sigma)\quad\text{and}\quad A_n\to_{p} A
\]</span> then <span class="math display">\[
A_nX_n\to_{d}\mathcal{N}_K(0,A\Sigma A').
\]</span></p>
</section><section id="law-of-large-numbers-and-central-limit-theorems" class="level3" data-number="7.1.4"><h3 data-number="7.1.4" class="anchored" data-anchor-id="law-of-large-numbers-and-central-limit-theorems">
<span class="header-section-number">7.1.4</span> Law of Large Numbers and Central Limit Theorems</h3>
<p>So far, we discussed the definitions of the four most important convergence modes, their relations among each other, and basic theorems about functionals of stochastic sequences (CMT and Slutsky). Though, we still lack of tools that allow us to actually show that a stochastic sequence convergences (in some of the discussed modes) to some limit.</p>
<p>In the following we consider the stochastic sequences <span class="math display">\[
\bar{Z}_1,\bar{Z}_2,\dots,\bar{Z}_n,\quad\text{as}\quad n \to\infty,
\]</span> of sample means <span class="math display">\[
\bar{Z}_n:=n^{-1}\sum_{i=1}^nZ_i,
\]</span> where <span class="math inline">\(Z_i\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>, are (scalar, vector, or matrix-valued) <em>random variables</em>.</p>
<p><strong>Remember:</strong> The sample mean <span class="math inline">\(\bar{Z}_n\)</span> is an estimator of the deterministic population mean <span class="math inline">\(\mu.\)</span></p>
<p>Weak Law of Large Numbers (WLLNs), Strong LLNs (SLLNs), and Central Limit Theorems (CLTs) tell us conditions under which arithmetic means <span class="math display">\[
\bar{Z}_n=n^{-1}\sum_{i=1}^nZ_i
\]</span> converge in probability, almost surely, and in distribution, respectively:</p>
<ul>
<li><p>Weak LLN: <span class="math inline">\(\bar{Z}_n \to_{p}\mu\)</span></p></li>
<li><p>Strong LLN: <span class="math inline">\(\bar{Z}_n\to_{as}\mu\)</span></p></li>
<li><p>CLT: <span class="math inline">\(\sqrt{n}(\bar{Z}_n-\mu)\to_{d}\mathcal{N}(0,\sigma^2)\)</span></p></li>
</ul>
<p>In the following we introduce the most well-known versions of a WLLN, SLLN, and a CLT.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-WLLN1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.4 (Weak LLN (by Chebychev))</strong></span> </p>
<p>If</p>
<ul>
<li>
<span class="math inline">\(\lim_{n\to\infty} E(\bar{Z}_n)=\mu\)</span> and</li>
<li>
<span class="math inline">\(\lim_{n\to\infty}Var(\bar{Z}_n)=0,\)</span> then <span class="math display">\[
\bar{Z}_n\to_{p}\mu.
\]</span>
</li>
</ul>
</div>
</div>
</div>
</div>
<p>A proof of <a href="#thm-WLLN1" class="quarto-xref">Theorem&nbsp;<span>7.4</span></a> can be found, for instance, here: <a href="https://www.statlect.com/asymptotic-theory/law-of-large-numbers">https://www.statlect.com/asymptotic-theory/law-of-large-numbers</a></p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-SLLN1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.5 (Strong LLN (by Kolmogorov))</strong></span> </p>
<p>If <span class="math inline">\(\{Z_i\}\)</span> is an</p>
<ul>
<li>iid sequence with</li>
<li>
<span class="math inline">\(E(Z_i)=\mu,\)</span> then <span class="math display">\[
\bar{Z}_n\to_{as}\mu.
\]</span>
</li>
</ul>
</div>
</div>
</div>
</div>
<p>A proof of <a href="#thm-SLLN1" class="quarto-xref">Theorem&nbsp;<span>7.5</span></a> can be found, e.g., in <em>Linear Statistical Inference and Its Applications</em>, Rao (1973), pp.&nbsp;112-114.</p>
<!-- 
> **Interactive visualization of the Law of Large Numbers:** [http://shiny.webpopix.org/sia/LLN/](http://shiny.webpopix.org/sia/LLN/) 
-->
<p><strong>Note:</strong> The WLLN and the SLLN for <strong>random vectors</strong> follow from applying the the theorem separately for each element of the random vectors.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-CLT1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.6 (CLT (Lindeberg-Levy))</strong></span> </p>
<p>If <span class="math inline">\(\{Z_i\}\)</span> is</p>
<ul>
<li>an iid sequence with<br>
</li>
<li>
<span class="math inline">\(E(Z_i)=\mu\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and</li>
<li>
<span class="math inline">\(Var(Z_i)=\sigma^2\)</span> for all <span class="math inline">\(i=1,\dots,n,\)</span>
</li>
</ul>
<p>then <span class="math display">\[
\sqrt{n}(\bar{Z}_n-\mu)\to_{d} \mathcal{N}(0,\sigma^2),\quad\text{as}\quad n\to\infty
\]</span></p>
</div>
</div>
</div>
</div>
<p>A proof of <a href="#thm-CLT1" class="quarto-xref">Theorem&nbsp;<span>7.6</span></a> can be found, e.g., in <span class="citation" data-cites="Vaart2000">Van der Vaart (<a href="#ref-Vaart2000" role="doc-biblioref">2000</a>)</span> (see Theorem 2.17).</p>
<!-- 
> **Interactive visualization of the CLT:** [https://tuomonieminen.shinyapps.io/CLTdemo/](https://tuomonieminen.shinyapps.io/CLTdemo/)
 -->
<p>Using the Cramér-Wold device (<a href="#thm-CramerWold" class="quarto-xref">Theorem&nbsp;<span>7.1</span></a>), the Lindeberg-Levy CLT (<a href="#thm-CLT1" class="quarto-xref">Theorem&nbsp;<span>7.6</span></a>) can also be applied to <span class="math inline">\(K\)</span>-dimensional random vectors. To show that <span class="math display">\[
\sqrt{n}(\bar{Z}_n-\mu)\to_d\mathcal{N}_K(0,\Sigma)
\]</span> converges to the <strong>multivariate</strong>, <span class="math inline">\(K\)</span>-dimensional, normal distribution <span class="math inline">\(\mathcal{N}_K(0,\Sigma)\)</span> as <span class="math inline">\(n\to\infty\)</span>, we need to check whether <strong>for any</strong> <span class="math inline">\(\lambda\in\mathbb{R}^K\)</span>:</p>
<ul>
<li>the <strong>uni</strong>variate stochastic sequence <span class="math inline">\(\{\lambda'Z_i\}\)</span> is i.i.d.</li>
<li>with <span class="math inline">\(E(\lambda'Z_i)=\lambda'\mu\)</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> where <span class="math inline">\(E(Z_i)=\mu\in\mathbb{R}^K\)</span>, and</li>
<li>with <span class="math inline">\(Var(\lambda'Z_i)=\lambda'\Sigma\lambda\)</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> where <span class="math inline">\(Var(Z_i)=\Sigma\)</span> denotes the <span class="math inline">\((K\times K)\)</span> dimensional variance-covariance matrix.</li>
</ul>
<p>These points are fulfilled if the multivariate stochastic sequence <span class="math inline">\(\{Z_i\}\)</span> is an i.i.d. sequence with <span class="math inline">\(E(Z_i)=\mu\)</span> and <span class="math inline">\(Var(Z_i)=\Sigma.\)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The LLNs and the CLT are stated with respect to sequences of sample means <span class="math inline">\(\{\bar{Z}_n\}\)</span>; i.e., the simplest estimators you probably can think of. We will see, however, that this is all we need in order to analyze also more complicated estimators such as the OLS estimator.</p>
</div>
</div>
</section><section id="estimators-sequences-of-random-variables" class="level3" data-number="7.1.5"><h3 data-number="7.1.5" class="anchored" data-anchor-id="estimators-sequences-of-random-variables">
<span class="header-section-number">7.1.5</span> Estimators: Sequences of Random Variables</h3>
<p>The concepts introduced above readily apply to univariate or multivariate (<span class="math inline">\(K\)</span>-dimensional) estimators <span class="math display">\[
\hat\theta_n\equiv\hat\theta((X_1,Y_1),\dots,(X_n,Y_n))
\]</span> which are functions of the random sample with sample size <span class="math inline">\(n.\)</span></p>
<p>An increasing sample size <span class="math display">\[
n\to\infty
\]</span> makes an <strong>estimator</strong> <span class="math display">\[
\{\hat\theta_n\}
\]</span> nothing but a <strong>sequence of random variables</strong> converging (hopefully) to the correct limit; namely, to the parameter value <span class="math inline">\(\theta\)</span> we aim to estimate.</p>
<p>If an estimator <span class="math inline">\(\hat\theta_n\)</span> converges in probability to its limit <span class="math inline">\(\theta\)</span>, we call the estimator <strong>weakly consistent</strong> or simply <strong>consistent</strong>. If it converges almost surely to <span class="math inline">\(\theta,\)</span> we call the estimator <strong>strongly consistent</strong>.</p>
<!-- **Note:**Under the asymptotic perspective ($n\to\infty$) it's not necessary anymore to condition on $X$ since as $n\to\infty$ the stochastic influence of $X$ on  vanishes  -->
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-WConsist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.5 ((Weak) Consistency)</strong></span> </p>
<p>We say that an estimator <span class="math inline">\(\hat\theta_n\)</span> is <em>(weakly) consistent for <span class="math inline">\(\theta\)</span></em> if <span class="math display">\[
\hat\theta_n\to_{p}\theta,\quad\text{as}\quad n\to\infty.
\]</span></p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-SConsist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.6 (Strong Consistency)</strong></span> </p>
<p>We say that an estimator <span class="math inline">\(\hat\theta_n\)</span> is <em>strongly consistent for <span class="math inline">\(\theta\)</span></em> if <span class="math display">\[
\hat\theta_n\to_{as}\theta,\quad\text{as}\quad n\to\infty.
\]</span></p>
</div>
</div>
</div>
</div>
<p>A necessary requirement for weak and strong consistency is that the estimator is asymptotically unbiased.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-ABias" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.7 (Asymptotic Bias)</strong></span> </p>
<p>The asymptotic bias of an estimator <span class="math inline">\(\hat\theta_n\)</span> of some parameter <span class="math inline">\(\theta\)</span> is defined as <span class="math display">\[
\begin{align*}
\operatorname{ABias}(\hat\theta_n,\theta)
&amp;=\lim_{n\to\infty}\operatorname{Bias}(\hat\theta_n,\theta)\\
&amp;=\lim_{n\to\infty}E(\hat\theta_n)-\theta.
\end{align*}
\]</span> If <span class="math display">\[
\operatorname{ABias}(\hat\theta_n,\theta)=0,
\]</span> then <span class="math inline">\(\hat\theta_n\)</span> is called an <strong>asymptotically unbiased</strong>.</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-ANorm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.8 (Asymptotic Normality, Asymptotic Variance, <span class="math inline">\(\sqrt{n}\)</span>-Consistent)</strong></span> </p>
<p>An estimator <span class="math inline">\(\hat\theta_n\)</span> is called <strong>asymptotically normal</strong> if <span class="math display">\[
\sqrt{n}(\hat\theta_n-\theta)\to_{d} \mathcal{N}(0,\operatorname{AVar}(\hat\theta_n)),\quad\text{as}\quad n\to\infty,
\]</span> where <span class="math display">\[
\begin{align}
\operatorname{AVar}(\hat\theta_n)
%\sigma^2
&amp; = \lim_{n\to\infty}Var(\sqrt{n}(\hat\theta_n-\theta))\\
&amp; = \lim_{n\to\infty}n Var(\hat\theta_n-\theta)\\
&amp; = \lim_{n\to\infty}n Var(\hat\theta_n)\\
\end{align}
\]</span> is called the <strong>asymptotic variance</strong> of <span class="math inline">\(\hat\theta_n.\)</span></p>
<p>Due to the <span class="math inline">\(\sqrt{n}\)</span>-scaling, <span class="math inline">\(\theta_n\)</span> is called <strong><span class="math inline">\(\sqrt{n}\)</span>-consistent</strong>.</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-MSE" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.9 (Mean Squared Error (MSE))</strong></span> <span class="math display">\[
\operatorname{MSE}(\hat\theta_n) = E\left((\hat\theta_n - \theta)^2\right)
\]</span> is called the <strong>mean squared error (MSE)</strong> of the estimator <span class="math inline">\(\hat\theta_n.\)</span></p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we can show that the MSE of <span class="math inline">\(\hat\theta_n\)</span> converges to zero, <span class="math display">\[
\operatorname{MSE}(\hat\theta_n) \to 0,\quad n\to\infty,
\]</span> then we have shown that <span class="math inline">\(\hat\theta_n\)</span> converges to <span class="math inline">\(\theta\)</span> in the mean square sense <span class="math display">\[
\hat\theta_n  \to_{ms} \theta,\quad n\to\infty,
\]</span> which implies that <span class="math inline">\(\hat\theta_n\)</span> is weakly consistent, i.e. <span class="math display">\[
\hat\theta_n \to_{p} \theta,\quad n\to\infty.
\]</span></p>
</div>
</div>
<p>The MSE of <span class="math inline">\(\hat\theta_n\)</span> can be decomposed into the squared bias of <span class="math inline">\(\hat\theta_n\)</span> and the variance of <span class="math inline">\(\hat\theta_n:\)</span> <span class="math display">\[
\begin{align*}
\operatorname{MSE}(\hat\theta_n)
&amp;=E\Big[(\hat\theta_n-\theta)^2\Big]\\[2ex]
&amp;=E\Big[\Big(\overbrace{E(\hat\theta_n) - E(\hat\theta_n)}^{=0} + \hat\theta_n-\theta\Big)^2\Big]\\[2ex]
&amp;=E\left[\left((E(\hat\theta_n) -\theta) + (\hat\theta_n - E(\hat\theta_n))\right)^2\right]\\[2ex]
&amp;=E\Big[\left(E(\hat\theta_n) -\theta\right)^2 + \left(\hat\theta_n - E(\hat\theta_n)\right)^2\\[2ex]
&amp;\quad +2\left(E(\hat\theta_n) -\theta\right)\left(\hat\theta_n - E(\hat\theta_n)\right)\Big]\\[2ex]
&amp;=\left(E(\hat\theta_n) -\theta\right)^2 + E\Big[\left(\hat\theta_n - E(\hat\theta_n)\right)^2\Big]\\[2ex]
&amp;\quad +2\left(E(\hat\theta_n) -\theta\right)\overbrace{\Big(E(\hat\theta_n) - E(\hat\theta_n)\Big)}^{=0}\\[2ex]
%&amp;=\left(E(\hat\theta_n)-\theta\right)^2 + Var(\hat\theta_n)\\
&amp;=\left(\operatorname{Bias}(\hat\theta_n)\right)^2 + Var(\hat\theta_n).
\end{align*}
\]</span> Thus, to show that an estimator <span class="math inline">\(\hat\theta_n\)</span> converges in the mean square sense to <span class="math inline">\(\theta,\)</span> we need to show that:</p>
<ol type="1">
<li>The estimator is asymptotically unbiased<br><span class="math display">\[
\operatorname{Bias}(\hat\theta_n)\to 0,\quad n\to\infty
\]</span>
</li>
<li>The variance of the estimator converges to zero <span class="math display">\[
Var(\hat\theta_n)\to 0,\quad n\to\infty.
\]</span>
</li>
</ol></section></section><section id="asymptotics-under-the-classic-regression-model" class="level2" data-number="7.2"><h2 data-number="7.2" class="anchored" data-anchor-id="asymptotics-under-the-classic-regression-model">
<span class="header-section-number">7.2</span> Asymptotics under the Classic Regression Model</h2>
<p>Given the above introduced machinery, we can now proof that the OLS estimators <span class="math display">\[
\hat\beta_n=(X'X)^{-1}X'Y
\]</span> and <span class="math display">\[
s^2_{ub,n}=\frac{1}{n-K}\sum_{i=1}^n\hat\epsilon_i^2
\]</span> are both consistent, and that <span class="math inline">\(\hat\beta_n\)</span> is asymptotically normal distributed.</p>
<!-- applied to the classic regression model (defined by Assumptions 1-4 in @sec-MLR are both consistent, and that $\hat\beta_n$ is asymptotically normal distributed.  -->
<p>Using asymptotic statistics, allows us to drop the unrealistic normality and spherical errors assumption (Assumption 4<span class="math inline">\(^\ast\)</span>) of <a href="06-Small-Sample-Inference.html" class="quarto-xref"><span>Chapter 6</span></a>, but still use our inference tools (<span class="math inline">\(t\)</span>-tests, <span class="math inline">\(F\)</span>-tests) from <a href="06-Small-Sample-Inference.html" class="quarto-xref"><span>Chapter 6</span></a>; as long as the sample size <span class="math inline">\(n\)</span> is “large.”</p>
<!-- However, before we can formally state the asymptotic properties, we, first, need to adjust our "data generating process"assumption (Assumption 1) such that we can apply Kolmogorov's strong LLN and Lindeberg-Levy's CLT. Second, we need to adjust the rank assumption (Assumption 3), such that the full column rank of $X$ is guaranteed for the limiting case as $n\to\infty$, too. Assumptions 2 and 4 from @sec-MLR are assumed to hold (and do not need to be changed).  -->
<!-- **Assumption 1$^\ast$: Data Generating Process (for Asymptotics)** Assumption 1 of @sec-MLR applies, but *additionally* we assume that $(\varepsilon_i, X_i)\in\mathbb{R}^{K+1}$ (or equivalently $(Y_i,X_i)\in\mathbb{R}^{K+1}$) is jointly i.i.d. for all $i=1,\dots,n$, with existing and finite second moments for $X_i$ and fourth moments for $\varepsilon_i$.


**Note 1:** The fourth moment of $\varepsilon_i$ is actually only needed for @thm-Consistency_s1; for the rest two moments are sufficient.


**Note 2:** The above adjustment of Assumption 1 is far less restrictive than assuming that the error-terms $\varepsilon_i$ are i.i.d. normally distributed and independent from $X_i$ (as it's necessary for small sample inference in @sec-ssinf).  -->
<p>For the following, it will be useful to introduce some notation that allows us to consider the different parts of the OLS estimator <span class="math inline">\(\hat\beta_n\)</span> separately.<br><span class="math display">\[\begin{align*}
\hat\beta_n
&amp;=\left(X'X\right)^{-1}X'Y\\[2ex]
&amp;=\left(\frac{1}{n}X'X\right)^{-1}\frac{1}{n} X'Y\\[2ex]
&amp;=\;\;\;\;S_{X'X}^{-1}\;\;\;\;\frac{1}{n} X'Y,
\end{align*}\]</span> where <span class="math display">\[
\underset{(K\times K)}{S_{X'X}}=\frac{1}{n}X'X=\frac{1}{n}\sum_{i=1}^nX_iX_i',
\]</span> and where the mean of the <span class="math inline">\((K\times K)\)</span> matrix <span class="math inline">\(S_{X'X}\)</span> will be denoted as <span class="math display">\[\begin{align*}
\Sigma_{X'X}
&amp;=E\left(S_{X'X}\right)\\[2ex]
&amp;=E\left(\frac{1}{n}X'X\right)\\[2ex]
&amp;=E\left(\frac{1}{n}\sum_{i=1}^nX_iX_i'\right)\\[2ex]
&amp;=\frac{n}{n}E\left(X_iX_i'\right)\\[2ex]
&amp;=E\left(X_iX_i'\right).
\end{align*}\]</span> For the below results, we need to restate the full rank assumption (Assumption 3 of <a href="05-Multiple-Linear-Regression.html" class="quarto-xref"><span>Chapter 5</span></a>) with respect to <span class="math inline">\(\Sigma_{X'X}.\)</span></p>
<p><strong>Assumption 3<span class="math inline">\(^\ast\)</span>: (Population) Rank Condition</strong>  The <span class="math inline">\((K\times K)\)</span> matrix <span class="math display">\[
\Sigma_{X'X}=E(S_{X'X})
\]</span> <!-- $$ --> <!-- n^{-1}X'X=S_{X'X}\to_{p}\Sigma_{X'X}\quad\text{as}\quad n\to\infty --> <!-- $$ --> has full rank <span class="math inline">\(K\)</span>. I.e., <span class="math inline">\(\Sigma_{X'X}\)</span> is nonsingular and invertible. <!-- (Note, this assumption does not assume that $S_{X'X}\to_{p}\Sigma_{X'X}$, but if this convergence hold, it assumes that the limiting matrix is of full rank $K$.) --></p>
<!-- **Note:**The crucial part of Assumption 3$^\ast$ is that the limit matrix has full rank $K$. The convergence in probability statement ($S_{X'X}\to_{p}\Sigma_{X'X}$) follows already from Assumption 1$^\ast$.   -->
<!-- **Note:** Assumption 3$^\ast$ implies the existence and finiteness of the first two moments of $X_i$ (even without Assumption 1$^\ast$). -->
<!-- % Under the Assumptions 1$^\ast$, 2, 3$^\ast$, and 4, we can show the following results. -->
<p><!-- 1$^\ast$ and 3$^\ast$  --></p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-Sxx1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.7 (Consistency of <span class="math inline">\(S_{X'X}^{-1}\)</span>)</strong></span> Under Assumptions 1, 2, 3<span class="math inline">\(^\ast\)</span>, and 4, and under the assumption that <span class="math inline">\(X_i\)</span> has finite second moments, we have that <span class="math display">\[
S_{X'X}^{-1} = \left(\frac{1}{n}X'X\right)^{-1}\quad\to_{p}\quad\Sigma_{X'X}^{-1},\quad\text{as}\quad n\to\infty.
\]</span></p>
</div>
</div>
</div>
</div>
<p>The proof of <a href="#thm-Sxx1" class="quarto-xref">Theorem&nbsp;<span>7.7</span></a> is done in the lecture.</p>
<!-- Under Assumption 1$^\ast$, 2 and 3$^\ast$ we have that -->
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-bconsistent1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.8 (Consistency of <span class="math inline">\(\hat\beta\)</span>)</strong></span> Under Assumptions 1, 2, 3<span class="math inline">\(^\ast\)</span>, and 4, and under the assumption that <span class="math inline">\(X_i\)</span> has finite second moments, and <span class="math inline">\(\varepsilon_i\)</span> has finite first moments, we have that <span class="math display">\[
\hat\beta_n\to_{p}\beta\quad\text{as}\quad n\to\infty.
\]</span></p>
</div>
</div>
</div>
</div>
<p>The proof of <a href="#thm-bconsistent1" class="quarto-xref">Theorem&nbsp;<span>7.8</span></a> is done in the lecture.</p>
<!-- **Note:** In @thm-bconsistent1 it would suffice to assume that $\varepsilon_i$ has only a finite *first* moment, but since we need also a finite second moment for the error term in the following theorems, we may also require two moments in @thm-bconsistent1.   -->
<p>Moreover, we can show that the appropriately scaled OLS estimator is asymptotically normal distributed. The following theorem is stated for the simpler homoskedastic case, the heteroskedastic case is presented in <a href="#sec-CaseHetero" class="quarto-xref"><span>Section 7.2.1</span></a>.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-OLSnormality1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.9 (Asymptotic Normality of <span class="math inline">\(\hat{\beta}\)</span> (Homoskedastic Case))</strong></span> Under Assumption 1, 2, 3<span class="math inline">\(^\ast\)</span>, and 4, and under the assumption that <span class="math inline">\(X_i\)</span> and <span class="math inline">\(\varepsilon_i\)</span> have finite second moments, and under the simplifying assumption of spherical errors <span class="math display">\[
Var(\varepsilon|X)=\sigma^2I_n,
\]</span> we have that <span class="math display">\[
\sqrt{n}(\hat\beta_n-\beta)\to_{d} \mathcal{N}_K\left(0,\sigma^2 \Sigma^{-1}_{X'X}\right),\quad\text{as}\quad n\to\infty,
\]</span> where <span class="math display">\[
\begin{align*}
\sigma^2 \Sigma^{-1}_{X'X}
&amp; = \lim_{n\to\infty} Var\left(\sqrt{n}(\hat\beta_n-\beta)\right)\\[2ex]
&amp; = \lim_{n\to\infty} n Var\left(\hat\beta_n\right)
\end{align*}
\]</span> is the asymptotic variance of <span class="math inline">\(\hat\beta_n.\)</span></p>
</div>
</div>
</div>
</div>
<p>The proof of <a href="#thm-OLSnormality1" class="quarto-xref">Theorem&nbsp;<span>7.9</span></a> is done in the lecture.</p>
<p><a href="#thm-OLSnormality1" class="quarto-xref">Theorem&nbsp;<span>7.9</span></a> implies that for spherical errors and for largish sample sizes <span class="math inline">\(n,\)</span> the OLS estimator <span class="math inline">\(\beta_n\)</span> is <strong>approximately</strong> normal distributed, i.e. <span class="math display">\[
\begin{align*}
\sqrt{n}(\hat\beta_n-\beta)&amp;\overset{a}{\sim}\mathcal{N}_K\left(0,\sigma^2 \Sigma^{-1}_{X'X}\right)\\[2ex]
\Rightarrow\qquad\hat\beta_n &amp;\overset{a}{\sim}\mathcal{N}_K\left(\beta, n^{-1}\sigma^2 \Sigma^{-1}_{X'X}\right),
\end{align*}
\]</span> where the approximation error becomes arbitrarily small as <span class="math inline">\(n\to\infty.\)</span></p>
<p>Usually, we do not know the asymptotic variance <span class="math inline">\(\sigma^2\Sigma_{X'X}^{-1},\)</span> but have to plug-in the (consistent) estimator<br><span class="math display">\[
s_{ub}^2 S_{X'X}^{-1}.
\]</span></p>
<blockquote class="blockquote">
<p><strong>Note:</strong> Consistency of <span class="math inline">\(S_{X'X}^{-1}\)</span> is provided by <a href="#thm-Sxx1" class="quarto-xref">Theorem&nbsp;<span>7.7</span></a> and consistency of <span class="math inline">\(s_{ub}^2\)</span> is provided by <a href="#thm-Consistency_s1" class="quarto-xref">Theorem&nbsp;<span>7.10</span></a>.</p>
</blockquote>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-Consistency_s1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.10 (Consistency of <span class="math inline">\(s^2_{UB}\)</span>)</strong></span> Under the assumptions of <a href="#thm-OLSnormality1" class="quarto-xref">Theorem&nbsp;<span>7.9</span></a>, but with the additional requirement that <span class="math inline">\(\varepsilon_i\)</span> has finite fourth moments, we have that <span class="math display">\[
s_{ub}^2\to_{p}\sigma^2,\quad\text{as}\quad n\to\infty.
\]</span></p>
</div>
</div>
</div>
</div>
<p>The proof of <a href="#thm-Consistency_s1" class="quarto-xref">Theorem&nbsp;<span>7.10</span></a> is skipped, but a detailed proof can be found here: <a href="https://www.statlect.com/fundamentals-of-statistics/OLS-estimator-properties">https://www.statlect.com/fundamentals-of-statistics/OLS-estimator-properties</a></p>
<section id="sec-CaseHetero" class="level3" data-number="7.2.1"><h3 data-number="7.2.1" class="anchored" data-anchor-id="sec-CaseHetero">
<span class="header-section-number">7.2.1</span> The Case of Heteroskedasticity</h3>
<p><a href="#thm-OLSnormality1" class="quarto-xref">Theorem&nbsp;<span>7.9</span></a> can also be stated (and proofed) for conditionally heteroskedastic error terms. In this case, <span class="math inline">\(\beta_n\)</span> is for large sample sizes <span class="math inline">\(n\)</span> <strong>approximately</strong> normal distributed, <span id="eq-OLSnormality1Rob"><span class="math display">\[
\begin{align*}
\sqrt{n}(\hat\beta_n-\beta)\overset{a}{\sim}\mathcal{N}_K\left(0,\Sigma_{X'X}^{-1}E(\varepsilon^2_iX_iX_i')\Sigma_{X'X}^{-1}\right)\\[2ex]
\Rightarrow\qquad\hat\beta_n\overset{a}{\sim}\mathcal{N}_K\left(\beta,n^{-1}\Sigma_{X'X}^{-1}E(\varepsilon^2_iX_iX_i')\Sigma_{X'X}^{-1}\right),
\end{align*}
\qquad(7.1)\]</span></span> where the approximation error becomes arbitrarily small as <span class="math inline">\(n\to\infty.\)</span></p>
<p>The asymptotic variance <span class="math display">\[
\lim_{n\to\infty}Var(\sqrt{n}(\hat\beta_n-\beta))=\underbrace{\Sigma_{X'X}^{-1}E(\varepsilon_i^2X_iX_i')\Sigma_{X'X}^{-1}}_{(K\times K)}
\]</span> is, of course, usually unknown and needs to be estimated from the data by some consistent estimator such that <span class="math display">\[
S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\to_{p} \Sigma_{X'X}^{-1}E(\varepsilon^2_iX_iX_i')\Sigma_{X'X}^{-1}
\]</span> as <span class="math inline">\(n\to\infty.\)</span></p>
<p>The estimator <span class="math display">\[
\widehat{E}(\varepsilon^2_iX_iX_i')
\]</span> is here a placeholder for one of the existing <strong>Heteroskedasticity Consistent (HC)</strong> estimators of <span class="math inline">\(E(\varepsilon^2X_iX_i')\)</span>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 66%">
</colgroup>
<thead><tr class="header">
<th>HC-Type</th>
<th>Formular</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>HC0</td>
<td><span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\hat\varepsilon_i^2X_iX_i'\)</span></td>
</tr>
<tr class="even">
<td>HC1</td>
<td><span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\frac{n}{n-K}\hat\varepsilon_i^2X_iX_i'\)</span></td>
</tr>
<tr class="odd">
<td>HC2</td>
<td><span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{1-h_{i}}X_iX_i'\)</span></td>
</tr>
<tr class="even">
<td>HC3</td>
<td><span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{2}}X_iX_i'\)</span></td>
</tr>
<tr class="odd">
<td>HC4</td>
<td><span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{\delta_{i}}}X_iX_i'\)</span></td>
</tr>
</tbody>
</table>
<p>HC3 is the most often used HC-estimator.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The statistic <span class="math inline">\(h_i:=[P_X]_{ii}\)</span> is called the <strong>leverage statistic</strong> of <span class="math inline">\(X_i,\)</span> where</p>
<ul>
<li>
<span class="math inline">\(1/n\leq h_i\leq 1\)</span> and</li>
<li>
<span class="math inline">\(\bar{h}=n^{-1}\sum_{i=1}^nh_i=K/n\)</span>.</li>
</ul>
<p>Observations <span class="math inline">\(X_i\)</span> with leverage statistics <span class="math inline">\(h_i\)</span> that greatly exceed the average leverage value <span class="math inline">\(K/n\)</span> are referred to as “high leverage” observations. High leverage observations <span class="math inline">\(X_i\)</span> are observations that are far away from all other observations <span class="math inline">\(X_j\)</span>, <span class="math inline">\(i\neq j=1,\dots,n.\)</span></p>
<p>High leverage observations <span class="math inline">\(X_i\)</span> have the potential to distort the estimation results, <span class="math inline">\(\hat\beta_n\)</span>. Indeed, a high leverage observation <span class="math inline">\(X_i\)</span> will have an distorting effect on the estimation results if the absolute value of the corresponding residual <span class="math inline">\(|\hat{\varepsilon}_i|\)</span> is unusually large—such observations are called <strong>influential outliers</strong>. Such observations increase the estimation uncertainty.</p>
<p>General idea of the HC2-HC4 estimators is to increase the estimated variance in order to account for the effects of influential outliers. The residuals <span class="math inline">\(\hat\varepsilon_i\)</span> belonging to <span class="math inline">\(X_i\)</span> values that have a large leverage <span class="math inline">\(h_i\)</span> receive a higher weight and thus increase the value of <span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i').\)</span> This strategy takes into account increased estimation uncertainties due to single influential outliers. <!-- (Too small variance estimates lead to false inference, too large variance estimates lead to conservative inference.) --></p>
</div>
</div>
<p>The estimator HC0 was suggested in the econometrics literature by <span class="citation" data-cites="White1980">White (<a href="#ref-White1980" role="doc-biblioref">1980</a>)</span> and is justified by asymptotic (<span class="math inline">\(n\to\infty\)</span>) arguments. The estimators HC1, HC2 and HC3 were suggested by <span class="citation" data-cites="MacKinnon_White_1985">MacKinnon and White (<a href="#ref-MacKinnon_White_1985" role="doc-biblioref">1985</a>)</span> to improve the finite sample performance of HC0. Using an extensive Monte Carlo simulation study comparing HC0-HC3, <span class="citation" data-cites="Long_Ervin_2000">Long and Ervin (<a href="#ref-Long_Ervin_2000" role="doc-biblioref">2000</a>)</span> concludes that HC3 provides the best overall performance in finite samples. <span class="citation" data-cites="Cribari_2004">Cribari-Neto (<a href="#ref-Cribari_2004" role="doc-biblioref">2004</a>)</span> suggested the estimator HC4 to further improve the performance in finite sample behavior, especially in the presence of influential observations (large <span class="math inline">\(h_i\)</span> values).</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> Besides <strong>Heteroskedasticity Consistent (HC)</strong> estimators, there are also <strong>Heteroskedasticity and Autocorrelation Consistent (HAC)</strong> estimators.</p>
</blockquote>
</section><section id="robust-inference" class="level3" data-number="7.2.2"><h3 data-number="7.2.2" class="anchored" data-anchor-id="robust-inference">
<span class="header-section-number">7.2.2</span> Robust Inference</h3>
<!-- From our asymptotic results under the classic regression model (Assumptions 1$^\ast$, 2, 3$^\ast$, and 4) we get the following results important for testing statistical hypothesis.  -->
<section id="robust-hypothesis-testing-multiple-parameters" class="level4" data-number="7.2.2.1"><h4 data-number="7.2.2.1" class="anchored" data-anchor-id="robust-hypothesis-testing-multiple-parameters">
<span class="header-section-number">7.2.2.1</span> Robust Hypothesis Testing: Multiple Parameters</h4>
<p>Let us reconsider the following system of <span class="math inline">\(q\)</span>-many null hypotheses: <span class="math display">\[\begin{align*}
H_0: \underset{(q\times K)}{R}\underset{(K\times 1)}{\beta}  = \underset{(q\times 1)}{r^{(0)}}\\
H_1: \underset{(q\times K)}{R}\underset{(K\times 1)}{\beta}  \neq \underset{(q\times 1)}{r^{(0)}}\\
\end{align*}\]</span> where the <span class="math inline">\((q \times K)\)</span> matrix <span class="math inline">\(R\)</span> and the <span class="math inline">\(q\)</span>-vector <span class="math inline">\(r=(r_{1},\dots,r_{q})'\)</span> are chosen by the statistician to specify her/his null hypothesis about the unknown true parameter vector <span class="math inline">\(\beta\)</span>. To make sure that there are no redundant equations, it is required that <span class="math inline">\(\operatorname{rank}(R)=q\)</span>.</p>
<p>By contrast to the multiple parameter tests for small samples (see <a href="06-Small-Sample-Inference.html#sec-testmultp" class="quarto-xref"><span>Section 6.2</span></a>), we can work here with a <strong>heteroskedasticity robust</strong> test statistic which is applicable for heteroskedastic error terms: <span id="eq-Ftestasymp"><span class="math display">\[
\begin{align*}
W&amp;=n(R\hat\beta_n -r)'[R\,S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\,R']^{-1}(R\hat\beta_n-r)\\[2ex]
W&amp;\overset{H_0}{\to}_d\chi^2(q), \quad\text{as}\quad n\to\infty.
\end{align*}
\qquad(7.2)\]</span></span> The price to pay is that the distribution of the test statistic under the null hypothesis is only valid asymptotically; i.e.&nbsp;for large <span class="math inline">\(n\)</span>. That is, the critical values taken from the asymptotic distribution will be useful only for “largish” samples sizes.</p>
<p>In case of <strong>homoskedastic error</strong> terms, one can substitute <span class="math display">\[
S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}
\]</span> by <span class="math display">\[
s_{ub}^2S_{X'X}^{-1}.
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Finite-sample correction
</div>
</div>
<div class="callout-body-container callout-body">
<p>In order to improve the finite-sample performance of this test, one usually uses the <span class="math inline">\(F_{q,n-K}\)</span> distribution with <span class="math inline">\(q\)</span> and <span class="math inline">\(n-K\)</span> degrees of freedoms instead of the <span class="math inline">\(\chi^2(q)\)</span> distribution.</p>
<p>Asymptotically (<span class="math inline">\(n\to\infty\)</span>), <span class="math inline">\(F_{q,n-K}\)</span> is equivalent to <span class="math inline">\(\chi^2(q)\)</span>. However, for any finite sample size <span class="math inline">\(n\)</span> (i.e., the practically relevant case) <span class="math inline">\(F_{q,n-K}\)</span> leads to larger critical values which helps to account for the estimation errors in <span class="math inline">\(S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\)</span> (or in <span class="math inline">\(s_{ub}^2S_{X'X}^{-1}\)</span>) which are otherwise neglected by the pure asymptotic perspective.</p>
</div>
</div>
</section><section id="robust-hypothesis-testing-single-parameters" class="level4" data-number="7.2.2.2"><h4 data-number="7.2.2.2" class="anchored" data-anchor-id="robust-hypothesis-testing-single-parameters">
<span class="header-section-number">7.2.2.2</span> Robust Hypothesis Testing: Single Parameters</h4>
<p>Let us reconsider the case of hypotheses about only one parameter <span class="math inline">\(\beta_k,\)</span> with <span class="math inline">\(k=1,\dots,K\)</span> <span class="math display">\[
\begin{equation*}
\begin{array}{ll}
H_0: &amp; \beta_k=\beta_k^{(0)}\\
H_1: &amp; \beta_k\ne \beta_k^{(0)}\\
\end{array}
\end{equation*}
\]</span> Selecting the <span class="math inline">\(k\)</span>th diagonal element of the test-statistic in <a href="#eq-Ftestasymp" class="quarto-xref">Equation&nbsp;<span>7.2</span></a> and taking the square root yields <span class="math display">\[
\begin{align*}
T&amp;=\frac{\sqrt{n}\left(\hat{\beta}_k-\beta_k^{(0)}\right)}{\sqrt{\left[S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\right]_{(k,k)}}}
\end{align*}
\]</span> where <span class="math display">\[
\begin{align*}
T&amp;\overset{H_0}{\to}_d\mathcal{N}(0,1),\quad\text{as}\quad n\to\infty,
\end{align*}
\]</span> and where <span class="math display">\[
\left[S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\right]_{(k,k)}
\]</span> denotes the element in the <span class="math inline">\(k\)</span>th row and <span class="math inline">\(k\)</span>th column of the <span class="math inline">\(K\times K\)</span> matrix <span class="math inline">\(S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}.\)</span></p>
<p>This <span class="math inline">\(t\)</span>-test statistic allows for <strong>heteroskedastic</strong> error terms.</p>
<p>In case of homoskedastic error terms, one can substitute <span class="math display">\[
[S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{(k,k)}
\]</span> by <span class="math display">\[
s_{ub}^2[S_{X'X}^{-1}]_{(k,k)}
\]</span> where <span class="math inline">\([S_{X'X}^{-1}]_{(k,k)}\)</span> denotes the element in the <span class="math inline">\(k\)</span>th row and <span class="math inline">\(k\)</span>th column of the <span class="math inline">\(K\times K\)</span> matrix <span class="math inline">\(S_{X'X}^{-1}.\)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Finite-sample correction
</div>
</div>
<div class="callout-body-container callout-body">
<p>In order to improve the finite-sample performance of this <span class="math inline">\(t\)</span> test, one usually uses the <span class="math inline">\(t_{(n-K)}\)</span> distribution with <span class="math inline">\(n-K\)</span> degrees of freedoms instead of the <span class="math inline">\(\mathcal{N}(0,1)\)</span> distribution.</p>
<p>Asymptotically (<span class="math inline">\(n\to\infty\)</span>), <span class="math inline">\(t_{(n-K)}\)</span> is equivalent to <span class="math inline">\(\mathcal{N}(0,1)\)</span>. However, for finite sample sizes <span class="math inline">\(n\)</span> (i.e., the practically relevant case) <span class="math inline">\(t_{n-K}\)</span> leads to larger critical values which helps to account for the estimation errors in <span class="math inline">\([S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{(k,k)}\)</span> (or in <span class="math inline">\(s_{ub}^2[S_{X'X}^{-1}]_{(k,k)}\)</span>) which are otherwise neglected by the pure asymptotic perspective.</p>
</div>
</div>
</section><section id="robust-confidence-intervals" class="level4" data-number="7.2.2.3"><h4 data-number="7.2.2.3" class="anchored" data-anchor-id="robust-confidence-intervals">
<span class="header-section-number">7.2.2.3</span> Robust Confidence Intervals</h4>
<p>Following the derivations in Chapter <a href="06-Small-Sample-Inference.html#sec-CIsmallsample" class="quarto-xref"><span>Section 6.4</span></a>, but using the expression for the robust standard errors, we get the following heteroskedasticity robust (random) <span class="math inline">\((1-\alpha)\cdot 100\%\)</span> confidence interval <span class="math display">\[
\operatorname{CI}_{1-\alpha}=
\left[\hat\beta_k\pm t_{1-\alpha/2,n-K}\sqrt{n^{-1}[S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{(k,k)}}\right].
\]</span> Here, the coverage probability is an asymptotic coverage probability with <span class="math inline">\(P(\beta_k\in\operatorname{CI}_{1-\alpha})\to \gamma\)</span> as <span class="math inline">\(n\to\infty\)</span>, where <span class="math inline">\(\gamma\geq 1-\alpha.\)</span></p>
</section></section></section><section id="sec-MCLS" class="level2" data-number="7.3"><h2 data-number="7.3" class="anchored" data-anchor-id="sec-MCLS">
<span class="header-section-number">7.3</span> Monte Carlo Simulations</h2>
<p>Let’s apply the above asymptotic inference methods using <code>R</code>. As in Chapter <a href="06-Small-Sample-Inference.html#sec-PSSI" class="quarto-xref"><span>Section 6.6</span></a> we, first, program a function <code>myDataGenerator()</code> which allows us to generate data from the following model, i.e., from the following fully specified data generating process: <span class="math display">\[\begin{align*}
Y_i &amp;=\beta_1+\beta_2X_{i2}+\beta_3X_{i3}+\varepsilon_i,\qquad i=1,\dots,n\\
\beta &amp;=(\beta_1,\beta_2,\beta_3)'=(2,3,4)'\\
X_{i2}&amp;\sim U[-4,4]\\
X_{i3}&amp;\sim U[-5,5]\\
\varepsilon_i|X_i&amp;\sim U[-0.5 |X_{i2}|, 0.5 |X_{i2}|],
\end{align*}\]</span> where <span class="math inline">\((Y_i,X_i)\)</span> is assumed i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span> with <span class="math inline">\(X_{i2}\)</span> and <span class="math inline">\(X_{i3}\)</span> being independent of each other.</p>
<p>By contrast to our simulations in Chapter <a href="06-Small-Sample-Inference.html#sec-PSSI" class="quarto-xref"><span>Section 6.6</span></a>, we consider here a <strong>non-Gaussian</strong> and <strong>heteroskedastic</strong> error term <span class="math display">\[
Var(\varepsilon_i|X_i)=\frac{1}{12}X_{i2}^2.
\]</span></p>
<blockquote class="blockquote">
<p>As a side note: The unconditional variance follows by the law of total variance and is given by <span class="math display">\[\begin{align*}
Var(\varepsilon_i)
&amp;=E(Var(\varepsilon_i|X_i))+Var(E(\varepsilon_i|X_i))\\
&amp;=E\left(\frac{1}{12}X_{i2}^2\right)+0\\
&amp;=\frac{1}{12}\;E\left(X_{i2}^2\right)\\
&amp;=\frac{1}{12}\;Var\left(X_{i2}\right)\quad\text{(since $E(X_{i2}=0)$)}\\
&amp;=\frac{1}{12}\left(\frac{1}{12}(4-(-4))^2\right)=\frac{4}{9},
\end{align*}\]</span> where the last steps follows from applying the variance formula for uniform random variables. The following <code>R</code>-function <code>myDataGenerator()</code> allows us to generate data from the above described data generating processes:</p>
</blockquote>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Function to generate artificial data</span></span>
<span><span class="va">myDataGenerator</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n</span>, <span class="va">beta</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="co">##</span></span>
<span>  <span class="va">X</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">n</span><span class="op">)</span>, </span>
<span>                 <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="op">-</span><span class="fl">4</span>, <span class="fl">4</span><span class="op">)</span>, </span>
<span>                 <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="op">-</span><span class="fl">5</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="co">##</span></span>
<span>  <span class="va">eps</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, min <span class="op">=</span> <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span><span class="op">)</span>, </span>
<span>                   max <span class="op">=</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">Y</span>    <span class="op">&lt;-</span> <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta</span> <span class="op">+</span> <span class="va">eps</span></span>
<span>  <span class="co">##</span></span>
<span>  <span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="st">"Y"</span>   <span class="op">=</span> <span class="va">Y</span>, </span>
<span>                     <span class="st">"X_1"</span> <span class="op">=</span> <span class="va">X</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, </span>
<span>                     <span class="st">"X_2"</span> <span class="op">=</span> <span class="va">X</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span>, </span>
<span>                     <span class="st">"X_3"</span> <span class="op">=</span> <span class="va">X</span><span class="op">[</span>,<span class="fl">3</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="co">##</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="check-distribution-of-hatbeta_n" class="level3" data-number="7.3.1"><h3 data-number="7.3.1" class="anchored" data-anchor-id="check-distribution-of-hatbeta_n">
<span class="header-section-number">7.3.1</span> Check: Distribution of <span class="math inline">\(\hat\beta_n\)</span>
</h3>
<p>The above data generating process fulfills our regulatory assumptions of this chapter. So, by theory, the estimators <span class="math inline">\(\hat\beta_k\)</span> should be normal distributed for sufficiently large sample sizes <span class="math inline">\(n\)</span>. <span class="math display">\[
\sqrt{n}\left(\hat\beta_{n,k}-\beta_k\right)\to_d\mathcal{N}\left(0,\left[\Sigma_{X'X}^{-1}E(\varepsilon^2_iX_iX_i')\Sigma_{X'X}^{-1}\right]_{(k,k)}\right)
\]</span> <span class="math display">\[
\Rightarrow\qquad\hat\beta_{n,k}\overset{a}{\sim}\mathcal{N}\left(\beta_k, \;n^{-1}\;\left[\Sigma_{X'X}^{-1}E(\varepsilon^2_iX_iX_i')\Sigma_{X'X}^{-1}\right]_{(k,k)}\right).
\]</span></p>
<p>For our above specified data generating process, we can derive all (usually unknown) population quantities.</p>
<ul>
<li><p>From the assumed distributions of <span class="math inline">\(X_{i2}\)</span> and <span class="math inline">\(X_{i3}\)</span> we have that: <span class="math display">\[
\begin{align*}
\Sigma_{X'X}
&amp;=E(S_{X'X})\\[2ex]
&amp;=E(X_iX_i')\\[2ex]
&amp;=\left(\begin{matrix}1&amp;0&amp;0\\0&amp;E(X_{i2}^2)&amp;0\\0&amp;0&amp;E(X_{i3}^2)\end{matrix}\right)\\[2ex]
&amp;=\left(\begin{matrix}1&amp;0&amp;0\\0&amp;\frac{16}{3}&amp;0\\0&amp;0&amp;\frac{25}{3}\end{matrix}\right)
\end{align*}
\]</span> The above result follows from observing that <span class="math inline">\(E(X^2)=Var(X)\)</span> if <span class="math inline">\(X\)</span> has mean zero, and that the variance of uniform <span class="math inline">\(U[a,b]\)</span> distributed random variables is given by <span class="math inline">\(\frac{1}{12}(b-a)^2\)</span>.</p></li>
<li><p>Moreover, <span class="math display">\[
\begin{align*}
E(\varepsilon^2_iX_iX_i')
&amp; =E(X_iX_i'E(\varepsilon^2_i|X_i))\\[2ex]
&amp; =E\left(X_iX_i'\left(\frac{1}{12}X_{i2}^2\right)\right)
\end{align*}
\]</span> such that <span class="math display">\[
\begin{align*}
E(\varepsilon^2_iX_iX_i')
&amp;=\left(\begin{matrix}E\left(\frac{1}{12}X_{i2}^2\right)&amp;0&amp;0\\[2ex]
                 0&amp;E\left(X_{i2}^2\cdot\frac{1}{12}X_{i2}^2\right)&amp;0\\0&amp;0&amp;E\left(X_{i3}^2\cdot\frac{1}{12}X_{i2}^2\right)
    \end{matrix}\right)\\[2ex]
&amp;=\left(\begin{matrix}\frac{1}{12}E\left(X_{i2}^2\right)&amp;0&amp;0\\
     0&amp;\frac{1}{12}E\left(X_{i2}^4\right)&amp;0\\0&amp;0&amp;\frac{1}{12}E\left(X_{i2}^2\right)\,E\left(X_{i3}^2\right)
\end{matrix}\right)\\[2ex]      
&amp;=\left(\begin{matrix}\frac{1}{12}\frac{16}{3}&amp;0&amp;0\\
                    0&amp;\frac{1}{12}\frac{256}{5}&amp;0\\
                    0&amp;0&amp;\frac{1}{12}\frac{16}{3}\frac{25}{3}\end{matrix}\right)\\[2ex]
&amp;=\left(\begin{matrix}\frac{4}{9}&amp;0&amp;0\\0&amp;\frac{64}{15}&amp;0\\0&amp;0&amp;\frac{100}{27}\end{matrix}\right)
\end{align*}
\]</span> The above result follow from observing that for <span class="math inline">\(X\sim U[a,b]\)</span> one has <span class="math inline">\(E(X^k)=\frac{b^{k+1}-a^{k+1}}{(k+1)(b-a)}\)</span>, <span class="math inline">\(k=1,2,\dots\)</span>; see, for instance, <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution">Wikipedia</a>.</p></li>
</ul>
<p>So, for instance, for <span class="math inline">\(\hat{\beta}_2\)</span> we have the following theoretical large sample distribution: <span class="math display">\[
\begin{align}
\hat\beta_{n,2}\overset{a}{\sim}&amp;\mathcal{N}\left(\beta_2, \;\frac{1}{n}\;\left[\left(\begin{matrix}1&amp;0&amp;0\\0&amp;\frac{16}{3}&amp;0\\0&amp;0&amp;\frac{25}{3}\end{matrix}\right)^{-1}\left(\begin{matrix}\frac{4}{9}&amp;0&amp;0\\0&amp;\frac{64}{15}&amp;0\\0&amp;0&amp;\frac{100}{27}\end{matrix}\right)\left(\begin{matrix}1&amp;0&amp;0\\0&amp;\frac{16}{3}&amp;0\\0&amp;0&amp;\frac{25}{3}\end{matrix}\right)^{-1}\right]_{22}\right)\\[2ex]
\hat\beta_{n,2}\overset{a}{\sim}&amp;\mathcal{N}\left(\beta_2, \;\frac{1}{n}\;\left[
  \left(
  \begin{matrix}
  0.444 &amp; 0 &amp;0\\
  0 &amp; 0.15 &amp;0\\
  0&amp;0&amp;0.053
  \end{matrix}\right)\right]_{22}\right)\\[2ex]
\hat\beta_{n,2}\overset{a}{\sim}&amp;\mathcal{N}\left(\beta_2, \;\frac{1}{n}\;0.15\right)
\end{align}
\]</span> Let’s use a Monte Carlo simulation to check how well the theoretical large sample (<span class="math inline">\(n\to\infty\)</span>) distribution of <span class="math inline">\(\hat\beta_2\)</span> works as an approximative distribution for a practical largish sample size of <span class="math inline">\(n=100\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>           <span class="op">&lt;-</span> <span class="fl">100</span>      <span class="co"># a largish sample size</span></span>
<span><span class="va">beta_true</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">3</span>,<span class="fl">4</span><span class="op">)</span> <span class="co"># true data vector</span></span>
<span></span>
<span><span class="co">## Mean and variance of the true asymptotic </span></span>
<span><span class="co">## normal distribution of beta_hat_2:</span></span>
<span><span class="co"># true mean</span></span>
<span><span class="va">beta_true_2</span>     <span class="op">&lt;-</span> <span class="va">beta_true</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> </span>
<span><span class="co"># true variance</span></span>
<span><span class="va">var_true_beta_2</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">16</span><span class="op">/</span><span class="fl">3</span>, <span class="fl">25</span><span class="op">/</span><span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>    <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> </span>
<span>                          <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span><span class="op">/</span><span class="fl">9</span>, <span class="fl">64</span><span class="op">/</span><span class="fl">15</span>, <span class="fl">100</span><span class="op">/</span><span class="fl">27</span><span class="op">)</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> </span>
<span>                    <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">16</span><span class="op">/</span><span class="fl">3</span>, <span class="fl">25</span><span class="op">/</span><span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">]</span><span class="op">/</span><span class="va">n</span></span>
<span></span>
<span><span class="co">## Let's generate 5000 realizations from beta_hat_2, and check </span></span>
<span><span class="co">## whether their distribution is close to the true normal </span></span>
<span><span class="co">## distribution.</span></span>
<span><span class="co">## (We don't condition on X since the theoretical limit </span></span>
<span><span class="co">## distribution is unconditional on X)</span></span>
<span><span class="va">rep</span>        <span class="op">&lt;-</span> <span class="fl">5000</span> <span class="co"># MC replications</span></span>
<span><span class="va">beta_hat_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="cn">NA</span>, times<span class="op">=</span><span class="va">rep</span><span class="op">)</span></span>
<span><span class="co">##</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">r</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">rep</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="va">MC_data</span> <span class="op">&lt;-</span> <span class="fu">myDataGenerator</span><span class="op">(</span>n    <span class="op">=</span> <span class="va">n</span>, </span>
<span>                               beta <span class="op">=</span> <span class="va">beta_true</span><span class="op">)</span></span>
<span>    <span class="va">lm_obj</span>        <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">X_3</span>, data <span class="op">=</span> <span class="va">MC_data</span><span class="op">)</span></span>
<span>    <span class="va">beta_hat_2</span><span class="op">[</span><span class="va">r</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lm_obj</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co">## Compare:</span></span>
<span><span class="co">## True beta_2 versus average of beta_hat_2 estimates</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">beta_true_2</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">beta_hat_2</span><span class="op">)</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.0000 2.9998</code></pre>
</div>
</div>
<p>Good! As expected, the average of the <code>5000</code> simulated realizations of <span class="math inline">\(\hat\beta_2\)</span> is basically equal to the theoretical true mean <span class="math inline">\(E(\hat\beta_2)=\beta_2=3\)</span> which indicates a bias of zero.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## True variance of beta_hat_2 versus </span></span>
<span><span class="co">## empirical variance of beta_hat_2 estimates</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">var_true_beta_2</span>, <span class="fl">5</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">beta_hat_2</span><span class="op">)</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.00150 0.00147</code></pre>
</div>
</div>
<p>Great! The variance of the <code>5000</code> simulated realizations of <span class="math inline">\(\hat\beta_2\)</span> is basically equal to the theoretical true variance <span class="math inline">\(Var(\hat\beta_{n,2})=0.15/n=0.0015\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## True normal distribution of beta_hat_2 versus </span></span>
<span><span class="co">## empirical density of beta_hat_2 estimates</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://scales.r-lib.org">"scales"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span>expr <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span>, mean <span class="op">=</span> <span class="va">beta_true_2</span>, </span>
<span>                   sd<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">var_true_beta_2</span><span class="op">)</span><span class="op">)</span>, </span>
<span>      xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>, col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/grDevices/gray.html">gray</a></span><span class="op">(</span><span class="fl">.2</span><span class="op">)</span>, lwd<span class="op">=</span><span class="fl">3</span>, lty<span class="op">=</span><span class="fl">1</span>, </span>
<span>xlim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="va">beta_hat_2</span><span class="op">)</span>,ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">14.1</span><span class="op">)</span>,main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"n="</span>,<span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">beta_hat_2</span>, bw <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/bandwidth.html">bw.SJ</a></span><span class="op">(</span><span class="va">beta_hat_2</span><span class="op">)</span><span class="op">)</span>, </span>
<span>      col<span class="op">=</span><span class="fu"><a href="https://scales.r-lib.org/reference/alpha.html">alpha</a></span><span class="op">(</span><span class="st">"blue"</span>,<span class="fl">.5</span><span class="op">)</span>, lwd<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span><span class="st">"topleft"</span>, lty<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span>, lwd<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">3</span><span class="op">)</span>, </span>
<span>     col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/grDevices/gray.html">gray</a></span><span class="op">(</span><span class="fl">.2</span><span class="op">)</span>, <span class="fu"><a href="https://scales.r-lib.org/reference/alpha.html">alpha</a></span><span class="op">(</span><span class="st">"blue"</span>,<span class="fl">.5</span><span class="op">)</span><span class="op">)</span>, bty<span class="op">=</span><span class="st">"n"</span>, legend<span class="op">=</span> </span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span></span>
<span>  <span class="st">"Theoretical (Asymptotic) Gaussian Density of"</span><span class="op">~</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hat</a></span><span class="op">(</span><span class="va">beta</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>, </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span></span>
<span>  <span class="st">"Empirical Density Estimation based on MC realizations from"</span><span class="op">~</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hat</a></span><span class="op">(</span><span class="va">beta</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="07-Asymptotics_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Great! The nonparametric density estimation (estimated via <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code>) computed from the <code>5000</code> simulated realizations of <span class="math inline">\(\hat\beta_2\)</span> is indicating that <span class="math inline">\(\hat\beta_2\)</span> is really normally distributed as described by our theoretical results.</p>
<p><!-- in @thm-OLSnormality1 (homoskedastic case) and in Equation @eq-OLSnormality1Rob (heterosceda# car::linearHypothesis(model = lm_obj, 
#                       hypothesis.matrix = c("X_2=3", "X_3=4"), 
#                       vcov = vcovHC3_mat)
stic case).  --></p>
<p>However, is the asymptotic distribution of <span class="math inline">\(\hat\beta_2\)</span> also usable for (very) small samples like <span class="math inline">\(n=5\)</span>? Let’s check that:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>           <span class="op">&lt;-</span> <span class="fl">5</span>       <span class="co"># a small sample size</span></span>
<span><span class="va">beta_true</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">3</span>,<span class="fl">4</span><span class="op">)</span> <span class="co"># true data vector</span></span>
<span></span>
<span><span class="co">## Mean and variance of the true asymptotic </span></span>
<span><span class="co">## normal distribution of beta_hat_2:</span></span>
<span><span class="co"># true mean</span></span>
<span><span class="va">beta_true_2</span>     <span class="op">&lt;-</span> <span class="va">beta_true</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> </span>
<span><span class="co"># true variance</span></span>
<span><span class="va">var_true_beta_2</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">16</span><span class="op">/</span><span class="fl">3</span>, <span class="fl">25</span><span class="op">/</span><span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> </span>
<span>                          <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span><span class="op">/</span><span class="fl">9</span>, <span class="fl">64</span><span class="op">/</span><span class="fl">15</span>, <span class="fl">100</span><span class="op">/</span><span class="fl">27</span><span class="op">)</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> </span>
<span>                    <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">16</span><span class="op">/</span><span class="fl">3</span>, <span class="fl">25</span><span class="op">/</span><span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">]</span><span class="op">/</span><span class="va">n</span></span>
<span></span>
<span><span class="co">## Let's generate 5000 realizations from beta_hat_2, and check </span></span>
<span><span class="co">## whether their distribution is close to the true normal </span></span>
<span><span class="co">## distribution.</span></span>
<span><span class="co">## (We don't condition on X since the theoretical limit </span></span>
<span><span class="co">## distribution is unconditional on X)</span></span>
<span><span class="va">rep</span>        <span class="op">&lt;-</span> <span class="fl">5000</span> <span class="co"># MC replications</span></span>
<span><span class="va">beta_hat_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="cn">NA</span>, times<span class="op">=</span><span class="va">rep</span><span class="op">)</span></span>
<span><span class="co">##</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">r</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">rep</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="va">MC_data</span> <span class="op">&lt;-</span> <span class="fu">myDataGenerator</span><span class="op">(</span>n    <span class="op">=</span> <span class="va">n</span>, </span>
<span>                               beta <span class="op">=</span> <span class="va">beta_true</span><span class="op">)</span></span>
<span>    <span class="va">lm_obj</span>        <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">X_3</span>, data <span class="op">=</span> <span class="va">MC_data</span><span class="op">)</span></span>
<span>    <span class="va">beta_hat_2</span><span class="op">[</span><span class="va">r</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lm_obj</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co">## Compare:</span></span>
<span><span class="co">## True beta_2 versus average of beta_hat_2 estimates</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">beta_true_2</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">beta_hat_2</span><span class="op">)</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.0000 2.9963</code></pre>
</div>
</div>
<p>OK, at least on average the <code>5000</code> simulated realizations of <span class="math inline">\(\hat\beta_2\)</span> are basically equal to the true mean <span class="math inline">\(E(\hat\beta_2)=\beta_2=3\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## True variance of beta_hat_2 versus </span></span>
<span><span class="co">## empirical variance of beta_hat_2 estimates</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">var_true_beta_2</span>, <span class="fl">4</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">beta_hat_2</span><span class="op">)</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0300 0.0562</code></pre>
</div>
</div>
<p>Ouch! The theoretical variance <span class="math inline">\(Var(\hat\beta_2)=0.15/n=0.03\)</span> is about 50% smaller than the actual (small sample) variance of <span class="math inline">\(\hat\beta_2\)</span> approximated by the empirical variance of the <code>5000</code> simulated realizations of <span class="math inline">\(\hat\beta_2\)</span>. That is, <strong>we cannot simply use a large sample result in small samples</strong>.</p>
<p>Reason: In small samples, the Law of Large Numbers has not kicked in yet; therefore, we cannot neglect the variability in the sample statistic <span class="math inline">\(S_{X'X}^{-1}.\)</span></p>
<p>This issue can also seen when comparing the theoretical large sample distribution of <span class="math inline">\(\hat\beta_2\)</span> with an estimate of the actual finite-sample distribution of <span class="math inline">\(\hat\beta_2.\)</span></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## True normal distribution of beta_hat_2 versus </span></span>
<span><span class="co">## empirical density of beta_hat_2 estimates</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://scales.r-lib.org">"scales"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span>expr <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span>, </span>
<span>                   mean <span class="op">=</span> <span class="va">beta_true_2</span>, </span>
<span>                   sd   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">var_true_beta_2</span><span class="op">)</span><span class="op">)</span>, </span>
<span>      xlab <span class="op">=</span> <span class="st">""</span>, ylab <span class="op">=</span> <span class="st">""</span>, col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/grDevices/gray.html">gray</a></span><span class="op">(</span><span class="fl">.2</span><span class="op">)</span>, lwd<span class="op">=</span><span class="fl">3</span>, lty<span class="op">=</span><span class="fl">1</span>, </span>
<span>xlim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">4</span><span class="op">)</span>, ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">3</span><span class="op">)</span>, main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"n="</span>,<span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">beta_hat_2</span>, bw <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/bandwidth.html">bw.SJ</a></span><span class="op">(</span><span class="va">beta_hat_2</span><span class="op">)</span><span class="op">)</span>, </span>
<span>      col<span class="op">=</span><span class="fu"><a href="https://scales.r-lib.org/reference/alpha.html">alpha</a></span><span class="op">(</span><span class="st">"blue"</span>,<span class="fl">.5</span><span class="op">)</span>, lwd<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">## Legend      </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span><span class="st">"topleft"</span>, lty<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span>, lwd<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">3</span><span class="op">)</span>, </span>
<span>       col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/grDevices/gray.html">gray</a></span><span class="op">(</span><span class="fl">.2</span><span class="op">)</span>, <span class="fu"><a href="https://scales.r-lib.org/reference/alpha.html">alpha</a></span><span class="op">(</span><span class="st">"blue"</span>,<span class="fl">.5</span><span class="op">)</span><span class="op">)</span>, bty<span class="op">=</span><span class="st">"n"</span>, </span>
<span>       legend<span class="op">=</span> </span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span></span>
<span>  <span class="st">"Theoretical (Asymptotic) Gaussian Density of"</span><span class="op">~</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hat</a></span><span class="op">(</span><span class="va">beta</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>, </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span></span>
<span>  <span class="st">"Empirical Density Estimation based on MC realizations from"</span><span class="op">~</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hat</a></span><span class="op">(</span><span class="va">beta</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>      </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="07-Asymptotics_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Not good. The actual finite-sample distribution has substantially fatter tails. That is, if we would use the quantiles of the asymptotic distribution, we would falsely reject the null-hypothesis too often (probability of type I errors would be larger than the significance level).</p>
<p>Fortunately, asymptotics are usually kicking in relatively fast; here, things become much more reliable already for <span class="math inline">\(n\geq 15\)</span>.</p>
</section><section id="check-testing-multiple-parameters" class="level3" data-number="7.3.2"><h3 data-number="7.3.2" class="anchored" data-anchor-id="check-testing-multiple-parameters">
<span class="header-section-number">7.3.2</span> Check: Testing Multiple Parameters</h3>
<p>In the following, we do inference about multiple parameters. We test the (here correct) null hypothesis <span class="math display">\[\begin{align*}
H_0:\;&amp;\beta_2=3\quad\text{and}\quad\beta_3=4\\
\text{versus}\quad H_1:\;&amp;\beta_2\neq 3\quad\text{and/or}\quad\beta_3\neq 4.
\end{align*}\]</span> Or equivalently <span class="math display">\[\begin{align*}
H_0:\;&amp;R\beta -r^{(0)} = 0 \\
H_1:\;&amp;R\beta -r^{(0)} \neq 0,
\end{align*}\]</span> where <span class="math display">\[
R=\left(
\begin{matrix}
0&amp;1&amp;0\\
0&amp;0&amp;1\\
\end{matrix}\right)\quad\text{ and }\quad
r^{(0)}=\left(\begin{matrix}3\\5\\\end{matrix}\right).
\]</span> The following <code>R</code> code can be used to test this hypothesis. Note that we use HC3 robust variance estimation <code>sandwich::vcovHC(lm_obj, type="HC3")</code> to take into account that the error terms are heteroskedastic.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://r-forge.r-project.org/projects/car/">"car"</a></span><span class="op">)</span><span class="op">)</span> <span class="co"># for linearHyothesis()</span></span>
<span><span class="co"># ?linearHypothesis</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://sandwich.R-Forge.R-project.org/">"sandwich"</a></span><span class="op">)</span> <span class="co"># for vcovHC(), robust variance estimations</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1009</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Generate data</span></span>
<span><span class="va">MC_data</span> <span class="op">&lt;-</span> <span class="fu">myDataGenerator</span><span class="op">(</span>n    <span class="op">=</span> <span class="fl">100</span>, </span>
<span>                           beta <span class="op">=</span> <span class="va">beta_true</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Estimate the linear regression model parameters</span></span>
<span><span class="va">lm_obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">X_3</span>, data <span class="op">=</span> <span class="va">MC_data</span><span class="op">)</span></span>
<span></span>
<span><span class="va">vcovHC3_mat</span> <span class="op">&lt;-</span> <span class="fu">sandwich</span><span class="fu">::</span><span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">lm_obj</span>, type<span class="op">=</span><span class="st">"HC3"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Option 1:</span></span>
<span><span class="co"># car::linearHypothesis(model = lm_obj, </span></span>
<span><span class="co">#                       hypothesis.matrix = c("X_2=3", "X_3=4"), </span></span>
<span><span class="co">#                       vcov = vcovHC3_mat)</span></span>
<span></span>
<span><span class="co">## Option 2:</span></span>
<span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span>,<span class="fl">0</span><span class="op">)</span>,</span>
<span>           <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">linearHypothesis</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">lm_obj</span>, </span>
<span>                      hypothesis.matrix <span class="op">=</span> <span class="va">R</span>, </span>
<span>                      rhs  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">4</span><span class="op">)</span>,</span>
<span>                      vcov <span class="op">=</span> <span class="va">vcovHC3_mat</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear hypothesis test

Hypothesis:
X_2 = 3
X_3 = 4

Model 1: restricted model
Model 2: Y ~ X_2 + X_3

Note: Coefficient covariance matrix supplied.

  Res.Df Df     F Pr(&gt;F)
1     99                
2     97  2 0.032 0.9685</code></pre>
</div>
</div>
<p>The large <span class="math inline">\(p\)</span>-value does not allow us to reject the null-hypothesis at any of the usual significance levels. This is good, since we test here a correct null-hypothesis and rejecting it would mean to do a false rejection (type I error, false positive).</p>
<p>On average, however, there will be some false rejections of the null-hypothesis, but this type I error rate needs to be samler or equal to <span class="math inline">\(\alpha\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1110</span><span class="op">)</span></span>
<span></span>
<span><span class="va">B</span>        <span class="op">&lt;-</span> <span class="fl">10000</span> </span>
<span><span class="va">p_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">B</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">r</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">B</span><span class="op">)</span><span class="op">{</span> </span>
<span>  <span class="va">MC_data</span>     <span class="op">&lt;-</span> <span class="fu">myDataGenerator</span><span class="op">(</span>n    <span class="op">=</span> <span class="fl">100</span>, beta <span class="op">=</span> <span class="va">beta_true</span><span class="op">)</span></span>
<span>  <span class="va">lm_obj</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">X_3</span>, data <span class="op">=</span> <span class="va">MC_data</span><span class="op">)</span></span>
<span>  <span class="va">vcovHC3_mat</span> <span class="op">&lt;-</span> <span class="fu">sandwich</span><span class="fu">::</span><span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">lm_obj</span>, type<span class="op">=</span><span class="st">"HC3"</span><span class="op">)</span></span>
<span>  <span class="va">Ftest</span>       <span class="op">&lt;-</span> <span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">linearHypothesis</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">lm_obj</span>, </span>
<span>                        hypothesis.matrix <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"X_2=3"</span>, <span class="st">"X_3=4"</span><span class="op">)</span>, </span>
<span>                        vcov <span class="op">=</span> <span class="va">vcovHC3_mat</span><span class="op">)</span></span>
<span>  <span class="va">p_values</span><span class="op">[</span><span class="va">r</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">Ftest</span><span class="op">$</span><span class="st">'Pr(&gt;F)'</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>                      </span>
<span><span class="op">}</span></span>
<span><span class="co">## Nominal type I error rate </span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">0.05</span> </span>
<span><span class="co">## Empirical type I error rate</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">p_values</span><span class="op">[</span><span class="va">p_values</span> <span class="op">&lt;</span> <span class="va">alpha</span><span class="op">]</span><span class="op">)</span><span class="op">/</span><span class="va">B</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0562</code></pre>
</div>
</div>
<p>This is more or less ok, but you see that the upper bound is not so strict here as it was in the small sample inference case in <a href="06-Small-Sample-Inference.html#sec-PSSI" class="quarto-xref"><span>Section 6.6</span></a>. A larger sample size <span class="math inline">\(n\geq 100\)</span> would improve this simulation result.</p>
</section><section id="check-testing-single-parameters" class="level3" data-number="7.3.3"><h3 data-number="7.3.3" class="anchored" data-anchor-id="check-testing-single-parameters">
<span class="header-section-number">7.3.3</span> Check: Testing Single Parameters</h3>
<p>Next, we do inference about a single parameter. We test <span class="math display">\[\begin{align*}
H_0:&amp;\beta_3=5\\
\text{versus}\quad H_1:&amp;\beta_3\neq 5.
\end{align*}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Load libraries</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"lmtest"</span><span class="op">)</span><span class="op">)</span>  <span class="co"># for coeftest()</span></span>
<span></span>
<span><span class="co">## Generate data</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">MC_data</span> <span class="op">&lt;-</span> <span class="fu">myDataGenerator</span><span class="op">(</span>n    <span class="op">=</span> <span class="va">n</span>, </span>
<span>                           beta <span class="op">=</span> <span class="va">beta_true</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Estimate the linear regression model parameters</span></span>
<span><span class="va">lm_obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">X_3</span>, data <span class="op">=</span> <span class="va">MC_data</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Robust t test</span></span>
<span></span>
<span><span class="co">## Robust standard error for \hat{\beta}_3:</span></span>
<span><span class="va">SE_rob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">lm_obj</span>, type <span class="op">=</span> <span class="st">"HC3"</span><span class="op">)</span><span class="op">[</span><span class="fl">3</span>,<span class="fl">3</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">## hypothetical (H0) value of \beta_3:</span></span>
<span><span class="va">beta_3_H0</span> <span class="op">&lt;-</span> <span class="fl">4</span></span>
<span><span class="co">## estimate for beta_3:</span></span>
<span><span class="va">beta_3_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lm_obj</span><span class="op">)</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span></span>
<span><span class="co">## robust t-test statistic</span></span>
<span><span class="va">t_test_stat</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">beta_3_hat</span> <span class="op">-</span> <span class="va">beta_3_H0</span><span class="op">)</span><span class="op">/</span><span class="va">SE_rob</span></span>
<span><span class="co">## p-value</span></span>
<span><span class="va">K</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lm_obj</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">##</span></span>
<span><span class="va">p_value</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span>   <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span>q <span class="op">=</span> <span class="va">t_test_stat</span>, df <span class="op">=</span> <span class="va">n</span> <span class="op">-</span> <span class="va">K</span><span class="op">)</span>, </span>
<span>                   <span class="fl">1</span><span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span>q <span class="op">=</span> <span class="va">t_test_stat</span>, df <span class="op">=</span> <span class="va">n</span> <span class="op">-</span> <span class="va">K</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">p_value</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5234886</code></pre>
</div>
</div>
<p>Agian, the large <span class="math inline">\(p\)</span>-value does not allow us to reject the (here true) null-hypothesis at any of the usual significance levels. Let us check the empirical type I error rate.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1009</span><span class="op">)</span></span>
<span><span class="va">n</span>        <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">B</span>        <span class="op">&lt;-</span> <span class="fl">10000</span> </span>
<span><span class="va">p_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">B</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">r</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">B</span><span class="op">)</span><span class="op">{</span> </span>
<span>  <span class="va">MC_data</span>     <span class="op">&lt;-</span> <span class="fu">myDataGenerator</span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, beta <span class="op">=</span> <span class="va">beta_true</span><span class="op">)</span></span>
<span>  <span class="va">lm_obj</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">X_3</span>, data <span class="op">=</span> <span class="va">MC_data</span><span class="op">)</span></span>
<span>  <span class="va">SE_rob</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">lm_obj</span>, type <span class="op">=</span> <span class="st">"HC3"</span><span class="op">)</span><span class="op">[</span><span class="fl">3</span>,<span class="fl">3</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">beta_3_hat</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lm_obj</span><span class="op">)</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span></span>
<span>  <span class="co">## robust t-test statistic</span></span>
<span>  <span class="va">t_test_stat</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">beta_3_hat</span> <span class="op">-</span> <span class="va">beta_3_H0</span><span class="op">)</span><span class="op">/</span><span class="va">SE_rob</span></span>
<span>  <span class="va">p_values</span><span class="op">[</span><span class="va">r</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span>   <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span>q <span class="op">=</span> <span class="va">t_test_stat</span>, df <span class="op">=</span> <span class="va">n</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">beta_true</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                         <span class="fl">1</span><span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span>q <span class="op">=</span> <span class="va">t_test_stat</span>, df <span class="op">=</span> <span class="va">n</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">beta_true</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">## Nominal type I error rate </span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">0.05</span> </span>
<span><span class="co">## Empirical type I error rate</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">p_values</span><span class="op">[</span><span class="va">p_values</span> <span class="op">&lt;</span> <span class="va">alpha</span><span class="op">]</span><span class="op">)</span><span class="op">/</span><span class="va">B</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0496</code></pre>
</div>
</div>
<p>Good! The empirical type I error rate is bounded from above by the nominal type I error rate <span class="math inline">\(\alpha\)</span>.</p>
</section></section><section id="sec-RDLSInf" class="level2" data-number="7.4"><h2 data-number="7.4" class="anchored" data-anchor-id="sec-RDLSInf">
<span class="header-section-number">7.4</span> Real Data Example</h2>
<p>In the following, we revisit the read data study from <a href="06-Small-Sample-Inference.html#sec-RDSSInf" class="quarto-xref"><span>Section 6.7</span></a>. Now, we have the tools to allow for heteroskedastic and non-Gaussian errors.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## The AER package contains a lot of datasets </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AER</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Attach the DoctorVisits data to make it usable</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"DoctorVisits"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lm_obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">visits</span> <span class="op">~</span> <span class="va">gender</span> <span class="op">+</span> <span class="va">age</span> <span class="op">+</span> <span class="va">income</span>, data <span class="op">=</span> <span class="va">DoctorVisits</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The above <code>R</code> codes re-estimate the following regression model <span class="math display">\[
Y_i = \beta_1 + \beta_{gender} X_{gender,i}
              + \beta_{age} X_{age,i}
              + \beta_{income} X_{income,i} + \varepsilon_i,
\]</span> where <span class="math inline">\(i=1,\dots,n\)</span> and</p>
<ul>
<li>
<span class="math inline">\(X_{gender,i}=1\)</span> if the <span class="math inline">\(i\)</span>th subject is a woman and <span class="math inline">\(X_{gender,i}=0\)</span> if the <span class="math inline">\(i\)</span>th subject is a man</li>
<li>
<span class="math inline">\(X_{age,i}\)</span> is the age of subject <span class="math inline">\(i\)</span> measured in years divided by <span class="math inline">\(100\)</span>
</li>
<li>
<span class="math inline">\(X_{income,i}\)</span> is the annual income of subject <span class="math inline">\(i\)</span> in tens of thousands of dollars</li>
</ul>
<p>Now, to do heteroskedasticity consistent robust inference, one can use the <code>vcocHV()</code> frunction from the <code>R</code> package <code>sandwich</code> together with the <code><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html">coeftest()</a></code> function from the <code>R</code> package <code>lmtest</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## No robust standard errors:</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html">coeftest</a></span><span class="op">(</span><span class="va">lm_obj</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
t test of coefficients:

              Estimate Std. Error t value  Pr(&gt;|t|)    
(Intercept)   0.153710   0.036069  4.2616 2.066e-05 ***
genderfemale  0.062446   0.023455  2.6624  0.007782 ** 
age           0.402355   0.057131  7.0427 2.132e-12 ***
income       -0.082306   0.031670 -2.5989  0.009380 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## HC3 robust standard errors:</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html">coeftest</a></span><span class="op">(</span><span class="va">lm_obj</span>, vcov <span class="op">=</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">lm_obj</span>, type <span class="op">=</span> <span class="st">"HC3"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
t test of coefficients:

              Estimate Std. Error t value  Pr(&gt;|t|)    
(Intercept)   0.153710   0.034351  4.4747 7.815e-06 ***
genderfemale  0.062446   0.024582  2.5403   0.01110 *  
age           0.402355   0.060304  6.6721 2.785e-11 ***
income       -0.082306   0.032321 -2.5465   0.01091 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>You can do this also using the <code>R</code> package <code>stargazer</code> which gives you (almost) puplication ready regression output tables. The following <code>R</code> code produced two regression outputs—one without and one with robust standard errors:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"stargazer"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Adjust standard errors</span></span>
<span><span class="va">cov_beta_HC3</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">lm_obj</span>, type <span class="op">=</span> <span class="st">"HC3"</span><span class="op">)</span></span>
<span><span class="va">robust_HC3_se</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">cov_beta_HC3</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Stargazer output (with and without RSE)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/stargazer/man/stargazer.html">stargazer</a></span><span class="op">(</span><span class="va">lm_obj</span>, <span class="va">lm_obj</span>, </span>
<span>          se   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="cn">NULL</span>, <span class="va">robust_HC3_se</span><span class="op">)</span>,</span>
<span>          column.labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">""</span>, <span class="st">"Robust SE"</span><span class="op">)</span>,</span>
<span>          type <span class="op">=</span> <span class="st">"html"</span><span class="op">)</span> <span class="co"># alternatively: type = "text" or type = "latex"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<table style="text-align:center">
<tbody><tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td colspan="2">
<em>Dependent variable:</em>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td colspan="2">
visits
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
Robust SE
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(1)
</td>
<td>
(2)
</td>
</tr>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
genderfemale
</td>
<td>
0.062<sup>***</sup>
</td>
<td>
0.062<sup>**</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.023)
</td>
<td>
(0.025)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
age
</td>
<td>
0.402<sup>***</sup>
</td>
<td>
0.402<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.057)
</td>
<td>
(0.060)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
income
</td>
<td>
-0.082<sup>***</sup>
</td>
<td>
-0.082<sup>**</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.032)
</td>
<td>
(0.032)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Constant
</td>
<td>
0.154<sup>***</sup>
</td>
<td>
0.154<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.036)
</td>
<td>
(0.034)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
Observations
</td>
<td>
5,190
</td>
<td>
5,190
</td>
</tr>
<tr>
<td style="text-align:left">
R<sup>2</sup>
</td>
<td>
0.019
</td>
<td>
0.019
</td>
</tr>
<tr>
<td style="text-align:left">
Adjusted R<sup>2</sup>
</td>
<td>
0.018
</td>
<td>
0.018
</td>
</tr>
<tr>
<td style="text-align:left">
Residual Std. Error (df = 5186)
</td>
<td>
0.791
</td>
<td>
0.791
</td>
</tr>
<tr>
<td style="text-align:left">
F Statistic (df = 3; 5186)
</td>
<td>
33.218<sup>***</sup>
</td>
<td>
33.218<sup>***</sup>
</td>
</tr>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
<em>Note:</em>
</td>
<td colspan="2" style="text-align:right">
<sup><em></em></sup>p&lt;0.1; <sup><strong></strong></sup>p&lt;0.05; <sup></sup>p&lt;0.01
</td>
</tr>
</tbody></table>
</div>
</section><section id="exercises" class="level2" data-number="7.5"><h2 data-number="7.5" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">7.5</span> Exercises</h2>
<!-- * [Exercises for Chapter 6](https://www.dropbox.com/scl/fi/kcnfmyndye4gg1lsrctlm/Ch6_Exercises1.pdf?rlkey=w8api48mkhul3hlyojdkyo1z0&dl=0)

* [Exercises of Chapter 6 with Solutions](https://www.dropbox.com/scl/fi/0107x2hb1s0dujehyikqi/Ch6_Exercises_with_Solutions1.pdf?rlkey=kpmqia69uayo1c6ax43syee1a&dl=0)


* [Exercises of Chapter 6 with Solutions (annotated)](https://www.dropbox.com/scl/fi/d5eh17n1njvbzqeiwtwr9/Ch6_Exercises_with_Solutions1_annotated.pdf?rlkey=0pwdbgj7dnqxpy0qq6bm3a76f&dl=0) -->
</section><section id="references" class="level1 unnumbered"><h1 class="unnumbered">References</h1>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Billingsley2008" class="csl-entry" role="listitem">
Billingsley, Patrick. 2008. <em>Probability and Measure</em>. John Wiley &amp; Sons.
</div>
<div id="ref-Cribari_2004" class="csl-entry" role="listitem">
Cribari-Neto, Francisco. 2004. <span>“Asymptotic Inference Under Heteroskedasticity of Unknown Form.”</span> <em>Computational Statistics &amp; Data Analysis</em> 45 (2): 215–33.
</div>
<div id="ref-Hayashi2000" class="csl-entry" role="listitem">
Hayashi, Fumio. 2000. <em>Econometrics</em>. Princeton University Press.
</div>
<div id="ref-Long_Ervin_2000" class="csl-entry" role="listitem">
Long, J Scott, and Laurie H Ervin. 2000. <span>“Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model.”</span> <em>The American Statistician</em> 54 (3): 217–24.
</div>
<div id="ref-MacKinnon_White_1985" class="csl-entry" role="listitem">
MacKinnon, James G, and Halbert White. 1985. <span>“Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties.”</span> <em>Journal of Econometrics</em> 29 (3): 305–25.
</div>
<div id="ref-Vaart2000" class="csl-entry" role="listitem">
Van der Vaart, Aad W. 2000. <em>Asymptotic Statistics</em>. Vol. 3. Cambridge university press.
</div>
<div id="ref-White1980" class="csl-entry" role="listitem">
White, Halbert. 1980. <span>“A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.”</span> <em>Econometrica</em>, 817–38.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./06-Small-Sample-Inference.html" class="pagination-link" aria-label="Small Sample Inference">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Small Sample Inference</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>