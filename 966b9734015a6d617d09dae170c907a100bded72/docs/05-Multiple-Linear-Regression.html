<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>5&nbsp; Multiple Linear Regression – Basis Module Econometrics (M.Sc.)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./06-Small-Sample-Inference.html" rel="next">
<link href="./04-Monte-Carlo-Simulations.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="site_libs/kePrint-0.0.1/kePrint.js"></script><link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./05-Multiple-Linear-Regression.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none"></a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Basis Module Econometrics (M.Sc.)</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Organization of the Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-Introduction-to-R.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to R</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-Probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-Matrix-Algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-Monte-Carlo-Simulations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Estimation Theory and Monte Carlo Simulations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-Multiple-Linear-Regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-Small-Sample-Inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Small Sample Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-Asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Large Sample Inference</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-LinModAssumptions" id="toc-sec-LinModAssumptions" class="nav-link active" data-scroll-target="#sec-LinModAssumptions"><span class="header-section-number">5.1</span> Assumptions</a>
  <ul class="collapse">
<li><a href="#some-implications-of-the-exogeneity-assumption-ass-2" id="toc-some-implications-of-the-exogeneity-assumption-ass-2" class="nav-link" data-scroll-target="#some-implications-of-the-exogeneity-assumption-ass-2"><span class="header-section-number">5.1.1</span> Some Implications of the Exogeneity Assumption (Ass 2)</a></li>
  </ul>
</li>
  <li>
<a href="#deriving-the-estimator-hatbeta" id="toc-deriving-the-estimator-hatbeta" class="nav-link" data-scroll-target="#deriving-the-estimator-hatbeta"><span class="header-section-number">5.2</span> Deriving the Estimator <span class="math inline">\(\hat\beta\)</span></a>
  <ul class="collapse">
<li><a href="#least-squares-approach" id="toc-least-squares-approach" class="nav-link" data-scroll-target="#least-squares-approach"><span class="header-section-number">5.2.1</span> Least Squares Approach</a></li>
  <li><a href="#sec-MMEstimator" id="toc-sec-MMEstimator" class="nav-link" data-scroll-target="#sec-MMEstimator"><span class="header-section-number">5.2.2</span> Method of Moments Estimator Approach</a></li>
  <li><a href="#method-of-moments-versus-least-squares-estimation" id="toc-method-of-moments-versus-least-squares-estimation" class="nav-link" data-scroll-target="#method-of-moments-versus-least-squares-estimation"><span class="header-section-number">5.2.3</span> Method of Moments versus Least Squares Estimation</a></li>
  </ul>
</li>
  <li><a href="#some-quantities-of-interest" id="toc-some-quantities-of-interest" class="nav-link" data-scroll-target="#some-quantities-of-interest"><span class="header-section-number">5.3</span> Some Quantities of Interest</a></li>
  <li><a href="#sec-GMTheorem" id="toc-sec-GMTheorem" class="nav-link" data-scroll-target="#sec-GMTheorem"><span class="header-section-number">5.4</span> The Gauss-Markov Theorem</a></li>
  <li>
<a href="#practice" id="toc-practice" class="nav-link" data-scroll-target="#practice"><span class="header-section-number">5.5</span> Practice</a>
  <ul class="collapse">
<li><a href="#factor-variables-and-the-dummy-variable-trap" id="toc-factor-variables-and-the-dummy-variable-trap" class="nav-link" data-scroll-target="#factor-variables-and-the-dummy-variable-trap"><span class="header-section-number">5.5.1</span> Factor Variables and the Dummy-Variable Trap</a></li>
  <li><a href="#dummy-variable-trap" id="toc-dummy-variable-trap" class="nav-link" data-scroll-target="#dummy-variable-trap">Dummy-Variable Trap</a></li>
  <li><a href="#detecting-heteroskedasticity" id="toc-detecting-heteroskedasticity" class="nav-link" data-scroll-target="#detecting-heteroskedasticity"><span class="header-section-number">5.5.2</span> Detecting Heteroskedasticity</a></li>
  <li><a href="#checking-non-linearity-of-the-regression-line" id="toc-checking-non-linearity-of-the-regression-line" class="nav-link" data-scroll-target="#checking-non-linearity-of-the-regression-line"><span class="header-section-number">5.5.3</span> Checking (Non-)Linearity of the Regression Line</a></li>
  <li><a href="#behavior-of-the-ols-estimates-for-resampled-data" id="toc-behavior-of-the-ols-estimates-for-resampled-data" class="nav-link" data-scroll-target="#behavior-of-the-ols-estimates-for-resampled-data"><span class="header-section-number">5.5.4</span> Behavior of the OLS Estimates for Resampled Data</a></li>
  </ul>
</li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">5.6</span> Exercises</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-MLR" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="hidden">
<p><span class="math display">\[
\require{color}
%% Colorbox within equation-environments:
\newcommand{\highlight}[2][yellow]{\mathchoice%
  {\colorbox{#1}{$\displaystyle#2$}}%
  {\colorbox{#1}{$\displaystyle#2$}}%
  {\colorbox{#1}{$\displaystyle#2$}}%
  }%
\]</span></p>
</div>
<p>In the following we focus on the case of random designs <span class="math inline">\(X\)</span> (i.e.&nbsp;<span class="math inline">\(X\)</span> being a random variable), since</p>
<ol type="1">
<li>this is the more relevant case in econometrics and</li>
<li>it includes the case of fixed designs (i.e.&nbsp;<span class="math inline">\(X\)</span> being deterministic) as a special case (“degenerated random variable”).</li>
</ol>
<p><strong>Caution:</strong> A random <span class="math inline">\(X\)</span> requires us to consider conditional means and variances given <span class="math inline">\(X.\)</span> That is, if we were able to resample from the population (from the data-generating process), we would do so by fixing (conditioning on) the observed <span class="math inline">\(X_{obs}\)</span> and generating new realizations of all other random quantities involved.</p>
<section id="sec-LinModAssumptions" class="level2" data-number="5.1"><h2 data-number="5.1" class="anchored" data-anchor-id="sec-LinModAssumptions">
<span class="header-section-number">5.1</span> Assumptions</h2>
<p>The multiple linear regression model is defined by the following assumptions which together describe the relevant theoretical aspects of the underlying data generating process:</p>
<p><strong>Assumption 1: Model and Sampling</strong></p>
<p><strong>Part (a): Linear Model</strong></p>
<p><span id="eq-LinMod"><span class="math display">\[
\begin{align}
  Y_i=\sum_{k=1}^K\beta_k X_{ik}+\varepsilon_i, \quad i=1,\dots,n.
\end{align}
\qquad(5.1)\]</span></span> Usually, a constant (intercept) is included, in this case <span class="math inline">\(X_{i1}=1\)</span> for all <span class="math inline">\(i=1,\dots,n.\)</span> In the following we will always assume that <span class="math inline">\(X_{i1}=1\)</span> for all <span class="math inline">\(i\)</span>, unless otherwise stated.</p>
<ul>
<li>
<span class="math inline">\(Y_i\)</span> is called “dependent variable” or “outcome variable” or “regressand”.</li>
<li>
<span class="math inline">\(X_{ik}\)</span> is called the <span class="math inline">\(k\)</span>th “independent variable” or “predictor variable” or “regressor” or “explanatory variable” or “control variable.”</li>
<li>
<span class="math inline">\(\varepsilon_i\)</span> denotes the statistical error term.</li>
</ul>
<p>It is convenient to write <a href="#eq-LinMod" class="quarto-xref">Equation&nbsp;<span>5.1</span></a> using matrix notation <span class="math display">\[
\begin{eqnarray*}
  Y_i&amp;=&amp;\underset{(1\times K)}{X_i'}\underset{(K\times 1)}{\beta} +\varepsilon_i, \quad i=1,\dots,n,
\end{eqnarray*}
\]</span> where <span class="math display">\[
X_i=\left(\begin{matrix}X_{i1}\\ \vdots\\  X_{iK}\end{matrix}\right)
\quad\text{and}\quad
\beta=\left(\begin{matrix}\beta_1\\ \vdots\\ \beta_K\end{matrix}\right).
\]</span> Stacking all individual rows <span class="math inline">\(i=1,\dots,n\)</span> leads to <span class="math display">\[
\begin{eqnarray*}\label{LM}
  \underset{(n\times 1)}{Y}&amp;=&amp;\underset{(n\times K)}{X}\underset{(K\times 1)}{\beta} + \underset{(n\times 1)}{\varepsilon},
\end{eqnarray*}
\]</span> where <span class="math display">\[
\begin{equation*}
Y=\left(\begin{matrix}Y_1\\ \vdots\\Y_n\end{matrix}\right),\quad X=\left(\begin{matrix}X_{11}&amp;\dots&amp;X_{1K}\\\vdots&amp;\ddots&amp;\vdots\\ X_{n1}&amp;\dots&amp;X_{nK}\\\end{matrix}\right),\quad\text{and}\quad \varepsilon=\left(\begin{matrix}\varepsilon_1\\ \vdots\\ \varepsilon_n\end{matrix}\right).
\end{equation*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Simple Linear Regression and Polynomial Regression Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>The special case of <span class="math inline">\(K=2\)</span> <span class="math display">\[
Y_i = \beta_1 + \beta_2 X_{i2} + \varepsilon_i
\]</span> is called the <strong><em>simple</em> linear regression model</strong>. With the simple linear regression model, only straight line fits are possible.</p>
<p>By contrast, with the multiple linear regression model, we can also fit polynomials. For instance, we can define <span class="math display">\[
X_{i3} := X_{i2}^2
\]</span> which leads to a quadratic regression model (often used for life-cycle analyses that include the predictor <code>Age</code><span class="math inline">\(_i=X_{i2}\)</span>) <span class="math display">\[
Y_i = \beta_1 + \beta_2 X_{i2} + \beta_3 X_{i2}^2 + \varepsilon_i.
\]</span> Of course, further predictor variables <span class="math inline">\(X_{i3},\dots,X_{iK}\)</span> can (and should) be added to this model.</p>
<p>The same logic applies to polynomials with higher polynomial degrees <span class="math inline">\((\geq 2).\)</span> Large polynomial degrees, however, can lead to unstable estimation results.</p>
</div>
</div>
<p><strong>Part (b): Random Sample</strong></p>
<p>Moreover, we assume that the observed (“obs”) data points <span class="math display">\[
((Y_{1,obs},X_{11,obs},\dots,X_{1K,obs}),(Y_{2,obs},X_{21,obs},\dots,X_{2K,obs}),\dots,(Y_{n,obs},X_{n1,obs},\dots,X_{nK,obs}))
\]</span> are a realization of the <strong>random sample</strong> <span class="math display">\[
((Y_{1},X_{11},\dots,X_{1K}),(Y_{2},X_{21},\dots,X_{2K}),\dots,(Y_{n},X_{n1},\dots,X_{nK})).
\]</span></p>
<p>That is, the <span class="math inline">\(i\)</span>th observed <span class="math inline">\(K+1\)</span> dimensional data point <span class="math display">\[(Y_{i,obs},X_{i1,obs},\dots,X_{iK,obs})\in\mathbb{R}^{K+1}
\]</span> is a realization of a <span class="math inline">\(K+1\)</span> dimensional random variable <span class="math display">\[
(Y_{i},X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1},
\]</span> where</p>
<ol type="1">
<li>
<span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\)</span> has the identical <span class="math inline">\(K+1\)</span> dimensional distribution for all <span class="math inline">\(i=1,\dots,n.\)</span>
</li>
<li>
<span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\)</span> is independent of <span class="math inline">\((Y_{j},X_{j1},\dots,X_{jK})\)</span> for all <span class="math inline">\(i\neq j=1,\dots,n.\)</span>
</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Due to <a href="#eq-LinMod" class="quarto-xref">Equation&nbsp;<span>5.1</span></a>, this i.i.d. assumption is equivalent to assuming that the multivariate random variables <span class="math display">\[
(\varepsilon_i,X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1}
\]</span> are i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Remark:</strong> Usually, we do not use a different notation for observed realizations <span class="math inline">\((Y_{i,obs},X_{i1,obs},\dots,X_{iK,obs})\in\mathbb{R}^{K+1}\)</span> and for the corresponding random variable <span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1}\)</span> since often both interpretations (random variable and its realizations) can make sense in the same statement and then it depends on the considered context whether the random variables point if view or the realization point of view applies.</p>
</div>
</div>
<!--  That is, the multivariate distribution of $(\varepsilon_i,X_{i1},\dots,X_{iK})$ is assumed equal 
across $i=1,\dots,n$, but the multivariate random variables $(\varepsilon_i,X_{i1},\dots,X_{iK})$ 
and $(\varepsilon_j,X_{j1},\dots,X_{jK})$ are independent for all $i\neq j$. -->
<!-- **Note.:** Of course, in principle, Assumption 1 can be violated; however, in classic econometrics one usually does not bother with a possibly violated model assumption (Assumption 1). In modern econometrics, this is a big deal.  -->
<!-- The following assumptions are the so-called *classic assumptions* and we will be often concerned about violations of these assumptions.  -->
<p><strong>Assumption 2: Exogeneity</strong> <span id="eq-assExogen"><span class="math display">\[
E(\varepsilon_i|X_i)=0,\quad i=1,\dots,n
\qquad(5.2)\]</span></span> This assumption demands that the mean of the random error term <span class="math inline">\(\varepsilon_i\)</span> is zero irrespective of the realizations of <span class="math inline">\(X_i\)</span>. This exogeneity assumption is also called</p>
<ul>
<li>“orthogonality assumption” or</li>
<li>“mean independence assumption.”</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Together with the random sample assumption (Assumption 1, Part (b)) <a href="#eq-assExogen" class="quarto-xref">Equation&nbsp;<span>5.2</span></a> even implies <strong>strict exogeneity</strong> <span class="math display">\[
E(\varepsilon|X) = \underset{(n\times 1)}{0}
\]</span> since we have independence across <span class="math inline">\(i=1,\dots,n\)</span>. Under strict exogeneity, the mean of the random vector <span class="math inline">\(\varepsilon\)</span> is zero irrespective of the realizations of the <span class="math inline">\((n\times K)\)</span>-dimensional random predictor matrix <span class="math inline">\(X.\)</span></p>
</div>
</div>
<!-- For one example, it cannot be fulfilled when the regressors include lagged dependent variables. -->
<!-- Notice that in the presence of a constant explanatory variable, setting the expectation to zero is a normalization.  -->
<p><strong>Assumption 3: Rank Condition (no perfect multicollinearity)</strong></p>
<p><span class="math display">\[
\begin{align*}
\operatorname{rank}(X)&amp;=K\quad\text{a.s.}\\
\Leftrightarrow P\big(\operatorname{rank}(X)&amp;=K\big)=1
\end{align*}
\]</span> This assumption demands that, with probability one, no predictor variable <span class="math inline">\(X_{k}\in\mathbb{R}^n\)</span> is linearly dependent of the others. (This is the literal translation of the “almost surely (a.s.)” concept.)</p>
<p><strong>Note:</strong> The assumption implies that <span class="math inline">\(n\geq K,\)</span> since <span class="math display">\[
\operatorname{rank}(X)\leq \min\{n,K\}\quad(a.s.)
\]</span></p>
<p>This rank assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: dummy variable trap, multicollinearity, variance inflation). The violation of this assumption harms any economic interpretation since we cannot disentangle the explanatory variables’ individual effects on <span class="math inline">\(Y\)</span>. Therefore, this assumption is also often called an <strong>identification assumption</strong>.</p>
<!-- 
::: {.callout-tip}
# Crash Course on Ranks and Matrix Inverse:

Let $A$ be a $(n\times m)$-dimensional matrix. 

* The rank of a matrix $A$ can be defined as the maximal number of linearly independent columns (or rows) of the matrix $A.$
* A $(n\times 1)$-dimensional column vector $A_k$ of $A$ is said to be **linearly dependent** of the other column vectors $A_{1},\dots,A_{k-1},A_{k+1},\dots,A_m$ if $A_k$ can be written as a linear combination of the other column vectors, i.e., if there are linear coefficients $c_1,\dots,c_{k-1},c_{k+1},\dots,c_m\in\mathbb{R}$ such that
$$
A_k = c_1 A_1 + \dots +c_{k-1} A_{k-1} + c_{k+1} A_{k+1}+ \dots +c_m A_{m}
$$
Conversely, the column vectors $A_1, \dots, A_m$ are **linearly independent** of each other if the equation  
$$
\begin{align*}
0   & = c_1 A_1 + \dots +c_{m} A_{m}
\end{align*}
$$
has only the trivial solution $c_1=\dots=c_m=0.$ 
* If $A$ is a $(n\times m)$ dimensional matrix, then 
$$
\operatorname{rank}(A)\leq \min\{m,n\}.
$$
* The column rank and the row rank are always equal, therefore 
$$
\operatorname{rank}(A)=\operatorname{rank}(A').$$ 
So, it is sufficient to check either compute the column rank or the row rank.
* A $(n\times m)$ matrix $A$ has full row-rank if $\operatorname{rank}(A)=n$
* A $(n\times m)$ matrix $A$ has full column-rank if $\operatorname{rank}(A)=m$
* A $(m\times m)$ matrix $B$ has full rank if $\operatorname{rank}(B)=m$
* Let $B$ a $(m\times m)$-dimensional matrix. The $(m\times m)$-dimensional matrix $B^{-1}$ is called a matrix **inverse** of $B$ if
$$
B^{-1}B=I_m\quad\text{and}\quad BB^{-1}=I_m,
$$ 
where $I_m$ is the $(m\times m)$ dimensional identity matrix. 
* If a $(m\times m)$ matrix $B$ has full rank, i.e. $\operatorname{rank}(B)=m,$ then there exists an inverse matrix $B^{-1}.$
* The inverse of a $(2\times 2)$ matrix is given by
$$
\begin{bmatrix}
a & b\\
c & d\\
\end{bmatrix}^{-1} 
= \frac{1}{ad - bc} 
\begin{bmatrix}
d  & -b\\
-c & a\\
\end{bmatrix},
$$

where $ad - bc$ is the **determinant** of the matrix, which has to be non-zero, otherwise the matrix is not invertible.  

The above crash course on linear algebra contains all concepts relevant for this course. Further, way more extensive material on linear algebra can be found, for instance: 

* in Appendix A of Marno Verbeek's "A Guide to Modern Econometrics"
* in Frank Pinter's lecture notes: [LINK](https://www.frankpinter.com/notes/linear-algebra-notes.pdf)
* in Cesar Aguilar's lecture notes: [Link](lecture_scripts/Linear_Algebra_Aguilar.pdf)
* in the free book: [Linear Algebra and Optimization with Applications to Machine Learning](https://www.cis.upenn.edu/~jean/gbooks/linalg.html)
::: 
-->
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Under Assumption 3, we have that <span class="math inline">\(\operatorname{rank}(X)=K\)</span> (a.s.)</p></li>
<li><p>This implies that the <span class="math inline">\((K\times K)\)</span>-dimensional matrix <span class="math inline">\(X'X\)</span> has full rank, i.e.&nbsp;that <span class="math display">\[
\operatorname{rank}(X'X)=K\quad\text{(a.s.)}
\]</span></p></li>
<li><p>Thus <span class="math inline">\((X'X)\)</span> is invertible; i.e.&nbsp;there exists a <span class="math inline">\((K\times K)\)</span>-dimensional matrix <span class="math inline">\((X'X)^{-1}\)</span> such that <span class="math display">\[
(X'X)(X'X)^{-1} = (X'X)^{-1}(X'X) = I_K.
\]</span></p></li>
</ul>
</div>
</div>
<p><strong>Assumption 4: Error distribution</strong></p>
<p>Depending on the context (i.e., parameter estimation vs.&nbsp;hypothesis testing and small <span class="math inline">\(n\)</span> vs.&nbsp;large <span class="math inline">\(n\)</span>) there are different more or less restrictive assumptions. Some of the most common ones are the following:</p>
<ul>
<li>
<strong>Conditional distribution:</strong> <span class="math display">\[
\varepsilon_i|X_i \sim f_{\varepsilon|X}
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and for any distribution <span class="math inline">\(f_{\varepsilon|X}\)</span> with two (or more) finite moments.
<ul>
<li>
<strong>Example: Conditional normal distribution</strong> <span class="math display">\[
\varepsilon_i|X_i \sim \mathcal{N}(0,\sigma^2(X_i))
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span>.</li>
</ul>
</li>
<li>
<strong>Independence between error and predictors:</strong> <span class="math inline">\(\varepsilon_i\sim f_\varepsilon\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> such that <span class="math inline">\(f_\varepsilon=f_{\varepsilon|X}\)</span> and such that <span class="math inline">\(f_\varepsilon\)</span> has two (or more) finite moments.
<ul>
<li>
<strong>Example: Independence between Gaussian error and predictors</strong> <span class="math display">\[
    \varepsilon_i\sim \mathcal{N}(0,\sigma^2),
    \]</span> where <span class="math inline">\(\sigma^2\)</span> is independent of <span class="math inline">\(X.\)</span>
</li>
</ul>
</li>
<li>
<strong>Spherical errors (“Gauss-Markov assumptions”):</strong> The conditional distributions of <span class="math inline">\(\varepsilon_i|X_i\)</span> may generally depend on <span class="math inline">\(X_i,\)</span> but only such that <span class="math display">\[
E(\varepsilon|X)=\underset{(n\times 1)}{0}
\]</span> and <span class="math display">\[
\begin{align*}
&amp;\underset{(n\times n)}{Var\left(\varepsilon|X\right)}=\\[2ex]
&amp; = \left(\begin{matrix}
Var(\varepsilon_1|X)&amp;Cov(\varepsilon_1,\varepsilon_2|X)&amp;\dots&amp;Cov(\varepsilon_1,\varepsilon_n|X)\\
Cov(\varepsilon_2,\varepsilon_1|X)&amp;Var(\varepsilon_2|X)&amp;\dots&amp;Cov(\varepsilon_2,\varepsilon_n|X)\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
Cov(\varepsilon_n,\varepsilon_1|X)&amp;Cov(\varepsilon_n,\varepsilon_2|X)&amp;\dots&amp;Var(\varepsilon_n|X)
\end{matrix}\right)\\[2ex]
&amp; = \left(\begin{matrix}
\sigma^2&amp;0&amp;\dots&amp;0\\
0&amp;\sigma^2&amp;\dots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\dots&amp;\sigma^2
\end{matrix}\right)
= \sigma^2 I_n,
\end{align*}
\]</span> where <span class="math inline">\(I_n\)</span> denotes the <span class="math inline">\((n\times n)\)</span> identity matrix. Under the spherical errors assumption, one has, for all possible realizations of <span class="math inline">\(X\)</span>, that:
<ul>
<li>
<strong>uncorrelated:</strong> <span class="math display">\[
  Cov(\varepsilon_i,\varepsilon_j|X)=0
  \]</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and all <span class="math inline">\(j=1,\dots,n\)</span> such that <span class="math inline">\(i\neq j\)</span>
</li>
<li>
<strong>homoskedastic:</strong> <span class="math display">\[
  Var(\varepsilon_i|X)=\sigma^2
  \]</span> for all <span class="math inline">\(i=1,\dots,n\)</span>
</li>
</ul>
</li>
</ul>
<!-- **Technical Note:** When we write that $Var(\varepsilon_i|X)=\sigma^2$ or $Var(\varepsilon_i|X_i)=\sigma^2_i,$ we implicitly assume that these second moments exists and that they are finite.  --><section id="homoskedastic-versus-heteroskedastic-error-terms" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="homoskedastic-versus-heteroskedastic-error-terms">Homoskedastic versus Heteroskedastic Error Terms</h4>
<p>The i.i.d. assumption is not as restrictive as it may seem on first sight. It allows for dependence between <span class="math inline">\(\varepsilon_i\)</span> and <span class="math inline">\((X_{i1},\dots,X_{iK})\in\mathbb{R}^K\)</span>. That is, the error term <span class="math inline">\(\varepsilon_i\)</span> can have a conditional distribution which depends on <span class="math inline">\((X_{i1},\dots,X_{iK})\)</span>; see <a href="02-Probability.html#sec-condDistr" class="quarto-xref"><span>Section 2.2.2.5</span></a>.</p>
<!-- However, we need to rule out one certain 
dependency between $\varepsilon_i$ and $X_i$; namely the conditional mean of $\varepsilon_i$ is not 
allowed to depend on $X_i$ (see Assumption 2: Exogeneity). -->
<!-- Example: $\varepsilon|X_i\sim U[-0.5|X_{i2}+X_{i3}|, 0.5|X_{i2}+X_{i3}|]$, 
with $X_{i2}\sim U[-4,4]$. This error term is mean independent of $X_i$ since 
$E(\varepsilon_i|X_i)=0$, but it has heteroskedastic conditional variance 
$Var(\varepsilon_i|X_i)=\frac{1}{12}X_i^2$. -->
<p>The exogeneity assumption (Assumption 2: Exogeneity) requires that the conditional mean of <span class="math inline">\(\varepsilon_i\)</span> is independent of <span class="math inline">\(X_i\)</span>. Besides this, dependencies between <span class="math inline">\(\varepsilon_i\)</span> and <span class="math inline">\(X_{i1},\dots,X_{iK}\)</span> are allowed. For instance, the variance of <span class="math inline">\(\varepsilon_i\)</span> can be a function of <span class="math inline">\(X_{i1},\dots,X_{iK}.\)</span> If this is the case, <span class="math inline">\(\varepsilon_i\)</span> is said to be <strong>“heteroskedastic.”</strong></p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-heteroskedastic" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.1 (Heteroskedastic Error Terms)</strong></span> The error terms <span class="math inline">\(\varepsilon_i\)</span> are called heteroskedastic if the conditional variance <span class="math display">\[
Var(\varepsilon_i|X_i=x_i)=\sigma^2(x_i)
\]</span> equals a <strong>non-constant</strong> variance function <span class="math inline">\(\sigma^2(x_i)&gt;0\)</span> for every possible realization <span class="math inline">\(X_i=x_i.\)</span></p>
</div>
</div>
</div>
</div>
<p><strong>Example:</strong> <span class="math display">\[
\varepsilon_i|X_i\sim U[-0.5|X_{i2}|, 0.5|X_{i2}|],
\]</span> with <span class="math display">\[
X_{i2}\sim U[-4,4]
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span> This error term is mean independent of <span class="math inline">\(X_i\)</span> since <span class="math inline">\(E(\varepsilon_i|X_i)=0,\)</span> but it has a heteroskedastic conditional variance since <span class="math display">\[
Var(\varepsilon_i|X_i)=\frac{1}{12}X_{i2}^2
\]</span> depends on <span class="math inline">\(X_{i2}.\)</span></p>
<p>Sometimes, we need to be more restrictive by assuming that also the variances of the error terms <span class="math inline">\(\varepsilon_i\)</span> are independent of <span class="math inline">\(X_i;\)</span> for instance, to do small sample inference (see <a href="06-Small-Sample-Inference.html" class="quarto-xref"><span>Chapter 6</span></a>).</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-heteroskedastic" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.2 (Homoskedastic Error Terms)</strong></span> The error terms <span class="math inline">\(\varepsilon_i\)</span> are called homoskedastic if the conditional variance <span class="math display">\[
Var(\varepsilon_i|X_i=x_i)=\sigma^2
\]</span> equals some <strong>constant</strong> <span class="math inline">\(\sigma^2&gt;0\)</span> for every possible realization <span class="math inline">\(X_i=x_i.\)</span></p>
</div>
</div>
</div>
</div>
<p><strong>Example:</strong> <span class="math display">\[
\varepsilon_i\sim{\mathcal N} (0, \sigma^2)
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span> Here, the conditional variance of the error terms <span class="math inline">\(\varepsilon_i\)</span> given <span class="math inline">\(X_i\)</span> <span class="math display">\[
Var(\varepsilon_i|X_i)=Var(\varepsilon_i)=\sigma^2
\]</span> are equal to the constant <span class="math inline">\(\sigma^2&gt;0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and for every possible realization of <span class="math inline">\(X_i.\)</span></p>
</section><section id="some-implications-of-the-exogeneity-assumption-ass-2" class="level3" data-number="5.1.1"><h3 data-number="5.1.1" class="anchored" data-anchor-id="some-implications-of-the-exogeneity-assumption-ass-2">
<span class="header-section-number">5.1.1</span> Some Implications of the Exogeneity Assumption (Ass 2)</h3>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-a" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.1 (Unconditional Mean)</strong></span> If <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> then the also the unconditional mean of the error term is zero, i.e. <span class="math display">\[
E(\varepsilon_i)=0,\quad i=1,\dots,n.
\]</span></p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Using the <em>Law of Total Expectations</em> (i.e., <span class="math inline">\(E[E(Z|X)]=E(Z)\)</span>) we can rewrite <span class="math inline">\(E(\varepsilon_i)\)</span> as <span class="math display">\[
E(\varepsilon_i)=E[E(\varepsilon_i|X_i)]
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span> But the exogeneity assumption yields <span class="math display">\[
E[E(\varepsilon_i|X_i)]=E[0]=0
\]</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> which completes the proof. <span class="math inline">\(\square\)</span></p>
</div>
<p>Generally, two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be <strong>orthogonal</strong> if their cross moment is zero, i.e.&nbsp;if <span class="math display">\[
E(XY)=0.
\]</span> The exogeneity assumption (Assumption 2) is sometimes also called “orthogonality” assumption, due to the following result:</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-b" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.2 (Orthogonality)</strong></span> If <span class="math inline">\(E(\varepsilon_i|X_{i})=0\)</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> then the regressors and the error term are orthogonal to each other, i.e, <span class="math display">\[
E(X_{ik}\varepsilon_i)=0
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and all <span class="math inline">\(k=1,\dots,K.\)</span></p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
E(X_{ik}\varepsilon_i)
&amp;=E(E(X_{ik}\varepsilon_i|X_{ik}))\quad{\text{(By the Law of Total Expectations)}}\\
&amp;=E(X_{ik}E(\varepsilon_i|X_{ik}))\quad{\text{(By the linearity of cond. expectations)}}
\end{align*}\]</span> Now, to show that <span class="math inline">\(E(X_{ik}\varepsilon_i)=0\)</span>, we need to show that <span class="math inline">\(E(\varepsilon_i|X_{ik})=0,\)</span> which is done in the following:</p>
<p>Since <span class="math inline">\(X_{ik}\)</span> is an element of <span class="math inline">\(X_i,\)</span> a slightly more sophisticated use of the <em>Law of Total Expectations</em> (i.e., <span class="math inline">\(E(Y|X)=E(E(Y|X,Z)|X)\)</span>) implies that <span class="math display">\[
E(\varepsilon_i|X_{ik})=E(E(\varepsilon_i|X_i)|X_{ik}).
\]</span> So, the exogeneity assumption, <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> yields <span class="math display">\[
E(\varepsilon_i|X_{ik})=E(\underbrace{E(\varepsilon_i|X_i)}_{=0}|X_{ik})=E(0|X_{ik})=0.
\]</span> I.e., we have that <span class="math inline">\(E(\varepsilon_i|X_{ik})=0\)</span> which allows us to conclude that <span class="math display">\[
E(X_{ik}\varepsilon_i)=E(X_{ik}E(\varepsilon_i|X_{ik}))=E(X_{ik}0)=0
\]</span> which completes the proof. <span class="math inline">\(\square\)</span></p>
</div>
<p>Because the mean of the error term is zero (<span class="math inline">\(E(\varepsilon_i)=0\)</span> for all <span class="math inline">\(i\)</span> (see <a href="#thm-a" class="quarto-xref">Theorem&nbsp;<span>5.1</span></a>), it follows that the orthogonality property, <span class="math inline">\(E(X_{ik}\varepsilon_i)=0,\)</span> is equivalent to a zero correlation property.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-c" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.3 (No Correlation)</strong></span> If <span class="math inline">\(E(\varepsilon_i|X_{i})=0\)</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> then <span class="math display">\[\begin{eqnarray*}
  Cov(\varepsilon_i,X_{ik})&amp;=&amp;0\quad\text{for all}\quad i=1,\dots,n\quad\text{and all}\quad k=1,\dots,K.
\end{eqnarray*}\]</span></p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{eqnarray*}
  Cov(\varepsilon_i,X_{ik})&amp;=&amp;E(X_{ik}\varepsilon_i)-E(X_{ik})\,E(\varepsilon_i)\quad{\small\text{(Def. of Cov)}}\\[2ex]
  &amp;&amp;\text{By the mean zero result, $E(\varepsilon_i)=0,$ shown above:}\\[2ex]
  &amp;=&amp;E(X_{ik}\varepsilon_i)\\[2ex]
  &amp;&amp;\text{By the orthogonality result shown above:}\\[2ex]
  &amp;=&amp;0\quad\square
\end{eqnarray*}\]</span></p>
</div>
<!-- ## The Algebra of Least Squares -->
<!-- In this section, we take the point of view where $(Y^{obs},X^{obs})$ is an observed data-set, i.e., a given realization of the random sample $((Y_1,X_1),\dots,(Y_n,X_n))$. That is, we are not yet interested in the statistical properties of estimators, but are only in doing some linear algebra.  -->
<!-- ### Exogeneity and Causality -->
<!-- https://allthingsstatistics.com/miscellaneous/exogeneity-assumption/ -->
<!--  The exogeneity assumption (Assumption 2) means that the dependent variable $Y_i$ does not causally affect the independent variables $X_i$.

$$
Y_i = \beta_1 + \beta_2 X_i + \varepsilon_i
$$

* $Y_i$: Depression test score of person $i$
* $X_i$: Number of cigarettes smoked per day by person $i$
* $\beta$: Mean effect of smoking on depression

Can we identify $\beta$, i.e. the causal effect of smoking on depression? 

While it is reasonable that X causes Y 
$$
Y_i = \beta_1 + \beta_2 X_i + \varepsilon_i
$$
It's also reasonable that Y causes to some extend $X$
$$
X_i = \gamma_1 + \gamma_2 Y_i + u_i
$$

Thus 
$$
\begin{align*}
Y_i 
& = \beta_1 + \overbrace{\beta_2 X_i}^{=\gamma_1 + \gamma_2 Y_i + u_i} + \varepsilon_i\\
& = \beta_1 + \beta_2 X_i + \varepsilon_i\\
\end{align*}
$$




* It's very likely that smoking causes depression (X \to Y)
* However, *simultaneously* depression may cause increased smoking behavior (Y\to X)

The latter point would mean that large values of $Y_i$ 




The independent and dependent variables are correlated but the causal link is only in one direction – the independent variable affects the dependent variable not vice-versa. -->
</section></section><section id="deriving-the-estimator-hatbeta" class="level2" data-number="5.2"><h2 data-number="5.2" class="anchored" data-anchor-id="deriving-the-estimator-hatbeta">
<span class="header-section-number">5.2</span> Deriving the Estimator <span class="math inline">\(\hat\beta\)</span>
</h2>
<section id="least-squares-approach" class="level3" data-number="5.2.1"><h3 data-number="5.2.1" class="anchored" data-anchor-id="least-squares-approach">
<span class="header-section-number">5.2.1</span> Least Squares Approach</h3>
<p>We derive the expression for the OLS estimator <span class="math display">\[
\hat\beta=\begin{pmatrix}\hat\beta_1\\\vdots\\\hat\beta_K\end{pmatrix}\in\mathbb{R}^K
\]</span> as the vector-valued minimizing argument of the sum of squared residuals, <span class="math display">\[
S_n(b)=\sum_{i=1}^n\big(\underbrace{Y_i-X_i'b}_{\text{$i$th residual}}\big)^2
\]</span> with <span class="math inline">\(b\in\mathbb{R}^K.\)</span></p>
<p>Using matrix/vector notation we can write <span class="math inline">\(S(b)\)</span> as <span class="math display">\[
\begin{align*}
S_n(b)
&amp;=\sum_{i=1}^n(Y_i-X_i'b)^2\\[2ex]
&amp;=(Y-X b)^{\prime}(Y-X b)\\[2ex]
&amp;=Y^{\prime}Y-2 Y^{\prime} X b+b^{\prime} X^{\prime} X b.
\end{align*}
\]</span> To find the minimizing argument <span class="math display">\[
\hat\beta=\arg\min_{b\in\mathbb{R}^K}S_n(b)
\]</span> we compute the vector containing all partial derivatives <span class="math display">\[
\begin{aligned}
\underset{(K\times 1)}{\frac{\partial S(b)}{\partial b}} &amp;=-2\left(X^{\prime}Y -X^{\prime} Xb\right).
\end{aligned}
\]</span> Setting each partial derivative to zero leads to <span class="math inline">\(K\)</span> linear equations (“normal equations”) in <span class="math inline">\(K\)</span> unknowns. This system of linear equations defines the OLS estimator, <span class="math inline">\(\hat{\beta}.\)</span> <span id="eq-NormalEquations"><span class="math display">\[
\begin{align*}
-2\left(X^{\prime}Y -X^{\prime} X\hat{\beta}\right)
&amp;=\underset{(K\times 1)}{0}\\[2ex]
\Leftrightarrow\qquad X^{\prime} X\hat{\beta}
&amp;=\underset{(K\times 1)}{X^{\prime}Y}.
\end{align*}
\qquad(5.3)\]</span></span> From our rank assumption (Assumption 3) it follows that <span class="math inline">\(X^{\prime}X\)</span> is an invertible <span class="math inline">\((K\times K)\)</span>-dimensional matrix which allows us to solve the equation system in <a href="#eq-NormalEquations" class="quarto-xref">Equation&nbsp;<span>5.3</span></a> by <span class="math display">\[
\begin{aligned}
\underset{(K\times 1)}{\hat{\beta}} &amp;=\left(X^{\prime} X\right)^{-1} X^{\prime} Y.
\end{aligned}
\]</span></p>
<p>The following codes computes the estimate <span class="math inline">\(\hat{\beta}\)</span> for a given dataset with <span class="math inline">\(X_i\in\mathbb{R}^K\)</span>, <span class="math inline">\(K=3\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Some given data</span></span>
<span><span class="va">X_2</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.9</span>,<span class="fl">0.8</span>,<span class="fl">1.1</span>,<span class="fl">0.1</span>,<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">4.4</span>,<span class="fl">4.6</span>,<span class="fl">1.6</span>,<span class="fl">5.5</span>,<span class="fl">3.4</span><span class="op">)</span></span>
<span><span class="va">X_3</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">66</span>, <span class="fl">62</span>, <span class="fl">64</span>, <span class="fl">61</span>, <span class="fl">63</span>, <span class="fl">70</span>, <span class="fl">68</span>, <span class="fl">62</span>, <span class="fl">68</span>, <span class="fl">66</span><span class="op">)</span></span>
<span><span class="va">Y</span>       <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.7</span>,<span class="op">-</span><span class="fl">1.0</span>,<span class="op">-</span><span class="fl">0.2</span>,<span class="op">-</span><span class="fl">1.2</span>,<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">3.4</span>,<span class="fl">0.0</span>,<span class="fl">0.8</span>,<span class="fl">3.7</span>,<span class="fl">2.0</span><span class="op">)</span></span>
<span><span class="va">dataset</span> <span class="op">&lt;-</span>  <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="st">"X_2"</span> <span class="op">=</span> <span class="va">X_2</span>, <span class="st">"X_3"</span> <span class="op">=</span> <span class="va">X_3</span>, <span class="st">"Y"</span> <span class="op">=</span> <span class="va">Y</span><span class="op">)</span></span>
<span><span class="co">## Compute the OLS estimation</span></span>
<span><span class="va">lmobj</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X_2</span> <span class="op">+</span> <span class="va">X_3</span>, data <span class="op">=</span> <span class="va">dataset</span><span class="op">)</span></span>
<span><span class="co">## Plot sample regression surface</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"scatterplot3d"</span><span class="op">)</span> <span class="co"># library for 3d plots</span></span>
<span><span class="va">plot3d</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/scatterplot3d/man/scatterplot3d.html">scatterplot3d</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">X_2</span>, y <span class="op">=</span> <span class="va">X_3</span>, z <span class="op">=</span> <span class="va">Y</span>,</span>
<span>            angle <span class="op">=</span> <span class="fl">33</span>, scale.y <span class="op">=</span> <span class="fl">0.8</span>, pch <span class="op">=</span> <span class="fl">16</span>,</span>
<span>            color <span class="op">=</span><span class="st">"red"</span>, </span>
<span>            xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>,</span>
<span>            ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span><span class="op">)</span>,</span>
<span>            main <span class="op">=</span><span class="st">"OLS Regression Surface"</span><span class="op">)</span></span>
<span><span class="va">plot3d</span><span class="op">$</span><span class="fu">plane3d</span><span class="op">(</span><span class="va">lmobj</span>, lty.box <span class="op">=</span> <span class="st">"solid"</span>, col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/grDevices/gray.html">gray</a></span><span class="op">(</span><span class="fl">.5</span><span class="op">)</span>, draw_polygon<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="05-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section><section id="sec-MMEstimator" class="level3" data-number="5.2.2"><h3 data-number="5.2.2" class="anchored" data-anchor-id="sec-MMEstimator">
<span class="header-section-number">5.2.2</span> Method of Moments Estimator Approach</h3>
<p>Remember that the exogeneity assumption (Assumption 2), <span class="math display">\[
E(\varepsilon_i|X_i)=0,
\]</span> implies that <span class="math display">\[
E(X_{ik}\varepsilon_i)=0\quad\text{for all}\quad k=1,\dots,K.
\]</span> Thus, the exogeneity assumption (Assumption 2) gives us a system of <span class="math inline">\(K\)</span> linear equations: <span class="math display">\[
\left.
  \begin{array}{c}
  E(\varepsilon_i)=0\\
  E(X_{i2}\varepsilon_i)=0\\
  \vdots\\
  E(X_{iK}\varepsilon_i)=0
  \end{array}
\right\}\Leftrightarrow \underset{(K\times 1)}{E(X_i\varepsilon_i)}=\underset{(K\times 1)}{0}
\]</span></p>
<p>This linear equation system in terms of population moments (means) <span class="math display">\[
\underset{(K\times 1)}{E(X_i\varepsilon_i)}=\underset{(K\times 1)}{0}
\]</span> allows us to <strong><em>identify</em> the unknown (true) parameter vector</strong> <span class="math inline">\(\beta\in\mathbb{R}^K\)</span> in terms of population moments: <span class="math display">\[
\begin{align*}
E(X_i\varepsilon_i) &amp; = 0 \\[2ex]
E(X_i(\overbrace{Y_i - X_i'\beta}^{=\varepsilon_i})) &amp;= 0\\[2ex]
%\Leftrightarrow \hspace{1.5cm}
E(X_iY_i) - E(X_iX_i')\beta &amp; = 0\\[2ex]
E(X_iX_i')\beta &amp; =  E(X_iY_i)  \\[2ex]
\beta &amp; = \left(E(X_iX_i')\right)^{-1} E(X_iY_i)
\end{align*}
\]</span> The fundamental idea behind <strong>method of moments estimation</strong> is to define an estimator by <strong>substituting population moments by sample moment analogues</strong> (sample means): <span id="eq-MMEstimator"><span class="math display">\[
\begin{align*}
\hat\beta_{mm}
&amp; = \left(\frac{1}{n}\sum_{i=1}^n X_iX_i'\right)^{-1} \frac{1}{n}\sum_{i=1}^n X_iY_i,
\end{align*}
\qquad(5.4)\]</span></span> where <span class="math display">\[
\begin{align*}
E(X_iX_i') &amp; \quad \text{is substituted by}\quad \frac{1}{n}\sum_{i=1}^n X_iX_i'\\[2ex]
E(X_iY_i) &amp; \quad \text{is substituted by}\quad \frac{1}{n}\sum_{i=1}^n X_iY_i.
\end{align*}
\]</span></p>
<p>Note that <a href="#eq-MMEstimator" class="quarto-xref">Equation&nbsp;<span>5.4</span></a> can be further simplified as following: <span class="math display">\[
\begin{align*}
\hat\beta_{mm}
&amp; = \left(\frac{1}{n}\sum_{i=1}^n X_iX_i'\right)^{-1} \frac{1}{n}\sum_{i=1}^n X_iY_i  \\
&amp; = \left(\sum_{i=1}^n X_iX_i'\right)^{-1} \sum_{i=1}^n X_iY_i\\
&amp; = \left(X'X\right)^{-1} X'Y.\\
\end{align*}
\]</span> Thus the method of moments estimator, <span class="math inline">\(\hat\beta_{mm},\)</span> coincides with the OLS estimator <span class="math inline">\(\hat\beta\)</span> as derived above.</p>
</section><section id="method-of-moments-versus-least-squares-estimation" class="level3" data-number="5.2.3"><h3 data-number="5.2.3" class="anchored" data-anchor-id="method-of-moments-versus-least-squares-estimation">
<span class="header-section-number">5.2.3</span> Method of Moments versus Least Squares Estimation</h3>
<!-- Deriving the OLS estimator does not explicitly require a identification step; i.e. it does not require a statement that the parameter of interest, $\beta,$ can be written in terms of population moments of observables
$$
\beta = \left(E(X_iX_i')\right)^{-1} E(X_iY_i),
$$
where the latter statement only hold under Assumption 2 (exogeneity). -->
<p>The method of moments estimation approach, firstly, checks whether the parameter of interest, <span class="math inline">\(\beta,\)</span> is identified; i.e., whether <span class="math inline">\(\beta\)</span> can be written in terms of population moments of observables <span id="eq-IdentificationBeta"><span class="math display">\[
\beta = \left(E(X_iX_i')\right)^{-1} E(X_iY_i),
\qquad(5.5)\]</span></span> where the identification result of <a href="#eq-IdentificationBeta" class="quarto-xref">Equation&nbsp;<span>5.5</span></a> only holds under Assumption 2 (exogeneity). The identified parameter <span class="math inline">\(\beta\)</span> is then estimated using the simple method of moments idea of substituting population moments by sample moments.</p>
<p>Note: If Assumption 2 (exogeneity) is violated, then we generally have that <span class="math display">\[
\beta \neq \tilde \beta = \left(E(X_iX_i')\right)^{-1} E(X_iY_i).
\]</span></p>
<p>The derivation of the OLS estimator <span class="math display">\[
\begin{align*}
\hat\beta
&amp; = \left(X'X\right)^{-1} X'Y\\[2ex]
&amp; = \left(\frac{1}{n}\sum_{i=1}^n X_iX_i'\right)^{-1} \frac{1}{n}\sum_{i=1}^n X_iY_i  \\
&amp; = \underbrace{\left(\sum_{i=1}^n X_iX_i'\right)^{-1}}_{\approx (E(X_iX_i'))^{-1}} \; \underbrace{\sum_{i=1}^n X_iY_i}_{\approx E(X_iY_i)}
\end{align*}
\]</span> does not require a check whether <span class="math inline">\(\beta\)</span> is actually identified. By the law of large numbers (sample means converge in probability to population means; see <a href="07-Asymptotics.html" class="quarto-xref"><span>Chapter 7</span></a>), the OLS estimator is a consistent estimator of <span class="math inline">\(\tilde\beta,\)</span> <span class="math display">\[
\begin{align*}
\hat\beta \to_p  \tilde\beta = (E(X_iX_i'))^{-1} E(X_iY_i) \quad \text{as}\quad n\to\infty.
\end{align*}
\]</span></p>
<p>That is, the OLS estimator will estimate the parameter of interest <span class="math inline">\(\beta\)</span> only if<br><span class="math display">\[
\beta = \tilde \beta = \left(E(X_iX_i')\right)^{-1} E(X_iY_i);
\]</span> i.e.&nbsp;only if <span class="math inline">\(\beta\)</span> is identified.</p>
<p>Without identification, the OLS estimator estimates <span class="math inline">\(\tilde\beta,\)</span> but generally not our parameter of interest <span class="math inline">\(\beta.\)</span></p>
<p><strong>Summary:</strong> So, at the end, both estimation approaches require that the parameter of interest, <span class="math inline">\(\beta,\)</span> is identified. The method of moments estimation approach starts with checking the identification requirement and thus frames the identification requirement more prominently as what it is: a pre-request for consistent estimation of <span class="math inline">\(\beta.\)</span></p>
</section></section><section id="some-quantities-of-interest" class="level2" data-number="5.3"><h2 data-number="5.3" class="anchored" data-anchor-id="some-quantities-of-interest">
<span class="header-section-number">5.3</span> Some Quantities of Interest</h2>
<p><strong>Predicted values and residuals.</strong></p>
<ul>
<li>The (OLS) <strong>predicted values</strong>: <span class="math display">\[
\hat{Y}_i=X_i'\hat\beta, \quad i=1,\dots,n
\]</span> The <span class="math inline">\((n\times 1)\)</span> vector of predicted values <span class="math display">\[
\begin{align*}
\hat{Y} = \left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)
&amp;=X\hat{\beta}\\[-2ex]
&amp;=\underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\[2ex]
&amp;=P_X Y
\end{align*}
\]</span>
</li>
<li>The (OLS) <strong>residuals</strong>: <span class="math display">\[
\hat\varepsilon_i=Y_i-\hat{Y}_i, \quad i=1,\dots,n
\]</span> The <span class="math inline">\((n\times 1)\)</span> vector of residuals <span class="math display">\[
\begin{align*}
\hat{\varepsilon} =
\left(\begin{matrix}\hat{\varepsilon}_1\\\hat{\varepsilon}_2\\ \vdots\\ \hat{\varepsilon}_n\end{matrix}\right)
&amp;=
\left(\begin{matrix}Y_1\\[.5ex]Y_2\\[.5ex] \vdots\\[.5ex] Y_n\end{matrix}\right)-
\left(\begin{matrix}\hat{Y}_1\\\hat{Y}_2\\ \vdots\\ \hat{Y}_n\end{matrix}\right)\\[2ex]
&amp;=Y - \hat{Y}\\[2ex]
%&amp;=Y - X\hat{\beta}\\[-2ex]
%&amp;=Y - \underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\[2ex]
&amp;=Y - P_X Y\\[2ex]
&amp;=\underbrace{(I_n - P_X)}_{=M_X} Y\\[2ex]
&amp;=M_XY
\end{align*}
\]</span>
</li>
</ul>
<p><strong>Projection matrices.</strong></p>
<p>The matrix <span class="math display">\[
P_X=X(X'X)^{-1}X'
\]</span> is the <span class="math inline">\((n\times n)\)</span> <strong>projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the column space spanned by the column vectors of <span class="math inline">\(X\)</span> and <span class="math display">\[
M_X=I_n-X(X'X)^{-1}X'=I_n-P_X
\]</span> is the associated <span class="math inline">\((n\times n)\)</span> <strong>orthogonal projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the vector space that is orthogonal to that spanned by the column vectors of <span class="math inline">\(X.\)</span></p>
<p>The projection matrices <span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> have some nice properties:</p>
<ol type="a">
<li>
<span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> are <strong>symmetric</strong>, i.e.&nbsp; <span class="math display">\[
P_X=P_X'\quad\text{ and }\quad M_X=M_X'
\]</span>
</li>
<li>
<span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> are <strong>idempotent</strong>, i.e.&nbsp; <span class="math display">\[
P_XP_X=P_X\quad\text{ and }\quad M_X M_X=M_X
\]</span>
</li>
<li>Moreover, we have that
<ul>
<li>
<span class="math inline">\(X'P_X=X'\)</span> and <span class="math inline">\(P_X X=X\)</span>
</li>
<li>
<span class="math inline">\(X'M_X=0\)</span> and <span class="math inline">\(M_XX=0\)</span>
</li>
<li>
<span class="math inline">\(P_XM_X=0\)</span> and <span class="math inline">\(M_XP_X=0\)</span>
</li>
</ul>
</li>
</ol>
<p>The properties (a)-(c) follow directly from the definitions of <span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> (check it out).</p>
<p>Using properties (a)-(c), one can show that the residual vector <span class="math inline">\(\hat\varepsilon=(\hat\varepsilon_1,\dots,\hat\varepsilon_n)'\)</span> is orthogonal to each of the column vectors in <span class="math inline">\(X\)</span>, i.e <span id="eq-ResidRestrictions"><span class="math display">\[
\begin{eqnarray}
X'\hat\varepsilon&amp;=&amp;X'M_XY\quad{\small\text{(By Def. of $M_X$)}}\\[2ex]
\Leftrightarrow\;\; X'\hat\varepsilon&amp;=&amp;\underset{(K\times n)}{0}\underset{(n\times 1)}{Y}\quad{\small\text{(since $X'M_X=0$)}}\\[2ex]
\Leftrightarrow\;\; X'\hat\varepsilon&amp;=&amp;\underset{(K\times 1)}{0}.
\end{eqnarray}
\qquad(5.6)\]</span></span> Note that, in the case with intercept, <a href="#eq-ResidRestrictions" class="quarto-xref">Equation&nbsp;<span>5.6</span></a> implies that <span class="math display">\[
\begin{align*}
\sum_{i=1}^n 1\cdot \hat\varepsilon_i&amp; = 0\\[2ex]
\Leftrightarrow\;\;\underbrace{\frac{1}{n}\sum_{i=1}^n \hat\varepsilon_i}_{=\bar{\hat\varepsilon}} &amp; = 0\\[2ex]
\end{align*}
\]</span></p>
<p>Moreover, the equation <span class="math inline">\(X'\hat\varepsilon=0\)</span> implies also that the residual vector <span class="math inline">\(\hat{\varepsilon}\)</span> is orthogonal to the predicted values vector, since <span class="math display">\[
\begin{align*}
X'\hat\varepsilon&amp;=0\\
\Rightarrow\;\hat\beta'X'\hat\varepsilon&amp;=\hat\beta'0\\
\Leftrightarrow\;\hat Y'\hat\varepsilon&amp;=0.
\end{align*}
\]</span></p>
<p>Another insight from <a href="#eq-ResidRestrictions" class="quarto-xref">Equation&nbsp;<span>5.6</span></a> is that the vector <span class="math inline">\(\hat\varepsilon\)</span> has to satisfy <span class="math inline">\(K\)</span> linear restrictions which means we loose <span class="math inline">\(K\)</span> degrees of freedom in the data. Consequently, the vector of residuals <span class="math inline">\(\hat\varepsilon\)</span> has only <span class="math inline">\(n-K\)</span> so-called <em>degrees of freedom</em>. This loss of <span class="math inline">\(K\)</span> degrees of freedom also appears in the definition of the <em>unbiased</em> variance estimator <span class="math display">\[
s_{UB}^2=\frac{1}{n-K}\sum_{i=1}^n\hat\varepsilon_i^2.
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <span class="math inline">\(K\)</span> linear restrictions follow from the fact that <span class="math display">\[
X'\hat\varepsilon=\underset{(K\times 1)}{0}
\]</span> is a linear system of <span class="math inline">\(K\)</span> equations. That is, for each <span class="math inline">\(k=1,\dots,K\)</span> the residuals <span class="math inline">\(\hat\varepsilon_1,\dots,\hat\varepsilon_n\)</span> need to fulfill the equation<br><span class="math display">\[
\sum_{i=1}^nX_{ik}\hat\varepsilon_i=0.
\]</span></p>
</div>
</div>
<p><strong>Variance decomposition:</strong> A further useful result that can be shown using the properties of <span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> is that <span class="math inline">\(Y'Y=\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon\)</span>, i.e. <span class="math display">\[\begin{eqnarray*}
Y'Y&amp;=&amp;(\hat Y+\hat\varepsilon)'(\hat Y+\hat\varepsilon)\notag\\
  &amp;=&amp;(P_XY+M_XY)'(P_XY+M_XY)\notag\\
  &amp;=&amp;(Y'P_X'+Y'M_X')(P_XY+M_XY)\notag\\
  &amp;=&amp;Y'P_X'P_XY+Y'M_X'M_XY+0\notag\\
  &amp;=&amp;\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon
\end{eqnarray*}\]</span> The decomposition <span class="math display">\[
\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon
\]</span> is the basis for the well-known variance decomposition result for OLS regressions.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-vardecomp" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.4</strong></span> For the linear regression model with intercept (<a href="#eq-LinMod" class="quarto-xref">Equation&nbsp;<span>5.1</span></a>), the total sample variance of the dependent variable <span class="math inline">\(Y_1,\dots,Y_n\)</span> can be decomposed as following: <span class="math display">\[\begin{eqnarray}
\underset{\text{total sample variance}}{\frac{1}{n}\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}&amp;=&amp;\underset{\text{explained sample variance}}{\frac{1}{n}\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}+\underset{\text{unexplained sample variance}}{\frac{1}{n}\sum_{i=1}^n\hat\varepsilon_i^2,}\label{VarDecomp}
\end{eqnarray}\]</span> where <span class="math inline">\(\bar{Y}=\frac{1}{n}\sum_{i=1}^nY_i\)</span> and <span class="math inline">\(\bar{\hat{Y}}=\frac{1}{n}\sum_{i=1}^n\hat{Y}_i\)</span>.</p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From equation <span class="math inline">\(X'\hat\varepsilon=0\)</span> we have for regressions with intercept that <span class="math inline">\(\sum_{i=1}^n\hat\varepsilon_i=0\)</span>. Hence, from <span class="math inline">\(Y_i=\hat{Y}_i+\hat\varepsilon_i\)</span> it follows that <span class="math display">\[\begin{eqnarray*}
  \frac{1}{n}\sum_{i=1}^n Y_i&amp;=&amp;\frac{1}{n}\sum_{i=1}^n \hat{Y}_i+\frac{1}{n}\sum_{i=1}^n \hat\varepsilon_i\\
  \bar{Y}&amp;=&amp;\bar{\hat{Y}}+0
\end{eqnarray*}\]</span></p>
<p>Using the decomposition <span class="math inline">\(Y'Y=\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon\)</span>, we can now derive the result: <span class="math display">\[\begin{eqnarray*}
   Y'Y&amp;=&amp;\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon\\
   Y'Y-n\bar{Y}^2&amp;=&amp;\hat{Y}'\hat{Y}-n\bar{Y}^2+\hat\varepsilon'\hat\varepsilon\\
   Y'Y-n\bar{Y}^2&amp;=&amp;\hat{Y}'\hat{Y}-n\bar{\hat{Y}}^2+\hat\varepsilon'\hat\varepsilon\quad\text{(by $\bar{Y}=\bar{\hat{Y}}$)}\\
   \sum_{i=1}^nY_i^2-n\bar{Y}^2&amp;=&amp;\sum_{i=1}^n\hat{Y}_i^2-n\bar{\hat{Y}}^2+\sum_{i=1}^n\hat\varepsilon_i^2\\
   \sum_{i=1}^n(Y_i-\bar{Y})^2&amp;=&amp;\sum_{i=1}^n(\hat{Y}_i-\bar{\hat{Y}})^2+\sum_{i=1}^n\hat\varepsilon_i^2\quad\square\\
\end{eqnarray*}\]</span></p>
</div>
<section id="coefficients-of-determination-r2-and-overliner2" class="level4 unnumbered"><h4 class="unnumbered anchored" data-anchor-id="coefficients-of-determination-r2-and-overliner2">Coefficients of determination: <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\overline{R}^2\)</span>
</h4>
<p>The larger the proportion of the explained variance, the better is the fit of the model. This motivates the definition of the so-called <span class="math inline">\(R^2\)</span> coefficient of determination: <span class="math display">\[\begin{eqnarray*}
R^2=\frac{\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\;=\;1-\frac{\sum_{i=1}^n\hat{\varepsilon}_i^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}
\end{eqnarray*}\]</span></p>
<ul>
<li><p>Obviously, we have that <span class="math inline">\(0\leq R^2\leq 1\)</span>.</p></li>
<li><p>The closer <span class="math inline">\(R^2\)</span> lies to <span class="math inline">\(1\)</span>, the better is the fit of the model to the observed data.</p></li>
</ul>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>A high/low <span class="math inline">\(R^2\)</span> value only means that the predictors have high/low <em>predictive power</em>.</p></li>
<li><p>A high/low <span class="math inline">\(R^2\)</span> does not mean a validation/falsification of the estimated model. Any econometric model needs a plausible explanation from relevant economic theory.</p></li>
<li><p>The most often criticized disadvantage of the <span class="math inline">\(R^2\)</span> is that additional regressors (relevant or not) will increase the <span class="math inline">\(R^2\)</span>. The below <code>R</code>-codes demonstrates this problem.</p></li>
</ul>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span>     <span class="op">&lt;-</span> <span class="fl">100</span>                  <span class="co"># Sample size</span></span>
<span><span class="va">X</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>      <span class="co"># Relevant X variable</span></span>
<span><span class="va">X_ir</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">5</span>, <span class="fl">20</span><span class="op">)</span>      <span class="co"># Irrelevant X variable</span></span>
<span><span class="va">error</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">*</span><span class="fl">10</span>    <span class="co"># True error</span></span>
<span><span class="va">Y</span>     <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">X</span> <span class="op">+</span> <span class="va">error</span>    <span class="co"># Y variable</span></span>
<span><span class="va">lm1</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span><span class="op">~</span><span class="va">X</span><span class="op">)</span><span class="op">)</span>     <span class="co"># Correct OLS regression </span></span>
<span><span class="va">lm2</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span><span class="op">~</span><span class="va">X</span><span class="op">+</span><span class="va">X_ir</span><span class="op">)</span><span class="op">)</span><span class="co"># OLS regression with X_ir </span></span>
<span><span class="va">lm1</span><span class="op">$</span><span class="va">r.squared</span> <span class="op">&lt;</span> <span class="va">lm2</span><span class="op">$</span><span class="va">r.squared</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
<p>So, <span class="math inline">\(R^2\)</span> increases here even though <code>X_ir</code> is a completely irrelevant explanatory variable.</p>
<p>Because of this, the <span class="math inline">\(R^2\)</span> cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called <strong>adjusted</strong> <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\overline{R}^2,\)</span> defined as <span class="math display">\[\begin{eqnarray*}
  \overline{R}^2&amp;=&amp;1-\frac{\frac{1}{n-K}\sum_{i=1}^n\hat{\varepsilon}^2_i}{\frac{1}{n-1}\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\leq R^2%\\
  %=\dots=
  %&amp;=&amp;1-\frac{n-1}{n-K}\left(1-R^2\right)\quad{\small\text{(since $1-R^2=(\sum_i\hat\varepsilon_i^2)/(\sum_i(Y_i-\bar{Y}))$)}}\\
  %&amp;=&amp;1-\frac{n-1}{n-K}+\frac{n-1}{n-K}R^2\quad+\frac{K-1}{n-K}R^2-\frac{K-1}{n-K}R^2\\
  %&amp;=&amp;1-\frac{n-1}{n-K}+R^2\quad+\frac{K-1}{n-K}R^2\\
  %&amp;=&amp;-\frac{K-1}{n-K}+R^2\quad+\frac{K-1}{n-K}R^2\\
  %&amp;=&amp;R^2-\underbrace{\frac{K-1}{n-K}\left(1-R^2\right)}_{\geq 0\;\text{and}\;\leq(K-1)/(n-K)}\;\leq\;R^2
\end{eqnarray*}\]</span> The adjustment is in terms of the degrees of freedom <span class="math inline">\(n-K\)</span>.</p>
</section></section><section id="sec-GMTheorem" class="level2" data-number="5.4"><h2 data-number="5.4" class="anchored" data-anchor-id="sec-GMTheorem">
<span class="header-section-number">5.4</span> The Gauss-Markov Theorem</h2>
<div class="callout callout-style-simple callout-none no-icon" title="">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The OLS estimator <span class="math inline">\(\hat\beta\)</span> is</p>
<ul>
<li>a <strong>linear</strong> (in <span class="math inline">\(Y\)</span>) and</li>
<li>an <strong>unbiased</strong> estimator</li>
</ul>
<p>of <span class="math inline">\(\beta\)</span>.</p>
</div>
</div>
</div>
<p><strong>Showing linearity of <span class="math inline">\(\hat{\beta}\)</span>:</strong></p>
<p>A function <span class="math inline">\(f(Y)\)</span> is called linear in <span class="math inline">\(Y\in\mathbb{R}^n\)</span> if for any two vectors <span class="math inline">\(Y_1\in\mathbb{R}^n\)</span> and <span class="math inline">\(Y_2\in\mathbb{R}^n\)</span> such that <span class="math inline">\(Y=Y_1+Y_2\)</span> and for any scalar <span class="math inline">\(a\in\mathbb{R}\)</span> <span class="math display">\[
\begin{align*}
f(Y_1+Y_2)&amp;=f(Y_1)+f(Y_2)\\[2ex]
f(aY)&amp;=af(Y).
\end{align*}
\]</span> This property applies to the OLS estimator <span class="math display">\[
\hat\beta=f(Y)=(X'X)^{-1}X'Y
\]</span> since <span class="math display">\[
\begin{align*}
(X'X)^{-1}X'(Y_1+Y_2)
&amp; = (X'X)^{-1}X'Y_1 + (X'X)^{-1}X'Y_2
\end{align*}
\]</span> and <span class="math display">\[
\begin{align*}
(X'X)^{-1}X'aY &amp; = a (X'X)^{-1}X'Y
\end{align*}
\]</span> for any <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> such that <span class="math inline">\(Y=Y_1+Y_2\)</span> and for any <span class="math inline">\(a\in\mathbb{R}.\)</span></p>
<p><strong>Showing unbiasedness of <span class="math inline">\(\hat{\beta}\)</span>:</strong></p>
<p>The OLS estimator is unbiased if <span class="math display">\[
\operatorname{Bias}(\hat\beta) = E(\hat\beta) - \beta = 0
\]</span> This can be shown as following: Observe that <span class="math display">\[
\hat\beta=(X'X)^{-1}X'Y
\]</span> consists of two multivariate random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y.\)</span> Thus one needs to show first the conditional unbiasedness of <span class="math inline">\(\hat\beta\)</span> given <span class="math inline">\(X\)</span> which effectively allows us to focus on randomness due to <span class="math inline">\(\varepsilon,\)</span><br><span class="math display">\[
\begin{align*}
\operatorname{Bias}(\hat\beta|X)
&amp;= E(\hat\beta|X)                                        - \beta \\[2ex]
&amp;= E((X'X)^{-1}X'\underbrace{Y}_{=X\beta+\varepsilon}|X) - \beta \\[2ex]
&amp;= E((X'X)^{-1}X'(X\beta+\varepsilon)|X)                 - \beta \\[2ex]
&amp;= E(\underbrace{(X'X)^{-1}X'X}_{=I_K}\beta|X) + E((X'X)^{-1}X'\varepsilon|X) - \beta \\[2ex]
&amp;= \underbrace{E(\beta|X)}_{=\beta} + \underbrace{E((X'X)^{-1}X'\varepsilon|X)}_{=(X'X)^{-1}X'E(\varepsilon|X)} - \beta \\[2ex]
&amp;=  (X'X)^{-1}X'\underbrace{E(\varepsilon|X)}_{=0} =\underset{(K\times 1)}{0}  
\end{align*}
\]</span> Thus <span class="math inline">\(\hat\beta\)</span> is unbiased conditionally on <span class="math inline">\(X.\)</span> From this if follows, by the iterated law of expectations, that the OLS estimator is also unconditionally unbiased, i.e.<br><span class="math display">\[
\operatorname{Bias}(\hat\beta) = E\left(\operatorname{Bias}(\hat\beta|X)\right) = E(0) = 0.
\]</span></p>
<p>For the Gauss-Markov Theorem, we also need the <strong>conditional variance of</strong> <span class="math inline">\(\hat\beta\)</span> <strong>given</strong> <span class="math inline">\(X,\)</span> which can be derived as following: <span class="math display">\[
\begin{align*}
Var(\hat\beta|X)
&amp;=Var(\;\highlight{\hat\beta - \beta}\;|X)\\[2ex]
&amp;=Var((X'X)^{-1}X'\varepsilon|X),
\end{align*}
\]</span> where we used that <span class="math display">\[
\begin{align*}
\hat\beta
&amp;=(X'X)^{-1}X'Y\\
&amp;=(X'X)^{-1}X'(X\beta+\varepsilon)\\[2ex]
&amp;=\beta+(X'X)^{-1}X'\varepsilon\\[2ex]
\Leftrightarrow \highlight{\hat\beta - \beta}
&amp; = \highlight{(X'X)^{-1}X'\varepsilon}.
\end{align*}
\]</span></p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-CondVarMult" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.3 (Conditional Variance of a Multivariate Random Variable)</strong></span> The conditional variance of a multivariate random variable <span class="math inline">\(Z\in\mathbb{R}^K,\)</span> given <span class="math inline">\(X,\)</span> is defined as <span class="math display">\[
\begin{align*}
Var(Z|X)
&amp;=\underbrace{E\Big[\overbrace{(Z-E(Z|X))}^{(K\times 1)}\overbrace{(Z-E(Z|X))'}^{(1\times K)}|X\Big]}_{K\times K}\\[2ex]
&amp;=\underbrace{E\big[ZZ'|X\big]}_{K\times K} - \underbrace{E\big[Z|X\big]E\big[Z'|X\big]}_{K\times K}
\end{align*}
\]</span></p>
</div>
</div>
</div>
</div>
<p>Using the definition of the conditional variance (<a href="#def-CondVarMult" class="quarto-xref">Definition&nbsp;<span>5.3</span></a>) for multivariate random variables <span class="math inline">\((Z=(X'X)^{-1}X'\varepsilon)\)</span> we have that <span class="math display">\[
\begin{align*}
&amp;Var(\hat\beta|X)=\\[2ex]
%&amp;=Var(\hat\beta - \beta|X)\\[2ex]
&amp;=Var((X'X)^{-1}X'\varepsilon|X)\\[2ex]
&amp;=E\Big[\big((X'X)^{-1}X'\varepsilon-\underbrace{E((X'X)^{-1}X'\varepsilon|X)}_{=0}\big)\times\\[2ex]
&amp;\phantom{=\Big(}\,\times\big((X'X)^{-1}X'\varepsilon-\underbrace{E((X'X)^{-1}X'\varepsilon|X)}_{=0}\big)'|X\Big]\\[2ex]
&amp;=E\left[((X'X)^{-1}X'\varepsilon)((X'X)^{-1}X'\varepsilon)'|X\right]\\[2ex]
&amp;=E\left[(X'X)^{-1}X'\varepsilon\varepsilon' X(X'X)^{-1}|X\right]\\[2ex]
&amp;=\;\;\;(X'X)^{-1}X'\underbrace{E\left(\varepsilon\varepsilon'|X\right)}_{=Var(\varepsilon|X)}X(X'X)^{-1}
\end{align*}
\]</span></p>
<p>Under the assumption of <strong>spherical errors</strong> (see Assumption 4), we have that <span class="math display">\[
\begin{align*}
Var\left(\varepsilon|X\right)
&amp; = \left(\begin{matrix}
Var(\varepsilon_1|X)&amp;Cov(\varepsilon_1,\varepsilon_2|X)&amp;\dots&amp;Cov(\varepsilon_1,\varepsilon_n|X)\\
Cov(\varepsilon_2,\varepsilon_1|X)&amp;Var(\varepsilon_2|X)&amp;\dots&amp;Cov(\varepsilon_2,\varepsilon_n|X)\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
Cov(\varepsilon_n,\varepsilon_1|X)&amp;Cov(\varepsilon_n,\varepsilon_2|X)&amp;\dots&amp;Var(\varepsilon_n|X)
\end{matrix}\right)\\[2ex]
&amp; = \left(\begin{matrix}
\sigma^2&amp;0&amp;\dots&amp;0\\
0&amp;\sigma^2&amp;\dots&amp;0\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
0&amp;0&amp;\dots&amp;\sigma^2
\end{matrix}\right)
= \sigma^2 I_n,
%&amp; = E\left(\varepsilon\varepsilon'|X\right) - E\left(\varepsilon|X\right) E\left(\varepsilon'|X\right)\\[2ex]
%&amp; = E\left(\varepsilon\varepsilon'|X\right) - \underset{(K\times K)}{0}\\[2ex]
\end{align*}
\]</span> where <span class="math inline">\(I_n\)</span> is the <span class="math inline">\((n\times n)\)</span> dimensional identity matrix with ones on the diagonal and zeros everywhere else.</p>
<p>Thus <strong>under the assumption of spherical errors</strong>, we have that <span class="math display">\[
\begin{align*}
Var(\hat\beta|X)
&amp;=(X'X)^{-1}X' \left(\sigma^2I_n\right)X(X'X)^{-1}\\[2ex]
&amp;=\sigma^2(X'X)^{-1}X'X(X'X)^{-1}\\[2ex]
&amp;=\sigma^2(X'X)^{-1}.
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Summary:</strong></p>
<ul>
<li>The OLS estimator belongs to the large family of <strong>linear</strong> (in <span class="math inline">\(Y\)</span>) and <strong>unbiased</strong> estimators of <span class="math inline">\(\beta.\)</span>
</li>
<li>Under the assumption of spherical errors, the <strong>conditional variance</strong> of <span class="math inline">\(\hat\beta,\)</span> given <span class="math inline">\(X,\)</span> is <span class="math display">\[
Var(\hat\beta|X)=\sigma^2(X'X)^{-1}.
\]</span>
</li>
</ul>
</div>
</div>
<p>The famous <strong>Gauss-Markov Theorem</strong> states that the OLS estimator is the <strong>“best”</strong> (smallest conditional variance) estimator within the family of linear (in <span class="math inline">\(Y\)</span>) and unbiased estimators.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-GMTheorem" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.5 (Gauss-Markov Theorem)</strong></span> If it holds true that</p>
<ol type="1">
<li><span class="math inline">\(Y = X\beta + \varepsilon\)</span></li>
<li><span class="math inline">\(E(\varepsilon|X) = \underset{(n\times 1)}{0}\)</span></li>
<li>
<span class="math inline">\(\operatorname{rank}(X)=K\)</span> a.s.</li>
<li><span class="math inline">\(Var(\varepsilon|X) = \sigma^2I_n\)</span></li>
</ol>
<p>then the OLS estimator <span class="math display">\[
\underset{(K\times 1)}{\hat\beta}=(X'X)^{-1}X'Y
\]</span> has the smallest conditional variance (given <span class="math inline">\(X\)</span>) among all <strong>linear</strong> (in <span class="math inline">\(Y\)</span>) and <strong>unbiased</strong> estimators. That is, for any alternative linear and unbiased estimator <span class="math inline">\(\tilde{\beta}\)</span> we have that <span class="math display">\[
\begin{align*}
&amp;Var(\tilde\beta|X)\geq Var(\hat\beta|X)\quad\text{(in the matrix sense)}\\[2ex]
\Leftrightarrow\quad&amp;Var(\tilde\beta|X)-Var(\hat\beta|X)=\underset{(K\times K)}{D},
\end{align*}
\]</span> where <span class="math inline">\(D\)</span> is a <em>positive semidefinite</em> <span class="math inline">\((K\times K)\)</span> matrix which implies that <span class="math display">\[
Var(\tilde{\beta}_k|X) \geq Var(\hat\beta_k | X)
\]</span> for all <span class="math inline">\(k=1,\dots,K\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remember
</div>
</div>
<div class="callout-body-container callout-body">
<p>A <span class="math inline">\((K\times K)\)</span> matrix <span class="math inline">\(D\)</span> is called “positive semidefinite” if <span class="math display">\[
a'Da\geq 0
\]</span> for any <span class="math inline">\(K\)</span>-dimensional vector <span class="math inline">\(a\in\mathbb{R}^K\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Since <span class="math inline">\(\tilde{\beta}\)</span> is assumed to be linear in <span class="math inline">\(Y\)</span>, we can write <span class="math display">\[
\tilde{\beta}=CY,
\]</span> where <span class="math inline">\(C\)</span> is some <span class="math inline">\((K\times n)\)</span> matrix, which is a function of <span class="math inline">\(X\)</span> and/or nonrandom components. Adding a <span class="math inline">\((K\times n)\)</span> zero matrix <span class="math inline">\(0\)</span> yields <span class="math display">\[
\tilde{\beta}=\Big(C\;\;\overbrace{-\left(X'X\right)^{-1}X'+\left(X'X\right)^{-1}X'\;}^{=0}\;\Big)Y.
\]</span> Let now <span class="math inline">\(D=C-\left(X'X\right)^{-1}X'\)</span>, then <span class="math display">\[
\begin{align*}
\tilde{\beta}&amp;=\left(D+\left(X'X\right)^{-1}X'\right)Y\\
\tilde{\beta}&amp;=DY + \left(X'X\right)^{-1}X'Y\\
\tilde{\beta}&amp;=D\left(X{\beta}+{\varepsilon}\right) + \left(X'X\right)^{-1}X'Y
\end{align*}
\]</span> Thus <span id="eq-c3e16"><span class="math display">\[
\tilde{\beta}=DX{\beta}+D{\varepsilon} + \hat{\beta}.
\qquad(5.7)\]</span></span> Moreover, <span class="math display">\[
E(\tilde{\beta}|X)=\underbrace{E(DX{\beta}|X)}_{=DX\beta}+\underbrace{E(D\varepsilon|X)}_{=DE(\varepsilon|X)=0}+\underbrace{E(\hat{\beta}|X)}_{=\beta}
\]</span> and thus <span id="eq-c3e17"><span class="math display">\[
E(\tilde{\beta}|X)=DX{\beta}+0+{\beta}.
\qquad(5.8)\]</span></span></p>
<p>Since <span class="math inline">\(\tilde{\beta}\)</span> is (by assumption) unbiased, we have that <span class="math inline">\(E(\tilde{\beta}|X)={\beta}\)</span>. Therefore, <a href="#eq-c3e17" class="quarto-xref">Equation&nbsp;<span>5.8</span></a> implies that <span class="math inline">\(DX=0_{(K\times K)}\)</span> since we must have that <span class="math display">\[
E(\tilde{\beta}|X)=DX{\beta}+0+{\beta}=\beta.
\]</span> Plugging <span class="math inline">\(DX=0\)</span> into <a href="#eq-c3e16" class="quarto-xref">Equation&nbsp;<span>5.7</span></a> yields, <span class="math display">\[
\begin{align*}
\tilde{\beta}&amp;=D{\varepsilon} + \hat{\beta}\\[2ex]
\tilde{\beta}-{\beta}&amp;=D{\varepsilon} + \highlight{(\hat{\beta}-{\beta})}\\[2ex]
\tilde{\beta}-{\beta}&amp;=D{\varepsilon} + \highlight{\left(X'X\right)^{-1}X'{\varepsilon}}
\end{align*}
\]</span> such that <span id="eq-c3e18"><span class="math display">\[
\tilde{\beta}-{\beta}=\left(D + \left(X'X\right)^{-1}X'\right){\varepsilon},
\qquad(5.9)\]</span></span> where we used that <span class="math display">\[
\begin{align*}
\highlight{\hat\beta-\beta}&amp;=(X'X)^{-1}X'Y-\beta\\[2ex]
&amp;=(X'X)^{-1}X'(X\beta+\varepsilon)-\beta\\[2ex]
&amp;=\highlight{(X'X)^{-1}X'\varepsilon}.
\end{align*}
\]</span></p>
<p>Using that <span class="math inline">\(Var(\tilde{\beta}|X)= Var(\tilde{\beta}-{\beta}|X)\)</span> since <span class="math inline">\(\beta\)</span> is not random and using <a href="#eq-c3e18" class="quarto-xref">Equation&nbsp;<span>5.9</span></a> yields <span class="math display">\[
\begin{align*}
Var(\tilde{\beta}|X)
&amp;= Var((D + (X'X)^{-1}X'){\varepsilon}|X)\\[2ex]
&amp;= (D + (X'X)^{-1}X')Var({\varepsilon}|X)(D' + X(X'X)^{-1})\\[2ex]
&amp;= \sigma^2(D + (X'X)^{-1}X')I_n(D' + X(X'X)^{-1})\\[2ex]
&amp;= \sigma^2\left(DD'+(X'X)^{-1}\right)\quad \text{(using that $DX=0$)} \\[2ex]
&amp;\geq\sigma^2(X'X)^{-1} \quad \text{(using that $DD'\geq 0$)}\\[2ex]
&amp;= Var(\hat{\beta}|X).
\end{align*}
\]</span> Finally, we need to show that <span class="math inline">\(DD'\)</span> is really positive semidefinite (i.e.&nbsp;<span class="math inline">\(DD'\geq 0\)</span> in matrix sense): <span class="math display">\[
\begin{align*}
a'DD'a=(D'a)'(D'a)=\tilde{a}'\tilde{a}=\sum_{k=1}^K \tilde{a}^2_k \geq 0,
\end{align*}
\]</span> where <span class="math inline">\(a\in\mathbb{R}^K\)</span> is any <span class="math inline">\(K\)</span>-dimensional vector.</p>
<!-- Remember: -->
<!-- \begin{itemize} -->
<!-- \item $(A+\hat{\beta})'=A'+\hat{\beta}'$ -->
<!-- \item $(AB)'=\hat{\beta}'A'$ -->
<!-- \item $A' = A$ $\Leftrightarrow$ $A$ is a symmetric matrix -->
<!-- \end{itemize} -->
</div>
<!-- \paragraph*{Note.}  To reiterate: The unbiasedness of $\hat{\beta}$ did not depend on any assumptions about the distribution of $\varepsilon$,  except that $E(\varepsilon|X)=0$ which follows from our Assumption 2 together with the i.i.d. assumption in Assumption 1.  Once we imposed additionally the assumption of spherical errors $E(\varepsilon\varepsilon|X)=\sigma^{2}I_n$ we can show that $\hat{\beta}$ has the smallest variance of all linear unbiased estimators.  -->
</section><section id="practice" class="level2" data-number="5.5"><h2 data-number="5.5" class="anchored" data-anchor-id="practice">
<span class="header-section-number">5.5</span> Practice</h2>
<section id="factor-variables-and-the-dummy-variable-trap" class="level3" data-number="5.5.1"><h3 data-number="5.5.1" class="anchored" data-anchor-id="factor-variables-and-the-dummy-variable-trap">
<span class="header-section-number">5.5.1</span> Factor Variables and the Dummy-Variable Trap</h3>
<p>In the following, we consider a simple linear regression model <span class="math display">\[
Y_i = \beta_1 + \beta_2 X_{i2} + \varepsilon_i
\]</span> that aims to predict wages <span class="math inline">\(Y_i\in\mathbb{R}\)</span> in the year 2008 using gender <span class="math inline">\(X_{i2}\in\{\texttt{male},\texttt{female}\}\)</span> as the only predictor.</p>
<p>Here <span class="math display">\[
X_{i2}\in\{\texttt{male},\texttt{female}\}
\]</span> is a <em>categorical</em> variable also called <strong>factor variable</strong> with two categories (two <strong>factor levels</strong>).</p>
<p>We use data provided in the accompanying materials of Stock and Watson’s <em>Introduction to Econometrics</em> textbook <span class="citation" data-cites="stock2015">(<a href="#ref-stock2015" role="doc-biblioref">Stock and Watson 2015</a>)</span>. You can download the data stored as an xlsx-file <code>cps_ch3.xlsx</code> <a href="https://github.com/lidom/Script-Econometrics-MSc/raw/main/966b9734015a6d617d09dae170c907a100bded72/data/cps_ch3.xlsx">HERE</a>.</p>
<p>Let us first prepare the dataset:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## load the 'tidyverse' package</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://tidyverse.tidyverse.org">"tidyverse"</a></span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">## load the 'readxl' package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://readxl.tidyverse.org">"readxl"</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co">## import the data into R</span></span>
<span><span class="va">cps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://readxl.tidyverse.org/reference/read_excel.html">read_excel</a></span><span class="op">(</span>path <span class="op">=</span> <span class="st">"data/cps_ch3.xlsx"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># names(cps)</span></span>
<span><span class="co"># range(cps$year)</span></span>
<span><span class="co"># range(cps$a_sex) # 1 = male, 2 = female</span></span>
<span></span>
<span><span class="co">## Data wrangling</span></span>
<span><span class="va">cps_2008</span> <span class="op">&lt;-</span> <span class="va">cps</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span></span>
<span>    wage   <span class="op">=</span> <span class="va">ahe08</span>,      <span class="co"># rename "ahe08" as "wage"    </span></span>
<span>    gender <span class="op">=</span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/fct_recode.html">fct_recode</a></span><span class="op">(</span> <span class="co"># rename factor "a_sex" as "gender"</span></span>
<span>      <span class="fu"><a href="https://forcats.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="va">a_sex</span><span class="op">)</span>,     </span>
<span>                <span class="st">"male"</span> <span class="op">=</span> <span class="st">"1"</span>,  <span class="co"># rename factor level "1" to "male"</span></span>
<span>                <span class="st">"female"</span> <span class="op">=</span> <span class="st">"2"</span> <span class="co"># rename factor level "2" to "female"</span></span>
<span>             <span class="op">)</span> </span>
<span>  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>  </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">year</span> <span class="op">==</span> <span class="fl">2008</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>     <span class="co"># Only data from year 2008</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">wage</span>, <span class="va">gender</span><span class="op">)</span>         <span class="co"># Select only the variables "wage" and "gender"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first six lines of the dataset <code>cps_2008</code> look as following:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="table caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead><tr class="header">
<th style="text-align: right;" data-quarto-table-cell-role="th">wage</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">gender</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">38.46154</td>
<td style="text-align: left;">male</td>
</tr>
<tr class="even">
<td style="text-align: right;">12.50000</td>
<td style="text-align: left;">male</td>
</tr>
<tr class="odd">
<td style="text-align: right;">17.78846</td>
<td style="text-align: left;">male</td>
</tr>
<tr class="even">
<td style="text-align: right;">30.38461</td>
<td style="text-align: left;">male</td>
</tr>
<tr class="odd">
<td style="text-align: right;">23.66864</td>
<td style="text-align: left;">male</td>
</tr>
<tr class="even">
<td style="text-align: right;">12.01923</td>
<td style="text-align: left;">female</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Above in our theoretical part, we have not mentioned the possibility of categorical, i.e., factor variables.</p>
<ul>
<li>How can one compute the estimators from non-numeric factor levels like <code>female</code> and <code>male</code>?</li>
</ul>
<p>Well, we need to transform the non-numeric factor level to numeric data. <code>R</code> and other statistical software-packages do this for us in the background.</p>
</div>
</div>
<p>Computing the estimation results:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm_obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">wage</span> <span class="op">~</span> <span class="va">gender</span>, data <span class="op">=</span> <span class="va">cps_2008</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lm_obj</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>gives</p>
<ul>
<li>
<span class="math inline">\(\hat\beta_1=\)</span> 24.98</li>
<li>
<span class="math inline">\(\hat\beta_2=\)</span> -4.1</li>
</ul>
<p>To compute these estimation results, <code>R</code> assigns to each factor level (<code>male</code> and <code>female</code>) a numeric value. To see the numeric values used by <code>R</code> one can take a look at <code>model.matrix(lm_obj)</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># this is the internally used numeric X-matrix</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">lm_obj</span><span class="op">)</span> </span>
<span><span class="va">X</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span>,<span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  (Intercept) genderfemale
1           1            0
2           1            0
3           1            0
4           1            0
5           1            0
6           1            1</code></pre>
</div>
</div>
<p>Compare this with the factor variable <code>gender</code> in the dataset <code>cps_2008</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Factor variable 'gender' in the dataset cps_2008</span></span>
<span><span class="va">cps_2008</span><span class="op">$</span><span class="va">gender</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] male   male   male   male   male   female
Levels: male female</code></pre>
</div>
</div>
<p>Thus <code>R</code> internally recodes <span class="math inline">\(X_{i2}\in\{\texttt{male}, \texttt{female}\}\)</span> as a <strong>dummy variable</strong> <span class="math inline">\(X_{i2}\in\{0,1\}\)</span> such that <span class="math display">\[
X_{i2}=
\left\{\begin{array}{ll}
0 &amp;\text{ if subject $i$ is male}\\
1 &amp;\text{ if subject $i$ is female}.
\end{array} \right.
\]</span> Therefore, we can interpret the estimation result as <span class="math display">\[
\hat\beta_1  + \hat\beta_2 X_{i2}=
\left\{\begin{array}{ll}
\hat\beta_1              &amp;\text{ if subject $i$ is male}\\
\hat\beta_1 + \hat\beta_2&amp;\text{ if subject $i$ is female}
\end{array} \right.
\]</span></p>
<p>Interpretation:</p>
<ul>
<li>The average wage of male workers in 2008 was <span class="math inline">\(\hat{\beta}_1=\)</span> 24.98 (USD/Hour).</li>
<li>The average wage of female workers in 2008 was <span class="math inline">\(\hat{\beta}_1 + \hat{\beta}_2=\)</span> 20.87 (USD/Hour).</li>
<li>The average difference in the earnings between male and female workers in 2008 is <span class="math inline">\(\hat{\beta}_2=\)</span> -4.1 (USD/Hour).</li>
</ul></section><section id="dummy-variable-trap" class="level3 unnumbered"><h3 class="unnumbered anchored" data-anchor-id="dummy-variable-trap">Dummy-Variable Trap</h3>
<p>Above, we used <code>R</code>’s automatic recoding of categorical factor variables into numeric dummy variable which is very convenient.</p>
<p>Of course, we can also construct a dummy variable for each of the levels of a factor yourself. But be careful, and do not step into the <strong>dummy variable trap</strong>!</p>
<p>Let’s construct the <span class="math inline">\(Y\)</span> variable and the <em>numeric</em> <span class="math inline">\(X\)</span> variable “by hand”:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Dependent variable</span></span>
<span><span class="va">Y</span>        <span class="op">&lt;-</span> <span class="va">cps_2008</span><span class="op">$</span><span class="va">wage</span></span>
<span></span>
<span><span class="co">## Intercept variable</span></span>
<span><span class="va">X_1</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>, times <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">cps_2008</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## 1. Dummy variable for 'female' (indicates female subjects)</span></span>
<span><span class="va">X_female</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">cps_2008</span><span class="op">$</span><span class="va">gender</span> <span class="op">==</span> <span class="st">"female"</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## 2. Dummy variable for 'male' (indicates male subjects)</span></span>
<span><span class="va">X_male</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">cps_2008</span><span class="op">$</span><span class="va">gender</span> <span class="op">==</span> <span class="st">"male"</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Construct the numeric model matrix 'X'</span></span>
<span><span class="va">X</span>        <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">X_1</span>, <span class="va">X_female</span>, <span class="va">X_male</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Computing the estimation result for <span class="math inline">\(\beta\)</span> “by hand” yields</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Computing the estimator</span></span>
<span><span class="va">beta_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">Y</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>Error <span class="cf">in</span> <span class="fu">solve.default</span>(X <span class="sc">%*%</span> <span class="fu">t</span>(X)) <span class="sc">:</span> </span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  Lapack routine dgesv<span class="sc">:</span> system is exactly singular<span class="sc">:</span> U[<span class="dv">3</span>,<span class="dv">3</span>] <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>Calls<span class="sc">:</span> .main ... eval_with_user_handlers <span class="ot">-&gt;</span> eval <span class="ot">-&gt;</span> eval <span class="ot">-&gt;</span> solve <span class="ot">-&gt;</span> solve.default</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>Execution halted</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<font color="#FF0000">An error message!</font> 🤬
</div>
</div>
<div class="callout-body-container callout-body">
<p>We stepped into the <strong>dummy variable trap</strong>!</p>
<p>That is, we constructed a <span class="math inline">\((n\times K)\)</span>-dimensional matrix <span class="math inline">\(X\)</span> for which Assumption 3 <span class="math inline">\((\operatorname{rank}(X)=K)\)</span> is violated. Therefore <span class="math inline">\((X'X)\)</span> does not have full rank and is thus not invertible.</p>
</div>
</div>
The estimation result is not computable since <span class="math inline">\((X'X)\)</span> is not invertible due to the perfect multicollinearity between the intercept and the two dummy variables <code>X_female</code> and <code>X_male</code>, i.e.&nbsp;
<center>
<code>X_1</code> = <code>X_female</code> <span class="math inline">\(+\)</span> <code>X_male</code>
</center>
<p> which violates Assumption 3 (no perfect multicollinearity).</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Use one dummy-variable less than factor levels. I.e., in this example you can either drop <code>X_female</code> or <code>X_male</code>.</p>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## New model matrix after dropping X_male:</span></span>
<span><span class="va">X</span>        <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">X_1</span>, <span class="va">X_female</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Computing the estimator</span></span>
<span><span class="va">beta_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">Y</span></span>
<span><span class="va">beta_hat</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              [,1]
X_1      24.978404
X_female -4.103626</code></pre>
</div>
</div>
<p>This give the same result as computed by <code>R</code>’s <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function using the factor variable <code>gender</code>.</p>
</section><section id="detecting-heteroskedasticity" class="level3" data-number="5.5.2"><h3 data-number="5.5.2" class="anchored" data-anchor-id="detecting-heteroskedasticity">
<span class="header-section-number">5.5.2</span> Detecting Heteroskedasticity</h3>
<p>A very simple, but highly effective way to check for heteroskedastic error terms is to plot the residuals. If we have the correct model (hopefully we have), then (and only then) the residuals <span class="math inline">\(\hat\varepsilon_i\)</span> are good approximations to the realizations of the error terms <span class="math inline">\(\varepsilon_i\)</span>. Thus a plot of the residuals can be used to check for heteroskedasticity.</p>
<p>You can use <code>R</code>’s internal diagnostic plots that can be called using the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> method for <code>lm</code>-objects:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># install.packages("devtools")</span></span>
<span><span class="co"># library(devtools)</span></span>
<span><span class="co"># install_git("https://github.com/ccolonescu/PoEdata")</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">PoEdata</span><span class="op">)</span> <span class="co"># for the "food" dataset contained in this package</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"food"</span><span class="op">)</span>     <span class="co"># makes the dataset "food" usable</span></span>
<span></span>
<span><span class="va">lm_object</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">food_exp</span> <span class="op">~</span> <span class="va">income</span>, data <span class="op">=</span> <span class="va">food</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Diagnostic scatter plot of residuals vs fitted values </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lm_object</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="05-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Interpretation:</p>
<ul>
<li>The plot shows the residuals <span class="math inline">\(\hat{\varepsilon}_i\)</span> plotted against the fitted values <span class="math inline">\(\hat{Y}_i\)</span>.</li>
<li>The diagnostic plot indicates that the variance increases with <span class="math inline">\(\hat{Y}_i=X_i'\hat{\beta}.\)</span>
</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Note:</strong> Plotting against the fitted values is actually a quite smart idea since this also works for multiple predictors <span class="math inline">\(X_{i1},\dots,X_{iK}\)</span> with <span class="math inline">\(K\geq 3\)</span>.</p>
</div>
</div>
</section><section id="checking-non-linearity-of-the-regression-line" class="level3" data-number="5.5.3"><h3 data-number="5.5.3" class="anchored" data-anchor-id="checking-non-linearity-of-the-regression-line">
<span class="header-section-number">5.5.3</span> Checking (Non-)Linearity of the Regression Line</h3>
<p>To check whether there are non-linear relationships between the outcome and the predictors, one should take a look at the data using a pairs-plot function that procures scatter plots for all pairs of variables in a dataset.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Base <code>R</code></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Tidyverse <code>R</code></a></li>
</ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">car_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span>file <span class="op">=</span> <span class="st">"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">my_car_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    <span class="st">"MPG"</span>   <span class="op">=</span> <span class="va">car_data</span><span class="op">$</span><span class="va">MPG.city</span>,</span>
<span>    <span class="st">"HP"</span>    <span class="op">=</span> <span class="va">car_data</span><span class="op">$</span><span class="va">Horsepower</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/pairs.html">pairs</a></span><span class="op">(</span><span class="va">my_car_df</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="05-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## install.packages("tidyverse")</span></span>
<span><span class="co">## install.packages("GGally")</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://tidyverse.tidyverse.org">"tidyverse"</a></span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://ggobi.github.io/ggally/">"GGally"</a></span><span class="op">)</span><span class="op">)</span> <span class="co"># nice pairs plot </span></span>
<span></span>
<span><span class="va">car_data</span> <span class="op">&lt;-</span> <span class="fu">readr</span><span class="fu">::</span><span class="fu"><a href="https://readr.tidyverse.org/reference/read_delim.html">read_csv</a></span><span class="op">(</span></span>
<span>  file <span class="op">=</span> <span class="st">"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv"</span>,</span>
<span>  show_col_types <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="va">my_car_df</span> <span class="op">&lt;-</span> <span class="va">car_data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span> <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span></span>
<span>  <span class="st">"MPG"</span>   <span class="op">=</span> <span class="va">MPG.city</span>, </span>
<span>  <span class="st">"HP"</span>    <span class="op">=</span> <span class="va">Horsepower</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="st">"MPG"</span>, <span class="st">"HP"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://ggobi.github.io/ggally/reference/ggpairs.html">ggpairs</a></span><span class="op">(</span><span class="va">my_car_df</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="05-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
<p>The plots indicate a positive, but non-linear relationship between the outcome variable <code>MPG</code> (gasoline consumption in miles per gallon) and the predictor <code>HP</code> (power of the mashing in horsepower).</p>
<p>Let’s begin with a simple linear regression model <span class="math display">\[
Y_i = \beta_1 + \beta_2 X_{i2} +  \varepsilon_i
\]</span> with</p>
<ul>
<li>
<span class="math inline">\(Y_i\)</span> denoting <code>MPG</code> of car <span class="math inline">\(i\)</span> and</li>
<li>
<span class="math inline">\(X_{i2}\)</span> denoting <code>HP</code> of car <span class="math inline">\(i.\)</span>
</li>
</ul>
<p>Such a simple model is not able to take into account the non-linear relationship between <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_{i2}\)</span>. Therefore, we get bad model fits which can be spotted</p>
<ol type="1">
<li>in the scatter plot of <code>MPG</code> vs.&nbsp;<code>HP</code> with added linear regression line fit and</li>
<li>in the diagnostic plot of the residuals <span class="math inline">\(\hat{\varepsilon}_i\)</span> vs.&nbsp;the fitted values <span class="math inline">\(\hat{Y}_i=\hat\beta_1 + \hat\beta_2 X_{i2}.\)</span>
</li>
</ol>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Base <code>R</code></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Tidyverse <code>R</code></a></li>
</ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm_obj_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MPG</span> <span class="op">~</span> <span class="va">HP</span>, data <span class="op">=</span> <span class="va">my_car_df</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>y<span class="op">=</span><span class="va">my_car_df</span><span class="op">$</span><span class="va">MPG</span>, x<span class="op">=</span><span class="va">my_car_df</span><span class="op">$</span><span class="va">HP</span>, </span>
<span>     main <span class="op">=</span> <span class="st">"Simple Linear Regression"</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Horse Power"</span>, </span>
<span>     ylab <span class="op">=</span> <span class="st">"Miles per Gallon"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">lm_obj_1</span><span class="op">)</span></span>
<span><span class="co">##</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lm_obj_1</span>, which<span class="op">=</span><span class="fl">1</span>, </span>
<span>       main <span class="op">=</span> <span class="st">"Simple Linear Regression"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="05-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="480"></p>
</figure>
</div>
</div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## install.packages("tidyverse")</span></span>
<span><span class="co">## install.packages("ggfortify")</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://tidyverse.tidyverse.org">"tidyverse"</a></span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/sinhrks/ggfortify">"ggfortify"</a></span><span class="op">)</span> <span class="co"># tidy diagnostic plots</span></span>
<span></span>
<span><span class="va">lm_obj_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MPG</span> <span class="op">~</span> <span class="va">HP</span>, data <span class="op">=</span> <span class="va">my_car_df</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Plot: Simple Linear Regression</span></span>
<span><span class="va">my_car_df</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">HP</span>, y<span class="op">=</span><span class="va">MPG</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha<span class="op">=</span><span class="fl">0.8</span>, size<span class="op">=</span><span class="fl">3</span><span class="op">)</span><span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">stat_smooth</a></span><span class="op">(</span>method<span class="op">=</span><span class="st">'lm'</span>, formula <span class="op">=</span> <span class="va">y</span><span class="op">~</span><span class="va">x</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_classic</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Horse Power"</span>, y <span class="op">=</span> <span class="st">"Miles per Gallon"</span>, </span>
<span>         title <span class="op">=</span> <span class="st">"Simple Linear Regression"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="05-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Diagnostic Plot: Simple Linear Regression </span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">autoplot</a></span><span class="op">(</span><span class="va">lm_obj_1</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="05-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-17-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
<p>To model the non-linear, more or less quadratic relationship between <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_{i2}\)</span>, we can use a a polynomial regression model with polynomial degree 2: <span class="math display">\[
Y_i = \beta_1 + \beta_2 X_{i2} + \beta_3 X_{i2}^2 +  \varepsilon_i
\]</span> with</p>
<ul>
<li>
<span class="math inline">\(Y_i\)</span> denoting <code>MPG</code> of car <span class="math inline">\(i,\)</span>
</li>
<li>
<span class="math inline">\(X_{i2}\)</span> denoting <code>HP</code> of car <span class="math inline">\(i,\)</span> and</li>
<li>
<span class="math inline">\(X_{i2}^2\)</span> denoting the squared value of <code>HP</code> of car <span class="math inline">\(i.\)</span>
</li>
</ul>
<p>This more flexible model is indeed better in taking into account the non-linear relationship between <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_{i2}.\)</span> The improved model fits can be spotted</p>
<ol type="1">
<li>in the scatter plot of <code>MPG</code> vs.&nbsp;<code>HP</code> with added non-linear regression line fit and</li>
<li>in the diagnostic plot of the residuals <span class="math inline">\(\hat{\varepsilon}_i\)</span> vs.&nbsp;the fitted values <span class="math inline">\(\hat{Y}_i=\hat\beta_1 + \hat\beta_2 X_{i2} + \hat\beta_3 X_{i2}^2.\)</span>
</li>
</ol>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">Base <code>R</code></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">Tidyverse <code>R</code></a></li>
</ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Adding new variable HP squared:</span></span>
<span><span class="va">my_car_df</span><span class="op">$</span><span class="va">HP_sq</span> <span class="op">&lt;-</span> <span class="va">car_data</span><span class="op">$</span><span class="va">Horsepower</span><span class="op">^</span><span class="fl">2</span></span>
<span></span>
<span><span class="va">lm_obj_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MPG</span> <span class="op">~</span> <span class="va">HP</span> <span class="op">+</span> <span class="va">HP_sq</span>, data <span class="op">=</span> <span class="va">my_car_df</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>y<span class="op">=</span><span class="va">my_car_df</span><span class="op">$</span><span class="va">MPG</span>, x<span class="op">=</span><span class="va">my_car_df</span><span class="op">$</span><span class="va">HP</span>,</span>
<span>     main <span class="op">=</span> <span class="st">"Polynomial Regression (Degree: 2)"</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Horse Power"</span>, </span>
<span>     ylab <span class="op">=</span> <span class="st">"Miles per Gallon"</span><span class="op">)</span></span>
<span><span class="va">X_seq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">my_car_df</span><span class="op">$</span><span class="va">HP</span><span class="op">)</span>, </span>
<span>             to   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">my_car_df</span><span class="op">$</span><span class="va">HP</span><span class="op">)</span>, len <span class="op">=</span> <span class="fl">25</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lm_obj_2</span>, </span>
<span>                  newdata<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="st">"HP"</span>    <span class="op">=</span> <span class="va">X_seq</span>, </span>
<span>                                     <span class="st">"HP_sq"</span> <span class="op">=</span> <span class="va">X_seq</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span>, </span>
<span>      x <span class="op">=</span> <span class="va">X_seq</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lm_obj_2</span>, which<span class="op">=</span><span class="fl">1</span>, </span>
<span>       main <span class="op">=</span> <span class="st">"Polynomial Regression (Degree: 2)"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="05-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="480"></p>
</figure>
</div>
</div>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## install.packages("tidyverse")</span></span>
<span><span class="co">## install.packages("ggfortify")</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://tidyverse.tidyverse.org">"tidyverse"</a></span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/sinhrks/ggfortify">"ggfortify"</a></span><span class="op">)</span> <span class="co"># tidy diagnostic plots</span></span>
<span></span>
<span><span class="co">## Adding new variable HP squared:</span></span>
<span><span class="va">my_car_df</span> <span class="op">&lt;-</span> <span class="va">my_car_df</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span> <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span></span>
<span>  <span class="st">"HP_sq"</span> <span class="op">=</span> <span class="va">HP</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Polynomial Regression</span></span>
<span></span>
<span><span class="va">lm_obj_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MPG</span> <span class="op">~</span> <span class="va">HP</span> <span class="op">+</span> <span class="va">HP_sq</span>, data <span class="op">=</span> <span class="va">my_car_df</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Plot: Polynomial Regression</span></span>
<span><span class="va">my_car_df</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">HP</span>, y<span class="op">=</span><span class="va">MPG</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha<span class="op">=</span><span class="fl">0.8</span>, size<span class="op">=</span><span class="fl">3</span><span class="op">)</span><span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">stat_smooth</a></span><span class="op">(</span>method<span class="op">=</span><span class="st">'lm'</span>, formula <span class="op">=</span> <span class="va">y</span><span class="op">~</span><span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">x</span>, <span class="fl">2</span>, raw <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_classic</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Horse Power"</span>, y <span class="op">=</span> <span class="st">"Miles per Gallon"</span>, </span>
<span>         title <span class="op">=</span> <span class="st">"Polynomial Regression"</span>, </span>
<span>         subtitle <span class="op">=</span> <span class="st">"Polynomial Degree: 2"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="05-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Diagnostic Plot: Polynomial Regression</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">autoplot</a></span><span class="op">(</span><span class="va">lm_obj_2</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>     </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="05-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-19-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section><section id="behavior-of-the-ols-estimates-for-resampled-data" class="level3" data-number="5.5.4"><h3 data-number="5.5.4" class="anchored" data-anchor-id="behavior-of-the-ols-estimates-for-resampled-data">
<span class="header-section-number">5.5.4</span> Behavior of the OLS Estimates for Resampled Data</h3>
<p>Usually, we only observe <em>one</em> estimate <span class="math inline">\(\hat{\beta}\)</span> of <span class="math inline">\(\beta\)</span> computed based on <em>one</em> given dataset (one realization of the random sample).</p>
<p>However, in order to understand the statistical properties of the estimators <span class="math inline">\(\hat{\beta}\)</span> we need to view them as random variables which yield different realizations in repeated realizations of the random sample from Model (<a href="#eq-LinMod" class="quarto-xref">Equation&nbsp;<span>5.1</span></a>).</p>
<p>This view allows us then to think about questions like:</p>
<ul>
<li>Is the estimator able to estimate the unknown parameter-value correctly <em>on average</em>?<br>
</li>
<li>Are the estimation results more precise (small variance) if we have more data?</li>
</ul>
<p>A first idea about the statistical properties of the estimator <span class="math inline">\(\hat{\beta}\)</span> can be gained using Monte Carlo simulations as following.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## Sample sizes</span></span>
<span><span class="va">n_small</span>      <span class="op">&lt;-</span>  <span class="fl">30</span> <span class="co"># smallish sample size</span></span>
<span><span class="va">n_large</span>      <span class="op">&lt;-</span> <span class="fl">100</span> <span class="co"># largish sample size</span></span>
<span></span>
<span><span class="co">## True parameter values</span></span>
<span><span class="va">beta0</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">beta1</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span></span>
<span><span class="co">## Monte-Carlo (MC) Simulation </span></span>
<span><span class="co">## 1. Generate data</span></span>
<span><span class="co">## 2. Compute and store estimates</span></span>
<span><span class="co">## Repeat steps 1. and 2. many times</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">## Number of Monte Carlo repetitions</span></span>
<span><span class="co">## How many samples to draw from the models</span></span>
<span><span class="va">B</span>          <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span></span>
<span><span class="co">## Containers to store the estimation results</span></span>
<span><span class="va">beta0_estimates_n_small</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">B</span><span class="op">)</span></span>
<span><span class="va">beta1_estimates_n_small</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">B</span><span class="op">)</span></span>
<span><span class="va">beta0_estimates_n_large</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">B</span><span class="op">)</span></span>
<span><span class="va">beta1_estimates_n_large</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">B</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">b</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">B</span><span class="op">)</span><span class="op">{</span></span>
<span><span class="co">## Generate artificial samples (n_small)</span></span>
<span><span class="va">error_n_small</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n_small</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">X_n_small</span>         <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n_small</span>, min <span class="op">=</span> <span class="fl">1</span>, max <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">Y_n_small</span>         <span class="op">&lt;-</span> <span class="va">beta0</span> <span class="op">+</span> <span class="va">beta1</span> <span class="op">*</span> <span class="va">X_n_small</span> <span class="op">+</span> <span class="va">error_n_small</span></span>
<span><span class="va">lm_obj</span>            <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y_n_small</span> <span class="op">~</span> <span class="va">X_n_small</span><span class="op">)</span> </span>
<span><span class="co">## Save estimation results </span></span>
<span><span class="va">beta0_estimates_n_small</span><span class="op">[</span><span class="va">b</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">lm_obj</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">beta1_estimates_n_small</span><span class="op">[</span><span class="va">b</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">lm_obj</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span></span>
<span><span class="co">## Generate artificial samples (n_large)</span></span>
<span><span class="va">error_n_large</span>     <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n_large</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">X_n_large</span>         <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n_large</span>, min <span class="op">=</span> <span class="fl">1</span>, max <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">Y_n_large</span>         <span class="op">&lt;-</span> <span class="va">beta0</span> <span class="op">+</span> <span class="va">beta1</span> <span class="op">*</span> <span class="va">X_n_large</span> <span class="op">+</span> <span class="va">error_n_large</span></span>
<span><span class="va">lm_obj</span>            <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y_n_large</span> <span class="op">~</span> <span class="va">X_n_large</span><span class="op">)</span></span>
<span><span class="co">## Save estimation results </span></span>
<span><span class="va">beta0_estimates_n_large</span><span class="op">[</span><span class="va">b</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">lm_obj</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">beta1_estimates_n_large</span><span class="op">[</span><span class="va">b</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">lm_obj</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> </span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we have produced <code>B=1000</code> realizations of the estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> and saved these realizations in the vectors</p>
<ul>
<li><code>beta0_estimates_n_small</code></li>
<li><code>beta1_estimates_n_small</code></li>
<li><code>beta0_estimates_n_large</code></li>
<li><code>beta1_estimates_n_large</code></li>
</ul>
<p>These artificially generated realizations allow us to visualize the behavior of the OLS estimates for the repeatedly sampled data.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="05-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>This are promising plots:</p>
<ul>
<li>The realizations of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are scattered around the true (unknown) parameter values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. This indicates unbiasdness of the estimators.</li>
<li>In the case of a larger sample size, the realizations of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> concentrate stronger around the true (unknown) parameter values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1.\)</span> This indicates consistency of the estimators.</li>
</ul>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>However, this was only a simulation for one specific data generating process. Such a Monte Carlo simulation does not allow us to generalize the observed properties to other data generating processes.</p>
<p>In the following chapters, we use theoretical arguments to investigate under which assumptions we can make <em>general</em> statements about the distributional properties of the estimator <span class="math inline">\(\hat\beta.\)</span></p>
</div>
</div>
</section></section><section id="exercises" class="level2" data-number="5.6"><h2 data-number="5.6" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">5.6</span> Exercises</h2>
<ul>
<li><p><a href="https://www.dropbox.com/scl/fi/t6l36vku6aiii3o518voi/Ch5_Exercises1.pdf?rlkey=jgg4e5l5uld4cfakkyy5cztk1&amp;dl=0">Exercises for Chapter 5</a></p></li>
<li><p><a href="https://www.dropbox.com/scl/fi/2arzabv6ydiap8tyh0c4p/Ch5_Exercises_with_Solutions1.pdf?rlkey=dqz2lewoqjxwkqxhvq7jxxf5l&amp;dl=0">Exercises of Chapter 5 with Solutions</a></p></li>
</ul></section><section id="references" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-stock2015" class="csl-entry" role="listitem">
Stock, J. H., and M. W. Watson. 2015. <em><span>I</span>ntroduction to <span>E</span>conometrics</em>. <span>P</span>earson <span>E</span>ducation.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./04-Monte-Carlo-Simulations.html" class="pagination-link" aria-label="Estimation Theory and Monte Carlo Simulations">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Estimation Theory and Monte Carlo Simulations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./06-Small-Sample-Inference.html" class="pagination-link" aria-label="Small Sample Inference">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Small Sample Inference</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>