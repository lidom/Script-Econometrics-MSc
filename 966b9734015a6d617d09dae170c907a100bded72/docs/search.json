[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basis Module Econometrics (M.Sc.)",
    "section": "",
    "text": "Organization of the Course\nTimetable\n\n\n\n\nDay\nTime (UTC+2)\nLecture Hall\n\n\n\nMonday\n10:15-11:45\nJur / Hörsaal F\n\n\nWednesday\n14:15-15:45\nJur / Hörsaal F\n\n\n\n\n\nLecture Material\n\nThis online script\neWhiteboard for lecture notes\nR-Script used during the lectures\n\nZoom\nFor students not yet in Bonn, I’ll stream the lectures via Zoom (Meeting-ID: 992 8328 7705, Pwd: 708587) during the first lecture weeks.\nLiterature\nThe script is self-contained. To prepare well for the exam, it’s a good idea to read this script.\nFurther textbooks I can recommend:\n\n\nAll of Statistics: A Concise Course in Statistical Inference, by Larry Wasserman\n\nEconometrics, by F. Hayashi\n\nEconometrics, by Bruce E. Hansen\n\nProbability and Statistics for Economists, by Bruce E. Hansen\n\nEconometric theory and methods, by R. Davidson and J.G. MacKinnon  \n\nInformation on the Exam\n\nMock Exam with 5 multiple-choice problems and 1 free-text problem: Download\n\nNote: The actual exam will contain 10 multiple-choice problems and 2 free-text problems.\n\n\nYou are allowed to use a handwritten cheat-sheet (one side of a DIN A4 page).\nYou are allowed to use a non-programmable scientific calculator.\nPlease, do not use a pencil.\nExam dates and times are published by the examinations office",
    "crumbs": [
      "Organization of the Course"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html",
    "href": "01-Introduction-to-R.html",
    "title": "\n1  Introduction to R\n",
    "section": "",
    "text": "1.1 Short Glossary\nLets start the tutorial with a (very) short glossary:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#short-glossary",
    "href": "01-Introduction-to-R.html#short-glossary",
    "title": "\n1  Introduction to R\n",
    "section": "",
    "text": "Console: The thing with the &gt; sign at the beginning.\n\nScript file: An ordinary text file with suffix .R. For instance, yourFavoritFileName.R.\n\nWorking directory: The file-directory you are working in. Useful commands: with getwd() you get the location of your current working directory and setwd() allows you to set a new location for it.\n\nWorkspace: This is a hidden file (stored in the working directory), where all objects you use (e.g., data, matrices, vectors, variables, functions, etc.) are stored. Useful commands: ls() shows all elements in our current workspace and rm(list=ls()) deletes all elements in our current workspace.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#first-steps",
    "href": "01-Introduction-to-R.html#first-steps",
    "title": "\n1  Introduction to R\n",
    "section": "\n1.2 First Steps",
    "text": "1.2 First Steps\nA good idea is to use a script file such as yourFavoritFileName.R in order to store your R commands. You can send single lines or marked regions of your R-code to the console by pressing the keys STRG+ENTER.\nTo begin with baby steps, do some simple computations:\n\n2+2 # and all the others: *,/,-,^2,^3,... \n\n[1] 4\n\n\nNote: Everything that is written after the #-sign is ignored by R, which is very useful to comment your code.\nThe assignment operator will be your most often used tool. Here an example to create a scalar variable:\n\nx &lt;- 4 \nx\n\n[1] 4\n\n4 -&gt; x # possible but unusual\nx\n\n[1] 4\n\n\nNote: The R community loves the &lt;- assignment operator, which is a very unusual syntax. Alternatively, you can use the = operator.\nAnd now a more interesting object - a vector:\n\ny &lt;- c(2,7,4,1)\ny\n\n[1] 2 7 4 1\n\n\nThe command ls() shows the total content of your current workspace, and the command rm(list=ls()) deletes all elements of your current workspace:\n\nls()[1:5] # only the first 5 elements\n\n[1] \"pandoc_dir\"      \"quarto_bin_path\" \"x\"               \"y\"              \n[5] NA               \n\nrm(list=ls())\nls()\n\ncharacter(0)\n\n\nNote: RStudio’s Environment pane also lists all the elements in your current workspace. That is, the command ls() becomes a bit obsolete when working with RStudio.\nLet’s try how we can compute with vectors and scalars in R.\n\nx &lt;- 4\ny &lt;- c(2,7,4,1)\n\nx*y # each element in y (vector) is multiplied by x (scalar).\n\n[1]  8 28 16  4\n\ny*y # this is a term by term product of the elements in y\n\n[1]  4 49 16  1\n\n\nPerforming vector multiplications as you might expect from your last math-course, e.g., an outer product: \\(y\\,y^\\top\\):\n\ny %*% t(y)\n\n     [,1] [,2] [,3] [,4]\n[1,]    4   14    8    2\n[2,]   14   49   28    7\n[3,]    8   28   16    4\n[4,]    2    7    4    1\n\n\nOr an inner product \\(y^\\top y\\):\n\nt(y) %*% y\n\n     [,1]\n[1,]   70\n\n\nNote: Sometimes, R’s treatment of vectors can be annoying. The product y %*% y is treated as the product t(y) %*% y.\nThe term-by-term execution as in the above example, y*y, is actually a central strength of R. We can conduct many operations vector-wisely:\n\ny^2\n\n[1]  4 49 16  1\n\nlog(y)\n\n[1] 0.6931472 1.9459101 1.3862944 0.0000000\n\nexp(y)\n\n[1]    7.389056 1096.633158   54.598150    2.718282\n\ny-mean(y)\n\n[1] -1.5  3.5  0.5 -2.5\n\n(y-mean(y))/sd(y) # standardization \n\n[1] -0.5669467  1.3228757  0.1889822 -0.9449112\n\n\nThis is a central characteristic of so called matrix based languages like R (or Matlab). Other programming languages often have to use loops instead:\n\nN &lt;- length(y)\n1:N\n\ny.sq &lt;- numeric(N)\ny.sq\n\nfor(i in 1:N){\n  y.sq[i] &lt;- y[i]^2\n  if(i == N){\n    print(y.sq)\n  }\n}\n\nThe for()-loop is the most common loop. But there is also a while()-loop and a repeat()-loop. However, loops in R can be rather slow, therefore, try to avoid them!\n\nUseful commands to produce sequences of numbers:\n\n1:10\n-10:10\n?seq # Help for the seq()-function\nseq(from=1, to=100, by=7)\n\nUsing the sequence command 1:16, we can go for our first matrix:\n\n?matrix\nA &lt;- matrix(data=1:16, nrow=4, ncol=4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA &lt;- matrix(1:16, 4, 4)\n\nNote that a matrix has always two dimensions, but a vector has only one dimension:\n\ndim(A)    # Dimension of matrix A?\n\n[1] 4 4\n\ndim(y)    # dim() does not operate on vectors.\n\nNULL\n\nlength(y) # Length of vector y?\n\n[1] 4\n\n\nLets play a bit with the matrix A and the vector y. As we have seen in the loop above, the []-operator selects elements of vectors and matrices:\n\nA[,1]\nA[4,4]\ny[c(1,4)]\n\nThis can be done on a more logical basis, too. For example, if you want to know which elements in the first column of matrix A are strictly greater than 2:\n\nA[,1][A[,1]&gt;2]\n\n[1] 3 4\n\n# Note that this give you a boolean vector:\nA[,1]&gt;2\n\n[1] FALSE FALSE  TRUE  TRUE\n\n# And you can use it in a non-sense relation, too:\ny[A[,1]&gt;2]\n\n[1] 4 1\n\n\nNote: Logical operations return so-called boolean objects, i.e., either a TRUE or a FALSE. For instance, if we ask R whether 1&gt;2 we get the answer FALSE.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#further-data-objects",
    "href": "01-Introduction-to-R.html#further-data-objects",
    "title": "\n1  Introduction to R\n",
    "section": "\n1.3 Further Data Objects",
    "text": "1.3 Further Data Objects\nBesides classical data objects such as scalars, vectors, and matrices there are three further data objects in R:\n1. The array: As a matrix but with more dimensions. Here is an example of a \\(2\\times 2\\times 2\\)-dimensional array:\n\nmyFirst.Array &lt;- array(c(1:8), dim=c(2,2,2)) # Take a look at it!\n\n\n2. The list: In lists you can organize different kinds of data. E.g., consider the following example:\n\nmyFirst.List &lt;- list(\n  \"Some_Numbers\" = c(66, 76, 55, 12, 4, 66, 8, 99), \n  \"Animals\"      = c(\"Rabbit\", \"Cat\", \"Elefant\"),\n  \"My_Series\"    = c(30:1)\n) \n\nA very useful function to find specific values and entries within lists is the str()-function:\n\nstr(myFirst.List)\n\nList of 3\n $ Some_Numbers: num [1:8] 66 76 55 12 4 66 8 99\n $ Animals     : chr [1:3] \"Rabbit\" \"Cat\" \"Elefant\"\n $ My_Series   : int [1:30] 30 29 28 27 26 25 24 23 22 21 ...\n\n\n\n3. The data frame: A data.frame is a list-object but with some more formal restrictions (e.g., equal number of rows for all columns). As indicated by its name, a data.frame-object is designed to store data:\n\nmyFirst.Dataframe &lt;- data.frame(\n  \"Credit_Default\"   = c( 0, 0, 1, 0, 1, 1), \n  \"Age\"              = c(35,41,55,36,44,26), \n  \"Loan_in_1000_EUR\" = c(55,65,23,12,98,76)\n) \n# Take a look at it!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#simple-regression-analysis-using-r",
    "href": "01-Introduction-to-R.html#simple-regression-analysis-using-r",
    "title": "1  Introduction to R",
    "section": "1.4 Simple Regression Analysis using R",
    "text": "1.4 Simple Regression Analysis using R\nAlright, let’s do some statistics with real data. You can download the data HERE. Save it on your computer, at a place where you can find it, and give the path (e.g. \"C:\\textbackslash path\\textbackslash autodata.csv\", which references to the data, to the file-argument of the function read.csv():\n\n# ATTENTION! YOU HAVE TO CHANGE \"\\\" TO \"/\":\nauto.data <- read.csv(file   = \"C:/your_path/autodata.csv\", \n                      header = TRUE)\nhead(auto.data)\n\nIf you have problems to read the data into R, go on with these commands. (For this you need a working internet connection!):\n\n# install.packages(\"readr\")\nlibrary(\"readr\")\nauto.data <- suppressMessages(\n  read_csv(\n  file = \"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv\",\n  col_names = TRUE)\n)\n# head(auto.data)\n\n\n\nYou can select specific variables of the auto.data using the $-operator:\n\ngasolin.consumption      <- auto.data$MPG.city\ncar.weight               <- auto.data$Weight\n## Take a look at the first elements of these vectors:\nhead(cbind(gasolin.consumption,car.weight))\n\n     gasolin.consumption car.weight\n[1,]                  25       2705\n[2,]                  18       3560\n[3,]                  20       3375\n[4,]                  19       3405\n[5,]                  22       3640\n[6,]                  22       2880\n\n\nThis is how you can produce your first plot:\n\n## Plot the data:\nplot(y=gasolin.consumption, x=car.weight, \n     xlab=\"Car-Weight (US-Pounds)\", \n     ylab=\"Consumption (Miles/Gallon)\", \n     main=\"Buy Light-Weight Cars!\")\n\n\n\n\nAs a first step, we might assume a simple kind of linear relationship between the variables gasolin.consumption and car.weight. Let us assume that the data was generated by the following simple regression model: \\[\ny_i=\\alpha+\\beta_1 x_i+\\varepsilon_i,\\quad i=1,\\dots,n\n\\] where \\(y_i\\) denotes the gasoline-consumption, \\(x_i\\) the weight of car \\(i\\), and \\(\\varepsilon_i\\) is a mean zero constant variance noise term. (This is clearly a non-sense model!)\nThe command lm() computes the estimates of this linear regression model. The command (in fact it’s a method) summary() computes further quantities of general interest from the object that was returned from the lm() function.\n\nlm.result   <- lm(gasolin.consumption~car.weight)\nlm.summary  <- summary(lm.result)\nlm.summary\n\n\nCall:\nlm(formula = gasolin.consumption ~ car.weight)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7946 -1.9711  0.0249  1.1855 13.8278 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 47.048353   1.679912   28.01   <2e-16 ***\ncar.weight  -0.008032   0.000537  -14.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.038 on 91 degrees of freedom\nMultiple R-squared:  0.7109,    Adjusted R-squared:  0.7077 \nF-statistic: 223.8 on 1 and 91 DF,  p-value: < 2.2e-16\n\n\n\n\nOf course, we want to have a possibility to access all the quantities computed so far, e.g., in order to plot the results. This can be done as following:\n\n## Accessing the computed quantities\nnames(lm.summary) ## Alternatively: str(lm.summary)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\nalpha <- lm.summary$coefficients[1]\nbeta  <- lm.summary$coefficients[2]\n\n## Plot all:\nplot(y=gasolin.consumption, x=car.weight, \n     xlab=\"Car-Weight (US-Pounds)\", \n     ylab=\"Consumption (Miles/Gallon)\", \n     main=\"Buy light-weight Cars!\")\nabline(a=alpha, \n       b=beta, col=\"red\")"
  },
  {
    "objectID": "01-Introduction-to-R.html#programming-in-r",
    "href": "01-Introduction-to-R.html#programming-in-r",
    "title": "\n1  Introduction to R\n",
    "section": "\n1.4 Programming in R",
    "text": "1.4 Programming in R\n\n1.4.1 Simulating Data (Playing Data-God)\nDefining a Data Generating Process\nLet’s write, i.e., program our own R-function for estimating linear regression models. In order to be able to validate our function, we start with simulating data for which we then know all true parameters. Simulating data is like being the “Data-God”: For instance, we can generate realizations of the error term \\(\\varepsilon_i\\), i.e., something which we never observe in real data.\nLet us design the data generating process from which to generate artificial data. We consider the following multiple regression model:\n\\[\nY_i=\\beta_1  X_{i1} + \\beta_2 X_{i2}+\\beta_3 X_{i3}+\\varepsilon_{i},\\quad i=1,\\dots,n,\n\\] where \\(\\varepsilon_{i}\\) is a heteroskedastic error term \\[\n\\varepsilon_{i}\\sim \\mathcal{N}(0,\\sigma_i^2),\\quad \\sigma_i=|x_{3i}|,\n\\]\nfor all \\(i=1,\\dots,n,\\) and where\n\n\n\\(n=50\\) (sample size)\n\n\\(X_{i1}=1\\) for all \\(i=1,\\dots,n=50\\)\n\n\\(X_{i2}\\sim \\mathcal{N}(10,1.5^2)\\)\n\n\\(X_{i3}\\) comes from a t-distribution with 5 degrees of freedom and non-centrality parameter 2\n\n\\(\\beta_1 = 1,\\;\\) \\(\\beta_2 = -5,\\;\\) \\(\\beta_3 = 5,\\;\\)\n\nGenerating Artificial Data; I.e. Generating Realizations from a Data Generating Process\nThe above setup completely defines the (usually unknown) data generating process.\nNow we can use R to generate realizations from this data generating process. We begin with generating realizations of the regressors:\n\nset.seed(109) # Sets the \"seed\" of the random number generators:\nn   &lt;- 50     # Number of observations\n\n## Generate two explanatory variables plus an intercept-variable:\nX.1 &lt;- rep(1, n)                 # Intercept\nX.2 &lt;- rnorm(n, mean=10, sd=1.5) # Draw realizations form a normal distr.\nX.3 &lt;- rt(n, df=5, ncp=2)        # Draw realizations form a t-distr.\nX   &lt;- cbind(X.1, X.2, X.3)      # Save as a Nx3-dimensional data matrix.\n\nNow we define the elements of the \\(\\beta\\)-vector. Be aware of the difference: In real data sets we do not know the true \\(\\beta\\)-vector, but try to estimate it. However, when simulating data, we determine (as “Data-Gods”) the true \\(\\beta\\)-vector and can compare our estimates \\(\\hat{\\beta}_1,\\hat{\\beta}_2,\\) and \\(\\hat{\\beta}_3\\) with the true values \\(\\beta_1=1,\\) \\(\\beta_2=-5,\\) and \\(\\beta_3=5.\\)\n\n## Define the slope-coefficients\nbeta.vec  &lt;- c(1,-5,5)\n\nWe still need to simulate realizations of the dependent variable \\(y_i\\) for all \\(i=1,\\dots,n.\\) Remember that \\[\nY_i=\\beta_1 X_{i1} +\\beta_2 X_{i2}+\\beta_3 x_{i3} + \\varepsilon_{i}.\n\\] That is, we only need realizations from the error terms \\(\\varepsilon_i\\) in order to compute the realizations from \\(y_i\\). This is how you can simulate realizations from the heteroskedastic error terms \\(\\varepsilon_{i}\\sim \\mathcal{N}(0,\\sigma_i^2)\\) with \\(\\sigma_i=|x_{3i}|\\):\n\n## Generate realizations from the heteroscadastic error term\neps       &lt;- abs(X.3) * rnorm(n, mean=0, sd=1)\n\nTake a look at the heteroskedasticity in the error term:\n\nplot(y=eps, x=X.3, \n     main=\"Realizations of the \\nHeteroscedastic Error Term\")\n\n\n\n\n\n\n\nWith the (pseudo-random) realizations from \\(\\varepsilon_i\\), we can finally generate realizations from the dependent variable \\(y_i\\):\n\n## Dependent variable:\ny   &lt;- X %*% beta.vec + eps\n\nLet’s take a look at the artificial data:\n\nmydata    &lt;- data.frame(\"Y\"=y, \"X.1\"=X.1, \"X.2\"=X.2, \"X.3\"=X.3)\npairs(mydata[,-2]) # The '-2' removes the intercept variable \"X.1\"\n\n\n\n\n\n\n\n\n1.4.2 Writing an R Function\nOnce we have data, we can compute the OLS estimate of the true \\(\\beta\\) vector. Remember the formula: \\[\n\\hat{\\beta}=(X^\\top X)^{-1}X^\\top y\n\\] In R-Code this is: \\((X^\\top X)^{-1}=\\)solve(t(X) %*% X), i.e.:\n\n## Computation of the beta-Vector:\nbeta.hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nbeta.hat\n\n         [,1]\nX.1 -2.609634\nX.2 -4.692735\nX.3  5.078342\n\n\nWell done. Using the above lines of code we can easily program our own myOLSFun() function!\n\nmyOLSFun &lt;- function(y, x, add.intercept=FALSE){\n  \n  ## Number of Observations:\n  n         &lt;- length(y)\n  \n  ## Add an intercept to x:\n  if(add.intercept){\n    Intercept &lt;- rep(1, n)\n    x         &lt;- cbind(Intercept, x)\n  }\n  \n  ## Estimation of the slope-parameters:\n  beta.hat.vec &lt;- solve(t(x) %*% x) %*% t(x) %*% y\n  \n  ## Return the result:\n  return(beta.hat.vec)\n}\n\n## Run the function:\nmyOLSFun(y=y, x=X)\n\n         [,1]\nX.1 -2.609634\nX.2 -4.692735\nX.3  5.078342\n\n\n\nYou can extend the function for the computation of the covariance matrix of the slope-estimates, several measures of fits (e.g. R\\(^2\\) and adj.-R\\(^2\\)), etc.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#r-packages",
    "href": "01-Introduction-to-R.html#r-packages",
    "title": "\n1  Introduction to R\n",
    "section": "\n1.5 R-packages",
    "text": "1.5 R-packages\nOne of the best features in R are its contributed packages. The list of all packages on CRAN is impressive! Take a look at it HERE\nFor instance, nice plots can be produced using the R-package is ggplot2. You can find an intro do this package HERE.\n\n# install.packages(\"ggplot2\")\nlibrary(\"ggplot2\")\n\nqplot(Sepal.Length, Petal.Length, data = iris, color = Species)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nOf course, ggplot2 concerns “only” plotting, but you’ll find R-packages for almost any statistical method out there.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#tidyverse",
    "href": "01-Introduction-to-R.html#tidyverse",
    "title": "\n1  Introduction to R\n",
    "section": "\n1.6 Tidyverse",
    "text": "1.6 Tidyverse\nThe tidyverse package is a collection of packages that lets you import, manipulate, explore, visualize and model data in a harmonized and consistent way which helps you to be more productive.\nInstalling the tidyverse package:\n\ninstall.packages(\"tidyverse\")\n\nTo use the tidyverse package load it using the library() function:\n\nlibrary(tidyverse)\n\nChick Weight Data\nR comes with many datasets installed. We will use the ChickWeight dataset to learn about the tidyverse. The help system gives a basic summary of the experiment from which the data was collect:\n\n“The body weights of the chicks were measured at birth and every second day thereafter until day 20. They were also measured on day 21. There were four groups of chicks on different protein diets.”\n\nYou can get more information, including references by typing:\n\nhelp(\"ChickWeight\")\n\nThe Data:  There are 578 observations (rows) and 4 variables:\n\n\nChick – unique ID for each chick.\n\nDiet – one of four protein diets.\n\nTime – number of days since birth.\n\nweight – body weight of chick in grams.\n\nNote: weight has a lower case w (recall R is case sensitive).\nStore the data locally:\n\nChickWeight %&gt;%\n  select(Chick, Diet, Time, weight) %&gt;% \n  arrange(Chick, Diet, Time) %&gt;% \n  write_csv(\"data/ChickWeight.csv\")\n\nFirst we will import the data from a file called ChickWeight.csv using the read_csv() function from the readr package (part of the tidyverse). The first thing to do, outside of R, is to open the file ChickWeight.csv to check what it contains and that it makes sense. Now we can import the data as follows:\n\nCW &lt;- read_csv(\"data/ChickWeight.csv\")\n\nRows: 578 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): Chick, Diet, Time, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf all goes well then the data is now stored in an R object called CW. If you get the following error message then you need to change the working directory to where the data is stored.\nError: 'ChickWeight.csv' does not exist in current\nworking directory ...\nChanging the working directory: In RStudio you can use the menu bar (“Session - Set Working Directory - Choose Directory…”). Alternatively, you can use the function setwd().\nLooking at the Dataset: To look at the data type just type the object (dataset) name:\n\nCW\n\n# A tibble: 578 × 4\n   Chick  Diet  Time weight\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1    18     1     0     39\n 2    18     1     2     35\n 3    16     1     0     41\n 4    16     1     2     45\n 5    16     1     4     49\n 6    16     1     6     51\n 7    16     1     8     57\n 8    16     1    10     51\n 9    16     1    12     54\n10    15     1     0     41\n# ℹ 568 more rows\n\n\nIf there are too many variables then not all them may be printed. To overcome this issue we can use the glimpse() function which makes it possible to see every column in your dataset (called a “data frame” in R speak).\n\nglimpse(CW)\n\nRows: 578\nColumns: 4\n$ Chick  &lt;dbl&gt; 18, 18, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15,…\n$ Diet   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Time   &lt;dbl&gt; 0, 2, 0, 2, 4, 6, 8, 10, 12, 0, 2, 4, 6, 8, 10, 12, 14, 0, 2, 4…\n$ weight &lt;dbl&gt; 39, 35, 41, 45, 49, 51, 57, 51, 54, 41, 49, 56, 64, 68, 68, 67,…\n\n\nThe function View() allows for a spread-sheet type of view on the data:\n\nView(CW)\n\n\n1.6.1 Tidyverse: Plotting Basics\nTo visualize the chick weight data, we will use the ggplot2 package (part of the tidyverse). Our interest is in seeing how the weight changes over time for the chicks by diet. For the moment don’t worry too much about the details just try to build your own understanding and logic. To learn more try different things even if you get an error messages.\nLet’s plot the weight data (vertical axis) over time (horizontal axis).\n\n# An empty plot (the plot on the left)\nggplot(CW, aes(Time, weight))  \n# With data (the plot on the right)\nggplot(CW, aes(Time, weight)) + geom_point() \n\n\n\n\n\n\n\n\n\n\n\n\n\nAdd color for Diet. The graph above does not differentiate between the diets. Let’s use a different color for each diet.\n\n# Adding colour for diet\nggplot(CW,aes(Time,weight,colour=factor(Diet))) +\n  geom_point() \n\n\n\n\n\n\n\nIt is difficult to conclude anything from this graph as the points are printed on top of one another (with diet 1 underneath and diet 4 at the top).\nFactor Variables: Before we continue, we have to make an important change to the CW dataset by making Diet and Time factor variables. This means that R will treat them as categorical variables (see the &lt;fct&gt; variables below) instead of continuous variables. It will simplify our coding. The next section will explain the mutate() function.\n\nCW &lt;- mutate(CW, Diet = factor(Diet))\nCW &lt;- mutate(CW, Time = factor(Time))\nglimpse(CW)\n\nRows: 578\nColumns: 4\n$ Chick  &lt;dbl&gt; 18, 18, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15,…\n$ Diet   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Time   &lt;fct&gt; 0, 2, 0, 2, 4, 6, 8, 10, 12, 0, 2, 4, 6, 8, 10, 12, 14, 0, 2, 4…\n$ weight &lt;dbl&gt; 39, 35, 41, 45, 49, 51, 57, 51, 54, 41, 49, 56, 64, 68, 68, 67,…\n\n\nThe facet_wrap() function: To plot each diet separately in a grid using facet_wrap():\n\n# Adding jitter to the points\nggplot(CW, aes(Time, weight, colour=Diet)) +\n  geom_point() +\n  facet_wrap(~Diet) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Diet 4 has the least variability but we can’t really say anything about the mean effect of each diet although diet 3 seems to have the highest.\nNext we will plot the mean changes over time for each diet using the stat_summary() function:\n\nggplot(CW, aes(Time, weight, \n               group=Diet, colour=Diet)) +\n  stat_summary(fun=\"mean\", geom=\"line\") \n\n\n\n\n\n\n\nInterpretation: We can see that diet 3 has the highest mean weight gains by the end of the experiment. However, we don’t have any information about the variation (uncertainty) in the data.\nTo see variation between the different diets we use geom_boxplot to plot a box-whisker plot. A note of caution is that the number of chicks per diet is relatively low to produce this plot.\n\nggplot(CW, aes(Time, weight, colour=Diet)) +\n  facet_wrap(~Diet) +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  ggtitle(\"Chick Weight over Time by Diet\")\n\n\n\n\n\n\n\nInterpretation: Diet 3 seems to have the highest “average” weight gain but it has more variation than diet 4 which is consistent with our findings so far.\nLet’s finish with a plot that you might include in a publication.\n\nggplot(CW, aes(Time, weight, group=Diet, \n                             colour=Diet)) +\n  facet_wrap(~Diet) +\n  geom_point() +\n  # geom_jitter() +\n  stat_summary(fun=\"mean\", geom=\"line\",\n               colour=\"black\") +\n  theme(legend.position = \"none\") +\n  ggtitle(\"Chick Weight over Time by Diet\") + \n  xlab(\"Time (days)\") +\n  ylab(\"Weight (grams)\")\n\n\n\n\n\n\n\n\n1.6.2 Tidyverse: Data Wrangling Basics\nIn this section we will learn how to wrangle (manipulate) datasets using the tidyverse package. Let’s start with the mutate(), select(), rename(), filter() and arrange() functions.\n\nmutate(): Adds a new variable (column) or modifies an existing one. We already used this above to create factor variables.\n\n# Added a column\nCWm1 &lt;- mutate(CW, weightKg = weight/1000)\nCWm1\n\n# A tibble: 578 × 5\n  Chick Diet  Time  weight weightKg\n  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1    18 1     0         39    0.039\n2    18 1     2         35    0.035\n3    16 1     0         41    0.041\n# ℹ 575 more rows\n\n# Modify an existing column\nCWm2 &lt;- mutate(CW, Diet = str_c(\"Diet \", Diet))\nCWm2\n\n# A tibble: 578 × 4\n  Chick Diet   Time  weight\n  &lt;dbl&gt; &lt;chr&gt;  &lt;fct&gt;  &lt;dbl&gt;\n1    18 Diet 1 0         39\n2    18 Diet 1 2         35\n3    16 Diet 1 0         41\n# ℹ 575 more rows\n\n\n\nselect(): Keeps, drops or reorders variables.\n\n# Drop the weight variable from CWm1 using minus\nselect(CWm1, -weight)\n\n# A tibble: 578 × 4\n  Chick Diet  Time  weightKg\n  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt;\n1    18 1     0        0.039\n2    18 1     2        0.035\n3    16 1     0        0.041\n# ℹ 575 more rows\n\n# Keep variables Time, Diet and weightKg\nselect(CWm1, Chick, Time, Diet, weightKg)\n\n# A tibble: 578 × 4\n  Chick Time  Diet  weightKg\n  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt;\n1    18 0     1        0.039\n2    18 2     1        0.035\n3    16 0     1        0.041\n# ℹ 575 more rows\n\n\n\nrename(): Renames variables whilst keeping all variables.\n\nrename(CW, Group = Diet, Weight = weight)\n\n# A tibble: 578 × 4\n  Chick Group Time  Weight\n  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n1    18 1     0         39\n2    18 1     2         35\n3    16 1     0         41\n# ℹ 575 more rows\n\n\n\nfilter(): Keeps or drops observations (rows).\n\nfilter(CW, Time==21 & weight&gt;300)\n\n# A tibble: 8 × 4\n  Chick Diet  Time  weight\n  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n1     7 1     21       305\n2    29 2     21       309\n3    21 2     21       331\n# ℹ 5 more rows\n\n\nFor comparing values in vectors use: &lt; (less than), &gt; (greater than), &lt;= (less than and equal to), &gt;= (greater than and equal to), == (equal to) and != (not equal to). These can be combined logically using & (and) and | (or).\n\narrange(): Changes the order of the observations.\n\narrange(CW, Chick, Time)\n\n# A tibble: 578 × 4\n  Chick Diet  Time  weight\n  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n1     1 1     0         42\n2     1 1     2         51\n3     1 1     4         59\n# ℹ 575 more rows\n\narrange(CW, desc(weight))\n\n# A tibble: 578 × 4\n  Chick Diet  Time  weight\n  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n1    35 3     21       373\n2    35 3     20       361\n3    34 3     21       341\n# ℹ 575 more rows\n\n\nWhat does the desc() do? Try using desc(Time).\n\n1.6.3 The pipe operator %&gt;%\n\nIn reality you will end up doing multiple data wrangling steps that you want to save. The pipe operator %&gt;% makes your code nice and readable:\n\nCW21 &lt;- CW %&gt;% \n  filter(Time %in% c(0, 21)) %&gt;% \n  rename(Weight = weight) %&gt;% \n  mutate(Group = factor(str_c(\"Diet \", Diet))) %&gt;% \n  select(Chick, Group, Time, Weight) %&gt;% \n  arrange(Chick, Time) \nCW21\n\n# A tibble: 95 × 4\n  Chick Group  Time  Weight\n  &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;dbl&gt;\n1     1 Diet 1 0         42\n2     1 Diet 1 21       205\n3     2 Diet 1 0         40\n# ℹ 92 more rows\n\n\nHint: To understand the code above we should read the pipe operator %&gt;% as “then”.\n\nCreate a new dataset (object) called CW21 using dataset CW then keep the data for days 0 and 21 then rename variable weight to Weight then create a variable called Group then keep variables Chick, Group, Time and Weight and then finally arrange the data by variables Chick and Time.\n\nThis is the same code:\n\nCW21 &lt;- CW %&gt;% \n  filter(., Time %in% c(0, 21)) %&gt;% \n  rename(., Weight = weight) %&gt;% \n  mutate(., Group=factor(str_c(\"Diet \",Diet))) %&gt;% \n  select(., Chick, Group, Time, Weight) %&gt;% \n  arrange(., Chick, Time) \n\nThe pipe operator, %&gt;%, replaces the dots (.) with whatever is returned from code preceding it. For example, the dot in filter(., Time %in% c(0, 21)) is replaced by CW. The output of the filter(...) then replaces the dot in rename(., Weight = weight) and so on. Think of it as a data assembly line with each function doing its thing and passing it to the next.\n\n1.6.4 The group_by() function\nFrom the data visualizations above we concluded that the diet 3 has the highest mean and diet 4 the least variation. In this section, we will quantify the effects of the diets using summmary statistics. We start by looking at the number of observations and the mean by diet and time.\n\nmnsdCW &lt;- CW %&gt;% \n  group_by(Diet, Time) %&gt;% \n  summarise(N = n(), Mean = mean(weight)) %&gt;% \n  arrange(Diet, Time)\n\n`summarise()` has grouped output by 'Diet'. You can override using the\n`.groups` argument.\n\nmnsdCW\n\n# A tibble: 48 × 4\n# Groups:   Diet [4]\n  Diet  Time      N  Mean\n  &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;\n1 1     0        20  41.4\n2 1     2        20  47.2\n3 1     4        19  56.5\n# ℹ 45 more rows\n\n\nFor each distinct combination of Diet and Time, the chick weight data is summarized into the number of observations (N) and the mean (Mean) of weight.\nFurther summaries: Let’s also calculate the standard deviation, median, minimum and maximum values but only at days 0 and 21.\n\nsumCW &lt;-  CW %&gt;% \n  filter(Time %in% c(0, 21)) %&gt;% \n  group_by(Diet, Time) %&gt;% \n  summarise(N = n(),\n            Mean = mean(weight),\n            SD = sd(weight),\n            Median = median(weight),\n            Min = min(weight),\n            Max = max(weight)) %&gt;% \n  arrange(Diet, Time)\n\n`summarise()` has grouped output by 'Diet'. You can override using the\n`.groups` argument.\n\nsumCW\n\n# A tibble: 8 × 8\n# Groups:   Diet [4]\n  Diet  Time      N  Mean     SD Median   Min   Max\n  &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1     0        20  41.4  0.995   41      39    43\n2 1     21       16 178.  58.7    166      96   305\n3 2     0        10  40.7  1.49    40.5    39    43\n# ℹ 5 more rows\n\n\nLet’s make the summaries “prettier”, say, for a report or publication.\n\nlibrary(\"knitr\") # to use the kable() function\nprettySumCW &lt;- sumCW %&gt;% \n mutate(`Mean (SD)` = str_c(format(Mean, digits=1),\n           \" (\", format(SD, digits=2), \")\")) %&gt;% \n mutate(Range = str_c(Min, \" - \", Max)) %&gt;% \n select(Diet, Time, N, `Mean (SD)`, Median, Range) %&gt;%\n arrange(Diet, Time) %&gt;% \n kable(format = \"latex\")\nprettySumCW\n\n\n\n\n\nDiet\nTime\nN\nMean (SD)\nMedian\nRange\n\n\n\n1\n0\n20\n41 ( 0.99)\n41.0\n39 - 43\n\n\n1\n21\n16\n178 (58.70)\n166.0\n96 - 305\n\n\n2\n0\n10\n41 ( 1.5)\n40.5\n39 - 43\n\n\n2\n21\n10\n215 (78.1)\n212.5\n74 - 331\n\n\n3\n0\n10\n41 ( 1)\n41.0\n39 - 42\n\n\n3\n21\n10\n270 (72)\n281.0\n147 - 373\n\n\n4\n0\n10\n41 ( 1.1)\n41.0\n39 - 42\n\n\n4\n21\n9\n239 (43.3)\n237.0\n196 - 322\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: This summary table offers the same interpretation as before, namely that diet 3 has the highest mean and median weights at day 21 but a higher variation than group 4. However it should be noted that at day 21, diet 1 lost 4 chicks from 20 that started and diet 4 lost 1 from 10. This could be a sign of some health related issues.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#further-links",
    "href": "01-Introduction-to-R.html#further-links",
    "title": "\n1  Introduction to R\n",
    "section": "\n1.7 Further Links",
    "text": "1.7 Further Links\n\n1.7.1 Further R-Intros\n\nhttps://eddelbuettel.github.io/gsir-te/Getting-Started-in-R.pdf\nhttps://www.datacamp.com/courses/free-introduction-to-r\nhttps://swcarpentry.github.io/r-novice-gapminder/\nhttps://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects\n\n1.7.2 Version Control (Git/GitHub)\n\nhttps://support.rstudio.com/hc/en-us/articles/200532077-Version-Control-with-Git-and-SVN\nhttp://happygitwithr.com/\nhttps://www.gitkraken.com/\n\n1.7.3 R-Ladies\n\nhttps://rladies.org/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "02-Probability.html",
    "href": "02-Probability.html",
    "title": "2  Probability",
    "section": "",
    "text": "2.1 Basics in Probability Theory\nProbability is the mathematical language for quantifying uncertainty. We can apply probability theory to a diverse set of problems, from coin flipping to the analysis of econometric problems. The starting point is to specify the sample space, that is, the set of possible outcomes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-Probability.html#basics-in-probability-theory",
    "href": "02-Probability.html#basics-in-probability-theory",
    "title": "2  Probability",
    "section": "",
    "text": "2.1.1 Sample Spaces and (Elementary) Events\nThe sample space \\(\\Omega,\\) is the set of possible outcomes of an experiment. Points \\(\\omega\\) in \\(\\Omega\\) are called sample outcomes or realizations or elementary events. Events are subsets of \\(\\Omega\\).\nExample: If we toss a coin twice then \\(\\Omega=\\{H H, H T, T H, T T\\}.\\) The event that the first toss is heads is \\(A=\\{H H, H T\\}.\\)\nExample: Let \\(\\omega\\) be the outcome of a measurement of some physical quantity, for example, temperature. Then \\(\\Omega=\\mathbb{R}=(-\\infty, \\infty).\\) The event that the measurement is larger than 10 but less than or equal to 23 is \\(A=(10,23].\\)\nExample: If we toss a coin forever then the sample space is the infinite set \\(\\Omega=\\left\\{\\omega=\\left(\\omega_{1}, \\omega_{2}, \\omega_{3}, \\ldots,\\right)|\\omega_{i} \\in\\{H, T\\}\\right\\}\\) Let \\(A\\) be the event that the first head appears on the third toss. Then \\(A=\\left\\{\\left(\\omega_{1}, \\omega_{2}, \\omega_{3}, \\ldots,\\right)| \\omega_{1}=T, \\omega_{2}=T, \\omega_{3}=H, \\omega_{i} \\in\\{H, T\\} \\text { for } i&gt;3\\right\\}.\\)\nGiven an event \\(A,\\) let \\(A^{c}=\\{\\omega \\in \\Omega | \\omega \\notin A\\}\\) denote the complement of \\(A\\). Informally, \\(A^{c}\\) can be read as “not \\(A\\).” The complement of \\(\\Omega\\) is the empty set \\(\\emptyset\\). The union of events \\(A\\) and \\(B\\) is defined as \\[\nA\\cup B=\\{\\omega \\in \\Omega|\\omega\\in A\\text{ or }\\omega \\in B\\text{ or }\\omega\\in\\text{ both}\\}\n\\] which can be thought of as “\\(A\\) or \\(B\\).” If \\(A_{1}, A_{2}, \\ldots\\) is a sequence of sets then \\[\n\\bigcup_{i=1}^{\\infty} A_{i}=\\left\\{\\omega \\in \\Omega | \\omega \\in A_{i} \\text { for at least one i }\\right\\}.\n\\] The intersection of \\(A\\) and \\(B\\) is defined as \\[\nA \\cap B=\\{\\omega \\in \\Omega | \\omega \\in A\\text{ and }\\omega \\in B\\}\n\\] which reads as “\\(A\\) and \\(B\\).” Often \\(A \\cap B\\) is also written shortly as \\(AB\\) or as \\(A,B.\\)\nIf \\(A_{1}, A_{2}, \\ldots\\) is a sequence of sets then \\[\n\\bigcap_{i=1}^{\\infty} A_{i}=\\left\\{\\omega \\in \\Omega | \\omega \\in A_{i} \\text { for all i }\\right\\}.\n\\]\nIf every element of \\(A\\) is also contained in \\(B\\) we write \\(A \\subset B\\) or, equivalently, \\(B \\supset A\\). If \\(A\\) is a finite set, let \\(|A|\\) denote the number of elements in \\(A .\\) We say that \\(A_{1}, A_{2}, \\ldots\\) are disjoint or mutually exclusive if \\(A_{i} \\cap A_{j}=\\emptyset\\) whenever \\(i \\neq j\\). For example, \\(A_{1}=[0,1), A_{2}=[1,2), A_{3}=[2,3), \\ldots\\) are disjoint. A partition of \\(\\Omega\\) is a sequence of disjoint sets \\(A_{1}, A_{2}, \\ldots\\) such that \\(\\bigcup_{i=1}^{\\infty} A_{i}=\\Omega\\).\nSummary: Sample space and events\n\\[\n\\begin{array}{ll}\n\\Omega & \\text { sample space } \\\\\n\\omega & \\text { outcome, elementary event, realization}\\\\\nA      & \\text { event (subset of } \\Omega) \\\\\n|A|    & \\text { number of points in } A \\text { (if } A \\text { is finite) }\\\\\nA^{c}  & \\text { complement of } A (\\operatorname{not} A)\\\\\nA \\cup B &\\text{ union }(A\\text{ or }B)\\\\\nA \\cap B &\\text{ intersection }(A \\text { and } B);\\text{ short notations: }AB\\text{ or }A,B\\\\\nA \\subset B &\\text{ set inclusion }(A \\text{ is a subset of or equal to }B)\\\\\n\\emptyset   &\\text{ null event (always false)}\\\\\n\\Omega      &\\text{ true event (always true)}\n\\end{array}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1.2 Probability\nWe want to assign a real number \\(P(A)\\) to every event \\(A,\\) called the probability of \\(A .\\) We also call \\(P\\) a probability distribution or a probability measure. To qualify as a probability, \\(P\\) has to satisfy three axioms. That is, a function \\(P\\) that assigns a real number \\(P(A)\\in[0,1]\\) to each event \\(A\\) is a probability distribution or a probability measure if it satisfies the following three axioms:\n\n\nAxiom 1: \\(P(A) \\geq 0\\) for every \\(A\\)\n\n\nAxiom 2: \\(P(\\Omega)=1\\)\n\n\nAxiom 3: If \\(A_{1}, A_{2}, \\ldots\\) are disjoint then\n\n\\[\nP\\left(\\bigcup_{i=1}^{\\infty} A_{i}\\right)=\\sum_{i=1}^{\\infty} P\\left(A_{i}\\right).\n\\]\n\n\n\n\n\n\nSigma-Algebra\n\n\n\nIt is not always possible to assign a probability to every event \\(A\\) if the sample space is large. For instance, in the case of \\[\n\\Omega=\\mathbb{R}\n\\] strange things can happen. There are pathological sets (e.g. Vitali sets) that simply break down the mathematics since they are non-measurable (i.e. we cannot assign probabilities to them). Therefore, in cases like \\(\\Omega=\\mathbb{R}\\), we assign probabilities to a limited class of sets called a \\(\\sigma\\)-field or \\(\\sigma\\)-algebra.\nFor \\(\\Omega=\\mathbb{R}\\), the canonical \\(\\sigma\\)-algebra is the Borel \\(\\sigma\\)-algebra. The Borel \\(\\sigma\\)-algebra on \\(\\mathbb{R}\\) is generated by the collection of all open subsets of \\(\\mathbb{R}\\).\nIn this course, we use the Borel \\(\\sigma\\)-algebra and fortunately we do not have to bother with it any further. It’s there lurking in the background making things work for us.\n\n\nOne can derive many properties of \\(P\\) from the three axioms. Here are a few:\n\n\\(P(\\emptyset)=0\\)\n\\(A \\subset B\\Rightarrow P(A) \\leq P(B)\\)\n\\(0 \\leq P(A) \\leq 1\\)\n\\(P\\left(A^{c}\\right)=1-P(A)\\)\n\\(A \\cap B=\\emptyset \\Rightarrow P(A \\cup B)=P(A)+P(B)\\)\n\nA less obvious property is given in the following: For any events \\(A\\) and \\(B\\) we have that,\n\\[\nP(A \\cup B)=P(A)+P(B)-P(A B).\n\\]\nExample: Two consecutive coin tosses. Let \\(H_{1}\\) be the event that heads occurs on toss 1 and let \\(H_{2}\\) be the event that heads occurs on toss 2. Likewise for \\(T_1\\) and \\(T_2.\\) Let us consider “fair” coins, i.e., all outcomes are equally likely. Then\\[\n\\begin{align*}\n%&P\\left(\\left\\{H_{1}, H_{2}\\right\\} \\cup\n%        \\left\\{H_{1}, T_{2}\\right\\} \\cup\n%        \\left\\{T_{1}, H_{2}\\right\\} \\cup\n%        \\left\\{T_{1}, T_{2}\\right\\}\\right)\\\\[2ex]\n&P\\left(\\left\\{H_{1}, H_{2}\\right\\}\\right)+\n  P\\left(\\left\\{H_{1}, T_{2}\\right\\}\\right)+\n  P\\left(\\left\\{T_{1}, H_{2}\\right\\}\\right)+\n  P\\left(\\left\\{T_{1}, T_{2}\\right\\}\\right)=\\\\[2ex]\n&=\\frac{1}{4}+\\frac{1}{4}+\\frac{1}{4}+\\frac{1}{4}=1.               \n\\end{align*}\n\\] Therefore,\\[\n\\begin{align*}\nP\\left(H_{1} \\cup H_{2}\\right)&=P\\left(H_{1}\\right)+P\\left(H_{2}\\right)-P\\left(H_{1} H_{2}\\right)\\\\[2ex]\n&=\\frac{1}{2}+\\frac{1}{2}-\\frac{1}{4}=\\frac{3}{4}.\n\\end{align*}\n\\]\nProbabilities as relative frequencies\nOne can interpret \\(P(A)\\) in terms of frequencies. That is, \\(P(A)\\) is the (infinitely) long run proportion of times that \\(A\\) is true in repetitions. For example, if we say that the probability of heads is \\(1 / 2\\), i.e \\(P(H)=1/2\\) we mean that if we flip the coin many times then the proportion of times we get heads tends to \\(1 / 2\\) as the number of tosses increases. An infinitely long, unpredictable sequence of tosses whose limiting proportion tends to a constant is an idealization, much like the idea of a straight line in geometry.\nThe following R codes approximates the probability \\(P(H)=1/2\\) using 5, 50 and 5,000 many (pseudo) random coin flips:\n\nset.seed(869)\n## 5 (fair) coin-flips:\nresults &lt;- sample(x = c(\"H\", \"T\"), size = 5, replace = TRUE)\n## Relative frequency of \"H\" in 5 coin-flips\nlength(results[results==\"H\"])/5\n\n[1] 0.2\n\n## 50 (fair) coin-flips:\nresults &lt;- sample(x = c(\"H\", \"T\"), size = 50, replace = TRUE)\n## Relative frequency of \"H\" in 50 coin-flips\nlength(results[results==\"H\"])/50\n\n[1] 0.52\n\n## 5000 (fair) coin-flips:\nresults &lt;- sample(x = c(\"H\", \"T\"), size = 5000, replace = TRUE)\n## Relative frequency of \"H\" in 5000 coin-flips\nlength(results[results==\"H\"])/5000\n\n[1] 0.5024\n\n\n\n2.1.3 Independent Events\nIf we flip a fair coin twice, then the probability of two heads is \\(\\frac{1}{2} \\times \\frac{1}{2}\\). We multiply the probabilities because we regard the two tosses as independent. Two events \\(A\\) and \\(B\\) are called independent if\n\\[\nP(A B)=P(A) P(B).\n\\]\nOr more generally, a whole set of events \\(\\{A_i|i\\in I\\}\\) is independent if\n\\[\nP\\left(\\bigcap_{i \\in J} A_{i}\\right)=\\prod_{i \\in J}P\\left(A_{i}\\right)\n\\] for every finite subset \\(J\\) of \\(I\\), where \\(I\\) denotes the not necessarily finite index set \\((\\)e.g. \\(I=\\{1,2,\\dots\\}).\\)\nIndependence can arise in two distinct ways. Sometimes, we explicitly assume that two events are independent. For example, in tossing a coin twice, we usually assume the tosses are independent which reflects the fact that the coin has no memory of the first toss.\nIn other instances, we derive independence by verifying that the definition of independence \\(P(A B)=P(A)P(B)\\) holds. For example, in tossing a fair die once, let \\(A=\\{2,4,6\\}\\) be the event of observing an even number and let \\(B=\\{1,2,3,4\\}\\) be the event of observing no \\(5\\) and no \\(6\\). Then, \\(A \\cap B=\\{2,4\\}\\) is the event of observing either a \\(2\\) or a \\(4\\). Are the events \\(A\\) and \\(B\\) independent?\\[\nP(A B)=\\frac{2}{6}=P(A)P(B)=\\frac{1}{2}\\cdot \\frac{2}{3}\n\\] and so \\(A\\) and \\(B\\) are independent. In this case, we didn’t assume that \\(A\\) and \\(B\\) are independent it just turned out that they were.\nCautionary Note: Suppose that \\(A\\) and \\(B\\) are disjoint events, i.e.  \\[\nAB=\\emptyset,\n\\] each with positive probability, i.e., \\(P(A)&gt;0\\) and \\(P(B)&gt;0.\\) Can they be independent? No! This follows since\n\\[\nP(A B)=P(\\emptyset)=0\\neq \\underbrace{P(A)}_{&gt;0}\\;\\underbrace{P(B)}_{&gt;0}&gt;0.\n\\]\nGenerally, there is no way to judge (in-)dependence by looking at the sets in a Venn diagram.\nSummary: Independent Events\n\n\n\\(A\\) and \\(B\\) are independent if \\(P(A B)=P(A) P(B)\\).\nIndependence is sometimes assumed and sometimes derived.\nDisjoint events with strictly positive probabilities are not independent.\n\n2.1.4 Conditional Probability\nIf \\(P(B)&gt;0\\) then the conditional probability of \\(A\\) given \\(B\\) is \\[\nP(A \\mid B)=\\frac{P(A B)}{P(B)}.\n\\] Think of \\(P(A \\mid B)\\) as the fraction of times \\(A\\) occurs among those in which \\(B\\) occurs. Here are some facts about conditional probabilities:\n\nThe rules of probability apply to events on the left of the bar “\\(\\mid\\)”. That is, for any fixed \\(B\\) such that \\(P(B)&gt;0,\\) \\(P(\\cdot \\mid B)\\) is a probability, i.e., it satisfies the three axioms of probability:\n\n\\(P(A \\mid B) \\geq 0\\)\n\\(P(\\Omega \\mid B)=1\\)\nIf \\(A_{1}, A_{2}, \\ldots\\) are disjoint then \\(P\\left(\\bigcup_{i=1}^{\\infty} A_{i} \\mid B\\right)=\\sum_{i=1}^{\\infty} P\\left(A_{i} \\mid B\\right).\\)\n\n\n\nBut it’s generally not true that \\(P(A \\mid B \\cup C)=P(A \\mid B)+P(A \\mid C).\\)\n\n\nIn general it is also not the case that \\(P(A \\mid B)=P(B \\mid A)\\). People get this confused all the time. For example, the probability of spots given you have measles is 1 but the probability that you have measles given that you have spots is not \\(1 .\\) In this case, the difference between \\(P(A \\mid B)\\) and \\(P(B \\mid A)\\) is obvious but there are cases where it is less obvious. This mistake is made often enough in legal cases that it is sometimes called the “prosecutor’s fallacy”.\nExample: A medical test for a disease \\(D\\) has outcomes \\(+\\) and \\(-.\\) The probabilities are:\n\\[\n\\begin{array}{c|cc|c}\n& D & D^{c} \\\\\n\\hline\n+ & .0081 & .0900 &  .0981\\\\\n- & .0009 & .9010 &  .9019\\\\\n\\hline\n  & .0090 & .9910 &  1\n\\end{array}\n\\] From the definition of conditional probability, we have:\n\nSensitivity of the test: $$ \\[\\begin{align*}\nP(+\\mid D)\n& = \\frac{P(+\\cap D)}{P(D)}\\\\[2ex]\n& = \\frac{0.0081}{(0.0081+0.0009)}=0.9\n\\end{align*}\\]\n\nSpecificity of the test: \\[\\begin{align*}\nP(-\\mid D^{c})\n& = \\frac{P(-\\cap D^{c})}{P(D^{c})}\\\\[2ex]\n& = \\frac{0.9010}{(0.9010+0.0900)}\\approx 0.9\n\\end{align*}\\] $$\n\nApparently, the test is fairly accurate:\n\nSick people yield a positive test result 90 percent of the time.\nHealthy people yield a negative test result about 90 percent of the time.\n\nNow, suppose you go for a test and get a positive result. What is the probability you have the disease? Most people answer \\(0.90=90\\%\\). However, this conclusion is a fallacy (“prosecutor’s fallacy”) and not correct.\nThe correct answer is \\[\n\\begin{align*}\nP(D \\mid+)\n& = \\frac{P(+\\cap D)}{P(+)}\\\\[2ex]\n& = \\frac{0.0081}{(0.0081+0.0900)}=0.08 = 8\\%.\n\\end{align*}\n\\]\nThe lesson here is that you need to compute the answer numerically. Don’t trust your intuition.\nIndependence via Conditional Probabilities\nIf \\(A\\) and \\(B\\) are independent events then \\[\nP(A \\mid B)=\\frac{P(A B)}{P(B)}=\\frac{P(A) P(B)}{P(B)}=P(A)\n\\] So another interpretation of independence is that knowing \\(B\\) doesn’t change the probability of \\(A\\).\nFrom the definition of conditional probability we can write \\[\nP(A B)=P(A \\mid B) P(B)\n\\] and also \\[\nP(A B)=P(B \\mid A) P(A).\n\\] Often, these formulas give us a convenient way to compute \\(P(A B)\\) when \\(A\\) and \\(B\\) are not independent.\nExample: Draw two cards from a deck, without replacement. Let \\(A\\) be the event that the first draw is Ace of Clubs and let \\(B\\) be the event that the second draw is Queen of Diamonds. Then \\[\\begin{align*}\nP(A B)\n&=P(A) \\;\\;P(B \\mid A)\\\\[2ex]\n&=(1 / 52)\\; (1 / 51)\n\\end{align*}\\]\nSummary: Conditional Probability\n\nIf \\(P(B)&gt;0\\) then \\(P(A \\mid B)=P(A B)/P(B)\\)\n\n\n\\(P(\\cdot \\mid B)\\) satisfies the axioms of probability, for fixed \\(B\\). In general, \\(P(A \\mid \\cdot)\\) does not satisfy the axioms of probability, for fixed \\(A\\).\nIn general, \\(P(A \\mid B) \\neq P(B \\mid A)\\).\n\n\\(A\\) and \\(B\\) are independent if and only if \\(P(A \\mid B)=P(A)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-Probability.html#random-variables",
    "href": "02-Probability.html#random-variables",
    "title": "2  Probability",
    "section": "\n2.2 Random Variables",
    "text": "2.2 Random Variables\nStatistics and econometrics are concerned with data. How do we link sample spaces, events and probabilities to data? The link is provided by the concept of a random variable. A real-valued random variable is a mapping \\(X: \\Omega \\rightarrow \\mathbb{R}\\) that assigns a real number \\(X(\\omega)\\in\\mathbb{R}\\) to each outcome \\(\\omega\\).\nAt a certain point in most statistics/econometrics courses, the sample space, \\(\\Omega\\), is rarely mentioned and we work directly with random variables. But you should keep in mind that the sample space is really there, lurking in the background.\nExample: Flip a coin ten times. Let \\(X(\\omega)\\) be the number of heads in the sequence \\(\\omega.\\) For example, if \\(\\omega=\\text{HHTHHTHHTT}\\) then \\(X(\\omega)=6\\).\nExample: Let \\(\\Omega=\\left\\{(x, y)|x^{2}+y^{2} \\leq 1\\right\\}\\) be the unit disc. Consider drawing a point “at random” from \\(\\Omega\\). A typical outcome is then of the form \\(\\omega=(x, y) .\\) Some examples of random variables are \\(X(\\omega)=x, Y(\\omega)=y, Z(\\omega)=x+y, W(\\omega)=\\sqrt{x^{2}+y^{2}}\\).\nGiven a real-valued random variable \\(X\\in\\mathbb{R}\\) and a subset \\(A\\) of the real line (\\(A\\subset\\mathbb{R}\\)), define \\(X^{-1}(A)=\\{\\omega \\in \\Omega|X(\\omega) \\in A\\}\\). This allows us to link the probabilities on the random variable \\(X\\), i.e. the probabilities we are usually working with, to the underlying probabilities on the events, i.e. the probabilities lurking in the background.\nExample: Flip a coin twice and let \\(X\\) be the number of heads. Then, \\(P_X(X=0)=P(\\{T T\\})=1 / 4\\), \\(P_X(X=1)=P(\\{H T, T H\\})=1 / 2\\) and \\(P_X(X=2)=P(\\{H H\\})=1 / 4\\). Thus, the events and their associated probability distribution, \\(P\\), and the random variable \\(X\\) and its distribution, \\(P_X\\), can be summarized as follows:\n\n\n\\(\\omega\\)\n\\(P(\\{\\omega\\})\\)\n\\(X(\\omega)\\)\n\n\n\n\\(T T\\)\n\\(1 / 4\\)\n0\n\n\n\\(T H\\)\n\\(1 / 4\\)\n1\n\n\n\\(H T\\)\n\\(1 / 4\\)\n1\n\n\n\\(H H\\)\n\\(1 / 4\\)\n2\n\n\n\n\n\n\\(x\\)\n\\(P_X(X=x)\\)\n\n\n\n0\n\\(1 / 4\\)\n\n\n1\n\\(1 / 2\\)\n\n\n2\n\\(1 / 4\\)\n\n\n\nHere, \\(P_{X}\\) is not the same probability function as \\(P\\), because \\(P\\) maps from the sample space events, \\(\\omega\\), to \\([0,1],\\) while \\(P_X\\) maps from the random-variable events, \\(X(\\omega),\\) to \\([0,1].\\) We will typically forget about the sample space \\(\\Omega\\) and just think of the random variable as an experiment with real-valued (possible multivariate) outcomes. We will therefore write \\(P\\left(X=x_{k}\\right)\\) instead of \\(P_{X}\\left(X=x_{k}\\right)\\) to simplify the notation.\n\n2.2.1 Univariate Distribution and Probability Functions\n\n2.2.1.1 Cumulative Distribution Function\nThe cumulative distribution function (cdf) \\[F_{X}: \\mathbb{R} \\rightarrow [0,1]\\] of a real-valued random variable \\(X\\in\\mathbb{R}\\) is defined by \\[\nF_{X}(x)=\\mathbb{P}(X \\leq x).\n\\]\nYou might wonder why we bother to define the cdf. The reason is that it effectively contains all the information about the random variable. Indeed, let \\(X\\in\\mathbb{R}\\) have cdf \\(F\\) and let \\(Y\\in\\mathbb{R}\\) have cdf \\(G\\). If \\(F(x)=G(x)\\) for all \\(x\\in\\mathbb{R}\\) then \\(P(X \\in A)=P(Y \\in A)\\) for all \\(A\\subset\\mathbb{R}\\). In order to denote that two random variables, here \\(X\\) and \\(Y\\), have the same distribution, one can write shortly \\(X\\overset{d}{=}Y\\).\n\nThe defining properties of a cdf: A function \\(F\\) mapping the real line to \\([0,1]\\), short \\(F:\\mathbb{R}\\to[0,1],\\) is called a cdf for some probability measure \\(P\\) if and only if it satisfies the following three properties:\n\n\\(F\\) is non-decreasing i.e. \\(x_{1}&lt;x_{2}\\) implies that \\(F\\left(x_{1}\\right) \\leq F\\left(x_{2}\\right)\\).\n\\(F\\) is normalized: \\(\\lim_{x\\rightarrow-\\infty} F(x)=0\\) and \\(\\lim_{x \\rightarrow \\infty} F(x)=1\\)\n\\(F\\) is right-continuous, i. e. \\(F(x)=F\\left(x^{+}\\right)\\) for all \\(x\\), where \\[\n  F\\left(x^{+}\\right)=\\lim_{y\\to x, y&gt;x} F(y).\n  \\]\n\nAlternatively to cumulative distribution functions one can use probability (mass) functions in order to describe the probability law of discrete random variables and density functions in order to describe the probability law of continuous random variables.\n\n2.2.1.2 Probability Functions for Discrete Random Variables\nA random variable \\(X\\) is discrete if it takes only countably many values \\[\nX\\in\\{x_{1}, x_{2}, \\ldots\\}.\n\\] For instance, \\(X\\in\\{1,2,3\\}\\) or \\(X\\in\\{2,4,6,\\dots\\}\\) or \\(X\\in\\mathbb{Z}\\) or \\(X\\in\\mathbb{Q}\\).\nWe define the probability function or probability mass function (pmf) for \\(X\\) by \\[\nf_{X}(x)=\\mathbb{P}(X=x)\\quad\\text{for all}\\quad x\\in\\{x_1,x_2,\\dots\\}\n\\]\n\n2.2.1.3 Density Functions for Continuous Random Variables\nA random variable \\(X\\) is continuous if there exists a function \\(f_{X}\\) such that\n\n\n\\(f_{X}(x)\\geq 0\\quad\\) for all \\(\\quad x\\)\n\n\n\\(\\int_{-\\infty}^{\\infty}f_{X}(x)dx=1 \\quad\\) and\n\n\\(\\mathbb{P}(a&lt;X&lt;b)=\\int_{a}^{b} f_{X}(x) dx\\quad\\) for every \\(\\quad a\\leq b\\).\n\nThe function \\(f_{X}\\) is called the probability density function (pdf) or short density function.\nFor density functions, we have that \\[\nF_{X}(x)=\\int_{-\\infty}^{x} f_{X}(t) dt\\quad\\text{and}\\quad f_{X}(x)=F_{X}^{\\prime}(x)\n\\] at all points \\(x\\) at which \\(F_{X}\\) is differentiable.\n\n2.2.2 Multivariate Distribution and Probability Functions\nA \\(d\\)-dimensional random vector is a column-vector \\(X=(X_1,\\dots,X_d)^\\prime\\), where each element is a univariate random variable.\n\n2.2.2.1 Multidimensional Distribution Function\nThe multivariate distribution function \\(F\\) is given by \\[\nF(a_1,\\dots,a_d)=P(X_1\\le a_1,\\dots,X_d\\le a_d).\n\\]\n\n## Install the package if not installed yet\n# install.packages(\"mnormt\")\n\nlibrary(mnormt)\n\nx1     &lt;- seq(-5, 5, 0.25) \nx2     &lt;- seq(-5, 5, 0.25)\nmu     &lt;- c(0, 0)\nsigma  &lt;- matrix(c(2, -1, -1, 2), nrow = 2)\nf      &lt;- function(x_1, x_2) pmnorm(cbind(x_1, x_2), mu, sigma)\nf_x1x2 &lt;- outer(x1, x2, f)\n\npersp(x = x1, y = x2, z = f_x1x2, theta = -30, phi = 25, \n      shade = 0.75, col = \"blue\", expand = 0.5, r = 2, \n      ltheta = 25, ticktype = \"detailed\", \n      xlab = \"x\\u2081\", ylab = \"x\\u2082\", zlab = \"\")\n\n\n\n\n\n\n\n\n2.2.2.2 Multidimensional Probability Mass Function\nDiscrete random vectors: \\(X\\) takes only countably many (i.e. discrete) values \\[\nX\\in\\{x_1, x_2,\\dots\\}\n\\] with \\[\nx_i=\\left(\\begin{matrix}x_{i1}\\\\\\vdots \\\\ x_{id}\\end{matrix}\\right)\\in\\mathbb{R}^d\n\\] for \\(i=1,2,\\dots\\)\nMultidimensional probability mass function\n\\[\nf_{X}(x)=\\mathbb{P}(X=x)\\quad\\text{for all}\\quad x\\in\\{x_1,x_2,\\dots\\}\n\\]\n\n\n2.2.2.3 Multidimensional Density Function\nContinuous random vectors: \\(X\\) takes values in \\(\\mathbb{R}^d\\)\n\\[\nX=\\left(\\begin{matrix}X_{1}\\\\ X_{2}\\\\ \\vdots \\\\ X_{d}\\end{matrix}\\right)\\in\\mathbb{R}^d\n\\]\nand has a multidimensional density function \\(f(x_1,\\dots,x_d)\\). That is, \\[\n\\begin{align*}\n&P(X\\in [a_1,b_1]\\times\\dots\\times [a_d,b_d])=\\\\[2ex]\n=&\\int\\limits_{a_d}^{b_d}\\dots \\int\\limits _{a_1}^{b_1}f(x_1,\\dots,x_d)dx_1\\dots dx_d.\n\\end{align*}\n\\]\nProperties of multivariate density functions:\n\n\\(\\displaystyle f(x_1,\\dots,x_d)\\geq 0\\)\n\\(\\displaystyle \\int_{-\\infty}^{\\infty}\\dots \\int_{-\\infty}^{\\infty}f(x_1,\\dots,x_d)dx_1\\dots dx_d=1\\)\n\n\n## Load the package\nlibrary(mnormt)\n\nx1     &lt;- seq(-5, 5, 0.25) \nx2     &lt;- seq(-5, 5, 0.25)\nmu     &lt;- c(0, 0)\nsigma  &lt;- matrix(c(2, -1, -1, 2), nrow = 2)\nf      &lt;- function(x_1, x_2) dmnorm(cbind(x_1, x_2), mu, sigma)\nf_x1x2 &lt;- outer(x1, x2, f)\n\npersp(x = x1, y = x2, z = f_x1x2, theta = -30, phi = 25, \n      shade = 0.75, col = \"blue\", expand = 0.5, r = 2, \n      ltheta = 25, ticktype = \"detailed\", \n      xlab = \"x\\u2081\", ylab = \"x\\u2082\", zlab = \"\")\n\n\n\n\n\n\n\nIn the following we focus only on continuous random vectors, but the discrete cases are treated analogously.\n\n2.2.2.4 Marginal Distribution Functions and Marginal Density Functions\nLet\n\\[\nX=\\left(\\begin{matrix}X_{1}\\\\ X_{2}\\\\ \\vdots \\\\ X_{d}\\end{matrix}\\right)\\in\\mathbb{R}^d\n\\]\nEach random element, \\(X_j\\), with \\(j=1,\\dots,d\\), of the random vector \\(X\\) has its own marginal distribution \\(F_j\\). This is just the univariate distribution of \\(X_j\\) when ignoring all other random variables in \\(X\\). Formally we have:\n\n\nMarginal distribution function: \\(F_j(x)=P(X_j\\leq x)\\)\n\n\nMarginal density function: \\(f_j\\), for instance, for \\(j=1\\):\n\n\\[\nf_1({\\color{blue}x_1})=\\int_{-\\infty}^{\\infty}\\dots \\int_{-\\infty}^{\\infty}f({\\color{blue}x_1},x_2\\dots,\nx_d)dx_2\\dots  dx_d\n\\]\n\n\n\n\n\n\n\n\n\n2.2.2.5 Conditional Distributions\nOften, we are interested in the conditional distribution of \\(X_j\\) given certain values of all other random variables\n\\[\nX_1=x_1,\\ldots, X_{j-1}=x_{j-1}, X_{j+1}=x_{j+1},\\ldots,X_d=x_d.\n\\] That is, the distribution of \\(X_j\\) when fixing the values of\\[\nX_1=x_1,\\ldots, X_{j-1}=x_{j-1}, X_{j+1}=x_{j+1},\\ldots, X_d=x_d.\n\\]\nAn important tool is here the conditional density of, for instance, \\(X_1\\) given \\(X_2=x_2,\\ldots,X_d=x_d\\): \\[\nf(x_1\\mid x_2,\\ldots,x_d)=\\frac{f(x_1,x_2,\\ldots,x_d)}{f_{X_{2},\\ldots,X_{d}}(x_2,\\ldots,x_d)},\n\\] where \\(f_{X_{2},\\ldots,X_{d}}\\) denotes the joint density of \\(X_2,\\ldots,X_d\\).\n\n\n\n\n\n\n\n\n\n2.2.3 Means and Moments\n\n2.2.4 Unconditional Means\nThe unconditional mean of \\(X_1\\) is given by \\[\nE(X_1)= \\int x f_{X_1}(x)dx.\n\\] The unconditional mean of a random vector \\(X=(X_1,\\dots,X_d)'\\) is given by the vector of element-wise means \\[\nE(X)=(E(X_1),\\dots,E(X_d))'.\n\\]\n\n2.2.5 Conditional Means\nOf central importance in regression analysis is the conditional mean. The conditional mean of \\(X_1\\) for given values \\(X_2=x_2,\\ldots,X_d=x_d\\): \\[\n\\begin{align*}\n  m(x_2,\\dots,x_d):&=E(X_1|X_2=x_2,\\ldots,X_d=x_d)\\\\\n                   &= \\int x_1 f(x_1\\mid x_2,\\ldots,x_d)dx_1,\n\\end{align*}                  \n\\] where \\(m(x_2,\\dots,x_d)\\) denotes the regression function.\n\n2.2.6 Means of Transformed Random Variables and Moments\nThe mean of a transformed random variable \\(r(X)\\) is given by \\[\nE(r(X))=\\int r(x) f_{X}(x)dx.\n\\] Typical transformations are, for instance\n\ncentering \\(r(x)=x-E(X)\\),\ncentering and scaling \\(r(x)=(x-E(X))/\\sqrt{Var(X)}\\),\nor \\(r(x)=(x - E(X))^2\\),\n\nwhere the latter transformation leads to the second central moment, i.e. the variance of \\(X\\), \\(Var(X)=\\int (x - E(X))^2 f_{X}(x)dx\\).\n\nThe \\(k\\)th, \\(k&gt;0\\), moment is given by \\[\n\\mu_{k}=\\mathrm{E}\\left[X^{k}\\right]=\\int_{-\\infty}^{+\\infty}x^{k} f_X(x)d x.\n\\]\nThe \\(k\\)th, \\(k&gt;1\\), central moment is given by \\[\n\\mu^c_{k}=\\mathrm{E}\\left[(X-\\mathrm{E}[X])^{k}\\right]=\\int_{-\\infty}^{+\\infty}(x-\\mu)^{k} f_X(x)d x,\n\\] where \\(\\mu=E(X)\\).\n\nNote: Moments determine the tail of a distribution (but not much else); see Lindsay and Basak (2000). Roughly: The more moments a distribution has the faster converge the tails to zero. Distributions with compact supports (e.g. the uniform distribution \\(\\mathcal{U}[a,b]\\)) have infinitely many moments. The Normal distribution has also infinitely many moments, even though this distribution has not a compact support; see Section 2.2.10.2.\n\n2.2.6.1 Law of Total Expectation\nAs long as we do not fix the values of the conditioning variables, \\(X_2,\\dots,X_d\\), they are random variables. Consequently, the conditional mean is generally itself a random variable \\[\nE(X_1|X_2,\\ldots,X_d)=\\int x_1 f(x_1\\mid X_2,\\ldots,X_d)dx_1\n\\] due to the randomness in \\(X_2,\\ldots,X_d\\).\nNote that \\(f(x_1\\mid X_2,\\ldots,X_d)\\) is just a transformation of the random variables \\(X_2,\\dots,X_d\\). So we can easily compute the unconditional mean \\(E(X_1)\\) by taking the mean of \\(E(X_1|X_2,\\ldots,X_d)\\) as following, \\[\n\\begin{align*}\n&E\\big({\\color{RedViolet}E(X_1|X_2,\\ldots,X_d)}\\big)=\\\\\n&=\\int\\dots\\int\\;{\\color{RedViolet}\\int x_1 f(x_1\\mid x_2,\\ldots,x_d)dx_1}\\;f_{X_2,\\dots,X_d}(x_2,\\ldots,x_d)dx_2\\dots dx_d\\\\\n&=\\int x_1 \\left({\\color{blue}\\int\\dots\\int f(x_1,x_2,\\ldots,x_d)dx_2\\dots dx_d}\\right)dx_1\\\\\n&=\\int x_1 {\\color{blue}f_{X_1}(x_1)}dx_1\\\\\n&=E(X_1).\n\\end{align*}\n\\]\nThe result that \\[\nE\\big(E(X_1|X_2,\\ldots,X_d)\\big)=E(X_1)\n\\] is called law of total expectation or law of iterated expectation.\n\n2.2.7 Independent Random Variables\nRandom variables \\(X_1,\\dots,X_d\\) are mutually independent if for all \\(x=(x_1,\\dots,x_d)^\\prime\\) it is true that \\[\n\\begin{align*}\n  F(x_1,\\dots,x_d)&=F_1(x_1)\\cdot F_2(x_2)\\cdot\\ldots\\cdot F_d(x_d)\\\\\n  f(x_1,\\dots,x_d)&=f_1(x_1)\\cdot f_2(x_2)\\cdot\\ldots\\cdot f_d(x_d)\n\\end{align*}\n\\]\nThe following holds true:\n\nTwo real-valued random variables \\(X\\) and \\(Y\\) are independent from each other if and only if the marginal density of \\(X\\) equals the conditional density of \\(X\\) given \\(Y=y\\) for all \\(y\\in\\mathbb{R}\\), \\[\nf_X(x)=f_{X|Y}(x\\mid y)\\quad \\text{ for all } y\\in\\mathbb{R}.\n\\] Of course, the same statement applies to the marginal density of \\(Y\\) given \\(X=x\\) for all \\(x\\in\\mathbb{R}\\). That is, \\(X\\) and \\(Y\\) are two independent real-valued random variables if and only if \\(f_Y(y)=f_{Y|X}(y\\mid x)\\) for all \\(x\\in\\mathbb{R}.\\)\n\nIf a real-valued random variable \\(X\\) is independent from a real-valued random variable \\(Y\\), then the conditional mean of \\(X\\) given \\(Y=y\\) equals the unconditional mean of \\(X\\) for all \\(y\\in\\mathbb{R}\\) \\[\nE(X\\mid Y=y)=E(X)\n\\] and likewise \\[\nE(Y\\mid X=x)=E(Y)\n\\] for all \\(x\\in\\mathbb{R}\\).\n\nNote: The properties that \\(E(X\\mid Y=y)=E(X)\\) for all \\(y\\in\\mathbb{R}\\) or that \\(E(Y\\mid X=x)=E(Y)\\) for all \\(x\\in\\mathbb{R}\\), do not imply that \\(Y\\) and \\(X\\) are independent. It only means that \\(Y\\) has no effect on the mean of \\(X\\), but it may have, for instance, an effect on the variance of \\(X\\).\n\n2.2.8 Random Samples\nTradition dictates that the sample size is denoted by the natural number \\(n\\in\\{1,2,\\dots\\}\\). A collection of random variables \\((X_{1}, \\ldots, X_{n})\\) is called a random sample if its random variables are i.i.d. (independent and identically distributed), i.e., if\n\n\n\\(X_{1}, \\ldots, X_{n}\\) are all independent from each other and\n\n\\(X_{1}, \\ldots, X_{n}\\) have identical marginal distributions, i.e., \\(X_i\\sim F_X\\) for all \\(i=1,\\dots,n\\).\n\nIn micro-econometrics, random samples are the default sampling scheme. That is, we consider the collected data as a realization of an underlying random sample.\n\n2.2.9 Some Important Discrete Random Variables\n\n\n\n\n\n\n\n\n\n\n2.2.9.1 The Discrete Uniform Distribution\nLet \\(k&gt;1\\) be a given integer. Suppose that \\(X\\) has probability mass function given by \\[\nf(x)=\\left\\{\\begin{array}{ll}\n1 / k & \\text { for } x=1, \\ldots, k \\\\\n0 & \\text { otherwise. }\n\\end{array}\\right.\n\\] We say that \\(X\\) has a uniform distribution on \\(\\{1, \\ldots, k\\}\\).\n\nset.seed(51)\n## Set the parameter k\nk &lt;- 10\n## Draw one realization from the discrete uniform distribution\nsample(x = 1:k, size = 1, replace = TRUE)\n\n[1] 7\n\n\n\n2.2.9.2 The Bernoulli Distribution\nLet \\(X\\) represent a possibly unfair coin flip. Then \\(P(X=1)=p\\) and \\(P(X=0)=1-p\\) for some \\(p \\in[0,1]\\). We say that \\(X\\) has a Bernoulli distribution written \\(X\\sim\\operatorname{Bernoulli }(p)\\). The probability function is \\(f(x)=p^{x}(1-p)^{1-x}\\) for \\(x \\in\\{0,1\\}\\)\n\nset.seed(51)\n## Set the parameter p\np &lt;- 0.25\n## Draw n realization from the discrete uniform distribution\nn &lt;- 5\nsample(x = c(0,1), size = n, prob = c(1-p, p), replace=TRUE)\n\n[1] 1 0 0 1 0\n\n## Alternatively:\n## (Bernoulli(p) equals Binomial(1,p))\nrbinom(n = n, size = 1, prob = p)\n\n[1] 1 1 0 1 0\n\n\n\n2.2.9.3 The Binomial Distribution\nSuppose we have a coin which falls heads with probability \\(p\\) for some \\(p\\in[0,1]\\). Flip the coin \\(n\\) times and let \\(X\\) be the number of heads (or successes). Assume that the tosses are independent. Let \\(f(x)=P(X=x)\\) be the mass function. It can be shown that \\[\nf(x)=\\left\\{\n\\begin{array}{ll}\n\\left(\\begin{array}{l}\nn \\\\\nx\n\\end{array}\\right) p^{x}(1-p)^{n-x} & \\text { for } x=0, \\ldots, n \\\\\n0 & \\text { otherwise. }\n\\end{array}\\right.\n\\] A random variable with this probability mas function is called a binomial random variable and we write \\(X \\sim \\operatorname{Binomial}(n, p)\\).\nIf \\(X_{1} \\sim\\) Binomial \\(\\left(n_1, p\\right)\\) and \\(X_{2} \\sim\\) Binomial \\(\\left(n_2, p\\right)\\) and if \\(X_1\\) and \\(X_2\\) are independent, then \\(X_{1}+X_{2} \\sim \\operatorname{Binomial}\\left(n_1 + n_2, p\\right).\\)\n\nset.seed(51)\n## Set the parameters n and p\nsize &lt;-   10 # number of trials\np    &lt;- 0.25 # prob of success\n\n## Draw n realization from the binomial distribution:\nn &lt;- 5\nrbinom(n = n, size = size, prob = p)\n\n[1] 4 1 2 6 1\n\n\n\n2.2.10 Some Important Continuous Random Variables\n\n2.2.10.1 The Uniform Distribution\n\\(X\\) has a \\(\\operatorname{Uniform}(a, b)\\) distribution, written \\(X\\sim \\mathcal{U}(a, b),\\) if \\[\nf(x)=\\left\\{\\begin{array}{ll}\n\\frac{1}{b-a} & \\text { for } x \\in[a, b] \\\\\n0 & \\text { otherwise }\n\\end{array}\\right.\n\\] where \\(a&lt;b\\). The distribution function is \\[\nF(x)=\\left\\{\\begin{array}{ll}\n0 & x&lt;a \\\\\n\\frac{x-a}{b-a} & x \\in[a, b] \\\\\n1 & x&gt;b\n\\end{array}\\right.\n\\]\n\n## Drawing from the uniform distribution:\nn &lt;- 10\na &lt;- 0\nb &lt;- 1\nrunif(n = n, min = a, max = b)\n\n [1] 0.83442365 0.75138318 0.40601047 0.97101998 0.11233151 0.50750617\n [7] 0.69714201 0.17104008 0.25448233 0.01813812\n\n\n\n2.2.10.2 The Normal (or Gaussian) Distribution\n\\(X\\) has a Normal (or Gaussian) distribution with parameters \\(\\mu\\) and \\(\\sigma,\\) denoted by \\(X \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right),\\) if \\[\nf(x)=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left\\{-\\frac{1}{2 \\sigma^{2}}(x-\\mu)^{2}\\right\\}, \\quad x \\in \\mathbb{R}\n\\] where \\(\\mu \\in \\mathbb{R}\\) and \\(\\sigma&gt;0.\\) Later we shall see that \\(\\mu\\) is the “center” (or mean of the distribution and \\(\\sigma\\) is the “spread” (or standard deviation) of the distribution. The Normal plays an important role in probability and statistics. Many phenomena in nature have approximately Normal distributions. The Central Limit Theorem gives a special role to the Normal distribution by stating that the distribution of averages of random variables can be approximated by a Normal distribution.\nWe say that \\(X\\) has a standard Normal distribution if \\(\\mu=0\\) and \\(\\sigma=1\\). Tradition dictates that a standard Normal random variable is denoted by \\(Z\\). The pdf and cdf of a standard Normal are denoted by \\(\\phi(z)\\) and \\(\\Phi(z)\\). There is no closed-form expression for \\(\\Phi\\). Here are some useful facts:\n\nIf \\(X \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)\\) then \\(Z=(X-\\mu) / \\sigma \\sim \\mathcal{N}(0,1)\\)\nIf \\(Z \\sim \\mathcal{N}(0,1)\\) then \\(X=\\mu+\\sigma Z \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)\\)\nIf \\(X_{i} \\sim \\mathcal{N}\\left(\\mu_{i}, \\sigma_{i}^{2}\\right), i=1, \\ldots, n\\) are independent then \\[\n\\sum_{i=1}^{n} X_{i} \\sim \\mathcal{N}\\left(\\sum_{i=1}^{n} \\mu_{i}, \\sum_{i=1}^{n} \\sigma_{i}^{2}\\right).\n\\]\n\nThe following R-codes plots the standard Normal density function:\n\n# draw a plot of the N(0,1) pdf\ncurve(dnorm(x),\n      xlim = c(-3.5, 3.5),\n      ylab = \"Density\", \n      main = \"Standard Normal Density Function\") \n\n\n\n\n\n\n\nThis is how you can draw realizations from pseudo random Normal variables:\n\n## Drawing from the uniform distribution:\nn     &lt;- 12\nmu    &lt;- 0\nsigma &lt;- 1\nrnorm(n = n, mean = mu, sd = sigma) \n\n [1]  0.085602504 -0.695791615 -1.364410561 -0.183503290 -1.675347076\n [6]  0.007303551  0.346965187  0.037914318  0.881345676 -0.882815597\n[11] -0.883560071 -0.795629557\n\n\nAn extension of the normal distribution in a univariate setting is the multivariate normal distribution. Let \\(X=(X_1,\\dots,X_k)'\\) be a \\(k\\)-dimensional normal variable, short \\(X\\sim N_k(\\mu,\\Sigma)\\) with mean vector \\(E(X)=\\mu\\in\\mathbb{R}^k\\) and covariance matrix \\(\\operatorname{Cov}(X)=\\Sigma\\). The joint density function or probability density function (pdf) of the \\(k\\)-dimensional multivariate normal distribution is \\[\nf_{X}\\left(x_{1}, \\ldots, x_{k}\\right)=\\frac{\\exp \\left(-\\frac{1}{2}(x-\\mu)' \\Sigma^{-1}(x-\\mu)\\right)}{\\sqrt{(2 \\pi)^{k}|\\Sigma|}},\n\\] where \\(|\\Sigma|\\) denotes the determinant of \\(\\Sigma\\). For \\(k=2\\) we have the bivariate pdf of two random normal variables, \\(X\\) and \\(Y\\) say \\[\n\\begin{align*}\n&g_{X,Y}(x,y) = \\frac{1}{2\\pi\\sigma_X\\sigma_Y\\sqrt{1-\\rho_{XY}^2}} \\\\\n& \\cdot \\, \\exp \\left\\{ \\frac{1}{-2(1-\\rho_{XY}^2)} \\left[ \\left( \\frac{x-\\mu_x}{\\sigma_X} \\right)^2 - 2\\rho_{XY}\\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)\\left( \\frac{y-\\mu_Y}{\\sigma_Y} \\right) + \\left( \\frac{y-\\mu_Y}{\\sigma_Y} \\right)^2 \\right]  \\right\\}.\n\\end{align*}\n\\] Lets consider the special case where \\(X\\) and \\(Y\\) are independent standard normal random variables with densities \\(f_X(x)\\) and \\(f_Y(y)\\). We then have the parameters \\(\\sigma_X = \\sigma_Y = 1\\), \\(\\mu_X=\\mu_Y=0\\) (due to marginal standard normality) and correlation \\(\\rho_{XY}=0\\) (due to independence). The joint density of \\(X\\) and \\(Y\\) then becomes \\[\ng_{X,Y}(x,y) = f_X(x) f_Y(y) = \\frac{1}{2\\pi} \\cdot \\exp \\left\\{ -\\frac{1}{2}\\left[x^2 + y^2\\right]\\right\\}.\n\\]\n\n2.2.10.3 The Chi-Squared Distribution\nThe chi-squared distribution is another distribution relevant in econometrics. It is often needed when testing special types of hypotheses frequently encountered when dealing with regression models.\nThe sum of \\(\\nu=1,2,\\dots\\) squared independent standard normal distributed random variables, \\(Z_1,\\dots,Z_\\nu\\) follows a chi-squared distribution with \\(\\nu\\) degrees of freedom (df): \\[\n\\begin{align*}\nZ_1^2 + \\dots + Z_\\nu^2 = \\sum_{j=1}^\\nu Z_j^2 =:W \\sim \\chi^2_{\\nu}.\n\\end{align*}\n\\] A \\(\\chi^2\\) distributed random variable \\(W\\) with \\(\\nu\\) degrees of freedom has expectation \\(E(W)=\\nu\\), mode at \\(\\nu-2\\) for \\(\\nu \\geq 2\\) and variance \\(2 \\cdot \\nu.\\)\nUsing the code below, we can display the pdf and the distribution function or cumulated density function (cdf) of a \\(\\chi^2_3\\) random variable in a single plot. This is achieved by setting the argument add = TRUE\" in the second call of \"curve()\". Further we adjust limits of both axes using \"xlim\" and \"ylim\" and choose different colors to make both functions better distinguishable. The plot is completed by adding a legend with help of \"legend()\".\n\n# plot the pdf\ncurve(dchisq(x, df = 3), \n      xlim = c(0, 10), \n      ylim = c(0, 1), \n      col = \"blue\",\n      ylab = \"\",\n      main = \"pdf and cdf of Chi-Squared Distribution (df = 3)\")\n\n# add the cdf to the plot\ncurve(pchisq(x, df = 3), \n      xlim = c(0, 10), \n      add = TRUE, \n      col = \"red\")\n\n# add a legend to the plot\nlegend(\"topleft\", \n       c(\"pdf\", \"cdf\"), \n       col = c(\"blue\", \"red\"), \n       lty = c(1, 1))\n\n\n\n\n\n\n\nSince the outcomes of a \\(\\chi^2_\\nu\\) distributed random variable are always positive, the support of the related pdf and cdf is \\(\\mathbb{R}_{\\geq 0}\\).\nAs expectation and variance depend (solely!) on the degrees of freedom, the distribution’s shape changes drastically if we vary the degrees of freedoms.\n\n# plot the density for M=1\ncurve(dchisq(x, df = 1), \n      xlim = c(0, 15), \n      xlab = \"x\", \n      ylab = \"Density\", \n      main = \"Chi-Square Distributed Random Variables\")\n\n# add densities for M=2,...,7 to the plot using a 'for()' loop \nfor (M in 2:7) {\n  curve(dchisq(x, df = M),\n        xlim = c(0, 15), \n        add = T, \n        col = M)\n}\n\n# add a legend\nlegend(\"topright\", \n       as.character(1:7), \n       col = 1:7 , \n       lty = 1, \n       title = \"df\")\n\n\n\n\n\n\n\nIncreasing the degrees of freedom shifts the distribution to the right (the mode becomes larger) and increases the dispersion (the distribution’s variance grows).\n\n2.2.10.4 The Student \\(t\\) Distribution\nLet \\(Z\\) be a standard normal random variable, \\(W\\) a \\(\\chi^2_\\nu\\) random variable and further assume that \\(Z\\) and \\(W\\) are independent. Then it holds that \\[\n\\frac{Z}{\\sqrt{W/\\nu}} =:X \\sim t_\\nu\n\\] and \\(X\\) follows a Student \\(t\\) distribution (or simply \\(t\\) distribution) with \\(\\nu\\) degrees of freedom.\nThe shape of a \\(t_\\nu\\) distribution depends on \\(\\nu\\). \\(t\\) distributions are symmetric, bell-shaped and look similar to a normal distribution, especially when \\(\\nu\\) is large. This is not a coincidence: for a sufficiently large \\(\\nu\\), the \\(t_\\nu\\) distribution can be approximated by the standard normal distribution. This approximation works reasonably well for \\(\\nu\\geq 30\\).\nA \\(t_\\nu\\) distributed random variable \\(X\\) has an expectation if \\(\\nu&gt;1\\) and it has a variance if \\(\\nu&gt;2\\). \\[\n\\begin{align*}\n  E(X) =& 0, \\qquad \\nu &gt; 1 \\\\\n  \\text{Var}(X) =& \\frac{\\nu}{\\nu-2}, \\qquad \\nu &gt; 2\n\\end{align*}\n\\]\nLet us plot some \\(t\\) distributions with different degrees of freedoms \\(\\nu\\) and compare them to the standard normal distribution.\n\n# plot the standard normal density\ncurve(dnorm(x), \n      xlim = c(-4, 4), \n      xlab = \"x\", \n      lty = 2, \n      ylab = \"Density\", \n      main = \"Densities of t Distributions\")\n\n# plot the t density for M=2\ncurve(dt(x, df = 2), \n      xlim = c(-4, 4), \n      col = 2, \n      add = T)\n\n# plot the t density for M=4\ncurve(dt(x, df = 4), \n      xlim = c(-4, 4), \n      col = 3, \n      add = T)\n\n# plot the t density for M=25\ncurve(dt(x, df = 25), \n      xlim = c(-4, 4), \n      col = 4, \n      add = T)\n\n# add a legend\nlegend(\"topright\", \n       c(\"N(0, 1)\", \"df=2\", \"df=4\", \"df=25\"), \n       col = 1:4, \n       lty = c(2, 1, 1, 1))\n\n\n\n\n\n\n\nThe plot illustrates that as the degrees of freedom increase, the shape of the \\(t\\) distribution comes closer to that of a standard normal bell curve. Already for \\(\\nu=25\\) we find little difference to the standard normal density. If \\(\\nu\\) is small, we find the distribution to have heavier tails than a standard normal.\n\n2.2.10.5 Cauchy Distribution\nThe Cauchy distribution is a special case of the \\(t\\) distribution corresponding to \\(\\nu=1\\). The density is \\[\nf(x)=\\frac{1}{\\pi\\left(1+x^{2}\\right)}.\n\\]       \nFor the Cauchy distribution, the expectation does not exist – that is, it has no mean. Let’s try to compute the mean of a Cauchy distribution and see what goes wrong. Its mean should be \\[\n\\mu=E(X)=\\int_{-\\infty}^{\\infty} \\frac{x d x}{\\pi\\left(1+x^{2}\\right)}.\n\\] In order for this improper integral to exist, we need both integrals \\(\\int_{-\\infty}^{0}\\) and \\(\\int_{0}^{\\infty}\\) to be finite. Let’s look at the second integral. \\[\n\\int_{0}^{\\infty} \\frac{x d x}{\\pi\\left(1+x^{2}\\right)}=\\left.\\frac{1}{2 \\pi} \\log \\left(1+x^{2}\\right)\\right|_{0} ^{\\infty}=\\infty\n\\] Similarly, the other integral, \\(\\int_{-\\infty}^{0},\\) is \\(-\\infty\\). Since they’re not both finite, the integral \\(\\int_{-\\infty}^{\\infty}\\) doesn’t exist. In other words \\(\\infty-\\infty\\) is not a number. Thus, the Cauchy distribution has no mean.\nWhat this means in practice is that if you take a sample \\(x_{1}, x_{2}, \\ldots, x_{n}\\) from the Cauchy distribution, then the average \\(\\bar{x}\\) does not tend to a particular number. Instead, every so often you will get such a huge number, either positive or negative, that the average is overwhelmed by it.\n\n2.2.10.6 The F Distribution\nAnother ratio of random variables important to econometricians is the ratio of two independent \\(\\chi^2\\) distributed random variables that are divided by their degrees of freedom \\(m\\) and \\(n\\). The quantity\n\\[\n\\frac{W/m}{V/n} \\sim F_{m,n} \\ \\ \\text{with} \\ \\ W \\sim \\chi^2_m \\ \\ , \\ \\ V \\sim \\chi^2_n\n\\] follows an \\(F\\) distribution with numerator degrees of freedom \\(m\\) and denominator degrees of freedom \\(n\\), denoted \\(F_{m,n}\\). The distribution was first derived by George Snedecor but was named in honor of Sir Ronald Fisher.\nBy definition, the support of both pdf and cdf of an \\(F_{m,n}\\) distributed random variable is \\(\\mathbb{R}_{\\geq0}\\).\nSay we have an \\(F\\) distributed random variable \\(Y\\) with numerator degrees of freedom \\(3\\) and denominator degrees of freedom \\(14\\) and are interested in \\(P(Y \\geq 2)\\). This can be computed with help of the function \"pf()\". By setting the argument \"lower.tail\" to \"FALSE\" we ensure that R computes \\(1- P(Y \\leq 2)\\), i.e,the probability mass in the tail right of \\(2\\).\n\npf(2, df1 = 3, df2 = 14, lower.tail = F)\n\n[1] 0.1603538\n\n\nWe can visualize this probability by drawing a line plot of the related density and adding a color shading with \"polygon()\".\n\n# define coordinate vectors for vertices of the polygon\nx &lt;- c(2, seq(2, 10, 0.01), 10)\ny &lt;- c(0, df(seq(2, 10, 0.01), 3, 14), 0)\n\n# draw density of F_{3, 14}\ncurve(df(x ,3 ,14), \n      ylim = c(0, 0.8), \n      xlim = c(0, 10), \n      ylab = \"Density\",\n      main = \"Density Function\")\n\n# draw the polygon\npolygon(x, y, col = \"orange\")\n\n\n\n\n\n\n\nThe \\(F\\) distribution is related to many other distributions. An important special case encountered in econometrics arises if the denominator degrees of freedom are large such that the \\(F_{m,n}\\) distribution can be approximated by the \\(F_{m,\\infty}\\) distribution which turns out to be simply the distribution of a \\(\\chi^2_m\\) random variable divided by its degrees of freedom \\(m,\\) i.e.  \\[\n\\frac{W}{m} \\sim F_{m,\\infty} \\quad\\text{with}\\quad W \\sim \\chi^2_m.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-Probability.html#exercises",
    "href": "02-Probability.html#exercises",
    "title": "2  Probability",
    "section": "\n2.3 Exercises",
    "text": "2.3 Exercises\n\nExercises for Chapter 2\nExercises for Chapter 2 with Solutions",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "02-Probability.html#references",
    "href": "02-Probability.html#references",
    "title": "2  Probability",
    "section": "References",
    "text": "References\n\n\n\n\nLindsay, Bruce G., and Prasanta Basak. 2000. “Moments Determine the Tail of a Distribution (but Not Much Else).” The American Statistician 54 (4): 248–51.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. Vol. 26. Springer.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "03-Multiple-Linear-Regression.html",
    "href": "03-Multiple-Linear-Regression.html",
    "title": "3  Multiple Linear Regression",
    "section": "",
    "text": "In the following we focus on case of random designs \\(X\\) (i.e. \\(X\\) being a random variable), since, first, this is the more relevant case in econometrics and, second, it includes the case of fixed designs (i.e. \\(X\\) being deterministic) as a special case (“degenerated random variable”). Caution: A random \\(X\\) requires us to consider conditional means and variances “given \\(X\\).” That is, if we would be able to resample from the model, we do so by fixing (conditioning on) the in-principle random explanatory variable \\(X\\)."
  },
  {
    "objectID": "03-Multiple-Linear-Regression.html#sec-LinModAssumptions",
    "href": "03-Multiple-Linear-Regression.html#sec-LinModAssumptions",
    "title": "3  Multiple Linear Regression",
    "section": "3.1 Assumptions",
    "text": "3.1 Assumptions\nThe multiple linear regression model is defined by the following assumptions which together describe the relevant theoretical aspects of the underlying data generating process:\nAssumption 1: Model and Sampling\nPart (a): Linear Model\n\\[\n\\begin{align}\n  Y_i=\\sum_{k=1}^K\\beta_k X_{ik}+\\varepsilon_i, \\quad i=1,\\dots,n.\n\\end{align}\n\\tag{3.1}\\] Usually, a constant (intercept) is included, in this case \\(X_{i1}=1\\) for all \\(i=1,\\dots,n.\\) In the following we will always assume that \\(X_{i1}=1\\) for all \\(i\\), unless otherwise stated.\nIt is convenient to write Equation 3.1 using matrix notation \\[\\begin{eqnarray*}\n  Y_i&=&\\underset{(1\\times K)}{X_i'}\\underset{(K\\times 1)}{\\beta} +\\varepsilon_i, \\quad i=1,\\dots,n,\n\\end{eqnarray*}\\] where \\(X_i=(X_{i1},\\dots,X_{iK})'\\) and \\(\\beta=(\\beta_1,\\dots,\\beta_K)'\\). Stacking all individual rows \\(i\\) leads to \\[\\begin{eqnarray*}\\label{LM}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\\] where \\[\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\\]\nPart (b): Random Sample\nMoreover, we assume that the observed (“obs”) data points \\[\n((Y_{1,obs},X_{11,obs},\\dots,X_{1K,obs}),(Y_{2,obs},X_{21,obs},\\dots,X_{2K,obs}),\\dots,(Y_{n,obs},X_{n1,obs},\\dots,X_{nK,obs}))\n\\] are realizations of a random sample \\[\n((Y_{1},X_{11},\\dots,X_{1K}),(Y_{2},X_{21},\\dots,X_{2K}),\\dots,(Y_{n},X_{n1},\\dots,X_{nK})).\n\\]\nThat is, the \\(i\\)th observed \\(K+1\\) dimensional data point \\((Y_{i,obs},X_{i1,obs},\\dots,X_{iK,obs})\\in\\mathbb{R}^{K+1}\\) is a realization of a \\(K+1\\) dimensional random variable \\((Y_{i},X_{i1},\\dots,X_{iK})\\in\\mathbb{R}^{K+1},\\) where\n\n\\((Y_{i},X_{i1},\\dots,X_{iK})\\) has the identical \\(K+1\\) dimensional distribution for all \\(i=1,\\dots,n.\\)\n\\((Y_{i},X_{i1},\\dots,X_{iK})\\) is independent of \\((Y_{j},X_{j1},\\dots,X_{jK})\\) for all \\(i\\neq j=1,\\dots,n.\\)\n\nNote: Due to Equation 3.1, this i.i.d. assumption is equivalent to assuming that the multivariate random variables \\((\\varepsilon_i,X_{i1},\\dots,X_{iK})\\in\\mathbb{R}^{K+1}\\) are i.i.d. across \\(i=1,\\dots,n\\).\nRemark: Usually, we do not use a different notation for observed realizations \\((Y_{i,obs},X_{i1,obs},\\dots,X_{iK,obs})\\in\\mathbb{R}^{K+1}\\) and for the corresponding random variable \\((Y_{i},X_{i1},\\dots,X_{iK})\\in\\mathbb{R}^{K+1}\\) since often both interpretations (random variable and its realizations) can make sense in the same statement and then it depends on the considered context whether the random variables point if view or the realization point of view applies.\n\n\n\nAssumption 2: Exogeneity \\[E(\\varepsilon_i|X_i)=0,\\quad i=1,\\dots,n\\] This assumption demands that the mean of the random error term \\(\\varepsilon_i\\) is zero irrespective of the realizations of \\(X_i\\).\nThis exogeneity assumption is also called “orthogonality assumption” or “mean independence assumption.”\nNote: Together with the random sample assumption (Assumption 1, Part (b)) this assumption implies even strict exogeneity \\(E(\\varepsilon_i|X) = 0\\) since we have independence across \\(i=1,\\dots,n\\). Under strict exogeneity, the mean of \\(\\varepsilon_i\\) is zero irrespective of the realizations of \\(X_1,\\dots,X_n\\).\n\n\nAssumption 3: Rank Condition (no perfect multicollinearity)\n\\[\\operatorname{rank}(X)=K\\quad\\text{a.s.}\\] This assumption demands that the event of one explanatory variable being linearly dependent on the others occurs with a probability equal to zero. (This is the literal translation of the “almost surely (a.s.)” concept.) The assumption implies that \\(n\\geq K\\).\nThis assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: dummy variable trap, multicollinearity, variance inflation). The violation of this assumption harms any economic interpretation as we cannot disentangle the explanatory variables’ individual effects on \\(Y\\). Therefore, this assumption is also often called an “identification assumption.”\nAssumption 4: Error distribution\nDepending on the context (i.e., parameter estimation vs. hypothesis testing and small \\(n\\) vs. large \\(n\\)) there are different more or less restrictive assumptions. Some of the most common ones are the following (roughly order from least to most restrictive):\n\nConditional distribution: \\(\\varepsilon_i|X_i \\sim f_{\\varepsilon|X}\\) for all \\(i=1,\\dots,n\\) and for any distribution \\(f_{\\varepsilon|X}\\) such that \\(\\varepsilon_i|X_i\\) has two (or more) finite moments.\nConditional normal distribution: \\(\\varepsilon_i|X_i \\sim \\mathcal{N}(0,\\sigma^2(X_i))\\) for all \\(i=1,\\dots,n\\).\nIndependence between error and predictors: \\(\\varepsilon_i\\sim f_\\varepsilon\\) such that \\(f_\\varepsilon=f_{\\varepsilon|X}\\) and such that \\(f_\\varepsilon\\) has two (or more) finite moments.\n\nIndependence between error and predictors and normal: As above, but with \\(f_\\varepsilon=\\mathcal{N}(0,\\sigma^2)\\).\nSpherical errors (“Gauss-Markov assumptions”): The conditional distributions of \\(\\varepsilon_i|X_i\\) may generally depend on \\(X_i\\), but such that \\[\\begin{align*}\nE(\\varepsilon_i^2|X_i)         &=\\sigma^2>0\\quad\\text{for all }i=1,\\dots,n\\\\\nE(\\varepsilon_i\\varepsilon_j|X)&=0\\quad\\text{for all }i\\neq j\\quad\\text{with}\\quad i=1,\\dots,n\\quad\\text{and}\\quad j=1,\\dots,n.\n\\end{align*}\\] Thus, here one assumes that, for a given realization of \\(X_i\\), the error process is uncorrelated (i.e. \\(Cov(\\varepsilon_i,\\varepsilon_j|X)=E(\\varepsilon_i\\varepsilon_j|X)=0\\) for all \\(i\\neq j\\)) and homoskedastic (i.e. \\(Var(\\varepsilon_i|X)=\\sigma^2\\) for all \\(i\\)).\n\n\n\nHomoskedastic versus Heteroskedastic Error Terms\nThe i.i.d. assumption is not as restrictive as it may seem on first sight. It allows for dependence between \\(\\varepsilon_i\\) and \\((X_{i1},\\dots,X_{iK})\\in\\mathbb{R}^K\\). That is, the error term \\(\\varepsilon_i\\) can have a conditional distribution which depends on \\((X_{i1},\\dots,X_{iK})\\); see Section 2.2.2.5.\n\n\nThe exogeneity assumption (Assumption 2: Exogeneity) requires that the conditional mean of \\(\\varepsilon_i\\) is independent of \\(X_i\\). Besides this, dependencies between \\(\\varepsilon_i\\) and \\(X_{i1},\\dots,X_{iK}\\) are allowed. For instance, the variance of \\(\\varepsilon_i\\) can be a function of \\(X_{i1},\\dots,X_{iK}\\). If this is the case, \\(\\varepsilon_i\\) is said to be “heteroskedastic.”\n\nHeteroskedastic error terms: The conditional variances \\(Var(\\varepsilon_i|X_i=x_i)=\\sigma^2(x_i)\\) are equal to a non-constant variance-function \\(\\sigma^2(x_i)>0\\) which is a function of the realization \\(X_i=x_i.\\)\n\nExample: \\(\\varepsilon_i|X_i\\sim U[-0.5|X_{i2}|, 0.5|X_{i2}|],\\) with \\(X_{i2}\\sim U[-4,4]\\). This error term is mean independent of \\(X_i\\) since \\(E(\\varepsilon_i|X_i)=0\\), but it has a heteroskedastic conditional variance since \\(Var(\\varepsilon_i|X_i)=\\frac{1}{12}X_i^2\\) depends on \\(X_i\\).\nSometimes, we need to be more restrictive by assuming that also the variances of the error terms \\(\\varepsilon_i\\) are independent from \\(X_i\\). (Higher moments may still depend on \\(X_i\\).) This assumption leads to “homoskedastic” error terms.\n\nHomoskedastic error terms: The conditional variances \\(Var(\\varepsilon_i|X_i=x_i)=\\sigma^2\\) are equal to some constant \\(\\sigma^2>0\\) for every possible realization \\(X_i=x_i.\\)\n\n\n\n\n\n\n\n\nExample: For doing small sample inference (see Chapter 5), we need to assume that the error terms \\(\\varepsilon_i\\) are i.i.d. across \\(i=1,\\dots,n\\) plus the normality assumption, i.e., \\(\\varepsilon_i\\stackrel{\\textrm{i.i.d.}}{\\sim}{\\mathcal N} (0, \\sigma^2)\\) for all \\(i=1,\\dots,n\\) which leads to homoskedastic variances \\(Var(\\varepsilon_i|X_i)=\\sigma^2\\) for every possible realization of \\(X_i\\).\n\n\n3.1.1 Some Implications of the Exogeneity Assumption\n\nTheorem 3.1 (Unconditional Mean) If \\(E(\\varepsilon_i|X_i)=0\\) for all \\(i=1,\\dots,n\\), then the also the unconditional mean of the error term is zero, i.e. \\[\nE(\\varepsilon_i)=0,\\quad i=1,\\dots,n.\n\\]\n\n\nProof. Using the Law of Total Expectations (i.e., \\(E[E(Z|X)]=E(Z)\\)) we can rewrite \\(E(\\varepsilon_i)\\) as \\[\nE(\\varepsilon_i)=E[E(\\varepsilon_i|X_i)]\n\\] for all \\(i=1,\\dots,n.\\) But the exogeneity assumption yields \\[\nE[E(\\varepsilon_i|X_i)]=E[0]=0\n\\] for all \\(i=1,\\dots,n,\\) which completes the proof. \\(\\square\\)\n\nGenerally, two random variables \\(X\\) and \\(Y\\) are said to be orthogonal if their cross moment is zero, i.e. if \\(E(XY)=0\\). Exogeneity is sometimes also called “orthogonality,” due to the following result.\n\nTheorem 3.2 (Orthogonality) Under exogeneity, i.e. if \\(E(\\varepsilon_i|X_{i})=0\\), the regressors and the error term are orthogonal to each other, i.e, \\[\nE(X_{ik}\\varepsilon_i)=0\n\\] for all \\(i=1,\\dots,n\\) and \\(k=1,\\dots,K\\).\n\n\nProof. \\[\\begin{align*}\nE(X_{ik}\\varepsilon_i)\n&=E(E(X_{ik}\\varepsilon_i|X_{ik}))\\quad{\\small\\text{(By the Law of Total Expectations)}}\\\\\n&=E(X_{ik}E(\\varepsilon_i|X_{ik}))\\quad{\\small\\text{(By the linearity of cond.~expectations)}}\n\\end{align*}\\] Now, to show that \\(E(X_{ik}\\varepsilon_i)=0\\), we need to show that \\(E(\\varepsilon_i|X_{ik})=0,\\) which is done in the following:\nSince \\(X_{ik}\\) is an element of \\(X_i,\\) a slightly more sophisticated use of the Law of Total Expectations (i.e., \\(E(Y|X)=E(E(Y|X,Z)|X)\\)) implies that \\[\nE(\\varepsilon_i|X_{ik})=E(E(\\varepsilon_i|X_i)|X_{ik}).\n\\] So, the exogeneity assumption, \\(E(\\varepsilon_i|X_i)=0\\) yields \\[\nE(\\varepsilon_i|X_{ik})=E(\\underbrace{E(\\varepsilon_i|X_i)}_{=0}|X_{ik})=E(0|X_{ik})=0.\n\\] I.e., we have that \\(E(\\varepsilon_i|X_{ik})=0\\) which allows us to conclude that \\[\nE(X_{ik}\\varepsilon_i)=E(X_{ik}E(\\varepsilon_i|X_{ik}))=E(X_{ik}0)=0\n\\] which completes the proof. \\(\\square\\)\n\nBecause the mean of the error term is zero (\\(E(\\varepsilon_i)=0\\) for all \\(i\\) (see Theorem 3.1), it follows that the orthogonality property, \\(E(X_{ik}\\varepsilon_i)=0,\\) is equivalent to a zero correlation property.\n\nTheorem 3.3 (No Correlation) If \\(E(\\varepsilon_i|X_{i})=0\\), then \\[\\begin{eqnarray*}\n  Cov(\\varepsilon_i,X_{ik})&=&0\\quad\\text{for all}\\quad i=1,\\dots,n\\quad\\text{and}\\quad k=1,\\dots,K.\n\\end{eqnarray*}\\]\n\n\nProof. \\[\\begin{eqnarray*}\n  Cov(\\varepsilon_i,X_{ik})&=&E(X_{ik}\\varepsilon_i)-E(X_{ik})\\,E(\\varepsilon_i)\\quad{\\small\\text{(Def. of Cov)}}\\\\\n  &=&E(X_{ik}\\varepsilon_i)\\quad{\\small\\text{(By point (a): $E(\\varepsilon_i)=0$)}}\\\\\n  &=&0\\quad{\\small\\text{(By orthogonality result in point (b))}}\\quad\\square\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "03-Multiple-Linear-Regression.html#deriving-the-expression-of-the-ols-estimator",
    "href": "03-Multiple-Linear-Regression.html#deriving-the-expression-of-the-ols-estimator",
    "title": "3  Multiple Linear Regression",
    "section": "3.2 Deriving the Expression of the OLS Estimator",
    "text": "3.2 Deriving the Expression of the OLS Estimator\nWe derive the expression for the OLS estimator \\(\\hat\\beta=(\\hat\\beta_1,\\dots,\\hat\\beta_K)'\\in\\mathbb{R}^K\\) as the vector-valued minimizing argument of the sum of squared residuals, \\(S_n(b)\\) with \\(b\\in\\mathbb{R}^K\\), for a given sample \\(((Y_1,X_1),\\dots,(Y_n,X_n))\\). In matrix terms this is \\[\\begin{align*}\nS_n(b)&=(Y-X b)^{\\prime}(Y-X b)=Y^{\\prime}Y-2 Y^{\\prime} X b+b^{\\prime} X^{\\prime} X b.\n\\end{align*}\\] To find the minimizing argument \\[\\hat\\beta=\\arg\\min_{b\\in\\mathbb{R}^K}S_n(b)\\] we compute all partial derivatives \\[\n\\begin{aligned}\n\\underset{(K\\times 1)}{\\frac{\\partial S(b)}{\\partial b}} &=-2\\left(X^{\\prime}Y -X^{\\prime} Xb\\right)\n\\end{aligned}\n\\] and set them equal to zero which leads to \\(K\\) linear equations (the “normal equations”) in \\(K\\) unknowns. This system of equations defines the OLS estimates, \\(\\hat{\\beta}\\), for a given data-set: \\[\n\\begin{aligned}\n-2\\left(X^{\\prime}Y -X^{\\prime} X\\hat{\\beta}\\right)=\\underset{(K\\times 1)}{0}.\n\\end{aligned}\n\\] From our rank assumption (Assumption 3) it follows that \\(X^{\\prime}X\\) is an invertible matrix which allows us to solve the equation system by \\[\n\\begin{aligned}\n\\underset{(K\\times 1)}{\\hat{\\beta}} &=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} Y.\n\\end{aligned}\n\\]\nThe following codes computes the estimate \\(\\hat{\\beta}\\) for a given realization \\((Y,X)\\) of the random sample \\((Y,X)\\) with \\(X_i\\in\\mathbb{R}^3\\).\n\n# Some given data\nX_2     <- c(1.9,0.8,1.1,0.1,-0.1,4.4,4.6,1.6,5.5,3.4)\nX_3     <- c(66, 62, 64, 61, 63, 70, 68, 62, 68, 66)\nY       <- c(0.7,-1.0,-0.2,-1.2,-0.1,3.4,0.0,0.8,3.7,2.0)\ndataset <-  data.frame(\"X_2\" = X_2, \"X_3\" = X_3, \"Y\" = Y)\n## Compute the OLS estimation\nlmobj   <- lm(Y ~ X_2 + X_3, data = dataset)\n## Plot sample regression surface\nlibrary(\"scatterplot3d\") # library for 3d plots\nplot3d  <- scatterplot3d(x = X_2, y = X_3, z = Y,\n            angle = 33, scale.y = 0.8, pch = 16,\n            color =\"red\", \n            xlab = expression(X[2]),\n            ylab = expression(X[3]),\n            main =\"OLS Regression Surface\")\nplot3d$plane3d(lmobj, lty.box = \"solid\", col=gray(.5), draw_polygon=TRUE)"
  },
  {
    "objectID": "03-Multiple-Linear-Regression.html#some-quantities-of-interest",
    "href": "03-Multiple-Linear-Regression.html#some-quantities-of-interest",
    "title": "3  Multiple Linear Regression",
    "section": "3.3 Some Quantities of Interest",
    "text": "3.3 Some Quantities of Interest\nPredicted values and residuals.\n\nThe (OLS) predicted values: \\(\\hat{Y}_i=X_i'\\hat\\beta\\).\nIn matrix notation: \\(\\hat Y=X\\underbrace{(X'X)^{-1}X'Y}_{\\hat\\beta}=P_X Y\\)\nThe (OLS) residuals: \\(\\hat\\varepsilon_i=Y_i-\\hat{Y}_i\\). In matrix notation: \\(\\hat\\varepsilon=Y-\\hat{Y}=\\left(I_n-X(X'X)^{-1}X'\\right)Y=M_X Y\\)\n\nProjection matrices.\nThe matrix \\[\nP_X=X(X'X)^{-1}X'\n\\] is the \\((n\\times n)\\) projection matrix that projects any vector from \\(\\mathbb{R}^n\\) into the column space spanned by the column vectors of \\(X\\) and \\[\nM_X=I_n-X(X'X)^{-1}X'=I_n-P_X\n\\] is the associated \\((n\\times n)\\) orthogonal projection matrix that projects any vector from \\(\\mathbb{R}^n\\) into the vector space that is orthogonal to that spanned by \\(X\\).\nThe projection matrices \\(P_X\\) and \\(M_X\\) have some nice properties:\n\n\\(P_X\\) and \\(M_X\\) are symmetric, i.e. \\(P_X=P_X'\\) and \\(M_X=M_X'\\).\n\\(P_X\\) and \\(M_X\\) are idempotent, i.e. \\(P_XP_X=P_X\\) and \\(M_X M_X=M_X\\).\nMoreover, we have that \\(X'P_X=X'\\), \\(P_XX=X\\), \\(X'M_X=0\\), \\(M_XX=0\\), and \\(P_XM_X=0\\).\n\nThe properties (a)-(c) follow directly from the definitions of \\(P_X\\) and \\(M_X\\) (check it out). Using these properties one can show that the residual vector \\(\\hat\\varepsilon=(\\hat\\varepsilon_1,\\dots,\\hat\\varepsilon_n)'\\) is orthogonal to each of the column vectors in \\(X\\), i.e \\[\\begin{eqnarray}\nX'\\hat\\varepsilon&=&X'M_XY\\quad\\text{\\small(By Def.~of $M_X$)}\\\\\n\\Leftrightarrow X'\\hat\\varepsilon&=&\\underset{(K\\times n)}{0}\\underset{(n\\times 1)}{Y}\\quad\\text{\\small(since $X'M_X=0$)}\\\\\n\\Leftrightarrow X'\\hat\\varepsilon&=&\\underset{(K\\times 1)}{0}.\n\\end{eqnarray}\\]    Note that, in the case with intercept, the result \\(X'\\hat\\varepsilon=0\\) implies that \\(\\sum_{i=1}^n\\hat\\varepsilon_i=0\\). Moreover, the equation \\(X'\\hat\\varepsilon=0\\) implies also that the residual vector \\(\\hat{\\varepsilon}\\) is orthogonal to the predicted values vector, since \\[\\begin{align*}\nX'\\hat\\varepsilon&=0\\\\\n\\Rightarrow\\;\\hat\\beta'X'\\hat\\varepsilon&=\\hat\\beta'0\\\\\n\\Leftrightarrow\\;\\hat Y'\\hat\\varepsilon&=0.\n\\end{align*}\\]\nAnother insight from equation \\(X'\\hat\\varepsilon=0\\) is that the vector \\(\\hat\\varepsilon\\) has to satisfy \\(K\\) linear restrictions which means it looses \\(K\\) degrees of freedom.1 Consequently, the vector of residuals \\(\\hat\\varepsilon\\) has only \\(n-K\\) so-called degrees of freedom. This loss of \\(K\\) degrees of freedom also appears in the definition of the unbiased variance estimator \\[\\begin{align}\n  s_{UB}^2&=\\frac{1}{n-K}\\sum_{i=1}^n\\hat\\varepsilon_i^2\\label{EqVarEstim}.\n\\end{align}\\]\nVariance decomposition: A further useful result that can be shown using the properties of \\(P_X\\) and \\(M_X\\) is that \\(Y'Y=\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\\), i.e. \\[\\begin{eqnarray*}\nY'Y&=&(\\hat Y+\\hat\\varepsilon)'(\\hat Y+\\hat\\varepsilon)\\notag\\\\\n  &=&(P_XY+M_XY)'(P_XY+M_XY)\\notag\\\\\n  &=&(Y'P_X'+Y'M_X')(P_XY+M_XY)\\notag\\\\\n  &=&Y'P_X'P_XY+Y'M_X'M_XY+0\\notag\\\\\n  &=&\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\n\\end{eqnarray*}\\] The decomposition \\[\n\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\n\\] is the basis for the well-known variance decomposition result for OLS regressions.\n\nTheorem 3.4 For the linear regression model with intercept (Equation 3.1), the total sample variance of the dependent variable \\(Y_1,\\dots,Y_n\\) can be decomposed as following: \\[\\begin{eqnarray}\n\\underset{\\text{total sample variance}}{\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2}&=&\\underset{\\text{explained sample variance}}{\\frac{1}{n}\\sum_{i=1}^n\\left(\\hat{Y}_i-\\bar{\\hat{Y}}\\right)^2}+\\underset{\\text{unexplained sample variance}}{\\frac{1}{n}\\sum_{i=1}^n\\hat\\varepsilon_i^2,}\\label{VarDecomp}\n\\end{eqnarray}\\] where \\(\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^nY_i\\) and \\(\\bar{\\hat{Y}}=\\frac{1}{n}\\sum_{i=1}^n\\hat{Y}_i\\).\n\n\nProof. From equation \\(X'\\hat\\varepsilon=0\\) we have for regressions with intercept that \\(\\sum_{i=1}^n\\hat\\varepsilon_i=0\\). Hence, from \\(Y_i=\\hat{Y}_i+\\hat\\varepsilon_i\\) it follows that \\[\\begin{eqnarray*}\n  \\frac{1}{n}\\sum_{i=1}^n Y_i&=&\\frac{1}{n}\\sum_{i=1}^n \\hat{Y}_i+\\frac{1}{n}\\sum_{i=1}^n \\hat\\varepsilon_i\\\\\n  \\bar{Y}&=&\\bar{\\hat{Y}}+0\n\\end{eqnarray*}\\]\nUsing the decomposition \\(Y'Y=\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\\), we can now derive the result: \\[\\begin{eqnarray*}\n   Y'Y&=&\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\\\\\n   Y'Y-n\\bar{Y}^2&=&\\hat{Y}'\\hat{Y}-n\\bar{Y}^2+\\hat\\varepsilon'\\hat\\varepsilon\\\\\n   Y'Y-n\\bar{Y}^2&=&\\hat{Y}'\\hat{Y}-n\\bar{\\hat{Y}}^2+\\hat\\varepsilon'\\hat\\varepsilon\\quad\\text{(by $\\bar{Y}=\\bar{\\hat{Y}}$)}\\\\\n   \\sum_{i=1}^nY_i^2-n\\bar{Y}^2&=&\\sum_{i=1}^n\\hat{Y}_i^2-n\\bar{\\hat{Y}}^2+\\sum_{i=1}^n\\hat\\varepsilon_i^2\\\\\n   \\sum_{i=1}^n(Y_i-\\bar{Y})^2&=&\\sum_{i=1}^n(\\hat{Y}_i-\\bar{\\hat{Y}})^2+\\sum_{i=1}^n\\hat\\varepsilon_i^2\\quad\\square\\\\\n\\end{eqnarray*}\\]\n\n\nCoefficients of determination: \\(R^2\\) and \\(\\overline{R}^2\\)\nThe larger the proportion of the explained variance, the better is the fit of the model. This motivates the definition of the so-called \\(R^2\\) coefficient of determination: \\[\\begin{eqnarray*}\nR^2=\\frac{\\sum_{i=1}^n\\left(\\hat{Y}_i-\\bar{\\hat{Y}}\\right)^2}{\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2}\\;=\\;1-\\frac{\\sum_{i=1}^n\\hat{\\varepsilon}_i^2}{\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2}\n\\end{eqnarray*}\\] Obviously, we have that \\(0\\leq R^2\\leq 1\\). The closer \\(R^2\\) lies to \\(1\\), the better is the fit of the model to the observed data. However, a high/low \\(R^2\\) does not mean a validation/falsification of the estimated model. Any relation (i.e., model assumption) needs a plausible explanation from relevant economic theory. The most often criticized disadvantage of the \\(R^2\\) is that additional regressors (relevant or not) will always increase the \\(R^2\\). Here is an example of the problem.\n\nset.seed(123)\nn     <- 100                  # Sample size\nX     <- runif(n, 0, 10)      # Relevant X variable\nX_ir  <- runif(n, 5, 20)      # Irrelevant X variable\nerror <- rt(n, df = 10)*10    # True error\nY     <- 1 + 5 * X + error    # Y variable\nlm1   <- summary(lm(Y~X))     # Correct OLS regression \nlm2   <- summary(lm(Y~X+X_ir))# OLS regression with X_ir \nlm1$r.squared < lm2$r.squared\n\n[1] TRUE\n\n\nSo, \\(R^2\\) increases here even though X_ir is a completely irrelevant explanatory variable. Because of this, the \\(R^2\\) cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called adjusted \\(R^2\\), \\(\\overline{R}^2,\\) defined as \\[\\begin{eqnarray*}\n  \\overline{R}^2&=&1-\\frac{\\frac{1}{n-K}\\sum_{i=1}^n\\hat{\\varepsilon}^2_i}{\\frac{1}{n-1}\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2}\\leq R^2%\\\\\n  %=\\dots=\n  %&=&1-\\frac{n-1}{n-K}\\left(1-R^2\\right)\\quad{\\small\\text{(since $1-R^2=(\\sum_i\\hat\\varepsilon_i^2)/(\\sum_i(Y_i-\\bar{Y}))$)}}\\\\\n  %&=&1-\\frac{n-1}{n-K}+\\frac{n-1}{n-K}R^2\\quad+\\frac{K-1}{n-K}R^2-\\frac{K-1}{n-K}R^2\\\\\n  %&=&1-\\frac{n-1}{n-K}+R^2\\quad+\\frac{K-1}{n-K}R^2\\\\\n  %&=&-\\frac{K-1}{n-K}+R^2\\quad+\\frac{K-1}{n-K}R^2\\\\\n  %&=&R^2-\\underbrace{\\frac{K-1}{n-K}\\left(1-R^2\\right)}_{\\geq 0\\;\\text{and}\\;\\leq(K-1)/(n-K)}\\;\\leq\\;R^2\n\\end{eqnarray*}\\] The adjustment is in terms of the degrees of freedom \\(n-K\\)."
  },
  {
    "objectID": "03-Multiple-Linear-Regression.html#sec-MMEstimator",
    "href": "03-Multiple-Linear-Regression.html#sec-MMEstimator",
    "title": "3  Multiple Linear Regression",
    "section": "3.4 Method of Moments Estimator",
    "text": "3.4 Method of Moments Estimator\nRemember that the exogeneity assumption (Assumption 2), \\(E(\\varepsilon_i|X_i)=0,\\) implies that \\(E(X_{ik}\\varepsilon_i)=0\\) for all \\(k=1,\\dots,K\\). Thus, the exogeneity assumption gives us a system of \\(K\\) linear equations: \\[\n\\left.\n  \\begin{array}{c}\n  E(\\varepsilon_i)=0\\\\\n  E(X_{i2}\\varepsilon_i)=0\\\\\n  \\vdots\\\\\n  E(X_{iK}\\varepsilon_i)=0\n  \\end{array}\n\\right\\}\\Leftrightarrow \\underset{(K\\times 1)}{E(X_i\\varepsilon_i)}=\\underset{(K\\times 1)}{0}\n\\]\nThe linear equation system \\[\n\\underset{(K\\times 1)}{E(X_i\\varepsilon_i)}=\\underset{(K\\times 1)}{0}\n\\] allows us to identify the unknown parameter vector \\(\\beta\\in\\mathbb{R}^K\\) in terms of population moments: \\[\n\\begin{align*}\nE(X_i(\\overbrace{Y_i - X_i'\\beta}^{=\\varepsilon_i})) &= 0\\\\\n%\\Leftrightarrow \\hspace{1.5cm}\nE(X_iY_i) - E(X_iX_i')\\beta & = 0\\\\\nE(X_iX_i')\\beta & =  E(X_iY_i)  \\\\\n\\beta & = \\left(E(X_iX_i')\\right)^{-1} E(X_iY_i)  \\\\\n\\end{align*}\n\\] The fundamental idea behind “method of moments estimation” is to define an estimator by substituting population moments by sample moment analogues (sample means): \\[\n\\begin{align*}\n\\hat\\beta_{mm}\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\frac{1}{n}\\sum_{i=1}^n X_iY_i,\n\\end{align*}\n\\] where \\(\\hat\\beta_{mm}\\) can be simplified as following: \\[\n\\begin{align*}\n\\hat\\beta_{mm}\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\frac{1}{n}\\sum_{i=1}^n X_iY_i  \\\\\n& = \\left(\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\sum_{i=1}^n X_iY_i\\\\\n& = \\left(X'X\\right)^{-1} X'Y.\\\\\n\\end{align*}\n\\]\nThus the method of moments estimator, \\(\\hat\\beta_{mm},\\) coincides with the OLS estimator."
  },
  {
    "objectID": "03-Multiple-Linear-Regression.html#sec-GMTheorem",
    "href": "03-Multiple-Linear-Regression.html#sec-GMTheorem",
    "title": "3  Multiple Linear Regression",
    "section": "3.5 The Gauss-Markov Theorem",
    "text": "3.5 The Gauss-Markov Theorem\nThe OLS estimator \\[\n\\hat\\beta=(X'X)^{-1}X'Y\n\\] is a linear (in \\(Y\\)) and unbiased estimator (\\(E(\\hat\\beta)=\\beta\\)), i.e. \\[\n\\operatorname{Bias}(\\hat\\beta) = E(\\hat\\beta) - \\beta = 0.\n\\] The latter can be shown as following:\nObserve that \\[\n\\hat\\beta=(X'X)^{-1}X'Y\n\\] consists of two random variables \\(X\\) and \\(Y.\\) Thus one needs to show first the conditional unbiasedness of \\(\\hat\\beta\\) given \\(X\\) which allows us to focus on randomness due to \\(\\varepsilon\\),\n\\[\n\\begin{align*}\n\\operatorname{Bias}(\\hat\\beta|X)\n&= E(\\hat\\beta|X) - \\beta \\\\\n&= E((X'X)^{-1}X'\\underbrace{Y}_{=X\\beta+\\varepsilon}|X) - \\beta \\\\\n&= E(\\underbrace{(X'X)^{-1}X'X}_{I_K}\\beta|X) + E((X'X)^{-1}X'\\varepsilon|X) - \\beta \\\\\n&=  \\beta + E((X'X)^{-1}X'\\varepsilon|X) - \\beta \\\\\n&=  (X'X)^{-1}X'E(\\varepsilon|X) =0.  \n\\end{align*}\n\\] Thus \\(\\hat\\beta\\) is unbiased conditionally on \\(X.\\) From this if follows, by the iterated law of expectations, that the OLS estimator is also unconditionally unbiased, i.e.\n\\[\n\\operatorname{Bias}(\\hat\\beta) = E\\left(\\operatorname{Bias}(\\hat\\beta|X)\\right) = E(0) = 0.\n\\] Moreover, the conditional variance of \\(\\hat\\beta\\) given \\(X\\) can be derived as following: \\[\n\\begin{align*}\nVar(\\hat\\beta|X)\n&=Var(\\hat\\beta - \\beta|X)\\\\\n&=Var((X'X)^{-1}X'\\varepsilon|X)\n\\end{align*}\n\\]\n\nHint: The conditional variance of a multivariate random variable \\(Z\\in\\mathbb{R}^K\\), given \\(X\\), is defined as \\(Var(Z|X)=E((Z-E(Z|X))(Z-E(Z|X))'|X)\\).\n\nUsing the definition of the conditional variance for multivariate random variables we have that \\[\n\\begin{align*}\n&Var(\\hat\\beta|X)=\\\\\n&=Var(\\hat\\beta - \\beta|X)\\\\\n&=Var((X'X)^{-1}X'\\varepsilon|X)\\\\\n&=E\\Big(\\big((X'X)^{-1}X'\\varepsilon-\\underbrace{E((X'X)^{-1}X'\\varepsilon|X)}_{=0}\\big)\\\\\n&\\phantom{=E\\Big(}\\,\\big((X'X)^{-1}X'\\varepsilon-\\underbrace{E((X'X)^{-1}X'\\varepsilon|X)}_{=0}\\big)'|X\\Big)\\\\\n&=E\\left(((X'X)^{-1}X'\\varepsilon)((X'X)^{-1}X'\\varepsilon)'|X\\right)\\\\\n&=E\\left((X'X)^{-1}X'\\varepsilon\\varepsilon' X(X'X)^{-1}|X\\right)\\\\\n&=(X'X)^{-1}X'\\underbrace{E\\left(\\varepsilon\\varepsilon'|X\\right)}_{=Var(\\varepsilon|X)}X(X'X)^{-1},\n\\end{align*}\n\\] where we used that \\[\n\\begin{align*}\n\\hat\\beta\n&=(X'X)^{-1}X'Y\\\\\n&=(X'X)^{-1}X'(X\\beta+\\varepsilon)\\\\\n&=\\beta+(X'X)^{-1}X'\\varepsilon\\\\\n\\Leftrightarrow \\hat\\beta - \\beta & = (X'X)^{-1}X'\\varepsilon.\n\\end{align*}\n\\] Under the assumption of spherical errors (see Assumption 4), we have that \\(E\\left(\\varepsilon\\varepsilon'|X\\right)=\\sigma^2 I_n\\), where \\(I_n\\) is the \\((n\\times n)\\) dimensional identity matrix with ones on the diagonal and zeros everywhere else. Thus under the assumption of spherical errors, we have that \\[\n\\begin{align*}\nVar(\\hat\\beta|X)\n&=(X'X)^{-1}X' \\left(\\sigma^2I_n\\right)X(X'X)^{-1}\\\\\n&=\\sigma^2(X'X)^{-1}X'X(X'X)^{-1}\\\\\n&=\\sigma^2(X'X)^{-1}.\n\\end{align*}\n\\]\nSo, we can summarize:\n\nThe OLS estimator belongs to the large family of linear (in \\(Y\\)) and unbiased estimators of \\(\\beta.\\)\nUnder the assumption of spherical errors, the conditional variance of \\(\\hat\\beta\\), given \\(X,\\) is \\(Var(\\hat\\beta|X)=\\sigma^2(X'X)^{-1}.\\)\n\nThe famous Gauss-Markov Theorem states that the OLS estimator is the “best” (lowest conditional variance) estimator within this family of linear (in \\(Y\\)) and unbiased estimators.\n\nTheorem 3.5 (Gauss-Markov Theorem) Let’s assume Assumptions 1-4 hold with spherical errors, i.e., with \\(E(\\varepsilon\\varepsilon'|X)=\\sigma^{2} I_{n}\\). Then the Gauss-Markov theorem states that of all linear and unbiased estimators (i.e. \\(E(\\hat\\beta)=\\beta\\)), the OLS estimator \\(\\hat\\beta=(X'X)^{-1}X'Y\\) will have the smallest variance (conditionally on \\(X\\)). That is, for any alternative linear (in \\(Y\\)) and unbiased (i.e. \\(E(\\tilde\\beta)=\\beta\\)) estimator \\(\\tilde{\\beta}\\) we have that \\[\n\\begin{align*}\n&Var(\\tilde\\beta|X)\\geq Var(\\hat\\beta|X)\\quad\\text{(in the matrix sense)}\\\\\n\\Leftrightarrow&Var(\\tilde\\beta|X)-Var(\\hat\\beta|X)=\\underset{(K\\times K)}{D},\n\\end{align*}\n\\] where \\(D\\) is a positive semidefinite \\((K\\times K)\\) matrix, i.e., \\(a'Da\\geq 0\\) for any \\(K\\)-dimensional vector \\(a\\in\\mathbb{R}^K\\). Observe that this implies that \\(Var(\\tilde{\\beta}_k|X) \\geq Var(\\hat\\beta_k | X)\\) for any \\(k=1,\\dots,K\\).\n\n\nProof. Since \\(\\tilde{\\beta}\\) is assumed to be linear in \\(Y\\), we can write \\[\n\\tilde{\\beta}=CY,\n\\] where \\(C\\) is some \\((K\\times n)\\) matrix, which is a function of \\(X\\) and/or nonrandom components. Adding a \\((K\\times n)\\) zero matrix \\(0\\) yields \\[\n\\tilde{\\beta}=\\Big(C\\overbrace{-\\left(X'X\\right)^{-1}X'+\\left(X'X\\right)^{-1}X'}^{=0}\\Big)Y.\n\\] Let now \\(D=C-\\left(X'X\\right)^{-1}X'\\), then \\[\n\\begin{align*}\n\\tilde{\\beta}&=\\left(D+\\left(X'X\\right)^{-1}X'\\right)Y\\\\\n\\tilde{\\beta}&=DY + \\left(X'X\\right)^{-1}X'Y\\\\\n\\tilde{\\beta}&=D\\left(X{\\beta}+{\\varepsilon}\\right) + \\left(X'X\\right)^{-1}X'Y\n\\end{align*}\n\\] Thus \\[\n\\tilde{\\beta}=DX{\\beta}+D{\\varepsilon} + \\hat{\\beta}.\n\\tag{3.2}\\] Moreover, \\[\nE(\\tilde{\\beta}|X)=\\underbrace{E(DX{\\beta}|X)}_{=DX\\beta}+\\underbrace{E(D\\varepsilon|X)}_{=DE(\\varepsilon|X)=0}+\\underbrace{E(\\hat{\\beta}|X)}_{=\\beta}\n\\] and thus \\[\nE(\\tilde{\\beta}|X)=DX{\\beta}+0+{\\beta}.\n\\tag{3.3}\\]\nSince \\(\\tilde{\\beta}\\) is (by assumption) unbiased, we have that \\(E(\\tilde{\\beta}|X)={\\beta}\\). Therefore, Equation 3.3 implies that \\(DX=0_{(K\\times K)}\\) since we must have that \\[\nE(\\tilde{\\beta}|X)=DX{\\beta}+0+{\\beta}=\\beta.\n\\] Plugging \\(DX=0\\) into Equation 3.2 yields, \\[\n\\begin{align*}\n\\tilde{\\beta}&=D{\\varepsilon} + \\hat{\\beta}\\\\\n\\tilde{\\beta}-{\\beta}&=D{\\varepsilon} + (\\hat{\\beta}-{\\beta})\\\\\n\\tilde{\\beta}-{\\beta}&=D{\\varepsilon} + \\left(X'X\\right)^{-1}X'{\\varepsilon}\n\\end{align*}\n\\] such that \\[\n\\tilde{\\beta}-{\\beta}=\\left(D + \\left(X'X\\right)^{-1}X'\\right){\\varepsilon},\n\\tag{3.4}\\] where we used that \\[\n\\begin{align*}\n\\hat\\beta-\\beta&=(X'X)^{-1}X'Y-\\beta\\\\\n&=(X'X)^{-1}X'(X\\beta+\\varepsilon)-\\beta\\\\\n&=(X'X)^{-1}X'\\varepsilon.\n\\end{align*}\n\\]\nUsing that \\(Var(\\tilde{\\beta}|X)= Var(\\tilde{\\beta}-{\\beta}|X)\\) since \\(\\beta\\) is not random and using Equation 3.4 yields \\[\n\\begin{align*}\nVar(\\tilde{\\beta}|X)\n&= Var((D + (X'X)^{-1}X'){\\varepsilon}|X)\\\\\n&= (D + (X'X)^{-1}X')Var({\\varepsilon}|X)(D' + X(X'X)^{-1})\\\\\n&= \\sigma^2(D + (X'X)^{-1}X')I_n(D' + X(X'X)^{-1})\\\\\n&= \\sigma^2\\left(DD'+(X'X)^{-1}\\right)\\quad \\text{(using that $DX=0$)} \\\\\n&\\geq\\sigma^2(X'X)^{-1} \\quad \\text{(using that $DD'\\geq 0$)}\\\\\n&= Var(\\hat{\\beta}|X).\n\\end{align*}\n\\] Finally, we need to show that \\(DD'\\) is really positive semidefinite (i.e. \\(DD'\\geq 0\\) in matrix sense): \\[\n\\begin{align*}\na'DD'a=(D'a)'(D'a)=\\tilde{a}'\\tilde{a}\\geq 0,\n\\end{align*}\n\\] where \\(\\tilde{a}\\) is a \\(K\\) dimensional column-vector."
  },
  {
    "objectID": "03-Multiple-Linear-Regression.html#practice",
    "href": "03-Multiple-Linear-Regression.html#practice",
    "title": "3  Multiple Linear Regression",
    "section": "3.6 Practice",
    "text": "3.6 Practice\n\n3.6.1 Factor Variables and the Dummy-Variable Trap\nIn the following, we consider simple linear regression model that aims to predict wages in the year 2008 using gender as the only predictor. We use data provided in the accompanying materials of Stock and Watson’s Introduction to Econometrics textbook (Stock and Watson 2015). You can download the data stored as an xlsx-file cps_ch3.xlsx HERE.\nLet us first prepare the dataset:\n\n## load the 'tidyverse' package\nsuppressPackageStartupMessages(library(\"tidyverse\"))\n## load the 'readxl' package\nlibrary(\"readxl\")\n\n## import the data into R\ncps <- read_excel(path = \"data/cps_ch3.xlsx\")\n\n# names(cps)\n# range(cps$year)\n# range(cps$a_sex) # 1 = male, 2 = female\n\n## Data wrangling\ncps_2008 <- cps %>% \n  mutate(\n    wage   = ahe08,      # rename \"ahe08\" as \"wage\"    \n    gender = fct_recode( # rename factor \"a_sex\" as \"gender\"\n      as_factor(a_sex),     \n                \"male\" = \"1\",  # rename factor level \"1\" to \"male\"\n                \"female\" = \"2\" # rename factor level \"2\" to \"female\"\n             ) \n  ) %>%  \n  filter(year == 2008) %>%     # Only data from year 2008\n  select(wage, gender)         # Select only the variables \"wage\" and \"gender\"\n\nThe first six lines of the dataset look as following:\n\n\n\n\n \n  \n    wage \n    gender \n  \n \n\n  \n    38.46154 \n    male \n  \n  \n    12.50000 \n    male \n  \n  \n    17.78846 \n    male \n  \n  \n    30.38461 \n    male \n  \n  \n    23.66864 \n    male \n  \n  \n    12.01923 \n    female \n  \n\n\n\n\n\nComputing the estimation results:\n\nlm_obj <- lm(wage ~ gender, data = cps_2008)\n\ncoef(lm_obj)\n\n\n\n\ngives\n\n\\(\\hat\\beta_1=\\) 24.98\n\\(\\hat\\beta_2=\\) -4.1\n\nTo compute these estimation results, one needs to assign a numeric 0/1 coding to the factor levels male and female of the factor variable gender. To see the numeric values used by R one can take a look at model.matrix(lm_obj):\n\nX <- model.matrix(lm_obj) # this is the internally used X-matrix\nX[1:6,]\n\n  (Intercept) genderfemale\n1           1            0\n2           1            0\n3           1            0\n4           1            0\n5           1            0\n6           1            1\n\ncps_2008$gender[1:6]\n\n[1] male   male   male   male   male   female\nLevels: male female\n\n\nThus R internally codes female subjects by a 1 and male subjects by a 0, such that \\[\n\\hat\\beta_1  + \\hat\\beta_2 X_{i,gender}=\\left\\{\\begin{array}{ll}\\hat\\beta_1&\\text{ if }X_{i,gender}=\\text{male}\\\\\\hat\\beta_1 + \\hat\\beta_2&\\text{ if }X_{i,gender}=\\text{female}\\end{array} \\right.\n\\]\nInterpretation:\n\nThe average wage of male workers in 2008 was \\(\\hat{\\beta}_1=\\) 24.98 (USD/Hour).\nThe average wage of female workers in 2008 was \\(\\hat{\\beta}_1 + \\hat{\\beta}_2=\\) 20.87 (USD/Hour).\nThe difference in the earnings between male and female works in 2008 is \\(\\hat{\\beta}_2=\\) -4.1 (USD/Hour).\n\n\n\nDummy-Variable Trap\nAbove, we used R’s internal handling of factor variables which you should always do. If you construct a dummy variable for each of the levels of a factor yourself, however, you may blundered into the dummy variable trap:\n\n## Intercept variable\nX_1      <- rep(1, times = nrow(cps_2008))\n\n## 1. Dummy variable for 'female'\nX_female <- ifelse(cps_2008$gender == \"female\", 1, 0)\n\n## 2. Dummy variable for 'male'\nX_male   <- ifelse(cps_2008$gender == \"male\", 1, 0)\n\n## Construct the model matrix 'X'\nX        <- cbind(X_1, X_female, X_male)\n\n\n\n\nComputing the estimation result for \\(\\beta\\) “by hand” yields\n\n## Dependent variable\nY        <- cps_2008$wage\n\n## Computing the estimator\nbeta_hat <- solve(t(X) %*% X) %*% t(X) %*% Y\n\n\nError in solve.default(X %*% t(X)) : \n  Lapack routine dgesv: system is exactly singular: U[3,3] = 0\nCalls: .main ... eval_with_user_handlers -> eval -> eval -> solve -> solve.default\nExecution halted\n\nAn error message! 🤬 We blundered into the dummy variable trap!\nThe estimation result is not computable since \\((X'X)\\) is not invertible due to the perfect multicollinearity between the intercept and the two dummy variables X_female and X_male\n\nX_1 = X_female \\(+\\) X_male\n\nwhich violates Assumption 3 (no perfect multicollinearity).\nSolution: Use one dummy-variable less than factor levels. I.e., in this example you can either drop X_female or X_male.\n\n## New model matrix after dropping X_male:\nX        <- cbind(X_1, X_female)\n\n## Computing the estimator\nbeta_hat <- solve(t(X) %*% X) %*% t(X) %*% Y\nbeta_hat\n\n              [,1]\nX_1      24.978404\nX_female -4.103626\n\n\nThis give the same result as computed by R’s lm() function using the factor variable gender.\n\n\n3.6.2 Detecting Heteroskedasticity\nA very simple, but highly effective way to check for heteroskedastic error terms is to plot the residuals. If we have the correct model (hopefully we have), then (and only then) the residuals are good approximations to the realizations of the error terms \\(\\varepsilon_i\\). Thus a plot of the residuals can be used to check for heteroskedasticity.\nYou can use R’s internal diagnostic plots that can be called using the plot() method for lm-objects:\n\n# install.packages(\"devtools\")\n# library(devtools)\n# install_git(\"https://github.com/ccolonescu/PoEdata\")\n\nlibrary(PoEdata) # for the \"food\" dataset contained in this package\n\ndata(\"food\")     # makes the dataset \"food\" usable\n\nlm_object <- lm(food_exp ~ income, data = food)\n\n## Diagnostic scatter plot of residuals vs fitted values \nplot(lm_object, which = 1)\n\n\n\n\nInterpretation:\n\nThe plot shows the residuals \\(\\hat{\\varepsilon}_i\\) plotted against the fitted values \\(\\hat{Y}_i\\).\nThe diagnostic plot indicates that the variance increases with income.\n\nNote: Plotting against the fitted values is actually a quite smart idea since this also works for multiple predictors \\(X_{i1},\\dots,X_{iK}\\) with \\(K\\geq 3\\).\n\n\n3.6.3 Checking (Non-)Linearity of the Regression Line\nTo check whether there are non-linear relationships between the outcome and the predictors, on should take a look at the data, using a pairs-plot function that procures scatter plots for all pairs of variables in a dataset.\n\nBase RTidyverse R\n\n\n\ncar_data <- read.csv(file = \"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv\")\n\nmy_car_df <- data.frame(\n    \"MPG\"   = car_data$MPG.city,\n    \"HP\"    = car_data$Horsepower\n    )\n\npairs(my_car_df)\n\n\n\n\n\n\n\n## install.packages(\"tidyverse\")\n## install.packages(\"GGally\")\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nsuppressPackageStartupMessages(library(\"GGally\")) # nice pairs plot \n\ncar_data <- readr::read_csv(\n  file = \"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv\",\n  show_col_types = FALSE)\n\nmy_car_df <- car_data %>% \n dplyr::mutate(\n  \"MPG\"   = MPG.city, \n  \"HP\"    = Horsepower) %>%\n select(\"MPG\", \"HP\")\n\nggpairs(my_car_df) + theme_bw()\n\n\n\n\n\n\n\nThe plots indicate a positive, but non-linear relationship between the outcome variable MPG (gasoline consumption in milles per gallon) and the predictor HP (power of the mashing in horsepower).\nIf we do not take this into account, we get bad model fits which can be seen in the scatter plot of MPG vs. HP and in the diagnostic plot of the residuals vs. the fitted values. (The latter works also for \\(K\\geq 3\\), the former not.)\n\nBase RTidyverse R\n\n\n\nlm_obj_1 <- lm(MPG ~ HP, data = my_car_df)\n\npar(mfrow=c(2,1))\nplot(y=my_car_df$MPG, x=my_car_df$HP, \n     main = \"Simple Linear Regression\",\n     xlab = \"Horse Power\", \n     ylab = \"Miles per Gallon\")\nabline(lm_obj_1)\n##\nplot(lm_obj_1, which=1, \n       main = \"Simple Linear Regression\")\n\n\n\n\n\n\n\n## install.packages(\"tidyverse\")\n## install.packages(\"ggfortify\")\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nlibrary(\"ggfortify\") # tidy diagnostic plots\n\nlm_obj_1 <- lm(MPG ~ HP, data = my_car_df)\n\n## Plot: Simple Linear Regression\nmy_car_df %>%\nggplot(aes(x=HP, y=MPG)) + \n    geom_point(alpha=0.8, size=3)+\n    stat_smooth(method='lm', formula = y~x) +\n    theme_classic() + \n    labs(x = \"Horse Power\", y = \"Miles per Gallon\", \n         title = \"Simple Linear Regression\")\n\n\n\n## Diagnostic Plot: Simple Linear Regression \nautoplot(lm_obj_1, which = 1)\n\n\n\n\n\n\n\nA polynomial regression model with polynomial degree 2 improves the model fit.\n\nBase RTidyverse R\n\n\n\n## Adding new variable HP squared:\nmy_car_df$HP_sq <- car_data$Horsepower^2\n\nlm_obj_2 <- lm(MPG ~ HP + HP_sq, data = my_car_df)\n\npar(mfrow=c(2,1))\nplot(y=my_car_df$MPG, x=my_car_df$HP,\n     main = \"Polynomial Regression (Degree: 2)\",\n     xlab = \"Horse Power\", \n     ylab = \"Miles per Gallon\")\nX_seq <- seq(from = min(my_car_df$HP), \n             to   = max(my_car_df$HP), len = 25)\nlines(y = predict(lm_obj_2, \n                  newdata=data.frame(\"HP\"    = X_seq, \n                                     \"HP_sq\" = X_seq^2)), \n      x = X_seq)\nplot(lm_obj_2, which=1, \n       main = \"Polynomial Regression (Degree: 2)\")\n\n\n\n\n\n\n\n## install.packages(\"tidyverse\")\n## install.packages(\"ggfortify\")\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nlibrary(\"ggfortify\") # tidy diagnostic plots\n\n## Adding new variable HP squared:\nmy_car_df <- my_car_df %>% \n dplyr::mutate(\n  \"HP_sq\" = HP^2\n)\n\n## Polynomial Regression\n\nlm_obj_2 <- lm(MPG ~ HP + HP_sq, data = my_car_df)\n\n## Plot: Polynomial Regression\nmy_car_df %>%\nggplot(aes(x=HP, y=MPG)) + \n    geom_point(alpha=0.8, size=3)+\n    stat_smooth(method='lm', formula = y~poly(x, 2, raw = TRUE)) +\n    theme_classic() + \n    labs(x = \"Horse Power\", y = \"Miles per Gallon\", \n         title = \"Polynomial Regression\", \n         subtitle = \"Polynomial Degree: 2\")\n\n\n\n## Diagnostic Plot: Polynomial Regression\nautoplot(lm_obj_2, which = 1)     \n\n\n\n\n\n\n\n\n\n3.6.4 Behavior of the OLS Estimates for Resampled Data\nUsually, we only observe one estimate \\(\\hat{\\beta}\\) of \\(\\beta\\) computed based on one given dataset (one realization of the random sample). However, in order to understand the statistical properties of the estimators \\(\\hat{\\beta}\\) we need to view them as random variables which yield different realizations in repeated realizations of the random sample from Model (Equation 3.1). This allows us then to think about questions like:\n\nIs the estimator able to estimate the unknown parameter-value correctly on average?\n\nAre the estimation results more precise if we have more data?\n\nA first idea about the statistical properties of the estimator \\(\\hat{\\beta}\\) can be gained using Monte Carlo simulations as following.\n\n## Sample sizes\nn_small      <-  30 # smallish sample size\nn_large      <- 100 # largish sample size\n\n## True parameter values\nbeta0 <- 1\nbeta1 <- 1\n\n## Monte-Carlo (MC) Simulation \n## 1. Generate data\n## 2. Compute and store estimates\n## Repeat steps 1. and 2. many times\nset.seed(3)\n## Number of Monte Carlo repetitions\n## How many samples to draw from the models\nB          <- 1000\n\n## Containers to store the estimation results\nbeta0_estimates_n_small <- numeric(B)\nbeta1_estimates_n_small <- numeric(B)\nbeta0_estimates_n_large <- numeric(B)\nbeta1_estimates_n_large <- numeric(B)\n\nfor(b in 1:B){\n## Generate artificial samples (n_small)\nerror_n_small     <- rnorm(n_small, mean = 0, sd = 5)\nX_n_small         <- runif(n_small, min = 1, max = 10)\nY_n_small         <- beta0 + beta1 * X_n_small + error_n_small\nlm_obj            <- lm(Y_n_small ~ X_n_small) \n## Save estimation results \nbeta0_estimates_n_small[b] <- lm_obj$coefficients[1]\nbeta1_estimates_n_small[b] <- lm_obj$coefficients[2]\n\n## Generate artificial samples (n_large)\nerror_n_large     <- rnorm(n_large, mean = 0, sd = 5)\nX_n_large         <- runif(n_large, min = 1, max = 10)\nY_n_large         <- beta0 + beta1 * X_n_large + error_n_large\nlm_obj            <- lm(Y_n_large ~ X_n_large)\n## Save estimation results \nbeta0_estimates_n_large[b] <- lm_obj$coefficients[1]\nbeta1_estimates_n_large[b] <- lm_obj$coefficients[2] \n}\n\nNow, we have produced B=1000 realizations of the estimators \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) and saved these realizations in the vectors\n\nbeta0_estimates_n_small\nbeta1_estimates_n_small\nbeta0_estimates_n_large\nbeta1_estimates_n_large\n\nThese artificially generated realizations allow us to visualize the behavior of the OLS estimates for the repeatedly sampled data.\n\n\n\n\n\n\n\n\n\nThis are promising plots:\n\nThe realizations of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are scattered around the true (unknown) parameter values \\(\\beta_0\\) and \\(\\beta_1\\). This indicates unbiasdness of the estimators.\nIn the case of a larger sample size, the realizations of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) concentrate stronger around the true (unknown) parameter values \\(\\beta_0\\) and \\(\\beta_1.\\) This indicates consistency of the estimators.\n\nHowever, this was only a simulation for one specific data generating process. Such a Monte Carlo simulation does not allow us to generalize these properties. In the following chapters, we use theoretical arguments to investigate under which assumptions we can make general statements about the distributional properties of the estimator \\(\\hat\\beta.\\)"
  },
  {
    "objectID": "03-Multiple-Linear-Regression.html#exercises",
    "href": "03-Multiple-Linear-Regression.html#exercises",
    "title": "3  Multiple Linear Regression",
    "section": "3.7 Exercises",
    "text": "3.7 Exercises\n\n\nExercises of Chapter 3 with Solutions"
  },
  {
    "objectID": "03-Multiple-Linear-Regression.html#references",
    "href": "03-Multiple-Linear-Regression.html#references",
    "title": "3  Multiple Linear Regression",
    "section": "References",
    "text": "References\n\n\n\n\nStock, J. H., and M. W. Watson. 2015. Introduction to Econometrics. Pearson Education."
  },
  {
    "objectID": "04-Monte-Carlo-Simulations.html",
    "href": "04-Monte-Carlo-Simulations.html",
    "title": "\n4  Estimation Theory and Monte Carlo Simulations\n",
    "section": "",
    "text": "4.1 Estimator vs. Estimate\nLet’s assume that we have an iid random sample \\(\\{X_1,\\dots,X_n\\}\\) with \\[\nX_i\\overset{iid}{\\sim} F_X\n\\] for all \\(i=1,\\dots,n\\), and let \\[\n\\theta\\in\\mathbb{R}\n\\] denote some parameter (e.g. the mean or the variance) of the distribution \\(F_X\\).\nAn estimator \\(\\hat\\theta_n\\) of \\(\\theta\\) is a function of the random sample \\(X_1,\\dots,X_n\\), \\[\n\\hat\\theta_n:=\\hat\\theta(X_1,\\dots,X_n).\n\\]\nSince \\(\\hat\\theta_n\\) is a function of the random variables \\(X_1,\\dots,X_n\\), the estimator \\(\\hat\\theta_n\\) is itself a random variable.\nThe observed data \\[\nX_{1,obs},\\dots,X_{n,obs}\n\\] is assumed to be a certain realization of the random sample \\[\nX_1,\\dots,X_n.\n\\] The corresponding realization of the estimator is called an estimate of \\(\\theta\\) \\[\n\\hat\\theta_{n,obs}=\\hat\\theta(X_{1,obs},\\dots,X_{n,obs}).\n\\]\nExamples:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation Theory and Monte Carlo Simulations</span>"
    ]
  },
  {
    "objectID": "04-Monte-Carlo-Simulations.html#estimator-vs.-estimate",
    "href": "04-Monte-Carlo-Simulations.html#estimator-vs.-estimate",
    "title": "\n4  Estimation Theory and Monte Carlo Simulations\n",
    "section": "",
    "text": "The sample mean as an estimator of the population mean \\(E(X_i) =\\theta:\\) \\[\n\\hat\\theta_n=\\bar{X}_n=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\] For given data, we observe the realization \\[\n\\hat\\theta_{n,obs}=\\bar{X}_{n,obs}=\\frac{1}{n}\\sum_{i=1}^nX_{i,obs}\n\\]\nThe sample variance as an estimator of the population variance \\(Var(X_i) =\\theta:\\) \\[\n\\hat\\theta_n=s_{UB}^2=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i - \\bar{X}_n\\right)^2\n\\] For given data, we observe the realization \\[\n\\hat\\theta_{n,obs}=s_{UB,obs}^2=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_{i,obs} - \\bar{X}_{n,obs}\\right)^2\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nOften we do not use a distinguishing notation, but denote both the estimator \\(\\hat\\theta_{n}\\) and its realization \\(\\hat\\theta_{n,obs}\\) as \\(\\hat\\theta_{n}\\). This ambiguity is often convenient since both points of views can make sense in a given context. Sometimes, also the subscript \\(n\\) in \\(\\hat\\theta_{n}\\) is dropped and one simply writes \\(\\hat\\theta.\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation Theory and Monte Carlo Simulations</span>"
    ]
  },
  {
    "objectID": "04-Monte-Carlo-Simulations.html#deriving-the-distribution-of-estimators",
    "href": "04-Monte-Carlo-Simulations.html#deriving-the-distribution-of-estimators",
    "title": "\n4  Estimation Theory and Monte Carlo Simulations\n",
    "section": "\n4.4 Deriving the Distribution of Estimators",
    "text": "4.4 Deriving the Distribution of Estimators\nUsually, we do not know the distribution \\(F_X\\) of the random sample \\(X_1,\\dots,X_n\\) and thus we neither know the value of \\(\\theta\\) nor the distribution of the estimator \\[\n\\hat\\theta_n=\\hat\\theta(X_1,\\dots,X_n).\n\\] This is the fundamental statistical problem that we need to overcome in statistical inference (estimating \\(\\theta\\), hypothesis testing about \\(\\theta\\), etc.).\nThere are (roughly) three different possibilities to derive/approximate the distribution of an estimator \\(\\hat\\theta_n:\\)\n\n\nOption 1: Mathematical derivation using a complete distributional assumption on \\(F_X\\). Assuming a certain distribution \\(F_X\\) for the random sample \\(X_1,\\dots,X_n\\) may allow us to derive mathematically the exact distribution of \\(\\hat\\theta_n\\) for given sample sizes \\(n.\\) ➡️ We consider this option in Chapter 6.\n\nPro: If the distributional assumption is correct, one has exact inference for each sample size \\(n\\).\nCon: This option can fail miserably if the distributional assumption on \\(F_X\\) is wrong.\nCon: This option is often only possible for rather simple distributions \\(F_X\\) like the normal distribution.\n\n\n\nOption 2: Mathematical derivation using only an incomplete distributional assumption on \\(F_X\\) and asymptotic statistics. Large sample \\((n\\to\\infty)\\) approximations (i.e. laws of large numbers and central limit theorems) often allows us to derive the approximate distribution of \\(\\hat\\theta_n\\) for large sample sizes \\(n.\\) ➡️ We consider this option in Chapter 7.\n\nPro: Only a few qualitative distributional assumptions are needed. (Typically: A random sample with finite variances.)\n\nCon: The derived asymptotic (\\(n\\to\\infty\\)) distribution is only exact for the practically impossible case where \\(n=\\infty\\) and thus can fail to approximate the exact distribution of \\(\\hat\\theta_n\\) for given (finite) sample sizes \\(n\\); particularly if \\(n\\) is small.\n\n\n\nOption 3: Monte Carlo (MC) Simulations using a complete distributional assumption on \\(F_X\\). Assuming a certain distribution \\(F_X\\) for the random sample \\(X_1,\\dots,X_n\\) we can approximate (with arbitrary precision) the exact distribution of \\(\\hat\\theta_n\\) for given sample sizes \\(n;\\) see the Algorithm “MC-Simulation”.  ➡️ We use this option to check the behavior of estimators under different scenarios for \\(F_X\\) and \\(n\\) throughout the rest of this script.\nPro: Works for a basically every distribution \\(F_X\\) and sample size \\(n.\\)\n\nCon: This option can fail miserably if the distributional assumption on \\(F_X\\) is wrong.\n\n\n\nAlgorithm “MC-Simulation”:1. Step: Generate realizations of \\(\\hat{\\theta}_n\\). Use a (pseudo-)random number generator to draw a large number of \\(B\\) (e.g. \\(B=10,000\\)) many realizations of the random sample \\(\\{X_1,\\dots,X_n\\}\\) for a given distribution \\(F_X\\) and a given sample size \\(n:\\) \\[\\begin{align*}\n&(X_{1,1,obs},\\dots,X_{n,1,obs})\\\\\n&(X_{1,2,obs},\\dots,X_{n,2,obs})\\\\\n& \\hspace{2cm}\\vdots \\\\\n&(X_{1,B,obs},\\dots,X_{n,B,obs})\n\\end{align*}\\] Compute for each realization of the random sample a realization of \\(\\hat{\\theta}_n:\\) \\[\\begin{align*}\n\\hat\\theta_{n,1,obs} &= \\hat\\theta(X_{1,1,obs},\\dots,X_{n,1,obs})\\\\\n\\hat\\theta_{n,2,obs} &= \\hat\\theta(X_{1,2,obs},\\dots,X_{n,2,obs})\\\\\n&\\vdots\\\\\n\\hat\\theta_{n,B,obs} &= \\hat\\theta(X_{1,B,obs},\\dots,X_{n,B,obs})\n\\end{align*}\\] 2. Step: Approximate the distribution of \\(\\hat{\\theta}_n\\) (or a features of it). Use the realizations \\(\\hat\\theta_{n,1,obs},\\dots,\\hat\\theta_{n,B,obs}\\) to approximate the exact distribution of \\(\\hat\\theta_n\\) for a given \\(F_X\\) and a give sample size \\(n.\\)\n\n\n\n\n\n\nNote\n\n\n\nStep 2 of the above algorithm works, since the empirical distribution function \\[\n\\hat{F}_{\\hat{\\theta}_n,B}(x)=\\frac{1}{B}\\sum_{j=1}^BI_{(\\hat\\theta_{n,j} \\leq x)}\n\\] approximates the true (unknown) distribution function of \\(\\hat{\\theta}_n\\) \\[\nF_{\\hat{\\theta}_n}(x)=P(\\hat\\theta_{n} \\leq x)\n\\] arbitrarily well as \\(B\\to\\infty.\\)\nThis hold true, since by the famous Glivenko–Cantelli theorem\\[\n\\sup_x\\left| \\hat{F}_{\\hat{\\theta}_n,B}(x) - F_{\\hat{\\theta}_n}(x)\\right|\\to 0\\quad\\text{as}\\quad B\\to\\infty.\n\\] almost surely as \\(B\\to\\infty.\\)\nInstead of approximating the whole distribution of \\(\\hat{\\theta}_n,\\) we may only be interested in approximating specific features of the this distribution, such as:\n\nthe bias of \\(\\hat{\\theta}_n\\)\n\nthe variance of \\(\\hat{\\theta}_n\\)\n\nthe standard error of \\(\\hat{\\theta}_n\\)\n\nthe mean squared error of \\(\\hat{\\theta}_n\\)\n\netc.\n\nNote: These features are simple functionals of \\(\\hat{F}_{\\hat{\\theta}_n,B},\\) and thus can also be approximated arbitrarily well as \\(B\\to\\infty.\\)\n\n\n\n4.4.1 Example: Sample Mean \\(\\bar{X}_n\\)\n\nLet \\(\\{X_1,\\dots,X_n\\}\\) be an iid random sample with \\[\nX_i\\overset{iid}{\\sim} F_X,\n\\] where\n\n\n\\(F_X\\) is a normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\) with\n\nmean \\((\\theta=)\\mu=10\\) and\nvariance \\(\\sigma^2=5\\).\n\n\n\nTo estimate the (usually unknown) mean value \\(\\mu=10,\\) we use the sample mean estimator \\[\n\\bar{X}_n =  \\frac{1}{n}\\sum_{i=1}^n X_i\n\\]\nWe consider two sample sizes \\(n=5\\) and \\(n=50.\\)\n\n\n\n\n\n\nMathematical Derivation using the Distributional Assumptions\n\n\n\nHere we have specified the distribution \\(F_X\\) completely by setting \\(F_X=\\mathcal{N}(\\mu=10,\\sigma^2=5).\\) This is such a simple case, that we can actually use mathematical derivations to derive the distribution of \\(\\bar{X}_n.\\) (Often, this is not possible.)\nObserve that since \\(X_i\\overset{\\text{iid}}{\\sim}\\mathcal{N}(\\mu,\\sigma^2),\\) \\[\n\\sum_{i=1}^nX_i\\sim\\mathcal{N}(n \\mu, n \\sigma^2).\n\\] Multiplying by \\(\\frac{1}{n}\\) yields \\[\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^nX_i = \\bar{X}_n\n&\\sim\\mathcal{N}\\left(\\frac{1}{n}n \\mu, \\frac{1}{n^2}n \\sigma^2\\right)\\\\[2ex]\n\\bar{X}_n &\\sim\\mathcal{N}\\left( \\mu, \\frac{1}{n} \\sigma^2\\right).\n\\end{align*}\\]\nSumming up: If \\(X_i\\overset{iid}{\\sim}\\mathcal{N}(\\mu,\\sigma^2),\\) then the exact (exact for each \\(n\\)) distribution of \\(\\bar{X}_n\\) is given by \\[\n\\bar{X}_n\\sim\\mathcal{N}\\left( \\mu, \\frac{1}{n} \\sigma^2\\right).\n\\]\n\nFor \\(\\mu=10,\\) \\(\\sigma=5,\\) \\(n=5\\): \\[\n\\bar{X}_n\\sim\\mathcal{N}\\left(10, 1\\right).\n\\]\n\nFor \\(\\mu=10,\\) \\(\\sigma=5,\\) \\(n=50\\): \\[\n\\bar{X}_n\\sim\\mathcal{N}\\left(10, 0.1\\right).\n\\]\n\n\n⚠️ Unfortunately, such a mathematical derivation works only for very simple estimators and only for simple (and completely specified) distributions \\(F_X.\\)\n🤓 But for this special case, we can now check, whether a Monte Carlo simulation is able to approximate the distribution of \\(\\bar{X}_n\\sim\\mathcal{N}\\left( \\mu, \\frac{1}{n} \\sigma^2\\right).\\)\n\n\nNext, we use a Monte Carlo simulation to approximate the distribution of the estimator \\[\n\\bar{X}_n =  \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\]\nThe following R code generates \\(B=10,000\\) many realizations of the random sample \\(X_i\\overset{iid}{\\sim}\\mathcal{N}(\\mu,\\sigma^2)\\) with \\(\\mu=10\\) and \\(\\sigma^2=5.\\)\n\\[\\begin{align*}\n&(X_{1,1,obs},\\dots,X_{n,1,obs})\\\\\n&(X_{1,2,obs},\\dots,X_{n,2,obs})\\\\\n& \\hspace{2cm}\\vdots \\\\\n&(X_{1,B,obs},\\dots,X_{n,B,obs})\n\\end{align*}\\] leading to \\(B\\) many realizations of the estimator \\(\\bar{X}_n\\) \\[\\begin{align*}\n\\bar{X}_{n,1,obs} &= \\hat\\theta(X_{1,1,obs},\\dots,X_{n,1,obs})\\\\\n\\bar{X}_{n,2,obs} &= \\hat\\theta(X_{1,2,obs},\\dots,X_{n,2,obs})\\\\\n&\\vdots\\\\\n\\bar{X}_{n,B,obs} &= \\hat\\theta(X_{1,B,obs},\\dots,X_{n,B,obs})\n\\end{align*}\\] These realizations are then used to approximate the true distribution of \\(\\bar{X}_n.\\)\n\n## True parameter value \nmu            &lt;- 10\n## Number of Monte Carlo repetitions:\nB             &lt;- 10000\n## Sequence of different sample sizes:\nn_seq         &lt;- c(5, 50)\n\n\n## #############################################\n## 1st Coding-Possibility: Using a for() loop ##\n## #############################################\n\n## Set seed for the random number generator to get reproducible results\nset.seed(3)\n\n# Container for the generated estimates:\nestimates_mat &lt;- matrix(NA, nrow = B, ncol = length(n_seq))\n\nfor(j in 1:length(n_seq)){\n  ## select the sample size\n  n &lt;- n_seq[j]\n  for(b in 1:B){\n    ## generate realization of the random sample \n    X_sample &lt;- rnorm(n = n, mean = mu, sd = sqrt(5))\n    ## compute the sample mean and safe it\n    estimates_mat[b,j] &lt;- mean(X_sample)\n  }\n}\n\n## ############################################\n## 2nd Coding-Possibility: Using replicate() ##\n## ############################################\n\n## Set seed for the random number generator to get reproducible results\nset.seed(3)\n\n## Function that generates estimator realizations \nmy_estimates_generator &lt;- function(n){\n  X_sample &lt;- rnorm(n = n, mean = mu, sd = sqrt(5))\n  ## compute the sample mean realization\n  return(mean(X_sample))\n}\n\nestimates_mat &lt;- cbind(\n  replicate(B, my_estimates_generator(n = n_seq[1])),\n  replicate(B, my_estimates_generator(n = n_seq[2]))\n)\n\nBased on the \\(B=10,000\\) realizations of the estimator \\(\\bar{X}_n\\), we can compute the empirical density functions \\(\\hat{F}_{X_n,B}\\) (see Figure 4.1) and histograms (see Figure 4.2) to get an idea about the true distribution of \\(\\bar{X}_n.\\)\nIn this simple case, we also know the theoretical distribution function \\(F_{X_n}\\) and density function which allows us to check the simulation results (see Figure 4.1 and Figure 4.2).\n\nlibrary(scales)\npar(mfrow=c(1,2))\nplot(ecdf(estimates_mat[,1]), main=\"n=5\", ylab=\"\", xlab=\"\", col = \"black\", xlim = range(estimates_mat[,1]), ylim=c(0,1.25))\nmtext(expression(mu==10), side = 1, at = 10, line = 2.5)\ncurve(pnorm(x, mean=10, sd=sqrt(5/5)), add=TRUE, col=\"red\", lty = 3, lwd=4)\nlegend(\"topleft\", legend = c(\"Empir. Distr.-Function\", \"True Distr.-Function\"), col = c(\"black\", \"red\"), lty = c(1, 3), lwd = c(1.3, 2), bty = \"n\")\n##      \nplot(ecdf(estimates_mat[,2]), main=\"n=50\", ylab=\"\", xlab=\"\", col = \"black\", xlim = range(estimates_mat[,1]), ylim=c(0,1.25))\nmtext(expression(mu==10), side = 1, at = 10, line = 2.5)\ncurve(pnorm(x, mean=10, sd=sqrt(5/50)), add=TRUE, col=\"red\", lty = 3, lwd=4)\nlegend(\"topleft\", legend = c(\"Empir. Distr.-Function\", \"True Distr.-Function\"), col = c(\"black\", \"red\"), lty = c(1, 3), lwd = c(1.3, 2), bty = \"n\")\n\n\n\n\n\n\nFigure 4.1: Empirical distribution functions \\(\\hat{F}_{X_{n},B}\\) computed from the \\((B=10000)\\) simulated realizations \\(\\bar{X}_{n,1,obs},\\dots,\\bar{X}_{n,B,obs}\\) and the theoretical distribution functions for \\(n=5,50.\\) The empirical and the theoretical distribution functions match perfectly.\n\n\n\n\n\nlibrary(scales)\npar(mfrow=c(1,2))\nhist(estimates_mat[,1], main=\"n=5\", xlab=\"\",  xlim = range(estimates_mat[,1]), prob = TRUE, ylim = c(0, 1.5))\nmtext(expression(mu==10), side = 1, at = 10, line = 2.5)\ncurve(dnorm(x, mean=10, sd=sqrt(5/5)), add=TRUE, lty = 3, lwd=4, col=\"red\")\nlegend(\"topleft\", legend = c(\"Histogram\",\"True Density\"), \n      col = c(\"black\", \"red\"), lty = c(1,3), lwd = c(1.3, 4), bty = \"n\")\n##\nhist(estimates_mat[,2], main=\"n=50\", xlab=\"\",  xlim = range(estimates_mat[,1]), prob = TRUE, ylim = c(0, 1.5))\nmtext(expression(mu==10), side = 1, at = 10, line = 2.5)\ncurve(dnorm(x, mean=10, sd=sqrt(5/50)), add=TRUE, lty = 3, lwd=4, col=\"red\")\nlegend(\"topleft\", legend = c(\"Histogram\",\"True Density\"), \n      col = c(\"black\", \"red\"), lty = c(1,3), lwd = c(1.3, 4), bty = \"n\")\n\n\n\n\n\n\nFigure 4.2: Histrograms of \\((B=10000)\\) simulated realizations \\(\\bar{X}_{n,1,obs},\\dots,\\bar{X}_{n,B,obs}\\) and true density functions for \\(n=5,50.\\) The empirical (simulation based) histrograms and the theoretical density functions match perfectly.\n\n\n\n\nObservations in Figure 4.1 and Figure 4.2: The empirical distribution functions and the histograms based on the simulated realizations \\[\n\\bar{X}_{n,1,obs},\\dots,\\bar{X}_{n,B,obs}\n\\] mach their theoretical counterparts almost perfectly since we chose a sufficient large number of \\(B=10000\\) simulations.\n\n\n\n\n\n\nTake away message\n\n\n\nWe can use Monte Carlo simulations to approximate the exact distribution of an estimator \\(\\hat{\\theta}_n\\) for given distributions \\(F_X\\) of the underlying random sample. These approximations become arbitrarily precise as \\(B\\to\\infty\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation Theory and Monte Carlo Simulations</span>"
    ]
  },
  {
    "objectID": "04-Monte-Carlo-Simulations.html#assessing-the-quality-of-estimators",
    "href": "04-Monte-Carlo-Simulations.html#assessing-the-quality-of-estimators",
    "title": "\n4  Estimation Theory and Monte Carlo Simulations\n",
    "section": "\n4.2 Assessing the Quality of Estimators",
    "text": "4.2 Assessing the Quality of Estimators\nAny reasonable estimator \\(\\hat\\theta_n\\) should be able to approximate the (usually unknown) parameter value \\(\\theta\\), \\[\n\\left(\\text{random quantity}\\right)\\quad\\hat\\theta_n\\approx\\theta\\quad\\left(\\text{deterministic parameter}\\right),\n\\] and the approximation should get better as the sample size increases, i.e. as \\(n\\to\\infty\\).\n\nStatisticians/econometricians use different metrics to assess the quality of an estimator \\(\\hat\\theta_n\\). The most prominent metrics are:\n\nbias of an estimator \\(\\hat{\\theta}_n\\)\n\nvariance and standard error of an estimator \\(\\hat{\\theta}_n\\)\n\nmean squared error (mse) of an estimator \\(\\hat{\\theta}_n\\)\n\n\n\n\n\n\n\n\n\nDefinition 4.1 (Bias of \\(\\theta\\)) The bias of an estimator \\(\\hat\\theta_n\\) is defined as\n\\[\n\\operatorname{Bias}\\left(\\hat\\theta_n\\right) = E\\left(\\hat\\theta_n\\right) - \\theta.\n\\]\n\n\n\n\nIf an estimator \\(\\hat\\theta_n\\) has no bias, i.e. if \\[\n\\operatorname{Bias}\\left(\\hat\\theta_n\\right)=E\\left(\\hat\\theta_n\\right) - \\theta=0\n\\] for all \\(\\theta\\in\\mathbb{R}\\) and all sample sizes \\(n,\\) we call it an unbiased estimator.\nMany modern estimators are not unbiased. However, every estimator should be at least asymptotically unbiased, i.e. \\[\n\\lim_{n\\to\\infty}\\operatorname{Bias}\\left(\\hat\\theta_n\\right)=0\n\\] for all \\(\\theta\\in\\mathbb{R}.\\)\nWe would like to have estimators \\(\\hat\\theta_n\\) with a small (or zero) bias.\nIf the bias of an estimator \\(\\hat\\theta_n\\) is small (or zero), we know that the distribution of the estimator \\(\\hat\\theta_n\\) is roughly (or exactly) centered around the true (usually unknown) parameter \\(\\theta.\\)\nHowever, also unbiased estimators \\(\\hat{\\theta}_n\\) may still vary a lot around the parameter \\(\\theta\\) to be estimated. Therefore, is is also important to assess the variance (or the standard deviation) of the estimator.\n\n\n\n\n\n\n\nDefinition 4.2 (Variance and Standard Error of \\(\\theta\\)) The variance of an estimator \\(\\hat\\theta_n\\) is defined equivalently to the variance of any other random variable\n\\[\nVar\\left(\\hat\\theta_n\\right) = E\\left[\\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)^2\\right].\n\\] The square root of the variance (i.e. the standard deviation) of an estimator \\(\\hat\\theta_n\\) is called standard error of \\(\\hat\\theta_n\\), \\[\n\\operatorname{SE}\\left(\\hat\\theta_n\\right) = \\sqrt{Var\\left(\\hat\\theta_n\\right)}.\n\\]\n\n\n\n\nWe would like to have estimators with a small as possible variance/standard error.\nThe variance (and thus also standard error) should decline as the sample size increases, such that \\[\n\\lim_{n\\to\\infty}Var\\left(\\hat\\theta_n\\right)=0.\n\\]\nTo combine bias and variance into one metric, one typically uses the Mean Squared Error (MSE) of an estimator \\(\\hat\\theta_n.\\)\n\n\n\n\n\n\n\nDefinition 4.3 (Mean Squared Error of \\(\\theta\\)) The mean squared error of an estimator \\(\\hat\\theta_n\\) is defined as\n\\[\n\\operatorname{MSE}\\left(\\hat\\theta_n\\right) =  E\\left[\\left(\\hat\\theta_n - \\theta\\right)^2\\right].\n\\]\n\n\n\n\nWe would like to have estimators with a small as possible mean squared error, and the mean squared error should decline as the sample size increases, such that \\[\n\\lim_{n\\to\\infty}\\operatorname{MSE}\\left(\\hat\\theta_n\\right)=0.\n\\]\nThe following holds true:\n\nThe mean squared error equals the sum of the squared bias and the variance:\n\n\\[\n\\operatorname{MSE}\\left(\\hat\\theta_n\\right) = \\left(\\operatorname{Bias}\\left(\\hat\\theta_n\\right)\\right)^2 +  Var\\left(\\hat\\theta_n\\right)\n\\]\n\nFor unbiased estimators (i.e. \\(E(\\hat\\theta_n)=\\theta\\)) the mean squared error equals the variance, i.e.\n\n\\[\n\\underbrace{E\\left[\\left(\\hat\\theta_n - \\theta\\right)^2\\right]}_{\\operatorname{MSE}\\left(\\hat\\theta_n\\right)} = \\underbrace{E\\left[\\left(\\hat\\theta_n - E\\left(\\hat\\theta_n\\right)\\right)^2\\right]}_{ Var\\left(\\hat\\theta_n\\right)}\n\\]\n\n\n\n\n\n\nMonte Carlo (MC) Simulations\n\n\n\n\nUnfortunately, it is often difficult to derive the above assessment metrics for given sample sizes \\(n\\) and given data distributions \\(F_X,\\) since for given \\(n\\) we typically do not know the distribution of the estimator \\(\\hat\\theta_n\\) and thus also not its bias and variance. Monte Carlo simulations allow us to solve this issue.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation Theory and Monte Carlo Simulations</span>"
    ]
  },
  {
    "objectID": "04-Monte-Carlo-Simulations.html#approximating-bias-variance-and-mean-squared-error-using-monte-carlo-simulations",
    "href": "04-Monte-Carlo-Simulations.html#approximating-bias-variance-and-mean-squared-error-using-monte-carlo-simulations",
    "title": "3  Estimation Theory and Monte Carlo Simulations",
    "section": "3.4 Approximating Bias, Variance, and Mean Squared Error using Monte Carlo Simulations",
    "text": "3.4 Approximating Bias, Variance, and Mean Squared Error using Monte Carlo Simulations\nWe can use Monte Carlo simulations to approximate the assessment metrics \\(\\operatorname{Bias}\\left(\\hat\\theta_n\\right),\\) \\(Var\\left(\\hat\\theta_n\\right),\\) and \\(\\operatorname{MSE}\\left(\\hat\\theta_n\\right)\\) for given sample sizes \\(n\\) and given data distributions \\(F_X\\) with arbitrary precision.\nAny of the the above assessment metrics require us to compute means of random variables:\n\nFor the \\(\\operatorname{Bias}\\left(\\hat\\theta_n\\right)\\) we need to compute \\(E\\left(\\hat\\theta_n\\right)-\\theta\\)\nFor the \\(Var\\left(\\hat\\theta_n\\right)\\) we need to compute \\(E\\left[\\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)^2\\right]\\).\nFor the \\(\\operatorname{MSE}\\left(\\hat\\theta_n\\right)\\) we need to compute \\(E\\left[\\left(\\hat\\theta_n - \\theta\\right)^2\\right]\\).\n\nA Monte Carlo simulation can approximate these means by using the law of large numbers which states that a sample mean over iid random variables is able to approximate the population mean of these random variables as the number of random variables to average over get large.1\nThus, to compute a very precise approximation to \\(E\\left(\\hat\\theta_n\\right)-\\theta\\), we can use a computer to execute the following algorithm:\nStep 1. Generate \\(B\\) many (e.g. \\(B=10,000\\)) realizations of the iid random sample\n\\[\n(X_{1,1},\\dots,X_{n,1}),\\; (X_{1,2},\\dots,X_{n,2}),\\dots, (X_{1,B},\\dots,X_{n,B})\n\\]\nStep 2. Compute the corresponding \\(B\\) many realizations of the estimator\n\\[\n\\underbrace{\\hat\\theta(X_{1,1},\\dots,X_{n,1})}_{=\\hat\\theta_{n,1}},\\;\\underbrace{\\hat\\theta(X_{1,2},\\dots,X_{n,2})}_{=\\hat\\theta_{n,2}},\\dots,\\underbrace{\\hat\\theta(X_{1,B},\\dots,X_{n,B})}_{=\\hat\\theta_{n,B}}\n\\] Step 3. Use the simulated realizations \\(\\hat{\\theta}_{n,1},\\dots,\\hat{\\theta}_{n,B}\\) to approximate the bias, variance, and the mean squared error of the estimator \\(\\hat{\\theta}_n\\):\n\nThe bias of \\(\\operatorname{Bias}\\left(\\hat\\theta_n\\right)=E\\left(\\hat\\theta_n\\right)-\\theta\\) can be approximated by\n\n\\[\n\\widehat{\\operatorname{Bias}}_{MC}\\left(\\hat\\theta_n\\right) = \\left(\\frac{1}{B}\\sum_{b=1}^B \\hat\\theta_{n,b}\\right) - \\theta\n\\]\n\nThe variance \\(Var\\left(\\hat\\theta_n\\right)=E\\left[\\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)^2\\right]\\) can be approximated by \\[\n\\widehat{Var}_{MC}\\left(\\hat\\theta_n\\right) = \\frac{1}{B}\\sum_{b=1}^B \\left(\\hat\\theta_{n,b} - \\left(\\frac{1}{B}\\sum_{b=1}^B \\hat\\theta_{n,b}\\right)\\right)^2\n\\]\nThe mean squared error \\(\\operatorname{MSE}\\left(\\hat\\theta_n\\right)=E\\left[\\left(\\hat\\theta_n - \\theta\\right)^2\\right]\\) can be approximated by \\[\n\\widehat{\\operatorname{MSE}}_{MC}\\left(\\hat\\theta_n\\right) = \\frac{1}{B}\\sum_{b=1}^B \\left(\\hat\\theta_{n,b} - \\theta\\right)^2\n\\]\n\nBy law of large numbers these approximations get arbitrarily precise as \\(B \\to \\infty\\).\n\nExample: Sample Mean\nThe following R code contains a Monte Carlo simulation ( B = 10000 replications) computing the bias, variance, and means squared error for the sample mean \\((\\hat\\theta_n=)\\bar{X}_n=\\sum_{i=1}^nX_i\\) as the estimator of the population mean \\((\\theta=)\\mu\\). The random sample \\(X_i\\overset{iid}{\\sim}F_X\\), \\(i=1,\\dots,n\\), is drawn from a normal distribution \\(F_X=\\mathcal{N}(\\mu,\\sigma^2)\\) with mean \\(\\mu=10\\) and variance \\(\\sigma^2=5\\). We investigate the accuracy of the estimator for different sample sizes \\(n\\in\\{5,15,50\\}\\). \n\n## Set seed for the random number generator to get reproducible results\nset.seed(3)\n## True parameter value ('theta' here 'mu')\nmu            <- 10\n## Number of Monte Carlo repetitions:\nB             <- 10000\n## Sequence of different sample sizes:\nn_seq         <- c(5, 15, 50)\n\n## Function that generates estimator realizations \nmy_estimates_generator <- function(n){\n  X_sample <- rnorm(n = n, mean = mu, sd = sqrt(5))\n  ## compute the sample mean realization\n  return(mean(X_sample))\n}\n\nestimates_mat <- cbind(\n  replicate(B, my_estimates_generator(n = n_seq[1])),\n  replicate(B, my_estimates_generator(n = n_seq[2])),\n  replicate(B, my_estimates_generator(n = n_seq[3]))\n)\n\nThe following R code computes the Monte Carlo (MC) approximations for the bias, variance, and mean squared error of \\(\\bar{X}_n\\). The results should capture our observations.\n\n## Bias of the sample mean for different sample sizes n\nMC_Bias_n_seq <- apply(estimates_mat, 2, mean) - mu\n\n## Variance of the sample mean for different sample sizes n\nMC_Var_n_seq  <- apply(estimates_mat, 2, var)\n\n## Mean squared error of the sample mean for different sample sizes n\nMC_MSE_n_seq  <- apply(estimates_mat, 2, function(x){mean((x - mu)^2)})\n\nThe table shows the numerical values of the Monte Carlo approximations for the true bias, true variance, and true mean squared error of \\(\\bar{X}_n\\):\n\n\n\n\nTable 3.1:  Monte Carlo approximations for the true bias, true variance, and true mean squared error of sample mean. \n \n  \n    n \n    Bias (MC-Sim)  \n    Variance (MC-Sim) \n    MSE (MC-Sim)  \n  \n \n\n  \n    5 \n    -0.001 \n    1.02 \n    1.02 \n  \n  \n    15 \n    0.000 \n    0.33 \n    0.33 \n  \n  \n    50 \n    0.001 \n    0.10 \n    0.10 \n  \n\n\n\n\n\n\nThese Monte Carlo approximations (Table 3.1) indicate that:\n\nThe true bias \\(\\operatorname{Bias}(\\bar{X}_n)\\) is very likely zero for all sample sizes \\(n\\in\\{5,15,50\\}\\)\n\n\n\nThe true mean squared error \\(\\operatorname{MSE}(\\bar{X}_n)\\) is very likely decreasing as the sample size \\(n\\) get larger.\n\n\nSince this example is chosen to be an extremely simple one, we can easily derive the true bias, variance and mean squared error values and compare them with their Monte Carlo approximations:\n\nTrue bias of \\(\\bar{X}_n\\): \\[\\begin{align*}\n\\operatorname{Bias}\\left(\\bar{X}_n\\right)\n&=E\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) - \\mu \\\\[2ex]\n&= \\left(\\frac{1}{n}\\sum_{i=1}^nE(X_i)\\right) -\\mu\\\\[2ex]\n&= \\frac{n}{n}\\mu-\\mu \\\\[2ex]\n&=0,\n\\end{align*}\\] thus the mean squared error of \\(\\bar{X}_n\\) equals the variance of \\(\\bar{X}_n\\).\nTrue variance of \\(\\bar{X}_n\\): \\[\\begin{align*}\nVar\\left(\\bar{X}_n\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right)\\\\[2ex]\n&= \\frac{1}{n^2} \\sum_{i=1}^nVar\\left(X_i\\right)\\\\[2ex]\n&= \\frac{n}{n^2}\\sigma^2 \\\\[2ex]\n&= \\frac{1}{n}\\sigma^2\n\\end{align*}\\] The following table shows the true bias, true variance and true mean squared error values:\n\n\n\n\n\nTable 3.2:  True bias, true variance, and true mean squared error of sample mean. (Only computable in simple special cases.) \n \n  \n    n \n    Bias (true)  \n    Variance (true) \n    MSE (true)  \n  \n \n\n  \n    5 \n    0 \n    1.00 \n    1.00 \n  \n  \n    15 \n    0 \n    0.33 \n    0.33 \n  \n  \n    50 \n    0 \n    0.10 \n    0.10 \n  \n\n\n\n\n\n\nObviously, the Monte Carlo approximations (Table 3.1) for these true values (Table 3.2) are very good. If we would further increase the number of Monte Carlo repetitions B, the Monte Carlo approximations would get even more precise since we can make them arbitrarily precise by letting B\\(\\to\\infty\\) using the law of large numbers."
  },
  {
    "objectID": "04-Monte-Carlo-Simulations.html#exercises",
    "href": "04-Monte-Carlo-Simulations.html#exercises",
    "title": "\n4  Estimation Theory and Monte Carlo Simulations\n",
    "section": "\n4.5 Exercises",
    "text": "4.5 Exercises\n\nExercises for Chapter 4\nExercises for Chapter 4 with Solutions",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation Theory and Monte Carlo Simulations</span>"
    ]
  },
  {
    "objectID": "05-Small-Sample-Inference.html",
    "href": "05-Small-Sample-Inference.html",
    "title": "5  Small Sample Inference",
    "section": "",
    "text": "The original version of this chapter was inspired by Chapter 1 of Hayashi (2000). The current version, however, deviates in many aspects from Hayashi (2000).\nIn this chapter, we focus on inference with small sample sizes. It’s is very hard to say when a sample size \\(n\\) is small. Often people say something like:\nbut these are only a very rough rules of thumb and may not apply in practice.\nThe core issue with small sample sizes is that we cannot do inference using the law of large numbers and the central limit theorem. Thus we need rather strict assumptions on the distribution of the error term, in order to do inference in finite samples. If these assumption are fulfilled, however, then we do exact inference.\nExact inference: By “exact inference” we mean correct inference for each sample size \\(n\\). That is, no asymptotic \\((n\\to\\infty)\\) arguments will be used.\nAssumptions: In Chapter 4, we did not impose a complete distributional assumption on \\(\\varepsilon\\) (see Assumption 4). For instance, the i.i.d. normal case in Assumption 4 was only one possible option. However, to do inference in small samples, the normality Assumption on the error terms is not a mere option, but a necessity.\nTherefore, in this chapter we assume that Assumptions 1-3 from Chapter 4 hold and that additionally the following assumption holds:\nAssumption 4\\(^\\boldsymbol{\\ast}\\): Conditional Gaussian error distribution: The error terms are Gaussian and homoskedastik, i.e., \\[\n\\varepsilon_i|X_i\\sim\\mathcal{N}(0,\\sigma^2)\n\\] for all \\(i=1,\\dots,n.\\)\nAssumption 4\\(^\\boldsymbol{\\ast}\\) together with the random sample assumption of Assumption 1, part (b), leads to Gaussian spherical errors conditionally on \\(X\\), \\[\n\\varepsilon|X\\sim\\mathcal{N}\\left(0,\\sigma^2I_n\\right),\n\\] where \\(\\varepsilon=(\\varepsilon_1,\\dots,\\varepsilon_n)'\\).\nRemark: The subscript \\(n\\) in \\(\\hat\\beta_n\\) is here only to emphasize that the distribution of \\(\\hat\\beta_n\\) depends on \\(n\\); we will, however, often simply write \\(\\hat\\beta\\)."
  },
  {
    "objectID": "05-Small-Sample-Inference.html#sec-testmultp",
    "href": "05-Small-Sample-Inference.html#sec-testmultp",
    "title": "5  Small Sample Inference",
    "section": "5.1 Hypothesis Tests about Multiple Parameters (F-Tests)",
    "text": "5.1 Hypothesis Tests about Multiple Parameters (F-Tests)\nLet us consider the following system of \\(q\\)-many null hypotheses: \\[\\begin{align*}\nH_0: \\underset{(q\\times K)}{R}\\underset{(K\\times 1)}{\\beta}  = \\underset{(q\\times 1)}{r^{(0)}},\n\\end{align*}\\] where\n\nthe \\((q \\times K)\\) matrix \\(R,\\) which describes the considered linear combinations of the unknown \\(\\beta=(\\beta_1,\\dots,\\beta_K)'\\) values, and\nthe \\((q\\times 1)\\) vector \\(r^{(0)}=(r^{(0)}_{1},\\dots,r^{(0)}_{q})'\\) of null hypothetical values\n\nare chosen by the statistician to specify the null hypothesis about the unknown true parameter vector \\(\\beta\\).\nTo make sure that there are no redundant equations, it is required that \\(\\operatorname{rank}(R)=q\\).\nWe must also specify the alternative against which we are testing the null hypothesis, for instance \\[\\begin{equation*}\nH_1: R\\beta  \\neq r^{(0)}\n\\end{equation*}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe above multiple parameter hypotheses cover also the special case of single parameter hypothesis; for instance, by setting\n\n\\(R=(0,1,0\\dots,0)\\) and\n\\(r^{(0)}=0\\)\n\nwe get the classic single parameter \\(H_0\\) and \\(H_1\\) that allows us to test the hypothesis that “\\(X_{i2}\\) has no effect”\n\\[\\begin{equation*}\n\\begin{array}{ll}\n&H_0:  \\beta_{2}=0 \\\\\n\\text{versus}\\quad &H_1:  \\beta_{2} \\ne 0 \\\\\n\\end{array}\n\\end{equation*}\\] We come back to this in Section 5.2.\n\n\nUnder our assumptions (Assumptions 1 to 4\\(^\\ast\\)), we have that \\[\n\\begin{align*}\n(R\\hat\\beta_n-r^{(0)})|X&\\sim\\mathcal{N}\\left(R\\beta -r^{(0)}, RVar(\\hat\\beta_n|X)R'\\right)\\\\\n(R\\hat\\beta_n-r^{(0)})|X&\\overset{{\\color{red}H_0}}{\\sim}\\mathcal{N}\\left(\\phantom{RV}{\\color{red}0}\\phantom{R},RVar(\\hat\\beta_n|X)R'\\right)\n\\end{align*}\n\\]\n\nThe realizations of \\[\n(R\\hat\\beta_n -r^{(0)})|X\n\\] will scatter around the unknown mean \\((R\\beta -r^{(0)})\\) in a Gaussian fashion.\nIf \\(H_0\\) is correct (i.e., if \\(R\\beta-r^{(0)}=0\\)), the realizations of \\[\n(R\\hat\\beta_n-r^{(0)})|X\n\\] will scatter around the \\((q\\times 1)\\) vector \\(0\\).\n\nWe use a test statistic to detect a systematic location shift away from the zero vector.\n\n\n\n5.1.1 The Test Statistic and its Null Distribution\nThe fact that \\[\n(R\\hat\\beta_n-r^{(0)})\\in\\mathbb{R}^q\n\\] is a \\(q\\)-dimensional random variable makes it a little bothersome to use as a test-statistic. Fortunately, we can turn \\((R\\hat\\beta_n-r^{(0)})\\) into a scalar-valued test statistic using the following quadratic form: \\[\nW=\\underbrace{(R\\hat\\beta_n -r^{(0)})'}_{(1\\times q)}\\underbrace{[RVar(\\hat\\beta_n|X)R']^{-1}}_{(q\\times q)}\\underbrace{(R\\hat\\beta_n -r^{(0)})}_{(q\\times 1)}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nNote that the test statistic \\(W\\) is simply measuring the distance (it’s a weighted L2-distance) between the \\((q\\times 1)\\) vectors \\(R\\hat\\beta_n\\) and \\(r^{(0)}.\\)\n\n\nUnder the null hypothesis (i.e., if \\(H_0\\) is true), \\(W|X\\) is a sum of \\(q\\)-many independent squared standard normal random variables. Therefore, under the null hypothesis, \\(W|X\\) is chi-square distributed with \\(q\\) degrees of freedom (see Section 2.2.10.3), \\[\n\\begin{align*}\nW|X&\\overset{H_0}{\\sim} \\chi^2_{(q)}\\\\\n\\Rightarrow \\quad\\quad W&\\overset{H_0}{\\sim} \\chi^2_{(q)}\n\\end{align*}\n\\] Note that the distribution of \\(W|X\\) does not depend on \\(X.\\) I.e. \\(W|X\\) follows a \\(\\chi^2_{(q)}\\)-distribution no matter the realization of \\(X.\\) Thus our test decisions do not depend on the values of \\(X.\\) (Good news!)\nUsually, we do not know \\(Var(\\hat\\beta_n|X),\\) and thus we need to estimate this quantity from the data. Unfortunately, in the small sample case, we can only deal with homoskedastic error terms. For truly exact finite sample inference, we need a variance estimator for which we can derive the exact small sample distribution. Therefore, we require Assumption 4\\(^*\\) of spherical errors (i.e., \\(Var(\\varepsilon|X)=I_n\\sigma^2\\)) which yields that \\(Var(\\hat\\beta_n|X)=\\sigma^2(X'X)^{-1}\\), and where \\(\\sigma^2\\) can be estimated by the unbiased (\\(UB\\)) variance estimator\n\\[\ns_{UB}^2=(n-K)^{-1}\\sum_{i=1}^n\\hat\\varepsilon_i^2.\n\\]\nFrom the normality assumption in Assumption 4\\(^*\\), it follows then that \\[\n\\frac{(n-K)}{\\sigma^{2}}s_{UB}^2\\sim\\chi^2_{(n-K)}.\n\\tag{5.2}\\]\nThe \\(F\\) test statistic uses then \\(s_{UB}^2\\) as an estimator of \\(\\sigma^2\\) \\[\nF=(R\\hat\\beta_n -r^{(0)})'[R(s_{UB}^2(X'X)^{-1})R']^{-1}(R\\hat\\beta_n -r^{(0)})/q\n\\] and takes into account the additional randomness (estimation errors) due to \\(s_{UB}^2\\), which leads to the following exact null distribution of the \\(F\\) test \\[\nF\\overset{H_0}{\\sim} F_{(q,n-K)},\n\\tag{5.3}\\] where \\(F_{(q,n-K)}\\) denotes the \\(F\\)-distribution with \\(q\\) numerator and \\(n-K\\) denominator degrees of freedom.\nAs in the case of \\(W\\), the distribution of \\(F\\) conditional on \\(X\\) does not depend on \\(X\\); i.e. \\(F|X\\overset{H_0}{\\sim}F_{(q,n-K)},\\) but \\(F_{(q,n-K)}\\) does not depend on \\(X,\\) thus we can write \\(F\\overset{H_0}{\\sim}F_{(q,n-K)}.\\)\nThe distributional statements in Equation 5.2 and Equation 5.3 are a little cumbersome to derive and we do not go into details here, but in case you’re interested you can find some more details, for instance, in Chapter 1 of Hayashi (2000).\nBy contrast to \\(W,\\) \\(F\\) is now a practically useful test statistic, and we can use the observed value \\(F_{\\text{obs}}\\) to measure the distance of our observed estimate \\(R\\hat\\beta_n\\) from its hypothetical value \\(r.\\)\nObserved values, \\(F_{\\text{obs}}\\), that are “unusually large” under the null hypothesis, lead to a rejection of the null hypothesis. The null distribution \\(F_{(q,n-K)}\\) of \\(F\\) is used to judge what’s “unusually large” under the null hypothesis.\nThe F distribution. The F distribution is a ratio of two \\(\\chi^2\\) distributions. It has two parameters: the numerator degrees of freedom, and the denominator degrees of freedom. Each combination of the parameters yields a different F distribution. See Section 2.2.10.6 for more information on the \\(F\\) distribution."
  },
  {
    "objectID": "05-Small-Sample-Inference.html#ch:testingsinglep",
    "href": "05-Small-Sample-Inference.html#ch:testingsinglep",
    "title": "5  Small Sample Inference",
    "section": "5.2 Tests about One Parameter (t-Tests)",
    "text": "5.2 Tests about One Parameter (t-Tests)\nA hypothesis about only one parameter \\[\\begin{equation*}\n\\begin{array}{ll}\nH_0: & \\beta_k=\\beta_k^{(0)}\\\\\n\\text{versus}\\quad H_1: & \\beta_k\\ne \\beta_k^{(0)}\\\\\n\\end{array}\n\\end{equation*}\\] is simply a special case of the general null hypothesis \\(H_0:R\\beta -r =0,\\) where \\(R\\) is a \\((1\\times K)\\) row-vector of zeros, but with a one as the \\(k\\)th element, and where the null hypothetical value is set by the statistician \\(r=\\beta_k^{(0)}\\) (e.g. \\(\\beta_k^{(0)}=0\\)).\nThus the \\(F\\)-test statistic simplifies to \\[\nF=\\frac{\\left(\\hat{\\beta}_k-\\beta_k^{(0)}\\right)^2}{\\widehat{Var}(\\hat{\\beta}_k|X)}\\overset{H_0}{\\sim}F_{(1,n-K)},\n\\] where \\[\\widehat{Var}(\\hat{\\beta}_k|X)=s^2_{UB}[(X'X)^{-1}]_{kk}.\n\\] Taking square roots yields the \\(t\\) test statistic \\[\nT=\\frac{\\hat{\\beta}_k-\\beta_k^{(0)}}{\\widehat{\\operatorname{SE}}(\\hat{\\beta}_k|X)}\\overset{H_0}{\\sim}t_{(n-K)},\n\\] where \\[\n\\widehat{\\operatorname{SE}}(\\hat{\\beta}_k|X)=s_{UB}[(X'X)^{-1/2}]_{kk},\n\\] and where \\(t_{(n-K)}\\) denotes the \\(t\\)-distribution with \\(n-K\\) degrees of freedom.\nThus the \\(t\\)-distribution with \\(n-K\\) degrees of freedom is the appropriate distribution to judge whether or not an observed value \\(T_{\\text{obs}}\\) of the test statistic is “unusually large” under the null hypothesis.\n\n\n\n\n\n\nTip\n\n\n\nAll commonly used statistical software packages report \\(t\\)-tests testing the null hypothesis \\[\nH_0:\\beta_k=0\n\\] for each \\(k=1,\\dots,K.\\) This means to test the null hypothesis that \\(X_k\\) has “no (linear) effect” on the conditional mean of \\(Y\\) given \\(X.\\)\n\n\nThe \\(t\\) distribution. The following plot illustrates that as the degrees of freedom increase, the shape of the \\(t\\) distribution comes closer to that of a standard normal bell curve. Already for \\(25\\) degrees of freedom we find little difference to the standard normal density. In case of small degrees of freedom values, we find the distribution to have heavier tails than a standard normal. See Section 2.2.10.4 for more information about the \\(t\\)-distribution."
  },
  {
    "objectID": "05-Small-Sample-Inference.html#testtheory",
    "href": "05-Small-Sample-Inference.html#testtheory",
    "title": "5  Small Sample Inference",
    "section": "5.3 Testtheory",
    "text": "5.3 Testtheory\nEvery statistical test statistic is a function of the random sample, i.e. \\[\nT\\equiv T((X_1,Y_1),\\dots,(X_n,Y_n))\n\\] and is thus a random variable.\nCaution: In this section, \\(T\\) denotes any real test statistic. Specific examples for \\(T\\) are, for instance, the \\(F\\)-test statistic (Section 5.1) or the \\(t\\)-test statistic (Section 5.2).\nGenerally, we can only derive the distribution of \\(T\\) under \\(H_0;\\) i.e. under the scenario that \\(H_0\\) is true. The distribution of a test statistic \\(T\\) under \\(H_1\\) is typically unknown.\n\nWe use the observed realization \\[\nT_{\\text{obs}}\\equiv T((X_{1,\\text{obs}},Y_{1,\\text{obs}}),\\dots,(X_{n,\\text{obs}},Y_{n,\\text{obs}}))\n\\] \\(T_{\\text{obs}},\\) of the random test statistic \\(T\\) to decide whether we cannot reject a null hypothesis \\(H_0\\) about some parameter \\(\\theta\\) or wether we can reject \\(H_0\\) in favor of the alternative hypothesis \\(H_1\\) \\[\\begin{equation*}\n\\begin{array}{ll}\n& H_0: \\theta\\in\\Theta_0\\\\\n\\text{versus}\\quad\n& H_1: \\theta\\in\\Theta_1,\\\\\n\\end{array}\n\\end{equation*}\\] where\n\n\\(\\Theta_0\\) is the set of parameter values \\(\\theta\\) under \\(H_0\\).\n\\(\\Theta_1\\) is the set of parameter values \\(\\theta\\) under \\(H_1\\).\n\\(\\Theta_0 \\cap \\Theta_1 = \\emptyset\\)\n\nSimple versus composite hypotheses:\n\nIf \\(\\Theta_j,\\) \\(j=1,2,\\) contains only one value \\[\n\\Theta_j=\\theta^{(j)},\n\\] it is called a simple hypothesis.\nIf \\(\\Theta_j,\\) \\(j=1,2,\\) contains multiple values, it is called a composite hypothesis.\n\n\n\n\n\n\n\nExample: Two-Sided Test with \\(\\theta\\in\\mathbb{R}\\)\n\n\n\n\\[\\begin{equation*}\n\\begin{array}{ll}\n& H_0: \\theta = \\theta^{(0)}\\\\\n\\text{versus}\\quad\n& H_1: \\theta\\neq \\theta^{(0)}\\\\\n\\end{array}\n\\end{equation*}\\]\n\nSimple null hypothesis: \\[\n\\Theta_0=\\theta^{(0)}\n\\]\nComposite alternative hypothesis: \\[\n\\Theta_1=\\mathbb{R}\\setminus\\theta^{(0)}\n\\]\n\n\n\n\n\n\n\n\n\nExample: One-Sided Test with \\(\\theta\\in\\mathbb{R}\\)\n\n\n\n\\[\\begin{equation*}\n\\begin{array}{ll}\n& H_0: \\theta \\geq  \\theta^{(0)}\\\\\n\\text{versus}\\quad\n& H_1: \\theta  < \\theta^{(0)}\\\\\n\\end{array}\n\\end{equation*}\\]\n\nComposite null hypothesis: \\[\n\\Theta_0=\\;[\\theta^{(0)},\\infty[\n\\]\nComposite alternative hypothesis: \\[\n\\Theta_1=\\;]-\\infty,\\theta^{(0)}[\n\\]\n\nLikewise for the other direction of the one-sided test.\n\n\n\n5.3.1 Decisions Errors, Size and Power\nHypothesis testing is like a legal trial. We assume someone is innocent unless the evidence strongly suggests that he is guilty. Similarly, we retain \\(H_0\\) unless there is strong evidence to reject \\(H_0.\\)\n\nWe reject \\(H_0\\) if \\[\nT_{\\text{obs}}\\in \\mathcal{R},\n\\] where \\(\\mathcal{R}\\) denotes the rejection region; i.e. a range of \\(T\\) values that we see only very rarely under \\(H_0.\\) \nWe cannot reject \\(H_0\\) \\[\nT_{\\text{obs}}\\not \\in \\mathcal{R}\n\\]\n\nWe differentiate two decision errors:\n\ntype-I-error: If we reject \\(H_0\\) even though \\(H_0\\) is true.\ntype-II-error: If we do not reject \\(H_0\\) even though \\(H_1\\) is true.\n\n\n\n\n\n\n\nSize\n\n\n\nThe probability of a type-I-error event is also called size of \\(T.\\)\nThe size of \\(T\\) is defined as the larges probability of rejecting \\(H_0\\) over all possible parameter values \\(\\theta\\in\\Theta_0\\) under \\(H_0,\\) \\[\\begin{align*}\n\\text{Size}\n=& \\sup_{\\theta\\in\\Theta_0} P(T \\in \\mathcal{R}).\n\\end{align*}\\]\nSince we want to be sure that we correctly reject \\(H_0,\\) we want test statistics for which we can control the their size from above by some chosen level; e.g. \n\n\\(\\text{Size}\\,\\leq 0.05\\) or\n\\(\\text{Size}\\,\\leq 0.01.\\)\n\n\n\n\n\n\n\n\n\nPower\n\n\n\nOne minus the probability of a type-II-error is called power of \\(T.\\)\nThe power of \\(T\\) is defined as \\[\\begin{align*}\n\\text{Power}=&1-P(\\text{type-II-error})\\\\[2ex]\n=&P(T\\in \\mathcal{R} | \\theta\\in \\Theta_1)\n\\end{align*}\\] Since we want to detect violations of \\(H_0,\\) we want test statistics with a large power.\n\n\n\n\n5.3.2 Significance Level or Nominal Size\nLet \\(\\alpha\\) with \\(0<\\alpha<1\\) denote the significance level chosen by the statistician/econometrician.\nA statistical hypothesis test, \\(T,\\) is a valid test if its size (probability of a type-I-error) can be bounded from above by the chosen significance level (nominal size) \\(\\alpha\\), i.e. if\n\\[\\begin{align*}\n\\text{Size}=P(\\text{reject } H_0| H_0\\text{ is true})\\; \\leq\\; \\alpha = \\text{Nominal Size}\n\\end{align*}\\]\nSince we want to keep the probability of falsely rejecting \\(H_0\\) small, we choose small singificance levels such as\n\n\\(\\alpha=0.05\\) or\n\\(\\alpha=0.01\\) or\n\\(\\alpha=0.001,\\)\n\nbut of course, these values are only conventions and you can also choose, for instance, \\(\\alpha=0.026.\\)\n\n\n\n\n\n\nExact vs conservative vs invalid\n\n\n\nA test statistic \\(T\\) is called exact (or non-conservative) if \\[\n\\text{Size}=\\sup_{\\theta\\in\\Theta_0} P(T \\in \\mathcal{R}) =\\alpha.\n\\] A test statistic \\(T\\) is called conservative if \\[\n\\text{Size}= \\sup_{\\theta\\in\\Theta_0} P(T \\in \\mathcal{R}) <\\alpha.\n\\] A test statistic \\(T\\) is called invalid if \\[\n\\text{Size}=\\sup_{\\theta\\in\\Theta_0} P(T \\in \\mathcal{R}) > \\alpha.\n\\]\n\n\n\n\n\n\nTip\n\n\n\nUnder Assumptions 1-4\\(^\\ast,\\) the \\(F\\)-test and the \\(t\\)-test are exact test statistics.\n\n\n\n\n\n\n\n5.3.3 Rejection Regions of the \\(F\\) and \\(t\\)-Test\nTo decide whether we can reject \\(H_0\\), we need to compare the observed value \\(T_{\\text{obs}}\\) with the distribution of \\(T\\) under \\(H_0.\\) This can be done using\n\nrejection regions/critical values,\n\\(p\\)-values (Section 5.3.5) or\n\nconfidence intervals (Section 5.4)\n\nAll of these options lead to equivalent test decisions.\nThe rejection region for a statistical test statistic \\(T\\) is defined using critical values which are certain quantiles of the distribution of \\(T\\) under \\(H_0.\\)\n\n5.3.3.1 Rejection Regions of the \\(F\\)-Test\nThe \\(F\\)-test allows us to test \\[\\begin{align*}\n&H_0: R\\beta = r^{(0)}\\\\[2ex]\n\\text{versus}\\quad\n&H_1: R\\beta\\neq r^{(0)},\n\\end{align*}\\] where \\(\\beta\\) denotes the true (unknown) parameter vector and \\(r\\) the null-hypothetical value specified by the statistician (e.g. \\(r=0\\)).\nUnder Assumptions 1-4\\(^\\ast\\) (see Section 5.1), we have that \\[\nF\\overset{H_0}{\\sim}F_{q,n-K}.\n\\tag{5.4}\\]\nLet \\(0<\\alpha<1\\) denote the significance level and let \\(c_{1-\\alpha}\\) denote the \\((1-\\alpha)\\) quantile of the \\(F\\)-distribution with \\((q,n-K)\\) degrees of freedom.\nThis quantile \\(c_{1-\\alpha}\\) is the critical value that defines the rejection region: \\[\n\\mathcal{R}=\\; ]c_{1-\\alpha},\\infty[\n\\] \n\nWe can rejection \\(H_0\\) if \\[\nF_{obs}\\in \\mathcal{R}=\\; ]c_{1-\\alpha},\\infty[\n\\]\nWe cannot rejection \\(H_0\\) if \\[\nF_{obs}\\not \\in \\mathcal{R}=\\; ]c_{1-\\alpha},\\infty[\n\\]\n\nEquation 5.4 allows us to show that the \\(F\\)-test is an exact test. Since the null hypothesis is a simple hypothesis, we have that \\[\\begin{align*}\n\\text{Size}\n=&\\sup_{\\theta\\in\\Theta_0}P(T \\in \\mathcal{R})\\\\[2ex]\n=&P(F \\in \\mathcal{R} \\;|\\; R\\beta = r^{(0)}).\n\\end{align*}\\] Thus, \\[\\begin{align*}\n\\text{Size}\n=&P(F\\in\\mathcal{R}\\;|\\;R\\beta = r^{(0)})\\\\[2ex]\n=&P\\Big(F > c_{1-\\alpha}\\;|\\;R\\beta = r^{(0)}\\Big)\\\\[2ex]\n=&1 - P\\Big(F \\leq c_{1-\\alpha}\\;|\\;R\\beta = r^{(0)}\\Big)=\\alpha,\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nThe rejection region: The rejection region describes a range of values of the test statistic \\(F\\) which we rarely see if the null hypothesis is true (only in at most \\(\\alpha \\cdot 100\\%\\) cases). If the observed value of the test statistic, \\(F_{\\text{obs}}\\), falls in this region, we will reject the null hypothesis and acknowledge a type-I-error rate of at most \\(\\alpha\\).\nThe non-rejection region: The non-rejection region describes a range of values of the test statistic \\(F\\) which we expect to see (in \\((1-\\alpha) \\cdot 100\\%\\) cases) if the null hypothesis is true. If the observed value of the test statistic, \\(F_{\\text{obs}}\\) falls in this region, we cannot reject the null hypothesis.\n\n\n\n\n\n\nDanger\n\n\n\nIf we cannot reject \\(H_0,\\) does not mean that we confirm \\(H_0.\\)\nA possible violation of the null hypothesis may only be too small to stand out from the estimation errors; i.e. we may do a type-II-error. However, we do not control the probability of type-II-errors—we only control the probability of type-I-errors.\nTherefore, if you were not able to reject \\(H_0,\\) never ever state something like: “I conclude \\(H_0\\) is true.”\n\n\n\n\n\n\nTip\n\n\n\nFor the special case of a “no-effect” null hypothesis \\[\nH_0:\\beta_k=0,\n\\] there’s a famous sentence which goes back to Altman and Bland (1995) :\n\n“Absence of evidence is not evidence of absence.”\n\n\n\n\n\nTo find the critical value \\(c_{1-\\alpha}\\) we can use R as following:\n\nalpha <- 0.05 # chosen significance level\ndf1   <- 9    # numerator df\ndf2   <- 120  # denominator df\n\n## Critical value:\ncrit_value <- qf(p = 1-alpha, df1 = df1, df2 = df2)\ncrit_value\n\n[1] 1.958763\n\n\nChanging the significance level from \\(\\alpha=0.05\\) to \\(\\alpha=0.01\\) makes the critical value \\(c_{1-\\alpha}\\) larger and, therefore, the rejection region smaller (smaller probability of type-I-errors).\n\nalpha <- 0.01 # chosen significance level\n## Critical value:\ncrit_value <- qf(p = 1-alpha, df1 = df1, df2 = df2)\ncrit_value\n\n[1] 2.558574\n\n\n\n\n5.3.3.2 Rejection Regions of the Two-Sided \\(t\\)-Test\nThe two-sided \\(t\\)-test allows us to test \\[\\begin{align*}\n& H_0: \\beta_k=\\beta_k^{(0)}\\\\\n\\text{versus}\\quad\n& H_1: \\beta_k\\ne \\beta_k^{(0)}\n\\end{align*}\\] where \\(\\beta_k\\) denotes the true (unknown) parameter value and \\(\\beta_k^{(0)}\\) the null hypothetical value specified by the statisticaian (e.g. \\(\\beta_k^{(0)}=0\\)).\nUnder Assumptions 1-4\\(^\\ast\\) (see Section 5.2), we have that \\[\nT\\overset{H_0}{\\sim}t_{n-K}.\n\\tag{5.5}\\]\nLet \\(0<\\alpha<1\\) denote the significance level and let \\(c_{\\alpha/2}\\) and \\(c_{1-\\alpha/2}\\) denote the \\(\\alpha/2\\) and the \\((1-\\alpha/2)\\) quantiles of the \\(t\\)-distribution with \\((n-K)\\) degrees of freedom.\nThese quantiles are the critical values that define the rejection region of the two-sided \\(t\\)-test: \\[\n\\mathcal{R}=\\;]-\\infty,c_{\\alpha/2}[\\;\\;\\cup\\;\\;]c_{1-\\alpha/2}, \\infty[\n\\]\n\nWe can reject \\(H_0\\) if \\[\\begin{align*}\nT_{obs}\n& \\in \\mathcal{R}%\\\\[2ex]\n%& \\in \\;]-\\infty,c_{\\alpha/2}[\\;\\;\\cup\\;\\;]c_{1-\\alpha/2}, \\infty[\n\\end{align*}\\]\nWe cannot reject \\(H_0\\) if \\[\\begin{align*}\nT_{obs}\n&\\not \\in \\mathcal{R}%\\\\[2ex]\n%&\\not \\in \\;]-\\infty,c_{\\alpha/2}[\\;\\;\\cup\\;\\;]c_{1-\\alpha/2}, \\infty[\n\\end{align*}\\]\n\nEquation 5.5 allows us to show that the \\(t\\)-test is an exact test. Since the two-sided null hypothesis is a simple hypothesis, we have that \\[\\begin{align*}\n\\text{Size}\n=&\\sup_{\\theta\\in\\Theta_0}P(T \\in \\mathcal{R})\\\\[2ex]\n=&P(T \\in \\mathcal{R} \\;|\\; \\beta_k = \\beta_k^{(0)}).\n\\end{align*}\\] Thus, \\[\\begin{align*}\n\\text{Size}\n=&P(T \\in \\mathcal{R} \\;|\\; \\beta_k = \\beta_k^{(0)})\\\\[2ex]\n=&P\\Big(T < c_{\\alpha/2}\\quad\\text{or}\\quad T>c_{1-\\alpha/2} \\;\\Big|\\; \\beta_k = \\beta_k^{(0)}\\Big)\\\\[2ex]\n=&P\\Big(T < c_{\\alpha/2} \\;\\Big|\\; \\beta_k = \\beta_k^{(0)}\\Big) +\n  P\\Big(T>c_{1-\\alpha/2} \\;\\Big|\\; \\beta_k = \\beta_k^{(0)}\\Big)\\\\[2ex]\n&=\\frac{\\alpha}{2}+\\frac{\\alpha}{2}=\\alpha.\n\\end{align*}\\]\nFigure 5.1 shows an example of the rejection region for the case of a significance level \\(\\alpha=0.05\\) and \\(n-K=12\\) degrees of freedom.\n\n\n\n\n\nFigure 5.1: \\(t\\)-distribution with \\(n-K=12\\) degrees of freedom and critical values \\(c_{\\alpha/2}=-2.18\\) and \\(c_{1-\\alpha/2}=2.18\\) for the significance level \\(\\alpha=0.05\\).\n\n\n\n\nTo find the \\(c_{\\alpha/2}\\) and \\(c_{1-\\alpha/2}\\) critical values we can use R as following:\n\nalpha <- 0.05 # chosen signficance level \ndf    <- 12   # degrees of freedom \n\n## Two-sided critical value (= (1-alpha/2) quantile):\nc_twoSided <- qt(p = 1-alpha/2, df = df)\n\n## lower critical value\n-c_twoSided\n\n[1] -2.178813\n\n## upper critical value\nc_twoSided\n\n[1] 2.178813\n\n\n\n\n5.3.3.3 Rejection Regions of the One-Sided \\(t\\)-Test\nPossible one-sided hypothesis: \\[\\begin{align*}\n&H_0: \\beta_k \\leq \\beta_k^{(0)}\\\\\n\\text{versus}\\quad\n& H_1: \\beta_k > \\beta_k^{(0)}\n\\end{align*}\\]\nThis is now slightly more involved since the null hypothesis is a compound hypothesis: \\[\nH_0: \\beta_k \\in \\;\\big]-\\infty,\\beta_k^{(0)}\\big]\n\\]\n\nOf course, under the scenario that \\(H_0\\) is true, only one of the infinitely many null hypotheses is actually true and all other null hypotheses are wrong.\n\n\n\n\n\n\nTip\n\n\n\nThe idea of a compound null hypothesis \\(H_0:\\theta\\in\\Theta_0\\) is to collect all hypotheses which we do not care to detect by the statistical test. This way, the set of alternative hypotheses \\(H_1:\\theta\\in\\Theta_1\\) becomes smaller which leads to more powerful tests.\n\n\nTo conduct the \\(t\\)-test we need to take one null hypothetical value \\[\n\\tilde\\beta_k^{(0)}\\in ]-\\infty,\\beta_k^{(0)}]\n\\] which leads to a \\(\\tilde\\beta_k^{(0)}\\) specific \\(t\\)-test \\[\\begin{align*}\nT(\\tilde\\beta_k^{(0)}) :=\n\\frac{\\hat\\beta_k - \\tilde\\beta_k^{(0)}}{\\widehat{SE}(\\hat\\beta_{k}|X)}\n\\end{align*}\\]\nUnder Assumptions 1-4\\(^\\ast,\\) we have that \\[\n\\begin{align*}\nT(\\tilde\\beta_k^{(0)})\n&=\n\\frac{\\hat\\beta_k \\overbrace{- \\beta_k + \\beta_k}^{=0} - \\tilde\\beta_k^{(0)}}{\\widehat{SE}(\\hat\\beta_{k}|X)}\\\\[2ex]\n&=\n\\underbrace{\\frac{\\hat\\beta_k - \\beta_k}{\\widehat{SE}(\\hat\\beta_{k}|X)}}_{\\sim t_{(n-K)}} +\n\\underbrace{\\frac{\\beta_k - \\tilde\\beta_k^{(0)}}{\\widehat{SE}(\\hat\\beta_{k}|X)}}_{❓},\n\\end{align*}\n\\tag{5.6}\\] where under \\(H_0:\\beta_k \\in ]-\\infty,\\beta_k^{(0)}].\\)\nThe second term in Equation 5.6 is challenging:\n\nGenerally, this term is non-zero since the selected null hypothetical value \\(\\tilde{\\beta}_k^{(0)}\\) generally does not equal the true (unknown) \\(\\beta_k.\\)\nIf it is non-zero, we do not know its distribution.\n\nSolution: Under \\(H_0,\\) we know that \\[\n\\beta_k \\leq \\beta_k^{(0)}.\n\\] Thus, when setting \\[\n\\tilde\\beta_k^{(0)} = \\beta_k^{(0)},\n\\] for conducting the \\(t\\)-test, we know, under \\(H_0,\\) that \\[\n\\begin{align*}\nT(\\beta_k^{(0)})\n&=\n\\frac{\\hat\\beta_k - \\beta_k^{(0)}}{\\widehat{SE}(\\hat\\beta_{k}|X)}\\\\[2ex]\n&=\n\\underbrace{\\frac{\\hat\\beta_k - \\beta_k}{\\widehat{SE}(\\hat\\beta_{k}|X)}}_{\\sim t_{(n-K)}} +\n\\underbrace{\\frac{\\beta_k - \\beta_k^{(0)}}{\\widehat{SE}(\\hat\\beta_{k}|X)}}_{{\\color{darkgreen}\\leq 0}}\\\\[2ex]\n&\\leq\n\\frac{\\hat\\beta_k - \\beta_k}{\\widehat{SE}(\\hat\\beta_{k}|X)} \\sim t_{(n-K)},\n\\end{align*}\n\\tag{5.7}\\] where the inequality holds with probability one (i.e. for any possible realization).\n\n\n\n\n\n\n\n\n\n\n\nLet \\(F_{T(\\beta_k^{(0)})}\\) and \\(F_{t_{(n-K)}}\\) denote cumulative distribution functions of \\(T(\\beta_k^{(0)})\\) and of the \\(t\\)-distribution with \\((n-K)\\) degrees of freedom.\nThe inequality in Equation 5.7 implies that:\n\nIf \\(\\beta_k = \\beta_k^{(0)},\\) the distribution of \\(T(\\beta_k^{(0)})\\) equals the \\(t\\)-distribution with \\((n-K)\\) degrees of freedom, i.e. \\[\\begin{align*}\nT(\\beta_k^{(0)}) &\\sim t_{(n-K)}\\\\[2ex]\n\\Leftrightarrow \\; F_{T(\\beta_k^{(0)})} (x) &= F_{t_{(n-K)}}(x) \\quad\\text{for all}\\quad x\\in\\mathbb{R}\n\\end{align*}\\]\nIf \\(\\beta_k < \\beta_k^{(0)},\\) the distribution of \\(T(\\beta_k^{(0)})\\) is strictly dominated by the \\(t\\)-distribution with \\((n-K)\\) degrees of freedom, i.e. \\[\nF_{T(\\beta_k^{(0)})} (x) > F_{t_{(n-K)}}(x) \\quad\\text{for all}\\quad x\\in\\mathbb{R}.\n\\]\n\n\n\nTherefore, we can use the \\((1-\\alpha)\\)-quantile of the \\(t\\)-distribution with \\((n-K)\\) degrees of freedom, \\(c_{1-\\alpha},\\) to define the rejection region \\[\n\\mathcal{R} = ]c_{1-\\alpha},\\infty[,\n\\] since this allows us to control the size simultaneously for all (infinitely many) null hypotheses \\(H_0:\\beta_k \\leq \\beta_k^{(0)},\\) \\[\n\\text{Size}=\\sup_{\\beta_k \\in ]-\\infty,\\beta_k^{(0)}]}P(T(\\beta_k^{(0)}) \\in\\mathcal{R}) = \\alpha.\n\\]\n\n\n\n\n\n\n\n\n\n\nThe inequality in Equation 5.7 implies that \\[\nP(T(\\beta_k^{(0)}) \\in\\mathcal{R}) = \\alpha\n\\quad\\text{if}\\quad\n\\beta_k = \\beta_k^{(0)}\n\\] and that \\[\nP(T(\\beta_k^{(0)}) \\in\\mathcal{R}) < \\alpha\n\\quad\\text{if}\\quad\n\\beta_k < \\beta_k^{(0)}.\n\\]\n\n\nThus, for testing \\[\\begin{align*}\n&H_0: \\beta_k \\leq \\beta_k^{(0)}\\\\\n\\text{versus}\\quad\n& H_1: \\beta_k >    \\beta_k^{(0)}\\\\\n\\end{align*}\\] the rejection region is \\[\n\\mathcal{R}=\\;]c_{1-\\alpha}, \\infty[\n\\]\n\nWe can reject \\(H_0\\) if \\[\\begin{align*}\nT_{obs}\n& \\in \\mathcal{R} = \\;]c_{1-\\alpha}, \\infty[\n\\end{align*}\\]\nWe cannot reject \\(H_0\\) if \\[\\begin{align*}\nT_{obs}\n&\\not \\in \\mathcal{R} = \\;]c_{1-\\alpha}, \\infty[\n\\end{align*}\\]\n\nFigure 5.2 shows an example of the rejection region for the case of a significance level \\(\\alpha=0.05\\) and \\(n-K=12\\) degrees of freedom.\n\n\n\n\n\nFigure 5.2: \\(t\\)-distribution with \\(n-K=12\\) degrees of freedom and critical value \\(c_{1-\\alpha}=1.78\\) for the significance level \\(\\alpha=0.05\\).\n\n\n\n\nTo find the \\(c_{1-\\alpha}\\) critical value we can use R as following:\n\nalpha <- 0.05 # chosen significance level \ndf    <- 12   # degrees of freedom \n\n## One-sided critical value (1-alpha) quantile:\nc_oneSided <- qt(p = 1-alpha, df = df)\nc_oneSided\n\n[1] 1.782288\n\n\nBy equivalent arguments, we can also do a one-sided \\(t\\)-test for the “other side.”\nFor testing \\[\\begin{align*}\n&H_0:  \\beta_k \\geq \\beta_k^{(0)}\\\\\n\\text{versus}\\quad\n&H_1:  \\beta_k <  \\beta_k^{(0)}\\\\\n\\end{align*}\\] the rejection region is \\[\n\\mathcal{R}=\\;]-\\infty,c_{\\alpha}[\n\\]\n\nWe can reject \\(H_0\\) if \\[\\begin{align*}\nT_{obs}\n& \\in \\mathcal{R} = \\;]-\\infty,c_{\\alpha}[\n\\end{align*}\\]\nWe cannot reject \\(H_0\\) if \\[\\begin{align*}\nT_{obs}\n&\\not \\in \\mathcal{R} = \\;]-\\infty,c_{\\alpha}[\n\\end{align*}\\]\n\nFigure 5.3 shows an example of the rejection region for the case of a significance level \\(\\alpha=0.05\\) and \\(n-K=12\\) degrees of freedom.\n\n\n\n\n\nFigure 5.3: \\(t\\)-distribution with \\(n-K=12\\) degrees of freedom and critical value \\(c_{\\alpha}=-1.78\\) for the significance level \\(\\alpha=0.05\\).\n\n\n\n\nTo find the \\(c_{\\alpha}\\) critical value we can use R as following:\n\nalpha <- 0.05 # chosen significance level \ndf    <- 12   # degrees of freedom \n\n## One-sided critical value (alpha) quantile:\nc_oneSided <- qt(p = alpha, df = df)\nc_oneSided\n\n[1] -1.782288\n\n\n\n\n\n5.3.4 Power\n\n\n\nSince we want to detect violations of the null-hypothesis, we want that our test has a large power \\[\\begin{align*}\n\\text{Power}\n& = 1 - P(\\text{type-II-error})\\\\[2ex]\n& = 1 - P(\\text{Not reject $H_0$ given $H_1$ is true})\\\\[2ex]\n& = P(\\text{Reject $H_0$ given $H_1$ is true})\\\\[2ex]\n& = P(\\text{Detect a true violation of the null})\n\\end{align*}\\]\nUnfortunately, computing the power of a statistical test is usually impossible, since this requires knowing the distribution of the test statistic under the alternative hypothesis \\(H_1.\\) The distribution of a test statistic under \\(H_1\\) can only be derived under quite restrictive setups.\nIn the following, we consider such a restrictive setup for the \\(t\\)-test statistic.\nLet’s consider the one-sided hypothesis \\[\\begin{align*}\n&H_0: \\beta_k\\leq \\beta_{k}^{(0)}\\\\\n&H_1: \\beta_k > \\beta_{k}^{(0)}\n\\end{align*}\\]\n\nlet \\(X\\) be deterministic such that \\(T|X \\overset{d}{=}T\\) and\nlet the true standard error of \\(\\hat\\beta_k\\) be\n\\[\n\\operatorname{SE}(\\hat\\beta_k|X)=\\frac{1}{\\sqrt{n}}4.5.\n\\]\n\n\n\n\n\n\n\nStandard error of \\(\\hat\\beta_k\\) is proportional to \\(1/\\sqrt{n}\\)\n\n\n\nOf course, usually we do not know the standard error of the estimator, but have to estimate it. But it is true that the standard error of the OLS estimator \\(\\hat{\\beta}_k\\) is proportional to \\(1/\\sqrt{n},\\) \\[\n\\operatorname{SE}(\\hat\\beta_k|X) = \\frac{1}{\\sqrt{n}}\\cdot \\texttt{constant},\n\\] where the \\(\\texttt{constant}\\) may depend on \\(X,\\) but not on \\(n.\\)\n\n\nUnder this setup and under Assumptions 1-4^\\(\\ast,\\) the \\(t\\)-test statistic is normally distributed.\nIf \\(H_0\\) is true with \\(\\beta_k=\\beta_k^{(0)},\\) then \\[\\begin{align*}\nT\n&=\\frac{\\hat\\beta_k-\\beta_k^{(0)}}{\\frac{1}{\\sqrt{n}}4.5}\\\\[2ex]\n&=\\frac{\\sqrt{n}(\\hat\\beta_k-\\beta_k)}{4.5} \\sim \\mathcal{N}(0,1).\n\\end{align*}\\]\nNote: It suffices to look at this specific null hypothesis \\(\\beta_k=\\beta_k^{(0)},\\) since the distribution of \\(T\\) is dominated by \\(\\mathcal{N}(0,1)\\) for all other null hypotheses \\(\\beta_k<\\beta_k^{(0)};\\) see our discussions above.\nIf \\(H_1: \\beta_k-\\beta_k^{(0)}>0\\) is true, then \\[\\begin{align*}\nT&=\\frac{\\hat\\beta_k-\\beta_k^{(0)}}{\\frac{1}{\\sqrt{n}}4.5}\n=\\frac{\\hat\\beta_k\\overbrace{-\\beta_k+\\beta_k}^{=0}-\\beta_k^{(0)}}{\\frac{1}{\\sqrt{n}}4.5}\\\\[2ex]\n&=\\underbrace{\\frac{\\sqrt{n}(\\hat\\beta_k-\\beta_k)}{4.5}}_{\\sim \\mathcal{N}(0,1)}+\\underbrace{\\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}}_{=\\text{mean-shift}}\\\\[2ex]\n&\\sim \\mathcal{N}\\left(\\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5},1\\right)\n\\end{align*}\\]\nPower: \\[\\begin{align*}\n\\text{Power}\n& = P(\\text{Reject $H_0$ given $H_1$ is true})\\\\[2ex]\n%& = P(T \\in \\left]z_{1-\\alpha},\\infty\\right[ \\;\\;|\\; H_1\\text{ is true})\\\\[2ex]\n& = P(T > z_{1-\\alpha}\\;|\\; H_1\\text{ is true}),\n\\end{align*}\\] where \\(z_{1-\\alpha}\\) denotes the \\((1-\\alpha)\\) quantile of the standard normal distribution \\(\\mathcal{N}(0,1),\\) and where \\[\nT\\sim \\mathcal{N}\\left(\\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5},1\\right).\n\\]\nThis allows us to compute the power as following: \\[\\begin{align*}\n\\text{Power}\n& = P(T > z_{1-\\alpha}\\;|\\; H_1\\text{ is true}),\n\\\\[2ex]\n& = P\\Bigg(\\overbrace{T - \\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}}^{=Z\\sim\\mathcal{N}(0,1)} > z_{1-\\alpha} - \\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}\\Bigg)\\\\[2ex]\n& = P\\left(Z > z_{1-\\alpha} - \\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}\\right)\\\\[2ex]%,\\quad\\text{where}\\quad Z\\sim\\mathcal{N}(0,1)\\\\[2ex]\n&=1-P\\left(Z \\leq z_{1-\\alpha} - \\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}\\right)\\\\[2ex]\n&=1-\\Phi\\left(z_{1-\\alpha} - \\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}\\right),\n\\end{align*}\\] where \\(\\Phi\\) denotes the cumulative distribution function of the standard normal distribution \\(\\mathcal{N}(0,1).\\)\nFigure 5.4 illustrates the probability of a type-II-error and the power for the case where\n\n\\(\\alpha = 0.05\\)\n\\(n=9\\)\n\\(\\beta_k - \\beta_k^{(0)}=3\\)\n\nsuch that \\[\\begin{align*}\n\\text{Power}\n&=1-\\Phi\\Bigg(z_{1-\\alpha} - \\overbrace{\\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}}^{=\\frac{3\\cdot 3}{4.5} = 2}\\Bigg)\\\\[2ex]\n&=1-\\Phi\\left(1.64  - 2 \\right)\\\\[2ex]\n&=1-0.359=0.641\n\\end{align*}\\] That is, we expect to detect the violation of the null hypothesis \\((\\beta_k - \\beta_k^{(0)}=3)\\) in \\(64\\%\\) of resamplings from the random sample (data generating process).\n\n\n\n\n\nFigure 5.4: Probability of a type-II-error and the power for a one-sided \\(t\\)-test at significance level \\(\\alpha = 0.05,\\) sample size \\(n=9,\\) and violation of the null hypothesis \\(\\beta_k - \\beta_k^{(0)}=3.\\)\n\n\n\n\n\n\n\n\n\n\nPower is a function of \\(\\alpha,\\) \\((\\beta_k-\\beta_k^{(0)}),\\) and \\(n\\)\n\n\n\n\n\nIndeed, for all reasonable test statistics we have that: \\[\\begin{align*}\n%&\\text{One-Sided $H_1:\\beta_k>\\beta_k^{(0)}$:}\\\\\n%&\\text{Power}\\left(\\alpha, (\\beta_k-\\beta_k^{(0)}), \\sqrt{n}\\right)\n%\\to 1 \\quad \\text{as}\\quad (\\beta_k-\\beta_k^{(0)})\\to\\infty\\\\[2ex]\n%&\\text{One-Sided $H_1:\\beta_k<\\beta_k^{(0)}$:}\\\\\n%&\\text{Power}\\left(\\alpha, (\\beta_k-\\beta_k^{(0)}), \\sqrt{n}\\right)\n%\\to 1 \\quad \\text{as}\\quad (\\beta_k-\\beta_k^{(0)})\\to-\\infty\\\\[2ex]\n&\\text{Two-Sided $H_1:\\beta_k\\neq\\beta_k^{(0)}$:}\\\\\n&\\text{Power}\\left(\\alpha, (\\beta_k-\\beta_k^{(0)}), \\sqrt{n}\\right)\n\\to 1 \\quad \\text{as}\\quad |\\beta_k-\\beta_k^{(0)}|\\to\\infty\\\\[2ex]\n&\\text{Power}\\left(\\alpha, (\\beta_k-\\beta_k^{(0)}), \\sqrt{n}\\right)\n\\to 1 \\quad \\text{as}\\quad n\\to\\infty\\\\[2ex]\n&\\text{Power}\\left(\\alpha, (\\beta_k-\\beta_k^{(0)}), \\sqrt{n}\\right)\n\\to 0 \\quad \\text{as}\\quad \\alpha\\to 0\n\\end{align*}\\] \n\n\n\n\n5.3.5 \\(p\\)-Value\n\n\\(F\\)-Test\nThe \\(F\\)-test allows us to test \\[\\begin{align*}\n&H_0: R\\beta = r\\\\[2ex]\n\\text{versus}\\quad\n&H_1: R\\beta\\neq r,\n\\end{align*}\\] where \\(\\beta\\) denotes the true (unknown) parameter vector and \\(r\\) the null-hypothetical value specified by the statistician (e.g. \\(r=0\\)).\nWe know that \\[\nF\\overset{H_0}{\\sim}F_{q,n-K}.\n\\]\nThe \\(p\\)-value of the \\(F\\)-test is the probability of seeing realizations of \\(F\\) that are equal to or larger than the observed value \\(F_{\\text{obs}}\\) given that the null hypothesis is true \\[\np_{\\text{obs}}=P(F\\geq F_{\\text{obs}}\\;|\\;H_0 \\text{ is true})\n\\]\n\nWe reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} < \\alpha\n\\]\nWe cannot reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} \\geq \\alpha\n\\]\n\n\n\nThe One-Sided \\(t\\)-Test\nPossible one-sided hypotheses:\n\\[\\begin{align*}\n&H_0: \\beta_k \\leq \\beta_k^{(0)}\\\\\n\\text{versus}\\quad & H_1:\\beta_k >    \\beta_k^{(0)}\\\\\n\\end{align*}\\]\nor\n\\[\\begin{align*}\n&H_0: \\beta_k \\geq \\beta_k^{(0)}\\\\\n\\text{versus}\\quad & H_1: \\beta_k <  \\beta_k^{(0)}\\\\\n\\end{align*}\\]\n\nWe know that \\[\nT\\overset{H_0}{\\sim}t_{n-K}.\n\\]\nThe \\(p\\)-value of the one-sided \\(t\\)-test for testing \\[\\begin{align*}\n&H_0: \\beta_k \\leq \\beta_k^{(0)}\\\\\n\\text{versus}\\quad & H_1: \\beta_k >    \\beta_k^{(0)}\\\\\n\\end{align*}\\] is the probability of seeing realizations of \\(T\\) that are equal to or larger than the observed value \\(T_{\\text{obs}}\\) given that the null hypothesis is true \\[\np_{\\text{obs}}=P(T\\geq T_{\\text{obs}}\\;|\\;H_0 \\text{ is true}).\n\\] * We reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} < \\alpha\n\\]\n\nWe cannot reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} \\geq \\alpha\n\\]\n\nThe \\(p\\)-value of the one-sided \\(t\\)-test for testing \\[\\begin{align*}\n&H_0: \\beta_k \\geq \\beta_k^{(0)}\\\\\n\\text{versus}\\quad & H_1: \\beta_k <    \\beta_k^{(0)}\\\\\n\\end{align*}\\] is the probability of seeing realizations of \\(T\\) that are equal to or smaller than the observed value \\(T_{\\text{obs}}\\) given that the null hypothesis is true \\[\np_{\\text{obs}}=P(T\\leq T_{\\text{obs}}\\;|\\;H_0 \\text{ is true}).\n\\]\n\nWe reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} < \\alpha\n\\]\nWe cannot reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} \\geq \\alpha\n\\]\n\n\n\nThe Two-Sided \\(t\\)-Test\nThe two-sided \\(t\\)-test allows us to test \\[\\begin{align*}\n& H_0: \\beta_k=\\beta_k^{(0)}\\\\\n\\text{versus}\\quad\n& H_1: \\beta_k\\ne \\beta_k^{(0)}\n\\end{align*}\\] where \\(\\beta_k\\) denotes the true (unknown) parameter value and \\(\\beta_k^{(0)}\\) the null hypothetical value specified by the statisticaian (e.g. \\(\\beta_k^{(0)}=0\\)).\nWe know that \\[\nT\\overset{H_0}{\\sim}t_{n-K}.\n\\]\nThe \\(p\\)-value of the two-sided \\(t\\)-test is the probability of seeing realizations of \\(T\\) that are equal to or more extreme than the observed value \\(T_{\\text{obs}}\\) given that the null hypothesis is true \\[\\begin{align*}\np_{\\text{obs}}\n&=P(|T|\\geq |T_{\\text{obs}}|\\;|\\;H_0 \\text{ is true})\\\\[2ex]\n&=2\\cdot\\min\\{P(T\\leq T_{\\text{obs}}\\;|\\;H_0 \\text{ is true}),\n            P(T\\geq T_{\\text{obs}}\\;|\\;H_0 \\text{ is true})\\}\n\\end{align*}\\]\n\nWe reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} < \\alpha\n\\]\nWe cannot reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} \\geq \\alpha\n\\]\n\n\n\n\n\n\n\nMarginal Significance Value\n\n\n\nThe \\(p\\)-value equals the significance level \\(\\alpha\\) for which we just fail to reject the null. Therefore, the \\(p\\)-value is sometimes also called “marginal significance value”."
  },
  {
    "objectID": "05-Small-Sample-Inference.html#sec-CIsmallsample",
    "href": "05-Small-Sample-Inference.html#sec-CIsmallsample",
    "title": "5  Small Sample Inference",
    "section": "5.4 Confidence Intervals",
    "text": "5.4 Confidence Intervals\nWe define a two-sided \\((1-\\alpha)\\cdot 100\\%\\) percent confidence interval for the deterministic (unknown) true \\(\\beta_k\\) as the random interval \\(\\operatorname{CI}_{k,1-\\alpha}\\) for which \\[\nP\\Big(\\beta_k\\in\\operatorname{CI}_{k,1-\\alpha}\\Big)\\geq 1-\\alpha.\n\\] Derivation of the random interval \\(\\operatorname{CI}_{k,1-\\alpha}\\):\nObserve that (under Ass 1-4\\(^\\ast\\)) \\[\n\\frac{\\hat\\beta_k-\\beta_k}{\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)}\\sim t_{(n-K)}\n\\tag{5.8}\\] Therefore, \\[\\begin{align*}\nP\\left(-c_{1-\\alpha/2}\\leq\\frac{\\hat\\beta_k-\\beta_k}{\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)}\\leq c_{1-\\alpha/2}\\right)=1-\\alpha,\n\\end{align*}\\] where \\(c_{1-\\alpha/2}\\) denotes the \\((1-\\alpha/2)\\) quantile of the \\(t\\)-distribution with \\((n-K)\\) degrees of freedom. Next, we can do the following equivalent transformations \\[\\begin{align*}\nP\\left(-c_{1-\\alpha/2}\\leq\\frac{\\hat\\beta_k-\\beta_k}{\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)}\\leq c_{1-\\alpha/2}\\right)&=1-\\alpha\\\\\n\\Leftrightarrow P\\left(\\hat\\beta_k-c_{1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)\\leq \\beta_k\\leq\\hat\\beta_k +c_{1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)\\right)&=1-\\alpha\\\\\n\\Leftrightarrow P\\left(\\beta_k\\in\\underbrace{\\left[\\hat\\beta_k-c_{1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X),\\;\\hat\\beta_k +c_{1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)\\right]}_{=:\\operatorname{CI}_{k,1-\\alpha}}\\right)&=1-\\alpha\n\\end{align*}\\] That is, the random interval \\[\n\\operatorname{CI}_{k,1-\\alpha}=\\left[\\hat\\beta_k-c_{1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X),\\;\\hat\\beta_k + c_{1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)\\right]\n\\] is our \\((1-\\alpha)\\cdot 100\\%\\) confidence interval for \\(\\beta_k\\).\nSince the confidence interval is based on the exact distribution (under Assumptions 1-4\\(^\\ast\\)) in Equation 5.8, the confidence interval has an exact coverage probability \\[\\begin{align*}\nP\\left(\\beta_k\\in\\operatorname{CI}_{k,1-\\alpha}\\right)&=1-\\alpha\n\\end{align*}\\] provided the Assumptions 1-4\\(^\\ast\\) are true.\n\n\n\n\n\n\nInterpretation of Confidence Intervals\n\n\n\nThe random interval \\(\\operatorname{CI}_{k,1-\\alpha}\\) for \\(\\beta_k\\) contains the true parameter value \\(\\beta_k\\) with probability \\(1-\\alpha;\\) i.e. we expect that \\(\\operatorname{CI}_{k,1-\\alpha}\\) covers \\(\\beta_k\\) in \\((1-\\alpha)\\cdot 100\\%\\) of resamplings from the random sample.\nIt’s best to take a look at dynamic visualizations like this one:\n\nhttps://rpsychologist.com/d3/ci/\n\nUnfortunately, this “frequentist” interpretation is not a statement about a single given \\(\\operatorname{CI}_{k,1-\\alpha}\\) realization computed for a given data set. A given, realized \\(\\operatorname{CI}_{k,1-\\alpha}\\) will either contain the true parameter \\(\\beta_k\\) or not, and usually we do not know the answer. So, confidence intervals are quite hard to interpret. However, they are very well suited as a tool to visualize estimation uncertainties in different parameter estimators, for instance, across \\(\\hat\\beta_k\\), \\(k=1,\\dots,K\\)."
  },
  {
    "objectID": "05-Small-Sample-Inference.html#sec-PSSI",
    "href": "05-Small-Sample-Inference.html#sec-PSSI",
    "title": "5  Small Sample Inference",
    "section": "5.5 Monte Carlo Simulations",
    "text": "5.5 Monte Carlo Simulations\nLet’s check the above exact inference results using Monte Carlo simulations. The check, of course, applies only to the considered special case and generally does not generalize to other data generating processes.\nFirst, we program a function myDataGenerator() which allows us to generate data from the following model, i.e., from the following fully specified data generating process: \\[\\begin{align*}\nY_i &=\\beta_1+\\beta_2X_{i2}+\\beta_3X_{i3}+\\varepsilon_i,\\qquad i=1,\\dots,n\\\\\n\\beta &=(\\beta_1,\\beta_2,\\beta_3)'=(2,3,4)'\\\\\nX_{i2}&\\sim U[2,10]\\\\\nX_{i3}&\\sim U[12,22]\\\\\n\\varepsilon_i|X&\\sim\\mathcal{N}(0,3^2),\n\\end{align*}\\] where \\((Y_i,X_i)\\) is i.i.d. across \\(i=1,\\dots,n\\).\nLet us consider a small sample size of \\(n=7\\).\nThe below function myDataGenerator() allows to sample new realizations of the random sample \\[\n((X_1,Y_1),\\dots,(X_n,Y_n))\n\\] You can provide your own values for the sample size \\(n\\) and for the parameter vector \\(\\beta=(\\beta_1,\\beta_2,\\beta_3)'\\).\n\n## Function to generate artificial data\n## If the user provides 'X_cond' data, \n## the sampling of new Y variables is \n## conditionally on the given X_cond variables.\n## If X_cond = NULL, sampling is done unconditionally. \n\nmyDataGenerator <- function(n, beta){\n\n## sampling predictors X:\nX   <- cbind(rep(1, n), \n            runif(n, 2, 10), \n            runif(n,12, 22))\n## sampling error terms: \neps  <- rnorm(n, sd = 3)\n## generate realizations of Y:\nY    <- X %*% beta + eps\n## safe X and Y as a data frame:\ndata <- data.frame(\"Y\"   = Y, \n                   \"X_1\" = X[,1], \n                   \"X_2\" = X[,2], \n                   \"X_3\" = X[,3])\n## return the data frame\nreturn(data)\n}\n\n## Small sample size\nn             <- 7        \n\n## Define the true beta vector\nbeta_true     <- c(2,3,4)\n\n## Generate Y and X data \ntest_data     <- myDataGenerator(n = n, beta=beta_true)\n\n## Look at the first six lines of the data frame\nround(head(test_data,     3), 2) \n\n      Y X_1  X_2   X_3\n1 79.16   1 4.48 15.32\n2 87.47   1 2.35 19.15\n3 81.84   1 6.17 16.36\n\n\n\n5.5.1 Check: Testing Multiple Parameters\nIn the following, we do inference about multiple parameters. We test \\[\\begin{align*}\nH_0:\\;&\\beta_2=3\\quad\\text{and}\\quad\\beta_3=4\\\\\n\\text{versus}\\quad H_1:\\;&\\beta_2\\neq 3\\quad\\text{and/or}\\quad\\beta_3\\neq 4.\n\\end{align*}\\] Or equivalently \\[\\begin{align*}\nH_0:\\;&R\\beta  = r^{(0)} \\\\\nH_1:\\;&R\\beta  \\neq r^{(0)},\n\\end{align*}\\] where \\[\nR=\\left(\n\\begin{matrix}\n0&1&0\\\\\n0&0&1\\\\\n\\end{matrix}\\right)\\quad\\text{ and }\\quad\nr^{(0)}=\\left(\\begin{matrix}3\\\\4\\\\\\end{matrix}\\right).\n\\] The following R code can be used to test this hypothesis:\n\n## Library containing the function 'linearHyothesis()' \n## for testing multiple parameters \nsuppressMessages(library(\"car\")) \n## See ?linearHypothesis\n\n## Generate one Monte Carlo sample (under H0)\ndata   <- myDataGenerator(n = n, beta = beta_true)\n\n## Estimate the linear regression model parameters\nlm_obj <- lm(Y ~ X_2 + X_3, data = data)\n\n## Option 1:\ntest_result <- car::linearHypothesis(\n      model = lm_obj, \n      hypothesis.matrix = c(\"X_2=3\", \"X_3=4\"))   \ntest_result        \n\nLinear hypothesis test\n\nHypothesis:\nX_2 = 3\nX_3 = 4\n\nModel 1: restricted model\nModel 2: Y ~ X_2 + X_3\n\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1      6 39.234                           \n2      4 33.017  2    6.2172 0.3766 0.7082\n\n\nThe \\(p\\)-value\n\n\\(p_{\\text{obs}}=\\) 0.7082 \\(\\;>\\alpha=0.05\\)\n\nis larger than the chosen significance level \\(\\alpha=0.05.\\) Thus we cannot reject the null hypothesis \\[\nH_0:\\;\\beta_2=3\\quad\\text{and}\\quad \\beta_3=4.\n\\]\nThe following codes gives an alternative, equivalent way to compute the test result:\n\n## Option 2:\nR <- rbind(c(0,1,0),\n           c(0,0,1))\ncar::linearHypothesis(model = lm_obj, \n                      hypothesis.matrix = R, \n                      rhs = c(3,4))\n\nWe simulated data under \\(H_0\\) and thus it is not surprising that we cannot reject \\(H_0.\\)\nHowever, in repeated samples we should nevertheless observe \\(\\alpha\\cdot 100\\%\\) type I errors (false rejections of \\(H_0\\)) under \\(H_0.\\) Let’s check the type-I-error rate using the following Monte Carlo simulation:\n\n## Let's generate 5000 F-test decisions and check \n## whether the empirical rate of type I errors is \n## close to the theoretical significance level. \nB               <- 5000 # MC replications\nF_test_pvalues  <- rep(NA, times=B)\n##\nfor(r in 1:B){\n  ## generate new data (under H0)\n  MC_data <- myDataGenerator(n = n, beta = beta_true)\n  ## estimate \n  lm_obj  <- lm(Y ~ X_2 + X_3, data = MC_data)\n  ## compute test and p-value\n  p       <- linearHypothesis(lm_obj, c(\"X_2=3\", \"X_3=4\"))$`Pr(>F)`[2]\n  ## save the p-value\n  F_test_pvalues[r] <- p\n}\n\nUsing the collection of \\(p\\)-value realizations (under \\(H_0\\)) we can check whether the size equals the nominal size (significance level) \\(\\alpha.\\)\nFor \\(\\alpha = 0.05:\\)\n\nalpha        <-  0.05          # signif level\nrejections   <- F_test_pvalues[F_test_pvalues < alpha]\nround(length(rejections)/B, 4) # actual type-I-error rate \n\n[1] 0.05\n\n\nFor \\(\\alpha = 0.01:\\)\n\nalpha        <-  0.01  # signif level\nrejections   <- F_test_pvalues[F_test_pvalues < alpha]\nround(length(rejections)/B, 4) # actual type-I-error rate \n\n[1] 0.0098\n\n\nObservations:\n\nWe correctly control for the type-I-error rate since the empirical type-I-error rate is not larger than the chosen significance level \\(\\alpha.\\)\nThe \\(F\\) test is not conservative since the empirical type-I-error rates essentially matches the chosen significance levels \\(\\alpha.\\)  In fact, if we would increase the number of Monte Carlo repetitions, the empirical type-I-error rate would converge to the nominal type-I-error rate \\(\\alpha\\) due to the law of large numbers.\nLast but not least: All this works unconditionally on \\(X\\) since the distribution of the \\(F\\) statistic (Equation 5.3) does not depend on \\(X\\).\n\nNext, we check how well the \\(F\\) test detects certain violations of the null hypothesis. We do this by using the same data generating process, but by testing the following incorrect null hypothesis: \\[\\begin{align*}\nH_0:\\;&{\\color{red}\\beta_2=4}\\quad\\text{and}\\quad\\beta_3=4\\\\\nH_1:\\;&\\beta_2\\neq 4\\quad\\text{and/or}\\quad\\beta_3\\neq 4\n\\end{align*}\\]\n\nB               <- 5000 # MC replications\nF_test_pvalues  <- rep(NA, times=B)\n##\nfor(r in 1:B){\n  ## generate new data \n  MC_data <- myDataGenerator(n    = n, beta = beta_true)\n  ## estimate \n  lm_obj  <- lm(Y ~ X_2 + X_3, data = MC_data)\n  ## compute test and p-value (for a false H0)\n  p       <- linearHypothesis(lm_obj, c(\"X_2=4\", \"X_3=4\"))$`Pr(>F)`[2]\n  ## save the p-value\n  F_test_pvalues[r] <- p\n}\n\n## Checking the power of the F test \n\nalpha       <-  0.05  # signif_level\nrejections  <- F_test_pvalues[F_test_pvalues < alpha]\nlength(rejections)/B  # power \n\n[1] 0.1966\n\n\nWe can now correctly reject the false null hypothesis in approximately 19.66 % of all Monte Carlo replications.\nCaution: This means that we are not able to detect the violation of the null hypothesis in 80.34 % of cases. Therefore, we can never use an insignificant test result (\\(p\\)-value \\(\\geq\\alpha\\)) as a confirmation of the null hypothesis. Obviously, there are type-II-error events (not rejecting a false \\(H_0\\)), but since we typically do not know the distribution of the test statistic under the alternative hypothesis, we cannot control the type-II-error rate. We can only control the type-I-error rate by using a small significance level \\(\\alpha\\).\nMoreover, note that the \\(F\\) test is not informative about which part of the null hypothesis (\\(\\beta_2=4\\) and/or \\(\\beta_3=4\\)) is violated. We only get the information that at least one of the multiple parameter hypotheses is violated. Test statistics with this property are called omnibus tests.\n\n\n5.5.2 Check: Dualty of Confidence Intervals and Hypothesis Tests\nConfidence intervals can be computed using R as following:\n\n## Significance level\nalpha        <- 0.05\n## Confidence level\nconf_level   <- 1 - alpha\n\n## 95% CI for beta_2\nconfint(lm_obj, parm = \"X_2\", level = conf_level)\n\n       2.5 %   97.5 %\nX_2 2.931918 4.656243\n\n## 95% CI for beta_3 \nconfint(lm_obj, parm = \"X_3\", level = conf_level)\n\n       2.5 %   97.5 %\nX_3 4.055278 6.408284\n\n\nWe can use these two-sided confidence intervals to conduct hypotheses tests. This property of confidence intervals is called the duality of confidence intervals and hypothesis tests.\nFor instance, when testing the null hypothesis \\[\\begin{align*}\nH_0:&\\;\\beta_2=3\\\\\n\\text{versus}\\quad H_1: &\\;\\beta_2\\neq 3\n\\end{align*}\\] we can either use a \\(t\\)-test or equivalently check whether the confidence interval \\(\\operatorname{CI}_{2,1-\\alpha}\\) for \\(\\beta_2\\) contains the hypothetical value \\(4\\) or not.\n\nIn case of \\(3 \\in\\operatorname{CI}_{2,1-\\alpha}\\), we cannot reject the null hypothesis \\(H_0\\): \\(\\beta_2=3.\\)\nIn case of \\(3\\not\\in\\operatorname{CI}_{2,1-\\alpha}\\), we can reject the null hypothesis \\(H_0\\): \\(\\beta_2=3.\\)\n\nIf the Assumptions 1-4\\(^\\ast\\) hold true, then \\(\\operatorname{CI}_{2,1-\\alpha}\\) is an exact confidence interval. That is, under the null hypothesis, it falsely rejects the null hypothesis in only \\(\\alpha\\cdot 100\\%\\) of resamplings. Let’s check this in the following Monte Carlo simulation:\n\n## Significance level\nalpha        <- 0.05\n\n## beta_2 safed separately\nbeta_true_2  <- beta_true[2]\n\n## Container to save all CI realizations\nconfint_m  <- matrix(NA, nrow=2, ncol=B)\n##\nfor(r in 1:B){\n  ## generate new data \n  MC_data <- myDataGenerator(n = n, beta = beta_true)\n  ## estimate\n  lm_obj  <- lm(Y ~ X_2 + X_3, data = MC_data)\n  ## compute confidence interval \n  CI <- confint(lm_obj, parm=\"X_2\", level = 1 - alpha)\n  ## save confidence interval\n  confint_m[,r] <- CI\n}\n## check whether true parameter is inside the CI\ninside_CI  <- confint_m[1,] <= beta_true_2 & \n                beta_true_2 <= confint_m[2,]\n\n## CI-lower, CI-upper, beta_true_2 inside?\nhead(cbind(t(confint_m), inside_CI))\n\n                         inside_CI\n[1,]  1.3447260 4.901192         1\n[2,] -0.4924544 4.271078         1\n[3,] -0.5045675 4.092939         1\n[4,]  0.1359987 6.560029         1\n[5,]  2.7438136 3.612304         1\n[6,]  0.2077626 3.963992         1\n\n\nThe following code computes the relative frequency of confidence intervals not containing the true parameter value \\((\\beta_2=3)\\):\n\nround(length(inside_CI[inside_CI == FALSE])/B, 4)\n\n[1] 0.0482\n\n\nThat’s good! The relative frequency is basically equal to the chosen \\(\\alpha=0.05\\) value.\nNext, we visualize a subsample of 100 confidence intervals from the total sample of 5000 generated confidence interval realizations:\n\nnCIs <- 100\nplot(x=0, y=0,type=\"n\", xlim=c(0,nCIs), ylim=range(confint_m[,1:nCIs]),\n     ylab=\"\", xlab=\"Resamplings\", main=\"Confidence Intervals\")\nfor(r in 1:nCIs){\n  if(inside_CI[r]==TRUE){\n      lines(x=c(r,r), y=c(confint_m[1,r], confint_m[2,r]), lwd=2, col=gray(.5,.5))\n  }else{\n      lines(x=c(r,r), y=c(confint_m[1,r], confint_m[2,r]), lwd=2, col=\"darkred\")\n    }\n}\naxis(4, at=beta_true_2, labels = expression(beta[2]))\nabline(h=beta_true_2)\n\n\n\n\n\n\n\n\nAs expected, only about \\(\\alpha\\cdot 100\\%=5\\%\\) of all confidence intervals do not contain the true parameter value \\(\\beta_2=3\\), but about \\((1-\\alpha)\\cdot 100\\%=95\\%\\) of all confidence intervals contain the true parameter value \\(\\beta_2=3\\)."
  },
  {
    "objectID": "05-Small-Sample-Inference.html#sec-RDSSInf",
    "href": "05-Small-Sample-Inference.html#sec-RDSSInf",
    "title": "5  Small Sample Inference",
    "section": "5.6 Real Data Example",
    "text": "5.6 Real Data Example\n\n## The AER package contains a lot of datasets \nsuppressPackageStartupMessages(library(AER))\n\n## Attach the DoctorVisits data to make it usable\ndata(\"DoctorVisits\")\n\nlm_obj <- lm(visits ~ gender + age + income, data = DoctorVisits)\n\nThe above R codes estimate the following regression model \\[\nY_i = \\beta_1 + \\beta_{gender} X_{gender,i}\n              + \\beta_{age} X_{age,i}\n              + \\beta_{income} X_{income,i} + \\varepsilon_i,\n\\] where \\(i=1,\\dots,n\\) and\n\n\\(X_{gender,i}=1\\) if the \\(i\\)th subject is a woman and \\(X_{gender,i}=0\\) if the \\(i\\)th subject is a man\n\\(X_{age,i}\\) is the age of subject \\(i\\) measured in years divided by \\(100\\)\n\\(X_{income,i}\\) is the annual income of subject \\(i\\) in tens of thousands of dollars\n\nThe following R codes produces the classic regression output table (simular tables are produced by all statistical/econometric software packages):\n\nlm_obj_summary <- summary(lm_obj)\nlm_obj_summary\n\n\nCall:\nlm(formula = visits ~ gender + age + income, data = DoctorVisits)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5009 -0.3435 -0.2306 -0.1682  8.6174 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.15371    0.03607   4.262 2.07e-05 ***\ngenderfemale  0.06245    0.02345   2.662  0.00778 ** \nage           0.40235    0.05713   7.043 2.13e-12 ***\nincome       -0.08231    0.03167  -2.599  0.00938 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7908 on 5186 degrees of freedom\nMultiple R-squared:  0.01885,   Adjusted R-squared:  0.01829 \nF-statistic: 33.22 on 3 and 5186 DF,  p-value: < 2.2e-16\n\n\nThe above regression output table contains the following information:\n\nEstimate: The column “Estimate” containes the estimates \\[\n\\hat\\beta_{j},\\quad j\\in\\{1,gender, age, income\\}\n\\] You can extract them using coef(lm_obj).\nStd. Error: The column “Std. Error” containes the estimates \\[\n\\widehat{\\operatorname{SE}}(\\hat\\beta_{j}|X),\\quad j\\in\\{1,gender, age, income\\}\n\\]\n\nYou can extract the total \\((K\\times K)=(4\\times 4)\\) variance-covariance matrix estimate \\(\\widehat{Var}(\\hat\\beta|X)\\) using vcov(lm_obj).\nThe diagonal diag(vcov(lm_obj)) contains the variance estimates \\(\\widehat{Var}(\\hat\\beta_j|X)\\), \\(j\\in\\{1,gender, age, income\\}\\).\nThe square root of the diagonal sqrt(diag(vcov(lm_obj))) allows you to compute the estimated standard errors shown in the regression table.\n\nt value: The column “t value” contains the observed \\(t\\) test statistics \\[\nT_{obs,j}=\\frac{\\hat\\beta_{j}-0}{\\widehat{\\operatorname{SE}}(\\hat\\beta_{j}|X)},\\quad j\\in\\{1,gender, age, income\\}\n\\] You can extract the values using lm_obj_summary$coefficients[,3].\nPr(>|t|): The column “Pr(>|t|)” contains the \\(p\\) values \\[\nP_{H_0}(|t|>t_{obs,j}),\\quad j\\in\\{1,gender, age, income\\}\n\\] You can extract the values using lm_obj_summary$coefficients[,4].\nResidual standard error \\(\\sqrt{\\frac{1}{n-K}\\sum_{i=1}^n\\hat\\varepsilon^2_i}=\\) sqrt(sum(resid(lm_obj)^2)/(n-4)) \\(=\\) 0.7908\nMultiple R-squared: \\(R^2=\\) lm_obj_summary$r.squared \\(=\\) 0.01885\nAdjusted R-squared: \\(\\bar{R}^2=\\) lm_obj_summary$adj.r.squared \\(=\\) 0.01829\nF-statistic: This is a standard \\(F\\) test that tests the null hypothesis that all parameters except the intercept are zero; i.e. \\(H_0\\): \\(\\beta_{gender}=\\beta_{age}=\\beta_{income}=0\\) versus \\(H_1\\): At least one parameter is not zero. R’s summary() functions reports an observed \\(F\\) statistic value of \\(33.22\\) which needs to be evaluated for an \\(F\\) distribution with \\(3\\) and \\(5186\\) degrees of freedom leading to a \\(p\\)-value \\(p< 0.00001.\\)  You can replicate this \\(F\\)-test result using the following R code:\n\n\ncar::linearHypothesis(\n      model = lm_obj, \n      hypothesis.matrix = c(\"genderfemale=0\", \"age=0\", \"income=0\"))  \n\nLinear hypothesis test\n\nHypothesis:\ngenderfemale = 0\nage = 0\nincome = 0\n\nModel 1: restricted model\nModel 2: visits ~ gender + age + income\n\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1   5189 3305.5                                  \n2   5186 3243.2  3     62.32 33.218 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nR Package Stargazer\nBeautiful and “publication ready” regression outputs can be produced using the R package stargazer and its function stargazer():\n\n\n\n\n\n## Hint: use type = \"latex\" \n## to produce a latex table\nstargazer(lm_obj, type=\"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nvisits\n\n\n\n\n\n\n\n\ngenderfemale\n\n\n0.062***\n\n\n\n\n\n\n(0.023)\n\n\n\n\n\n\n\n\n\n\nage\n\n\n0.402***\n\n\n\n\n\n\n(0.057)\n\n\n\n\n\n\n\n\n\n\nincome\n\n\n-0.082***\n\n\n\n\n\n\n(0.032)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.154***\n\n\n\n\n\n\n(0.036)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n5,190\n\n\n\n\nR2\n\n\n0.019\n\n\n\n\nAdjusted R2\n\n\n0.018\n\n\n\n\nResidual Std. Error\n\n\n0.791 (df = 5186)\n\n\n\n\nF Statistic\n\n\n33.218*** (df = 3; 5186)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\n\n\n\nCritical Discussion of the Regression above Results\nThe above real data analysis does not fit into the small sample inference framework we introduced in this chapter.\n\nThe dependent variable \\(Y_i\\) visits is a categorial variable taking finitely many discrete values, indeed\n\nunique(DoctorVisits$visits) = 1, 2, 3, 4, 8, 5, 7, 6, 9, 0.\n\nConsequently, the error term \\(\\varepsilon_i\\) cannot be normal distributed.\nThe diagnostic plot (“Residuals versus Fitted”) indicates a possible issue violation of the homoskedasticity assumption. In case of homokedastic variances, the data points \\((\\hat\\varepsilon_i,\\hat{Y}_i)\\), \\(i=1,\\dots,n\\) should roughly show a homogenous scattering across the fitted values \\(\\hat{Y}_i=X\\hat\\beta\\). This seems not to be the case here.\n\n\n## Diagonstic Plot \n## Residuals versus fitted values\nplot(lm_obj, which = 1)\n\n\n\n\nLukily, the data set DoctorVisits actually has a large sample size of \\(n=\\) 5190 and thus there is a way out of this problem: The large sample inference framework introduced in the next chapter."
  },
  {
    "objectID": "05-Small-Sample-Inference.html#exercises",
    "href": "05-Small-Sample-Inference.html#exercises",
    "title": "5  Small Sample Inference",
    "section": "5.7 Exercises",
    "text": "5.7 Exercises\n\nExercises for Chapter 5\nExercises of Chapter 5 with Solutions\nExercises of Chapter 5 with Solutions (annotated version)"
  },
  {
    "objectID": "05-Small-Sample-Inference.html#references",
    "href": "05-Small-Sample-Inference.html#references",
    "title": "5  Small Sample Inference",
    "section": "References",
    "text": "References\n\n\n\n\nAltman, Douglas G, and J Martin Bland. 1995. “Statistics Notes: Absence of Evidence Is Not Evidence of Absence.” British Medical Journal 311 (7003): 485.\n\n\nHayashi, Fumio. 2000. Econometrics. Princeton University Press."
  },
  {
    "objectID": "06-Asymptotics.html",
    "href": "06-Asymptotics.html",
    "title": "6  Large Sample Inference",
    "section": "",
    "text": "The content of this chapter is very much inspired by Chapter 2 of the textbook of Hayashi (2000)."
  },
  {
    "objectID": "06-Asymptotics.html#tools-for-asymptotic-statistics",
    "href": "06-Asymptotics.html#tools-for-asymptotic-statistics",
    "title": "6  Large Sample Inference",
    "section": "6.1 Tools for Asymptotic Statistics",
    "text": "6.1 Tools for Asymptotic Statistics\nBasically every modern econometric method is justified using the toolbox of asymptotic statistics. The following core concepts from asymptotic statistics will allow us to drop the restrictive normality assumption of Chapter 5 and to introduce robust standard errors:\n\nConcepts on stochastic convergence\nContinuous mapping theorem\nSlutsky’s theorem\nLaw of large numbers\nCentral limit theorems\nCramér-Wold device\n\n\n6.1.1 Modes of Convergence\nIn the following we will discuss the four most important convergence concepts for sequences of random variables \\[\n\\{Z_n\\}:=(Z_1,Z_2,\\dots,Z_n),\n\\] where \\[\nZ_i,\\quad i=1,\\dots,n,\n\\] is a scalar (uni-variate) or multivariate random variable.\nNon-random scalars (or vectors or matrices) will be denoted by Greek letters such as \\(\\alpha\\).\n\n\n\n\n\nFour Important Modes of Convergence\nProbably the most often used mode of convergence is convergence in probability. It states that a stochastic sequence \\(\\{Z_n\\}\\) concentrates around its limit \\(\\alpha\\) such that deviations \\(|Z_n-\\epsilon|\\) larger than some small \\(\\epsilon >0\\) occur eventually (as \\(n\\to\\infty\\)) with probability zero.\nIn order to show that some stochastic sequence converges in probability to its limit, one typically uses a (weak) law of large numbers.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 6.1 (Convergence in Probability) A sequence of real valued random scalars \\(\\{Z_n\\}\\) converges in probability to a constant (non-random) \\(\\alpha\\in\\mathbb{R}\\) if, for any (arbitrarily small) \\(\\varepsilon>0\\), \\[\\begin{eqnarray*}\n  \\lim_{n\\to\\infty} P\\left(|Z_n-\\alpha|>\\epsilon\\right)=0.\n\\end{eqnarray*}\\] We write: \\[\n\\operatorname{plim}_{n\\to\\infty}Z_n=\\alpha\n\\] or more shortly \\[\nZ_n\\to_{p}\\alpha,\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nNote: Convergence in probability of a sequence of random vectors or matrices \\(\\{Z_n\\}\\) to a constant vector or matrix \\(\\alpha\\) requires element-wise convergence in probability.\n\n\n\nA stricter mode of convergence is almost sure convergence. Almost sure convergence is (usually) rather hard to derive, since the probability is about an event concerning an infinite sequence. Fortunately, however, there are established strong laws of large numbers that we can use to argue that some stochastic sequence converges almost surely to its limit.\nEvery sequence that converges almost surely, also converges in probability.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 6.2 (Almost Sure Convergence) A sequence of real valued random scalars \\(\\{Z_n\\}\\) converges almost surely to a constant (non-random) \\(\\alpha\\in\\mathbb{R}\\) if \\[\\begin{eqnarray*}\nP\\left(\\lim_{n\\to\\infty}Z_n=\\alpha\\right)=1.\n\\end{eqnarray*}\\] We write: \\[\nZ_n\\to_{as}\\alpha,\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nNote: Almost sure convergence of a sequence of random vectors (or matrices) \\(\\{Z_n\\}\\) to a constant vector (or matrix) \\(\\alpha\\) requires element-wise almost sure convergence.\n\n\n\nConvergence in mean square is typically the most intuitive mode of convergence. If a stochastic sequence converges to a certain limit, then the mean of the stochastic sequence \\(E(Z_n)\\) must converge to this limit and the variance of the stochastic sequence must converge to zero. Convergence in mean square is typically easy to show.\nEvery sequence that converges in mean square, also converges in probability.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 6.3 (Convergence in Mean Square) A sequence of real valued random scalars \\(\\{z_n\\}\\) converges in mean square (or in quadratic mean) to a constant (non-random) \\(\\alpha\\in\\mathbb{R}\\) if \\[\n\\begin{align*}\n  \\lim_{n\\to\\infty}E\\left((Z_n-\\alpha)^2\\right)&=0.\n\\end{align*}\n\\] We write: \\[\nZ_n\\to_{ms}\\alpha,\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nNote: Mean square convergence of a sequence of random vectors (or matrices) \\(\\{Z_n\\}\\) to a deterministic vector (or matrix) \\(\\alpha\\) requires element-wise mean square convergence.\n\n\n\n\nConvergence in distribution is the weakest and at the same time most important mode of convergence.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 6.4 (Convergence in Distribution) Let \\(F_n\\) be the cumulative distribution function (cdf) of the real valued random scalar \\(Z_n\\in\\mathbb{R}\\) and \\(F\\) the cdf of the real valued random scalar \\(Z\\in\\mathbb{R}.\\)\nA sequence of real valued random scalars \\(\\{Z_n\\}\\) converges in distribution to a real valued random scalar \\(Z\\) if \\[\\begin{eqnarray*}\n  \\lim_{n\\to\\infty}F_n(x)=F(x)\n\\end{eqnarray*}\\] for all \\(x\\in\\mathbb{R}\\) at which \\(F(x)\\) is continuous.\nWe write: \\[\nZ_n\\to_{d}Z,\\quad\\text{as}\\quad n\\to\\infty\n\\] We call \\(F\\) the asymptotic (or limit) distribution of \\(Z_n\\).\n\n\n\nRemarks on Definition 6.4:\n\nOften you will see statements like \\[\nZ_n\\to_{d} N(0,1)\n\\] or \\[\nZ_n\\overset{a}{\\sim}N(0,1),\n\\] which should be read as \\[\n\\lim_{n\\to\\infty}F_n(x) = \\Phi(x)\n\\] for all \\(x\\in\\mathbb{R},\\) where \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\nA stochastic sequence \\(\\{Z_n\\}\\) can also convergence in distribution to a constant \\(\\alpha\\). In this case \\(\\alpha\\) is treated as a degenerated random variable with cdf \\[\nF_\\alpha(x)=\\left\\{\n  \\begin{matrix}\n  0&\\text{if}\\;\\;x   <\\alpha\\\\\n  1&\\text{if}\\;\\;x\\geq\\alpha.\n  \\end{matrix}\\right.\n\\]\n\nBy contrast to all other modes of convergence, Definition 6.4 only addresses the scalar (uni-variate) case. The reason for this is that Definition 6.4 cannot simply applied element-wise since this would ignore the possible dependencies between the different uni-variate random variables elements of a random vector.\nTo handle multivariate convergence in distribution, we need the following theorem known as the Cramér-Wold device.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 6.1 (Cramér-Wold) Let \\(Z_n\\in\\mathbb{R}^K\\) and \\(Z\\in\\mathbb{R}^K\\) be \\((K\\times 1)\\)-dimensional random vectors, then \\[\nZ_n\\to_{d} Z\\text{\\quad if and only if \\quad}\\lambda'Z_n\\to_{d}\\lambda'Z\n\\] for any \\(\\lambda\\in\\mathbb{R}^K\\).\n\n\n\nA proof of Theorem 6.1 can be found, e.g., in Billingsley (2008) (p. 383).\nThe Cramér-Wold theorem (Theorem 6.1) is needed since element-wise convergence in distribution does generally not imply convergence of the joint distribution of \\(Z_n\\) to the joint distribution of \\(Z\\); except, if all elements in the random vectors \\(Z_n\\) and \\(Z\\) are independent from each other.\n\n\n\nRelations among Modes of Convergence\n\n\n\n\n\n\n\n\n\n\n\nLemma 6.1 (Relationship among the four modes of convergence) The following relationships hold:\n\nMean square convergence implies convergence in probability: \\[\nZ_n\\to_{ms}\\alpha\\quad \\Rightarrow\\quad Z_n\\to_{p}\\alpha\n\\]\nAlmost sure convergence implies convergence in probability: \\[\nZ_n\\to_{as}\\alpha\\quad \\Rightarrow\\quad Z_n\\to_{p}\\alpha\n\\]\nConvergence in distribution to a constant is equivalent to convergence in probability to the same constant: \\[\nZ_n\\to_{d}\\alpha\\quad \\Leftrightarrow\\quad Z_n\\to_{p}\\alpha\n\\]\n\n\n\n\nProofs of the result in Lemma 6.1 can be found, e.g., here: https://www.statlect.com/asymptotic-theory/relations-among-modes-of-convergence\n\n\n6.1.2 Continuous Mapping Theorem (CMT)\n\n\n\n\n\n\n\n\n\n\n\nTheorem 6.2 (Preservation of convergence for continuous transformations (or “continuous mapping theorem (CMT)”)) Suppose \\(\\{Z_n\\}\\) is a stochastic sequence of random scalars, vectors, or matrices and that \\(f\\) is a continuous function that does not depended on \\(n\\). Then \\[\\begin{align*}\nZ_n\\to_{p}  \\alpha \\quad & \\Rightarrow\\quad f(Z_n)\\to_{p} f(\\alpha)\\\\[2ex]\nZ_n\\to_{as} \\alpha \\quad & \\Rightarrow\\quad f(Z_n)\\to_{as} f(\\alpha)\\\\[2ex]\nZ_n\\to_{d}  \\alpha \\quad & \\Rightarrow\\quad f(Z_n)\\to_{d} f(\\alpha)\n\\end{align*}\\]\n\n\n\nProofs of Theorem 6.2 can be found, e.g., in Van der Vaart (2000) (see Theorem 2.3) or here: https://www.statlect.com/asymptotic-theory/continuous-mapping-theorem\nNote: The CMT does not hold for m.s.-convergence except for the case where \\(f(.)\\) is a linear function.\n \nExamples: As a consequence of the CMT (Theorem 6.2) we have that the usual arithmetic operations preserve convergence in probability (and equivalently for almost sure convergence and convergence in distribution):\n\nIf \\(X_n\\to_{p} \\beta\\) and \\(Y_n\\to_{p} \\gamma,\\) then \\[\nX_n+Y_n\\to_{p} \\beta+\\gamma\n\\]\nIf \\(X_n\\to_{p} \\beta\\) and \\(Y_n\\to_{p} \\gamma,\\) then \\[\nX_n\\cdot Y_n\\to_{p} \\beta\\cdot\\gamma\n\\]\nIf \\(X_n\\to_{p} \\beta\\) and \\(Y_n\\to_{p} \\gamma,\\) then \\[\nX_n/Y_n\\to_{p} \\beta/\\gamma,\n\\] provided that \\(\\gamma\\neq 0\\)\nIf \\(\\frac{1}{n}X_n'X_n\\to_{p} \\Sigma_{X'X},\\) then \\[\n\\left(\\frac{1}{n}X_n'X_n\\right)^{-1}\\to_{p} \\Sigma_{X'X}^{-1},\n\\] provided \\(\\Sigma_{X'X}\\) is a nonsingular/invertible matrix.\n\n\n\n\n6.1.3 Slutsky’s Theorem\nThe following results are concerned with combinations of convergence in probability and convergence in distribution. These are particularly important for the derivation of the asymptotic distribution of estimators.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 6.3 (Slutsky’s Theorem) Let \\(X_n\\) and \\(Y_n\\) denote sequences of random scalars or vectors and let \\(A_n\\) denote a sequences of random matrices. Moreover, \\(\\alpha\\) and \\(A\\) are deterministic limits of appropriate dimensions and \\(X\\) is a random limit of appropriate dimension.\n\nIf \\(X_n\\to_{d} X\\quad\\) and \\(\\quad Y_n\\to_{p} \\alpha,\\quad\\) then \\[\nX_n+Y_n\\to_{d} X + \\alpha.\n\\]\nIf \\(X_n\\to_{d} X\\quad\\) and \\(\\quad Y_n\\to_{p} 0,\\quad\\) then \\[\nX_n'Y_n\\to_{p} 0.\n\\]\nIf \\(X_n\\to_{d} X\\quad\\) and \\(\\quad A_n\\to_{p} A,\\quad\\) then \\[\nA_n X_n\\to_{d} AX.\n\\]\n\nAbove, we assume that \\(X_n,\\) \\(Y_n,\\) and \\(A_n\\) are “conformable” (i.e., the matrix- and vector-dimensions fit to each other).\n\n\n\nProofs of Theorem 6.3 can be found, e.g., in Van der Vaart (2000) (see Theorem 2.8) or here: https://www.statlect.com/asymptotic-theory/Slutsky-theorem\nRemark: Sometimes, only the first two points in Theorem 6.3 are called “Slutsky’s theorem.”\nImportant special case of Theorem 6.3:\nLet \\(\\{X_n\\}\\) and \\(\\{X_n\\}\\) be sequences of real valued \\((K\\times 1)\\)-dimensional random vectors. If \\[\nX_n\\to_{d}\\mathcal{N}_K(0,\\Sigma)\\quad\\text{and}\\quad A_n\\to_{p} A\n\\] then \\[\nA_nX_n\\to_{d}\\mathcal{N}_K(0,A\\Sigma A').\n\\]\n\n\n6.1.4 Law of Large Numbers and Central Limit Theorems\nSo far, we discussed the definitions of the four most important convergence modes, their relations among each other, and basic theorems about functionals of stochastic sequences (CMT and Slutsky). Though, we still lack of tools that allow us to actually show that a stochastic sequence convergences (in some of the discussed modes) to some limit.\nIn the following we consider the stochastic sequences \\[\n\\bar{Z}_1,\\bar{Z}_2,\\dots,\\bar{Z}_n,\\quad\\text{as}\\quad n \\to\\infty,\n\\] of sample means \\[\n\\bar{Z}_n:=n^{-1}\\sum_{i=1}^nZ_i,\n\\] where \\(Z_i\\), \\(i=1,\\dots,n\\), are (scalar, vector, or matrix-valued) random variables.\nRemember: The sample mean \\(\\bar{Z}_n\\) is an estimator of the deterministic population mean \\(\\mu=E(Z_i)\\).\nWeak Law of Large Numbers (WLLNs), Strong LLNs (SLLNs), and Central Limit Theorems (CLTs) tell us conditions under which arithmetic means \\[\n\\bar{Z}_n=n^{-1}\\sum_{i=1}^nZ_i\n\\] converge in probability, almost surely, and in distribution, respectively:\n\nWeak LLN: \\(\\bar{Z}_n \\to_{p}\\mu\\)\nStrong LLN: \\(\\bar{Z}_n\\to_{as}\\mu\\)\nCLT: \\(\\sqrt{n}(\\bar{Z}_n-\\mu)\\to_{d}N(0,\\sigma^2)\\)\n\nIn the following we introduce the most well-known versions of a WLLN, SLLN, and a CLT.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 6.4 (Weak LLN (by Chebychev)) If\n\n\\(\\lim_{n\\to\\infty} E(\\bar{Z}_n)=\\mu\\) and\n\\(\\lim_{n\\to\\infty}Var(\\bar{Z}_n)=0,\\) then \\[\n\\bar{Z}_n\\to_{p}\\mu.\n\\]\n\n\n\n\nA proof of Theorem 6.4 can be found, for instance, here: https://www.statlect.com/asymptotic-theory/law-of-large-numbers\n\n\n\n\n\n\n\n\n\n\n\nTheorem 6.5 (Strong LLN (by Kolmogorov)) If \\(\\{Z_i\\}\\) is an\n\niid sequence with\n\\(E(Z_i)=\\mu,\\) then \\[\n\\bar{Z}_n\\to_{as}\\mu.\n\\]\n\n\n\n\nA proof of Theorem 6.5 can be found, e.g., in Linear Statistical Inference and Its Applications, Rao (1973), pp. 112-114.\n\nNote: The WLLN and the SLLN for random vectors follow from applying the the theorem separately for each element of the random vectors.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 6.6 (CLT (Lindeberg-Levy)) If \\(\\{Z_i\\}\\) is\n\nan iid sequence with\n\n\\(E(Z_i)=\\mu\\) for all \\(i=1,\\dots,n\\) and\n\\(Var(Z_i)=\\sigma^2\\) for all \\(i=1,\\dots,n,\\)\n\nthen \\[\n\\sqrt{n}(\\bar{Z}_n-\\mu)\\to_{d} N(0,\\sigma^2),\\quad\\text{as}\\quad n\\to\\infty\n\\]\n\n\n\nA proof of Theorem 6.6 can be found, e.g., in Van der Vaart (2000) (see Theorem 2.17).\n\nUsing the Cramér-Wold device (Theorem 6.1), the Lindeberg-Levy CLT (Theorem 6.6) can also be applied to \\(K\\)-dimensional random vectors. To show that \\[\n\\sqrt{n}(\\bar{Z}_n-\\mu)\\to_d\\mathcal{N}_K(0,\\Sigma)\n\\] converges to the multivariate, \\(K\\)-dimensional, normal distribution \\(\\mathcal{N}_K(0,\\Sigma)\\) as \\(n\\to\\infty\\), we need to check whether for any \\(\\lambda\\in\\mathbb{R}^K\\):\n\nthe univariate stochastic sequence \\(\\{\\lambda'Z_i\\}\\) is i.i.d.\nwith \\(E(\\lambda'Z_i)=\\lambda'\\mu\\) for all \\(i=1,\\dots,n,\\) where \\(E(Z_i)=\\mu\\in\\mathbb{R}^K\\), and\nwith \\(Var(\\lambda'Z_i)=\\lambda'\\Sigma\\lambda\\) for all \\(i=1,\\dots,n,\\) where \\(Var(Z_i)=\\Sigma\\) denotes the \\((K\\times K)\\) dimensional variance-covariance matrix.\n\nThese points are fulfilled if the multivariate stochastic sequence \\(\\{Z_i\\}\\) is an i.i.d. sequence with \\(E(Z_i)=\\mu\\) and \\(Var(Z_i)=\\Sigma.\\)\n\n\n\n\n\n\nTip\n\n\n\nThe LLNs and the CLT are stated with respect to sequences of sample means \\(\\{\\bar{Z}_n\\}\\); i.e., the simplest estimators you probably can think of. We will see, however, that this is all we need in order to analyze also more complicated estimators such as the OLS estimator.\n\n\n\n\n6.1.5 Estimators: Sequences of Random Variables\nThe concepts introduced above readily apply to univariate or multivariate (\\(K\\)-dimensional) estimators \\[\n\\hat\\theta_n\\equiv\\hat\\theta((X_1,Y_1),\\dots,(X_n,Y_n))\n\\] which are functions of the random sample with sample size \\(n.\\)\nAn increasing sample size \\[\nn\\to\\infty\n\\] makes an estimator \\[\n\\{\\hat\\theta_n\\}\n\\] nothing but a sequence of random variables converging (hopefully) to the correct limit; namely, to the parameter value \\(\\theta\\) we aim to estimate.\nIf an estimator \\(\\hat\\theta_n\\) converges in probability to its limit \\(\\theta\\), we call the estimator weakly consistent or simply consistent. If it converges almost surely to \\(\\theta,\\) we call the estimator strongly consistent.\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 6.5 ((Weak) Consistency) We say that an estimator \\(\\hat\\theta_n\\) is (weakly) consistent for \\(\\theta\\) if \\[\n\\hat\\theta_n\\to_{p}\\theta,\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 6.6 (Strong Consistency) We say that an estimator \\(\\hat\\theta_n\\) is strongly consistent for \\(\\theta\\) if \\[\n\\hat\\theta_n\\to_{as}\\theta,\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\nA necessary requirement for weak and strong consistency is that the estimator is asymptotically unbiased.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 6.7 (Asymptotic Bias) The asymptotic bias of an estimator \\(\\hat\\theta_n\\) of some parameter \\(\\theta\\) is defined as \\[\n\\begin{align*}\n\\operatorname{ABias}(\\hat\\theta_n,\\theta)\n&=\\lim_{n\\to\\infty}\\operatorname{Bias}(\\hat\\theta_n,\\theta)\\\\\n&=\\lim_{n\\to\\infty}E(\\hat\\theta_n)-\\theta.\n\\end{align*}\n\\] If \\(\\text{ABias}(\\hat\\theta_n,\\theta)=0\\), then \\(\\hat\\theta_n\\) is called an asymptotically unbiased.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 6.8 (Asymptotic Normality, Asymptotic Variance, \\(\\sqrt{n}\\)-Consistent) \nAn estimator \\(\\hat\\theta_n\\) is called asymptotically normal if \\[\n\\sqrt{n}(\\hat\\theta_n-\\theta)\\to_{d} \\mathcal{N}(0,\\sigma^2),\\quad\\text{as}\\quad n\\to\\infty,\n\\] where \\[\n\\begin{align}\n\\sigma^2\n& = \\lim_{n\\to\\infty}Var(\\sqrt{n}(\\hat\\theta_n-\\theta))\\\\\n& = \\lim_{n\\to\\infty}n Var(\\hat\\theta_n-\\theta)\\\\\n& = \\lim_{n\\to\\infty}n Var(\\hat\\theta_n)\\\\\n\\end{align}\n\\] is called the asymptotic variance of \\(\\hat\\theta_n.\\)\nDue to the \\(\\sqrt{n}\\)-scaling, \\(\\theta_n\\) is called \\(\\sqrt{n}\\)-consistent.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 6.9 (Mean Squared Error (MSE)) \\[\n\\operatorname{MSE}(\\hat\\theta_n) = E\\left((\\hat\\theta_n - \\theta)^2\\right)\n\\] is called the mean squared error (MSE) of the estimator \\(\\hat\\theta_n.\\)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf we can show that the MSE of \\(\\hat\\theta_n\\) converges to zero, \\[\n\\operatorname{MSE}(\\hat\\theta_n) \\to 0,\\quad n\\to\\infty,\n\\] then we have shown that \\(\\hat\\theta_n\\) converges to \\(\\theta\\) in the mean square sense \\[\n\\hat\\theta_n  \\to_{ms} \\theta,\\quad n\\to\\infty,\n\\] which implies that \\(\\hat\\theta_n\\) is weakly consistent, i.e. \\[\n\\hat\\theta_n \\to_{p} \\theta,\\quad n\\to\\infty.\n\\]\n\n\nThe MSE of \\(\\hat\\theta_n\\) can be decomposed into the squared bias of \\(\\hat\\theta_n\\) and the variance of \\(\\hat\\theta_n:\\) \\[\n\\begin{align*}\n\\operatorname{MSE}(\\hat\\theta_n)\n&=E\\Big[(\\hat\\theta_n-\\theta)^2\\Big]\\\\[2ex]\n&=E\\Big[\\Big(\\overbrace{E(\\hat\\theta_n) - E(\\hat\\theta_n)}^{=0} + \\hat\\theta_n-\\theta\\Big)^2\\Big]\\\\[2ex]\n&=E\\left[\\left((E(\\hat\\theta_n) -\\theta) + (\\hat\\theta_n - E(\\hat\\theta_n))\\right)^2\\right]\\\\[2ex]\n&=E\\Big[\\left(E(\\hat\\theta_n) -\\theta\\right)^2 + \\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)^2\\\\[2ex]\n&\\quad +2\\left(E(\\hat\\theta_n) -\\theta\\right)\\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)\\Big]\\\\[2ex]\n&=\\left(E(\\hat\\theta_n) -\\theta\\right)^2 + E\\Big[\\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)^2\\Big]\\\\[2ex]\n&\\quad +2\\left(E(\\hat\\theta_n) -\\theta\\right)\\overbrace{\\Big(E(\\hat\\theta_n) - E(\\hat\\theta_n)\\Big)}^{=0}\\\\[2ex]\n%&=\\left(E(\\hat\\theta_n)-\\theta\\right)^2 + Var(\\hat\\theta_n)\\\\\n&=\\left(\\operatorname{Bias}(\\hat\\theta_n)\\right)^2 + Var(\\hat\\theta_n).\n\\end{align*}\n\\] Thus, to show that an estimator \\(\\hat\\theta_n\\) converges in the mean square sense to \\(\\theta,\\) we need to show that:\n\nThe estimator is asymptotically unbiased\n\\[\n\\operatorname{Bias}(\\hat\\theta_n)\\to 0,\\quad n\\to\\infty\n\\]\nThe variance of the estimator converges to zero \\[\nVar(\\hat\\theta_n)\\to 0,\\quad n\\to\\infty.\n\\]"
  },
  {
    "objectID": "06-Asymptotics.html#asymptotics-under-the-classic-regression-model",
    "href": "06-Asymptotics.html#asymptotics-under-the-classic-regression-model",
    "title": "6  Large Sample Inference",
    "section": "6.2 Asymptotics under the Classic Regression Model",
    "text": "6.2 Asymptotics under the Classic Regression Model\nGiven the above introduced machinery, we can now proof that the OLS estimators \\[\n\\hat\\beta_n=(X'X)^{-1}X'Y\n\\] and \\[\ns^2_{ub,n}=\\frac{1}{n-K}\\sum_{i=1}^n\\hat\\epsilon_i^2\n\\] are both consistent, and that \\(\\hat\\beta_n\\) is asymptotically normal distributed.\n\nUsing asymptotic statistics, allows us to drop the unrealistic normality and spherical errors assumption (Assumption 4\\(^\\ast\\)) of Chapter 5, but still use our inference tools (\\(t\\)-tests, \\(F\\)-tests) from Chapter 5; as long as the sample size \\(n\\) is “large.”\n\n\nFor the following, it will be useful to introduce some notation that allows us to consider the different parts of the OLS estimator \\(\\hat\\beta_n\\) separately.\n\\[\\begin{align*}\n\\hat\\beta_n\n&=\\left(X'X\\right)^{-1}X'Y\\\\[2ex]\n&=\\left(\\frac{1}{n}X'X\\right)^{-1}\\frac{1}{n} X'Y\\\\[2ex]\n&=\\;\\;\\;\\;S_{X'X}^{-1}\\;\\;\\;\\;\\frac{1}{n} X'Y,\n\\end{align*}\\] where \\[\n\\underset{(K\\times K)}{S_{X'X}}=\\frac{1}{n}X'X=\\frac{1}{n}\\sum_{i=1}^nX_iX_i',\n\\] and where the mean of the \\((K\\times K)\\) matrix \\(S_{X'X}\\) will be denoted as \\[\\begin{align*}\n\\Sigma_{X'X}\n&=E\\left(S_{X'X}\\right)\\\\[2ex]\n&=E\\left(\\frac{1}{n}X'X\\right)\\\\[2ex]\n&=E\\left(\\frac{1}{n}\\sum_{i=1}^nX_iX_i'\\right)\\\\[2ex]\n&=\\frac{n}{n}E\\left(X_iX_i'\\right)\\\\[2ex]\n&=E\\left(X_iX_i'\\right).\n\\end{align*}\\] For the below results, we need to restate the full rank assumption (Assumption 3 of Chapter 4) with respect to \\(\\Sigma_{X'X}.\\)\nAssumption 3\\(^\\ast\\): (Population) Rank Condition  The \\((K\\times K)\\) matrix \\[\n\\Sigma_{X'X}=E(S_{X'X})\n\\]    has full rank \\(K\\). I.e., \\(\\Sigma_{X'X}\\) is nonsingular and invertible. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 6.7 (Consistency of \\(S_{X'X}^{-1}\\)) Under Assumptions 1, 2, 3\\(^\\ast\\), and 4, and under the assumption that \\(X_i\\) has finite second moments, we have that \\[\nS_{X'X}^{-1} = \\left(\\frac{1}{n}X'X\\right)^{-1}\\quad\\to_{p}\\quad\\Sigma_{X'X}^{-1},\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\nThe proof of Theorem 6.7 is done in the lecture.\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 6.8 (Consistency of \\(\\hat\\beta\\)) Under Assumptions 1, 2, 3\\(^\\ast\\), and 4, and under the assumption that \\(X_i\\) has finite second moments, and \\(\\varepsilon_i\\) has finite first moments, we have that \\[\n\\hat\\beta_n\\to_{p}\\beta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\nThe proof of Theorem 6.8 is done in the lecture.\n\nMoreover, we can show that the appropriately scaled OLS estimator is asymptotically normal distributed. The following theorem is stated for the simpler homoskedastic case, the heteroskedastic case is presented in Section 6.2.1.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 6.9 (Asymptotic Normality of \\(\\hat{\\beta}\\) (Homoskedastic Case)) Under Assumption 1, 2, 3\\(^\\ast\\), and 4, and under the assumption that \\(X_i\\) and \\(\\varepsilon_i\\) have finite second moments, and under the simplifying assumption of spherical errors \\[\nVar(\\varepsilon|X)=\\sigma^2I_n,\n\\] we have that \\[\n\\sqrt{n}(\\hat\\beta_n-\\beta)\\to_{d} \\mathcal{N}_K\\left(0,\\sigma^2 \\Sigma^{-1}_{X'X}\\right),\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\nThe proof of Theorem 6.9 is done in the lecture.\nIn principle, we can derive the usual test statistics from the latter result. Though, as long as we do not know (we usually don’t) \\(\\sigma^2\\) and \\(\\Sigma_{X'X}\\) we need to plug-in the (consistent!) estimators \\(S_{X'X}^{-1}\\) and \\(s_{ub}^2\\), where the consistency of the former estimator is provided by Theorem 6.7 and the consistency of \\(s_{ub}^2\\) is provided by the following result.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 6.10 (Consistency of \\(s^2_{UB}\\)) Under the assumptions of Theorem 6.9, but with the additional requirement that \\(\\varepsilon_i\\) has finite fourth moments, we have that \\[\ns_{ub}^2\\to_{p}\\sigma^2,\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\nThe proof of Theorem 6.10 is skipped, but a detailed proof can be found here: https://www.statlect.com/fundamentals-of-statistics/OLS-estimator-properties\n\n6.2.1 The Case of Heteroskedasticity\nTheorem 6.9 can also be stated (and proofed) for conditionally heteroskedastic error terms. In this case one gets \\[\n\\sqrt{n}(\\hat\\beta_n-\\beta)\\to_{d} \\mathcal{N}_K\\left(0,\\Sigma_{X'X}^{-1}E(\\varepsilon^2_iX_iX_i')\\Sigma_{X'X}^{-1}\\right)\n\\tag{6.1}\\] as \\(n\\to\\infty.\\)\nThe asymptotic variance \\[\n\\lim_{n\\to\\infty}Var(\\sqrt{n}(\\hat\\beta_n-\\beta))=\\underbrace{\\Sigma_{X'X}^{-1}E(\\varepsilon_i^2X_iX_i')\\Sigma_{X'X}^{-1}}_{(K\\times K)}\n\\] is, of course, usually unknown and needs to be estimated from the data by some consistent estimator such that \\[\nS_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\\to_{p} \\Sigma_{X'X}^{-1}E(\\varepsilon^2_iX_iX_i')\\Sigma_{X'X}^{-1}\n\\] as \\(n\\to\\infty.\\)\nThe \\(\\widehat{E}(\\varepsilon^2_iX_iX_i')\\) is here a placeholder for one of the existing Heteroskedasticity Consistent (HC) estimators of \\(E(\\varepsilon^2X_iX_i')\\):\n\n\n\n\n\n\n\nHC-Type\nFormular\n\n\n\n\nHC0\n\\(\\widehat{E}(\\varepsilon^2_iX_iX_i')=\\frac{1}{n}\\sum_{i=1}^n\\hat\\varepsilon_i^2X_iX_i'\\)\n\n\nHC1\n\\(\\widehat{E}(\\varepsilon^2_iX_iX_i')=\\frac{1}{n}\\sum_{i=1}^n\\frac{n}{n-K}\\hat\\varepsilon_i^2X_iX_i'\\)\n\n\nHC2\n\\(\\widehat{E}(\\varepsilon^2_iX_iX_i')=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\hat{\\varepsilon}_{i}^{2}}{1-h_{i}}X_iX_i'\\)\n\n\nHC3\n\\(\\widehat{E}(\\varepsilon^2_iX_iX_i')=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\hat{\\varepsilon}_{i}^{2}}{\\left(1-h_{i}\\right)^{2}}X_iX_i'\\)\n\n\nHC4\n\\(\\widehat{E}(\\varepsilon^2_iX_iX_i')=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\hat{\\varepsilon}_{i}^{2}}{\\left(1-h_{i}\\right)^{\\delta_{i}}}X_iX_i'\\)\n\n\n\nHC3 is the most often used HC-estimator.\n\n\n\n\n\n\nTip\n\n\n\nThe statistic \\(h_i:=[P_X]_{ii}\\) is called the leverage statistic of \\(X_i,\\) where\n\n\\(1/n\\leq h_i\\leq 1\\) and\n\\(\\bar{h}=n^{-1}\\sum_{i=1}^nh_i=K/n\\).\n\nObservations \\(X_i\\) with leverage statistics \\(h_i\\) that greatly exceed the average leverage value \\(K/n\\) are referred to as “high leverage” observations. High leverage observations \\(X_i\\) are observations that are far away from all other observations \\(X_j\\), \\(i\\neq j=1,\\dots,n.\\)\nHigh leverage observations \\(X_i\\) have the potential to distort the estimation results, \\(\\hat\\beta_n\\). Indeed, a high leverage observation \\(X_i\\) will have an distorting effect on the estimation results if the absolute value of the corresponding residual \\(|\\hat{\\varepsilon}_i|\\) is unusually large—such observations are called influential outliers. Such observations increase the estimation uncertainty.\nGeneral idea of the HC2-HC4 estimators is to increase the estimated variance in order to account for the effects of influential outliers. The residuals \\(\\hat\\varepsilon_i\\) belonging to \\(X_i\\) values that have a large leverage \\(h_i\\) receive a higher weight and thus increase the value of \\(\\widehat{E}(\\varepsilon^2_iX_iX_i').\\) This strategy takes into account increased estimation uncertainties due to single influential outliers. \n\n\nThe estimator HC0 was suggested in the econometrics literature by White (1980) and is justified by asymptotic (\\(n\\to\\infty\\)) arguments. The estimators HC1, HC2 and HC3 were suggested by MacKinnon and White (1985) to improve the finite sample performance of HC0. Using an extensive Monte Carlo simulation study comparing HC0-HC3, Long and Ervin (2000) concludes that HC3 provides the best overall performance in finite samples. Cribari-Neto (2004) suggested the estimator HC4 to further improve the performance in finite sample behavior, especially in the presence of influential observations (large \\(h_i\\) values).\n\n\n6.2.2 Robust Inference\n\n\n6.2.2.1 Robust Hypothesis Testing: Multiple Parameters\nLet us reconsider the following system of \\(q\\)-many null hypotheses: \\[\\begin{align*}\nH_0: \\underset{(q\\times K)}{R}\\underset{(K\\times 1)}{\\beta}  = \\underset{(q\\times 1)}{r^{(0)}}\\\\\nH_1: \\underset{(q\\times K)}{R}\\underset{(K\\times 1)}{\\beta}  \\neq \\underset{(q\\times 1)}{r^{(0)}}\\\\\n\\end{align*}\\] where the \\((q \\times K)\\) matrix \\(R\\) and the \\(q\\)-vector \\(r=(r_{1},\\dots,r_{q})'\\) are chosen by the statistician to specify her/his null hypothesis about the unknown true parameter vector \\(\\beta\\). To make sure that there are no redundant equations, it is required that \\(\\operatorname{rank}(R)=q\\).\nBy contrast to the multiple parameter tests for small samples (see Section 5.1), we can work here with a heteroskedasticity robust test statistic which is applicable for heteroskedastic error terms: \\[\n\\begin{align*}\nW&=n(R\\hat\\beta_n -r)'[R\\,S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\\,R']^{-1}(R\\hat\\beta_n-r)\\\\[2ex]\nW&\\overset{H_0}{\\to}_d\\chi^2(q), \\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\tag{6.2}\\] The price to pay is that the distribution of the test statistic under the null hypothesis is only valid asymptotically; i.e. for large \\(n\\). That is, the critical values taken from the asymptotic distribution will be useful only for “largish” samples sizes.\nIn case of homoskedastic error terms, one can substitute \\[\nS_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\n\\] by \\[\ns_{ub}^2S_{X'X}^{-1}.\n\\]\n\n\n\n\n\n\nFinite-sample correction\n\n\n\nIn order to improve the finite-sample performance of this test, one usually uses the \\(F_{q,n-K}\\) distribution with \\(q\\) and \\(n-K\\) degrees of freedoms instead of the \\(\\chi^2(q)\\) distribution.\nAsymptotically (\\(n\\to\\infty\\)), \\(F_{q,n-K}\\) is equivalent to \\(\\chi^2(q)\\). However, for any finite sample size \\(n\\) (i.e., the practically relevant case) \\(F_{q,n-K}\\) leads to larger critical values which helps to account for the estimation errors in \\(S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\\) (or in \\(s_{ub}^2S_{X'X}^{-1}\\)) which are otherwise neglected by the pure asymptotic perspective.\n\n\n\n\n6.2.2.2 Robust Hypothesis Testing: Single Parameters\nLet us reconsider the case of hypotheses about only one parameter \\(\\beta_k,\\) with \\(k=1,\\dots,K\\) \\[\\begin{equation*}\n\\begin{array}{ll}\nH_0: & \\beta_k=\\beta_k^{(0)}\\\\\nH_1: & \\beta_k\\ne \\beta_k^{(0)}\\\\\n\\end{array}\n\\end{equation*}\\] We can selecting the \\(k\\)th diagonal element of the test-statistic in Equation 6.2 and taking the square root yields \\[\n\\begin{align*}\nT&=\\frac{\\sqrt{n}\\left(\\hat{\\beta}_k-\\beta_k^{(0)}\\right)}{\\sqrt{\\left[S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\\right]_{kk}}}\\\\[1.5ex]\n\\text{where}\\qquad T&\\overset{H_0}{\\to}_d\\mathcal{N}(0,1),\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\] This \\(t\\)-test statistic allows for heteroskedastic error terms.\nIn case of homoskedastic error terms, one can substitute \\[\n[S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{kk}\n\\] by \\[\ns_{ub}^2[S_{X'X}^{-1}]_{kk}.\n\\]\n\n\n\n\n\n\nFinite-sample correction\n\n\n\nIn order to improve the finite-sample performance of this \\(t\\) test, one usually uses the \\(t_{(n-K)}\\) distribution with \\(n-K\\) degrees of freedoms instead of the \\(\\mathcal{N}(0,1)\\) distribution.\nAsymptotically (\\(n\\to\\infty\\)), \\(t_{(n-K)}\\) is equivalent to \\(\\mathcal{N}(0,1)\\). However, for finite sample sizes \\(n\\) (i.e., the practically relevant case) \\(t_{n-K}\\) leads to larger critical values which helps to account for the estimation errors in \\([S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{kk}\\) (or in \\(s_{ub}^2[S_{X'X}^{-1}]_{kk}\\)) which are otherwise neglected by the pure asymptotic perspective.\n\n\n\n\n6.2.2.3 Robust Confidence Intervals\nFollowing the derivations in Chapter Section 5.4, but using the expression for the robust standard errors, we get the following heteroskedasticity robust (random) \\((1-\\alpha)\\cdot 100\\%\\) confidence interval \\[\n\\operatorname{CI}_{1-\\alpha}=\n\\left[\\hat\\beta_k\\pm t_{1-\\alpha/2,n-K}\\sqrt{n^{-1}[S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{kk}}\\right].\n\\] Here, the coverage probability is an asymptotic coverage probability with \\(P(\\beta_k\\in\\operatorname{CI}_{1-\\alpha})\\to \\gamma\\) as \\(n\\to\\infty\\), where \\(\\gamma\\geq 1-\\alpha.\\)"
  },
  {
    "objectID": "06-Asymptotics.html#sec-MCLS",
    "href": "06-Asymptotics.html#sec-MCLS",
    "title": "6  Large Sample Inference",
    "section": "6.3 Monte Carlo Simulations",
    "text": "6.3 Monte Carlo Simulations\nLet’s apply the above asymptotic inference methods using R. As in Chapter Section 5.5 we, first, program a function myDataGenerator() which allows us to generate data from the following model, i.e., from the following fully specified data generating process: \\[\\begin{align*}\nY_i &=\\beta_1+\\beta_2X_{i2}+\\beta_3X_{i3}+\\varepsilon_i,\\qquad i=1,\\dots,n\\\\\n\\beta &=(\\beta_1,\\beta_2,\\beta_3)'=(2,3,4)'\\\\\nX_{i2}&\\sim U[-4,4]\\\\\nX_{i3}&\\sim U[-5,5]\\\\\n\\varepsilon_i|X_i&\\sim U[-0.5 |X_{i2}|, 0.5 |X_{i2}|],\n\\end{align*}\\] where \\((Y_i,X_i)\\) is assumed i.i.d. across \\(i=1,\\dots,n\\) with \\(X_{i2}\\) and \\(X_{i3}\\) being independent of each other.\nBy contrast to our simulations in Chapter Section 5.5, we consider here a non-Gaussian and heteroskedastic error term \\[\nVar(\\varepsilon_i|X_i)=\\frac{1}{12}X_{i2}^2.\n\\]\n\nAs a side note: The unconditional variance follows by the law of total variance and is given by \\[\\begin{align*}\nVar(\\varepsilon_i)\n&=E(Var(\\varepsilon_i|X_i))+Var(E(\\varepsilon_i|X_i))\\\\\n&=E\\left(\\frac{1}{12}X_{i2}^2\\right)+0\\\\\n&=\\frac{1}{12}\\;E\\left(X_{i2}^2\\right)\\\\\n&=\\frac{1}{12}\\;Var\\left(X_{i2}\\right)\\quad\\text{(since $E(X_{i2}=0)$)}\\\\\n&=\\frac{1}{12}\\left(\\frac{1}{12}(4-(-4))^2\\right)=\\frac{4}{9},\n\\end{align*}\\] where the last steps follows from applying the variance formula for uniform random variables. The following R-function myDataGenerator() allows us to generate data from the above described data generating processes:\n\n\n## Function to generate artificial data\nmyDataGenerator <- function(n, beta){\n  ##\n  X   <- cbind(rep(1, n), \n                 runif(n, -4, 4), \n                 runif(n, -5, 5))\n  ##\n  eps  <- runif(n, min = - 0.5 * abs(X[,2]), \n                   max = + 0.5 * abs(X[,2]))\n  Y    <- X %*% beta + eps\n  ##\n  data <- data.frame(\"Y\"   = Y, \n                     \"X_1\" = X[,1], \n                     \"X_2\" = X[,2], \n                     \"X_3\" = X[,3])\n  ##\n  return(data)\n}\n\n\n6.3.1 Check: Distribution of \\(\\hat\\beta_n\\)\nThe above data generating process fulfills our regulatory assumptions of this chapter. So, by theory, the estimators \\(\\hat\\beta_k\\) should be normal distributed for sufficiently large sample sizes \\(n\\). \\[\n\\sqrt{n}\\left(\\hat\\beta_{n,k}-\\beta_k\\right)\\to_d\\mathcal{N}\\left(0,\\left[\\Sigma_{X'X}^{-1}E(\\varepsilon^2_iX_iX_i')\\Sigma_{X'X}^{-1}\\right]_{kk}\\right)\n\\] or (slight abuse of notation): \\[\n\\hat\\beta_{n,k}\\to_d\\mathcal{N}\\left(\\beta_k, \\;n^{-1}\\;\\left[\\Sigma_{X'X}^{-1}E(\\varepsilon^2_iX_iX_i')\\Sigma_{X'X}^{-1}\\right]_{kk}\\right)\n\\] as \\(n\\to\\infty.\\)\nMathematically, the latter is a bit sloppy since the right hand side of \\(\\to_d\\) depends on \\(n\\), i.e., is not the stable limit object for \\(n\\to\\infty\\). However, this sloppiness is nevertheless instructive since it gives us the approximative distribution for given largish sample sizes like \\(n=100\\).\n\n\n\nFor our above specified data generating process, we can derive all (usually unknown) population quantities.\n\nFrom the assumed distributions of \\(X_{i2}\\) and \\(X_{i3}\\) we have that: \\[\\begin{align*}\n\\Sigma_{X'X}\n&=E(S_{X'X})\\\\[2ex]\n&=E(X_iX_i')\\\\[2ex]\n&=\\left(\\begin{matrix}1&0&0\\\\0&E(X_{i2}^2)&0\\\\0&0&E(X_{i3}^2)\\end{matrix}\\right)\\\\[2ex]\n&=\\left(\\begin{matrix}1&0&0\\\\0&\\frac{16}{3}&0\\\\0&0&\\frac{25}{3}\\end{matrix}\\right)\n\\end{align*}\\] The above result follows from observing that \\(E(X^2)=Var(X)\\) if \\(X\\) has mean zero, and that the variance of uniform \\(U[a,b]\\) distributed random variables is given by \\(\\frac{1}{12}(b-a)^2\\).\nMoreover, \\(E(\\varepsilon^2_iX_iX_i')=E(X_iX_i'E(\\varepsilon^2_i|X_i))=E\\left(X_iX_i'\\left(\\frac{1}{12}X_{i2}^2\\right)\\right)\\) such that \\[\\begin{align*}\nE(\\varepsilon^2_iX_iX_i')\n&=\\left(\\begin{matrix}E\\left(\\frac{1}{12}X_{i2}^2\\right)&0&0\\\\[2ex]\n                 0&E\\left(X_{i2}^2\\cdot\\frac{1}{12}X_{i2}^2\\right)&0\\\\0&0&E\\left(X_{i3}^2\\cdot\\frac{1}{12}X_{i2}^2\\right)\n    \\end{matrix}\\right)\\\\[2ex]\n&=\\left(\\begin{matrix}\\frac{1}{12}E\\left(X_{i2}^2\\right)&0&0\\\\\n     0&\\frac{1}{12}E\\left(X_{i2}^4\\right)&0\\\\0&0&\\frac{1}{12}E\\left(X_{i2}^2\\right)\\,E\\left(X_{i3}^2\\right)\n\\end{matrix}\\right)\\\\[2ex]      \n&=\\left(\\begin{matrix}\\frac{1}{12}\\frac{16}{3}&0&0\\\\\n                    0&\\frac{1}{12}\\frac{256}{5}&0\\\\\n                    0&0&\\frac{1}{12}\\frac{16}{3}\\frac{25}{3}\\end{matrix}\\right)\\\\[2ex]\n&=\\left(\\begin{matrix}\\frac{4}{9}&0&0\\\\0&\\frac{64}{15}&0\\\\0&0&\\frac{100}{27}\\end{matrix}\\right)\n\\end{align*}\\] The above result follow from observing that for \\(X\\sim U[a,b]\\) one has \\(E(X^k)=\\frac{b^{k+1}-a^{k+1}}{(k+1)(b-a)}\\), \\(k=1,2,\\dots\\); see, for instance, Wikipedia.\n\nSo, for instance, for \\(\\hat{\\beta}_2\\) we have the following theoretical large sample distribution: \\[\n\\begin{align}\n\\hat\\beta_{n,2}\\to_d&\\mathcal{N}\\left(\\beta_2, \\;\\frac{1}{n}\\;\\left[\\left(\\begin{matrix}1&0&0\\\\0&\\frac{16}{3}&0\\\\0&0&\\frac{25}{3}\\end{matrix}\\right)^{-1}\\left(\\begin{matrix}\\frac{4}{9}&0&0\\\\0&\\frac{64}{15}&0\\\\0&0&\\frac{100}{27}\\end{matrix}\\right)\\left(\\begin{matrix}1&0&0\\\\0&\\frac{16}{3}&0\\\\0&0&\\frac{25}{3}\\end{matrix}\\right)^{-1}\\right]_{22}\\right)\\\\[2ex]\n\\hat\\beta_{n,2}\\to_d&\\mathcal{N}\\left(\\beta_2, \\;\\frac{1}{n}\\;\\left[\n  \\left(\n  \\begin{matrix}\n  0.444 & 0 &0\\\\\n  0 & 0.15 &0\\\\\n  0&0&0.053\n  \\end{matrix}\\right)\\right]_{22}\\right)\\\\[2ex]\n\\hat\\beta_{n,2}\\to_d&\\mathcal{N}\\left(\\beta_2, \\;\\frac{1}{n}\\;0.15\\right)\n\\end{align}\n\\] Let’s use a Monte Carlo simulation to check how well the theoretical large sample (\\(n\\to\\infty\\)) distribution of \\(\\hat\\beta_2\\) works as an approximative distribution for a practical largish sample size of \\(n=100\\).\n\nset.seed(123)\nn           <- 100      # a largish sample size\nbeta_true   <- c(2,3,4) # true data vector\n\n## Mean and variance of the true asymptotic \n## normal distribution of beta_hat_2:\n# true mean\nbeta_true_2     <- beta_true[2] \n# true variance\nvar_true_beta_2 <- (solve(diag(c(1, 16/3, 25/3)))    %*% \n                          diag(c(4/9, 64/15, 100/27))%*% \n                    solve(diag(c(1, 16/3, 25/3))))[2,2]/n\n\n## Let's generate 5000 realizations from beta_hat_2, and check \n## whether their distribution is close to the true normal \n## distribution.\n## (We don't condition on X since the theoretical limit \n## distribution is unconditional on X)\nrep        <- 5000 # MC replications\nbeta_hat_2 <- rep(NA, times=rep)\n##\nfor(r in 1:rep){\n    MC_data <- myDataGenerator(n    = n, \n                               beta = beta_true)\n    lm_obj        <- lm(Y ~ X_2 + X_3, data = MC_data)\n    beta_hat_2[r] <- coef(lm_obj)[2]\n}\n\n## Compare:\n## True beta_2 versus average of beta_hat_2 estimates\nc(beta_true_2, round(mean(beta_hat_2), 4))\n\n[1] 3.0000 2.9998\n\n\nGood! As expected, the average of the 5000 simulated realizations of \\(\\hat\\beta_2\\) is basically equal to the theoretical true mean \\(E(\\hat\\beta_2)=\\beta_2=3\\) which indicates a bias of zero.\n\n## True variance of beta_hat_2 versus \n## empirical variance of beta_hat_2 estimates\nc(round(var_true_beta_2, 5), round(var(beta_hat_2), 5))\n\n[1] 0.00150 0.00147\n\n\nGreat! The variance of the 5000 simulated realizations of \\(\\hat\\beta_2\\) is basically equal to the theoretical true variance \\(Var(\\hat\\beta_{n,2})=0.15/n=0.0015\\).\n\n## True normal distribution of beta_hat_2 versus \n## empirical density of beta_hat_2 estimates\nlibrary(\"scales\")\ncurve(expr = dnorm(x, mean = beta_true_2, \n                   sd=sqrt(var_true_beta_2)), \n      xlab=\"\",ylab=\"\", col=gray(.2), lwd=3, lty=1, \nxlim=range(beta_hat_2),ylim=c(0,14.1),main=paste0(\"n=\",n))\nlines(density(beta_hat_2, bw = bw.SJ(beta_hat_2)), \n      col=alpha(\"blue\",.5), lwd=3)\nlegend(\"topleft\", lty=c(1,1), lwd=c(3,3), \n     col=c(gray(.2), alpha(\"blue\",.5)), bty=\"n\", legend= \nc(expression(\n  \"Theoretical (Asymptotic) Gaussian Density of\"~hat(beta)[2]), \n  expression(\n  \"Empirical Density Estimation based on MC realizations from\"~\n  hat(beta)[2])))\n\n\n\n\n\n\n\n\nGreat! The nonparametric density estimation (estimated via density()) computed from the 5000 simulated realizations of \\(\\hat\\beta_2\\) is indicating that \\(\\hat\\beta_2\\) is really normally distributed as described by our theoretical results.\n\nHowever, is the asymptotic distribution of \\(\\hat\\beta_2\\) also usable for (very) small samples like \\(n=5\\)? Let’s check that:\n\nset.seed(123)\nn           <- 5       # a small sample size\nbeta_true   <- c(2,3,4) # true data vector\n\n## Mean and variance of the true asymptotic \n## normal distribution of beta_hat_2:\n# true mean\nbeta_true_2     <- beta_true[2] \n# true variance\nvar_true_beta_2 <- (solve(diag(c(1, 16/3, 25/3)))%*% \n                          diag(c(4/9, 64/15, 100/27))%*% \n                    solve(diag(c(1, 16/3, 25/3))))[2,2]/n\n\n## Let's generate 5000 realizations from beta_hat_2, and check \n## whether their distribution is close to the true normal \n## distribution.\n## (We don't condition on X since the theoretical limit \n## distribution is unconditional on X)\nrep        <- 5000 # MC replications\nbeta_hat_2 <- rep(NA, times=rep)\n##\nfor(r in 1:rep){\n    MC_data <- myDataGenerator(n    = n, \n                               beta = beta_true)\n    lm_obj        <- lm(Y ~ X_2 + X_3, data = MC_data)\n    beta_hat_2[r] <- coef(lm_obj)[2]\n}\n\n## Compare:\n## True beta_2 versus average of beta_hat_2 estimates\nc(beta_true_2, round(mean(beta_hat_2), 4))\n\n[1] 3.0000 2.9963\n\n\nOK, at least on average the 5000 simulated realizations of \\(\\hat\\beta_2\\) are basically equal to the true mean \\(E(\\hat\\beta_2)=\\beta_2=3\\).\n\n## True variance of beta_hat_2 versus \n## empirical variance of beta_hat_2 estimates\nc(round(var_true_beta_2, 4), round(var(beta_hat_2), 4))\n\n[1] 0.0300 0.0562\n\n\nOuch! The theoretical variance \\(Var(\\hat\\beta_2)=0.15/n=0.03\\) is about 50% smaller than the actual (small sample) variance of \\(\\hat\\beta_2\\) approximated by the empirical variance of the 5000 simulated realizations of \\(\\hat\\beta_2\\). That is, we cannot simply use a large sample result in small samples.\nReason: In small samples, the Law of Large Numbers has not kicked in yet; therefore, we cannot neglect the variability in the sample statistic \\(S_{X'X}^{-1}.\\)\nThis issue can also seen when comparing the theoretical large sample distribution of \\(\\hat\\beta_2\\) with an estimate of the actual finite-sample distribution of \\(\\hat\\beta_2.\\)\n\n## True normal distribution of beta_hat_2 versus \n## empirical density of beta_hat_2 estimates\nlibrary(\"scales\")\ncurve(expr = dnorm(x, \n                   mean = beta_true_2, \n                   sd   = sqrt(var_true_beta_2)), \n      xlab = \"\", ylab = \"\", col=gray(.2), lwd=3, lty=1, \nxlim=c(2,4), ylim=c(0,3), main=paste0(\"n=\",n))\nlines(density(beta_hat_2, bw = bw.SJ(beta_hat_2)), \n      col=alpha(\"blue\",.5), lwd=3)\n## Legend      \nlegend(\"topleft\", lty=c(1,1), lwd=c(3,3), \n       col=c(gray(.2), alpha(\"blue\",.5)), bty=\"n\", \n       legend= \nc(expression(\n  \"Theoretical (Asymptotic) Gaussian Density of\"~hat(beta)[2]), \n  expression(\n  \"Empirical Density Estimation based on MC realizations from\"~\n  hat(beta)[2])))      \n\n\n\n\n\n\n\n\nNot good. The actual finite-sample distribution has substantially fatter tails. That is, if we would use the quantiles of the asymptotic distribution, we would falsely reject the null-hypothesis too often (probability of type I errors would be larger than the significance level).\nFortunately, asymptotics are usually kicking in relatively fast; here, things become much more reliable already for \\(n\\geq 15\\).\n\n\n6.3.2 Check: Testing Multiple Parameters\nIn the following, we do inference about multiple parameters. We test the (here correct) null hypothesis \\[\\begin{align*}\nH_0:\\;&\\beta_2=3\\quad\\text{and}\\quad\\beta_3=4\\\\\n\\text{versus}\\quad H_1:\\;&\\beta_2\\neq 3\\quad\\text{and/or}\\quad\\beta_3\\neq 4.\n\\end{align*}\\] Or equivalently \\[\\begin{align*}\nH_0:\\;&R\\beta -r^{(0)} = 0 \\\\\nH_1:\\;&R\\beta -r^{(0)} \\neq 0,\n\\end{align*}\\] where \\[\nR=\\left(\n\\begin{matrix}\n0&1&0\\\\\n0&0&1\\\\\n\\end{matrix}\\right)\\quad\\text{ and }\\quad\nr^{(0)}=\\left(\\begin{matrix}3\\\\5\\\\\\end{matrix}\\right).\n\\] The following R code can be used to test this hypothesis. Note that we use HC3 robust variance estimation sandwich::vcovHC(lm_obj, type=\"HC3\") to take into account that the error terms are heteroskedastic.\n\nsuppressMessages(library(\"car\")) # for linearHyothesis()\n# ?linearHypothesis\nlibrary(\"sandwich\") # for vcovHC(), robust variance estimations\n\nset.seed(1009)\n\n## Generate data\nMC_data <- myDataGenerator(n    = 100, \n                           beta = beta_true)\n\n## Estimate the linear regression model parameters\nlm_obj <- lm(Y ~ X_2 + X_3, data = MC_data)\n\nvcovHC3_mat <- sandwich::vcovHC(lm_obj, type=\"HC3\")\n\n## Option 1:\n# car::linearHypothesis(model = lm_obj, \n#                       hypothesis.matrix = c(\"X_2=3\", \"X_3=4\"), \n#                       vcov = vcovHC3_mat)\n\n## Option 2:\nR <- rbind(c(0,1,0),\n           c(0,0,1))\ncar::linearHypothesis(model = lm_obj, \n                      hypothesis.matrix = R, \n                      rhs  = c(3,4),\n                      vcov = vcovHC3_mat)\n\nLinear hypothesis test\n\nHypothesis:\nX_2 = 3\nX_3 = 4\n\nModel 1: restricted model\nModel 2: Y ~ X_2 + X_3\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df     F Pr(>F)\n1     99                \n2     97  2 0.032 0.9685\n\n\nThe large \\(p\\)-value does not allow us to reject the null-hypothesis at any of the usual significance levels. This is good, since we test here a correct null-hypothesis and rejecting it would mean to do a false rejection (type I error, false positive).\nOn average, however, there will be some false rejections of the null-hypothesis, but this type I error rate needs to be samler or equal to \\(\\alpha\\).\n\nset.seed(1110)\n\nB        <- 10000 \np_values <- numeric(B)\n\nfor(r in 1:B){ \n  MC_data     <- myDataGenerator(n    = 100, beta = beta_true)\n  lm_obj      <- lm(Y ~ X_2 + X_3, data = MC_data)\n  vcovHC3_mat <- sandwich::vcovHC(lm_obj, type=\"HC3\")\n  Ftest       <- car::linearHypothesis(model = lm_obj, \n                        hypothesis.matrix = c(\"X_2=3\", \"X_3=4\"), \n                        vcov = vcovHC3_mat)\n  p_values[r] <- Ftest$'Pr(>F)'[2]                      \n}\n## Nominal type I error rate \nalpha <- 0.05 \n## Empirical type I error rate\nlength(p_values[p_values < alpha])/B\n\n[1] 0.0562\n\n\nThis is more or less ok, but you see that the upper bound is not so strict here as it was in the small sample inference case in Section 5.5. A larger sample size \\(n\\geq 100\\) would improve this simulation result.\n\n\n6.3.3 Check: Testing Single Parameters\nNext, we do inference about a single parameter. We test \\[\\begin{align*}\nH_0:&\\beta_3=5\\\\\n\\text{versus}\\quad H_1:&\\beta_3\\neq 5.\n\\end{align*}\\]\n\n# Load libraries\nsuppressMessages(library(\"lmtest\"))  # for coeftest()\n\n## Generate data\nn <- 100\nMC_data <- myDataGenerator(n    = n, \n                           beta = beta_true)\n\n## Estimate the linear regression model parameters\nlm_obj <- lm(Y ~ X_2 + X_3, data = MC_data)\n\n## Robust t test\n\n## Robust standard error for \\hat{\\beta}_3:\nSE_rob <- sqrt(vcovHC(lm_obj, type = \"HC3\")[3,3])\n## hypothetical (H0) value of \\beta_3:\nbeta_3_H0 <- 4\n## estimate for beta_3:\nbeta_3_hat <- coef(lm_obj)[3]\n## robust t-test statistic\nt_test_stat <- (beta_3_hat - beta_3_H0)/SE_rob\n## p-value\nK <- length(coef(lm_obj))\n##\np_value <- 2 * min(   pt(q = t_test_stat, df = n - K), \n                   1- pt(q = t_test_stat, df = n - K))\np_value\n\n[1] 0.2504485\n\n\nAgian, the large \\(p\\)-value does not allow us to reject the (here true) null-hypothesis at any of the usual significance levels. Let us check the empirical type I error rate.\n\nset.seed(1009)\nn        <- 100\nB        <- 10000 \np_values <- numeric(B)\n\nfor(r in 1:B){ \n  MC_data     <- myDataGenerator(n = n, beta = beta_true)\n  lm_obj      <- lm(Y ~ X_2 + X_3, data = MC_data)\n  SE_rob      <- sqrt(vcovHC(lm_obj, type = \"HC3\")[3,3])\n  beta_3_hat  <- coef(lm_obj)[3]\n  ## robust t-test statistic\n  t_test_stat <- (beta_3_hat - beta_3_H0)/SE_rob\n  p_values[r] <- 2 * min(   pt(q = t_test_stat, df = n - length(beta_true)), \n                         1- pt(q = t_test_stat, df = n - length(beta_true)))\n}\n## Nominal type I error rate \nalpha <- 0.05 \n## Empirical type I error rate\nlength(p_values[p_values < alpha])/B\n\n[1] 0.0496\n\n\nGood! The empirical type I error rate is bounded from above by the nominal type I error rate \\(\\alpha\\)."
  },
  {
    "objectID": "06-Asymptotics.html#sec-RDLSInf",
    "href": "06-Asymptotics.html#sec-RDLSInf",
    "title": "6  Large Sample Inference",
    "section": "6.4 Real Data Example",
    "text": "6.4 Real Data Example\nIn the following, we revisit the read data study from Section 5.6. Now, we have the tools to allow for heteroskedastic and non-Gaussian errors.\n\n## The AER package contains a lot of datasets \nsuppressPackageStartupMessages(library(AER))\n\n## Attach the DoctorVisits data to make it usable\ndata(\"DoctorVisits\")\n\nlm_obj <- lm(visits ~ gender + age + income, data = DoctorVisits)\n\nThe above R codes re-estimate the following regression model \\[\nY_i = \\beta_1 + \\beta_{gender} X_{gender,i}\n              + \\beta_{age} X_{age,i}\n              + \\beta_{income} X_{income,i} + \\varepsilon_i,\n\\] where \\(i=1,\\dots,n\\) and\n\n\\(X_{gender,i}=1\\) if the \\(i\\)th subject is a woman and \\(X_{gender,i}=0\\) if the \\(i\\)th subject is a man\n\\(X_{age,i}\\) is the age of subject \\(i\\) measured in years divided by \\(100\\)\n\\(X_{income,i}\\) is the annual income of subject \\(i\\) in tens of thousands of dollars\n\nNow, to do heteroskedasticity consistent robust inference, one can use the vcocHV() frunction from the R package sandwich together with the coeftest() function from the R package lmtest.\n\n## No robust standard errors:\ncoeftest(lm_obj)\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(>|t|)    \n(Intercept)   0.153710   0.036069  4.2616 2.066e-05 ***\ngenderfemale  0.062446   0.023455  2.6624  0.007782 ** \nage           0.402355   0.057131  7.0427 2.132e-12 ***\nincome       -0.082306   0.031670 -2.5989  0.009380 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## HC3 robust standard errors:\ncoeftest(lm_obj, vcov = vcovHC(lm_obj, type = \"HC3\"))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(>|t|)    \n(Intercept)   0.153710   0.034351  4.4747 7.815e-06 ***\ngenderfemale  0.062446   0.024582  2.5403   0.01110 *  \nage           0.402355   0.060304  6.6721 2.785e-11 ***\nincome       -0.082306   0.032321 -2.5465   0.01091 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nYou can do this also using the R package stargazer which gives you (almost) puplication ready regression output tables. The following R code produced two regression outputs—one without and one with robust standard errors:\n\nsuppressPackageStartupMessages(library(\"stargazer\"))\n\n# Adjust standard errors\ncov_beta_HC3   <- vcovHC(lm_obj, type = \"HC3\")\nrobust_HC3_se  <- sqrt(diag(cov_beta_HC3))\n\n# Stargazer output (with and without RSE)\nstargazer(lm_obj, lm_obj, \n          se   = list(NULL, robust_HC3_se),\n          column.labels = c(\"\", \"Robust SE\"),\n          type = \"html\") # alternatively: type = \"text\" or type = \"latex\"\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nvisits\n\n\n\n\n\n\n\n\nRobust SE\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\ngenderfemale\n\n\n0.062***\n\n\n0.062**\n\n\n\n\n\n\n(0.023)\n\n\n(0.025)\n\n\n\n\n\n\n\n\n\n\n\n\nage\n\n\n0.402***\n\n\n0.402***\n\n\n\n\n\n\n(0.057)\n\n\n(0.060)\n\n\n\n\n\n\n\n\n\n\n\n\nincome\n\n\n-0.082***\n\n\n-0.082**\n\n\n\n\n\n\n(0.032)\n\n\n(0.032)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.154***\n\n\n0.154***\n\n\n\n\n\n\n(0.036)\n\n\n(0.034)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n5,190\n\n\n5,190\n\n\n\n\nR2\n\n\n0.019\n\n\n0.019\n\n\n\n\nAdjusted R2\n\n\n0.018\n\n\n0.018\n\n\n\n\nResidual Std. Error (df = 5186)\n\n\n0.791\n\n\n0.791\n\n\n\n\nF Statistic (df = 3; 5186)\n\n\n33.218***\n\n\n33.218***\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01"
  },
  {
    "objectID": "06-Asymptotics.html#exercises",
    "href": "06-Asymptotics.html#exercises",
    "title": "6  Large Sample Inference",
    "section": "6.5 Exercises",
    "text": "6.5 Exercises\n\nExercises for Chapter 6\nExercises of Chapter 6 with Solutions\nExercises of Chapter 6 with Solutions (annotated)"
  },
  {
    "objectID": "07-Instrumental-Variables.html",
    "href": "07-Instrumental-Variables.html",
    "title": "7  Instrumental Variables",
    "section": "",
    "text": "This chapter builds upon Chapter 12 of Hansen (2022)."
  },
  {
    "objectID": "07-Instrumental-Variables.html#introduction",
    "href": "07-Instrumental-Variables.html#introduction",
    "title": "7  Instrumental Variables",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nThe concepts of endogeneity and instrumental variable are fundamental to econometrics, and mark a substantial departure from other branches of statistics.\nThe ideas of endogeneity arise naturally in economics from the structural model approach leading to models of simultaneous equations such as, for instance, the classic supply/demand model of price determination.\n\nWe say that there is endogeneity in the linear model \\[\n\\begin{align}\n  Y_i=X_i'\\beta+\\varepsilon_i\n\\end{align}\n\\tag{7.1}\\]\n\nif \\(\\beta\\) is the parameter of interest and\nif \\(E(\\varepsilon_i|X_i)\\neq 0\\)\n\nand thus \\[\nE(X_i\\varepsilon_i)\\neq 0.\n\\tag{7.2}\\]\nWhen Equation 7.2 holds, \\(X_i\\) is endogenous for \\(\\beta.\\)\nThis situation constitutes a core problem in econometrics which is not so much in the focus of the statistics literature.\n\nEquation 7.1 is called a structural equation\n\\(\\beta\\) in Equation 7.1 is called a structural parameter\n\nIt is important to distinguish Equation 7.1 from the regression or projection models we considered so far. It may be the case that a structural model coincides with a regression/projection model, but this is not necessarily the case.\nEndogeneity cannot happen if the model coefficient is defined by a linear projection. We can define the linear (population) projection coefficient \\[\n\\beta^*=E(X_iX_i')^{-1}E(X_iY_i)\n\\] and the corresponding linear (population) projection equation \\[\nY_i = X_i'\\beta^* + \\varepsilon_i^*,\n\\] where then, by construction (projection properties), \\(\\varepsilon_i^* = Y_i - X_i'\\beta^*\\) is a projection error, i.e. \\[\nE(X_i\\varepsilon_i^*)=0.\n\\]\nCaution: Here we simply define \\(\\beta^*\\) as \\(\\beta^*=E(X_iX_i')^{-1}E(X_iY)\\) which results in the population version of the projection coefficient for the projection of \\(Y_i\\) into the space spanned by \\(X_i\\) (i.e. the population regression of \\(Y_i\\) on \\(X_i\\).) We did not derive the expression for \\(\\beta^*\\) using the exogeneity assumption as we did in ?sec-MMEstimator; indeed, the exogeneity assumption may be violated.\nThe (population) projection coefficient \\(\\beta^*\\) and the structural parameter \\(\\beta\\) coincide \\((\\beta^*=\\beta)\\) under exogeneity.\nHowever, under endogeneity (Equation 7.2) the projection coefficient \\(\\beta^*\\) does not equal the structural parameter \\((\\beta^*\\neq \\beta):\\) \\[\n\\begin{align*}\n\\beta^* & =E(X_iX_i')^{-1}E(X_iY_i)\\\\\n\\beta^* & =E(X_iX_i')^{-1}E(X_i (X_i'\\beta + \\varepsilon_i) )\\\\\n%\\beta^* & =E(X_iX_i')^{-1}E(X_i X_i') \\beta +\n%            E(X_iX_i')^{-1}E(X_i\\varepsilon_i)\\\\\n\\beta^* & = \\beta + E(X_iX_i')^{-1}\\underbrace{E(X_i \\varepsilon_i)}_{\\neq 0}\\neq \\beta\\\\\n\\end{align*}\n\\]\nThus under endogeneity we cannot simply use the projection coefficient to derive an estimator since the projection coefficient does not identify the structural parameter of interest.\nThat is, endogeneity implies that the least squares estimator is inconsistent for the structural parameter. Indeed, under i.i.d. sampling, least squares is consistent for the projection coefficient \\[\n\\hat\\beta\\to_pE(X_iX_i')^{-1}E(X_iY) = \\beta^* \\neq \\beta,\\quad n\\to\\infty.\n\\]\nBut since the structural parameter \\(\\beta\\) is here the parameter of interest, endogeneity requires the development of alternative estimation methods."
  },
  {
    "objectID": "07-Instrumental-Variables.html#examples-for-structural-equationsmodels",
    "href": "07-Instrumental-Variables.html#examples-for-structural-equationsmodels",
    "title": "7  Instrumental Variables",
    "section": "7.2 Examples for Structural Equations/Models",
    "text": "7.2 Examples for Structural Equations/Models\nThe structural model approach in econometrics goes back to the seminal paper The probability approach in econometrics by Haavelmo (1944).\nUnder the structural approach one, first, specifies a probabilistic economic model (taking into account economic theory), and then performs a quantitative analysis under the assumption that the economic model is correctly specified. This approach is reflected by Assumption 1 in ?sec-MLR. Researchers often describe this as “taking their model seriously”. A criticism of the structural approach is that it is misleading to treat an economic model as correctly specified. Rather, it is more accurate to view a model as a useful abstraction or approximation.\n\nExample 7.1 (Measurement error in the regressor) \nSuppose that:\n\n\\((Y_i,Z_i)\\) is a multivariate random variable i.i.d. across \\(i=1,\\dots,n\\)\n\\(E(Y_i|Z_i)=Z_i'\\beta\\)\n\\(\\beta\\) is the structural parameter\n\\(Y_i = Z_i'\\beta + \\varepsilon_i\\) is the structural equation for which our model assumptions of ?sec-LinModAssumptions can be justified.\n\nUnfortunately, \\(Z_i\\) is not observed (latent), instead we observe a noisy version of \\(Z_i\\) \\[\nX_i=Z_i + u_i,\n\\] where \\(u_i\\) is a \\((K\\times 1)\\) dimensional classic measurement error, i.e.\n\n\\(u_i\\) is independent of all other stochastic quantities in the model\n\\(E(u_i)=0\\)\n\nsuch that \\(X_i\\) is a noisy, but unbiased measure of \\(Z_i.\\)\nIt’s easy to express \\(Y_i\\) as a function of the observable \\(X_i\\) \\[\n\\begin{align*}\nY_i\n& = Z_i'\\beta + \\varepsilon_i \\\\\n& = (X_i - u_i)'\\beta + \\varepsilon_i \\\\\n& = X_i'\\beta  + v_i,\\\\\n\\end{align*}\n\\] where \\(v_i=\\varepsilon_i - u_i'\\beta\\).\nThat is, the relationship of \\(Y_i\\) and \\(X_i\\) is described by a linear equation \\[\nY_i = X_i'\\beta  + v_i,\n\\] with a stochastic error term \\(v_i\\), but this error term is (generally) not a projection error since \\[\n\\begin{align}\nE(X_iv_i)\n& = E((Z_i + u_i)\\,(\\varepsilon_i - u_i'\\beta)) \\\\\n& = \\underbrace{E(Z_i \\varepsilon_i)}_{=0} - \\underbrace{E(Z_iu_i'\\beta)}_{=0}  + \\underbrace{E(u_i \\varepsilon_i)}_{=0} - E(u_iu_i')\\beta \\\\\n& = - E(u_iu_i')\\beta \\\\\n&\\neq 0\n\\end{align}\n\\tag{7.3}\\] if \\(\\beta\\neq 0\\) and \\(E(u_iu_i')\\neq 0.\\)\nGenerally\n\n\\(\\beta\\) will not equal zero and\nthe \\((K\\times K)\\) variance-covariance matrix \\(E(u_iu_i')\\) will also not equal a zero matrix since this would mean that the measurement errors have variance zero.\n\nWe can calculate the (population) projection coefficient \\(\\beta^*\\) (which can be consistently estimated): \\[\n\\begin{align*}\n\\beta^*\n& = E(X_iX_i')^{-1} E(X_i Y_i)\\\\\n& = E(X_iX_i')^{-1} E(X_i (X_i'\\beta + v_i))\\\\\n& = \\beta + E(X_iX_i')^{-1} \\underbrace{E(X_i v_i)}_{\\neq 0}\n\\end{align*}\n\\] In the special case where \\(K=1\\), we have \\[\n\\begin{align}\n\\beta^*_1\n& = \\beta_1 + \\frac{E(X_i v_i)}{E(X_i^2)}\\\\\n& = \\beta_1 - \\frac{E(u_i^2)\\beta_1}{E(X_i^2)}\\\\\n& = \\beta_1\\left(1 - \\frac{ E(u_i^2)}{E(X_i^2)}\\right) < \\beta_1,\n\\end{align}\n\\tag{7.4}\\] where we used that, by Equation 7.3, \\(E(X_iv_i)=-E(u_i^2)\\beta_1\\), and where the inequality follows from observing that \\(E(u_i^2)/E(X_i^2)<1\\) since \\[\n\\begin{align*}\nE(X_i^2)\n&= E((Z_i + u_i)^2)\\\\\n&= E(Z_i^2) + 2E(Z_iu_i) + E(u_i^2) \\\\\n&= E(Z_i^2) + 0 + E(u_i^2)\\\\  \n&> E(u_i^2)\\\\  \n\\Leftrightarrow\\quad 0\\leq \\frac{E(u_i^2)}{E(X_i^2)}&<1.\n\\end{align*}\n\\] Equation 7.4 shows that projection coefficient \\(\\beta_1^*\\) shrinks the structural parameter \\(\\beta_1\\) towards zero. This is called measurement error bias or attenuation bias.\n\n\nExample 7.2 (Supply and Demand) \nThe variables \\(Q_i\\in\\mathbb{R}\\) and \\(P_i\\in\\mathbb{R}\\) (Quantity and Price) are determined jointly by the following simultaneous equation system consisting of the demand equation \\[\nQ_i = -\\beta_1 P_i + \\varepsilon_{1i}\n\\] and of the supply equation \\[\nQ_i =  \\beta_2 P_i + \\varepsilon_{2i},\n\\] where we assume that \\(Q_i\\) and \\(P_i\\) are centered such that we can drop the intercepts.\nAssume that the \\((2\\times 1)\\) dimensional joint random error \\(\\varepsilon_i=(\\varepsilon_{1i},\\varepsilon_{2i})'\\) satisfies \\(E(\\varepsilon_i)=0\\) and \\(E(\\varepsilon_i\\varepsilon_i')=I_2\\) (the latter for simplicity).\n\nWhat happens if we regress \\(Q_i\\) on \\(P_i\\)? \\[\nQ_i=\\beta^* P_i+\\varepsilon^*_i\n\\]\n\nTo answer this question, we need to solve \\(Q_i\\) and \\(P_i\\) in terms of the errors \\(\\varepsilon_{1i}\\) and \\(\\varepsilon_{2i}\\). \\[\n\\left(\\begin{matrix}\n1 & \\beta_1\\\\\n1 & -\\beta_2\\\\\n\\end{matrix}\n\\right)\n\\left(\\begin{matrix}\nQ_i\\\\\nP_i\\\\\n\\end{matrix}\n\\right) =\n\\left(\\begin{matrix}\n\\varepsilon_{1i}\\\\\n\\varepsilon_{2i}\\\\\n\\end{matrix}\n\\right)\n\\] Rewriting yields (assuming invertibility) \\[\n\\begin{align}\n\\left(\\begin{matrix}\nQ_i\\\\\nP_i\\\\\n\\end{matrix}\n\\right)\n&=\n\\left(\\begin{matrix}\n1 & \\beta_1\\\\\n1 & -\\beta_2\\\\\n\\end{matrix}\n\\right)^{-1}\n\\left(\\begin{matrix}\n\\varepsilon_{1i}\\\\\n\\varepsilon_{2i}\\\\\n\\end{matrix}\n\\right)\\\\[2ex]\n&=\n\\frac{1}{-\\beta_2 - \\beta_1}\n\\left(\\begin{matrix}\n-\\beta_2 &   -\\beta_1\\\\\n- 1    &           1\\\\\n\\end{matrix}\n\\right)\n\\left(\\begin{matrix}\n\\varepsilon_{1i}\\\\\n\\varepsilon_{2i}\\\\\n\\end{matrix}\n\\right)\\\\[2ex]\n&=\n\\left(\\begin{matrix}\n\\beta_2 &   \\beta_1\\\\\n1    &     -1\\\\\n\\end{matrix}\\right)\n\\left(\\begin{matrix}\n\\varepsilon_{1i}\\\\\n\\varepsilon_{2i}\\\\\n\\end{matrix}\\right)\n\\frac{1}{\\beta_1 +\\beta_2}\\\\[2ex]\n&=\n\\left(\n\\begin{matrix}\n(\\beta_2 \\varepsilon_{1i} + \\beta_1  \\varepsilon_{2i})/(\\beta_1 + \\beta_2)\\\\\n(\\varepsilon_{1i} - \\varepsilon_{2i})/(\\beta_1 + \\beta_2)\n\\end{matrix}  \n\\right)\n\\end{align}\n\\] Thus, under the simplification that \\(E(\\varepsilon_i\\varepsilon_i')=I_2\\) \\[\n\\begin{align*}\nE(P_iQ_i)\n&=\\frac{E\\left((\\varepsilon_{1i} - \\varepsilon_{2i})(\\beta_2 \\varepsilon_{1i} + \\beta_1  \\varepsilon_{2i})\\right)}{(\\beta_1 + \\beta_2)^2}\\\\\n&=\\frac{\\beta_2 E\\left(\\varepsilon_{1i}^2\\right) - \\beta_1 E\\left(\\varepsilon_{2i}^2\\right)}{(\\beta_1 + \\beta_2)^2}=\\frac{\\beta_2 - \\beta_1}{(\\beta_1 + \\beta_2)^2}\n\\end{align*}\n\\] \\[\n\\begin{align*}\nE(P_i^2)\n&=\\frac{E\\left((\\varepsilon_{1i} - \\varepsilon_{2i})^2\\right)}{(\\beta_1 + \\beta_2)^2}\\\\\n&=\\frac{E\\left(\\varepsilon_{1i}^2\\right) + E\\left(\\varepsilon_{2i}^2\\right)}{(\\beta_1 + \\beta_2)^2} = \\frac{2}{(\\beta_1 + \\beta_2)^2}\n\\end{align*}\n\\]\nThe (population) projection of \\(Q_i\\) on \\(P_i\\) yield \\[\nQ_i=\\beta^* P_i+\\varepsilon^*_i\n\\] with \\(E(P_i\\varepsilon^*_i)=0\\) and \\[\n\\beta^* = \\frac{E(P_iQ_i)}{E(P_i^2)} = \\frac{\\beta_2 - \\beta_1}{2}\n\\] The projection coefficient equals the average of the demand slope \\(\\beta_1\\) and the supply slope \\(\\beta_2.\\) The OLS estimator satisfies \\(\\hat\\beta\\to_p\\beta^*\\) and thus the limit does not equal one of the structural parameters \\(\\beta_1\\) or \\(\\beta_2.\\) This is called simultaneous equation bias.\nGenerally, when both the dependent variable and a regressor are simultaneously determined then the regressor should be treated as endogenous."
  },
  {
    "objectID": "07-Instrumental-Variables.html#endogenous-regressors",
    "href": "07-Instrumental-Variables.html#endogenous-regressors",
    "title": "7  Instrumental Variables",
    "section": "7.3 Endogenous Regressors",
    "text": "7.3 Endogenous Regressors\n\nUsually, only a subset of the regressors need to be treated as endogenous. In the following, we partition the vector of regressors \\[\n\\underset{(K\\times 1)}{X_i}=\n\\left(\n\\begin{matrix}\n\\underset{(K_1\\times 1)}{X_{i1}}\\\\\n\\underset{(K_2\\times 1)}{Y_{i2}}\n\\end{matrix}\n\\right)\n\\] with \\(K=K_1 + K_2\\).\n\n\\(X_{i1}\\): \\((K_1\\times 1)\\) vector of exogenous regressors, i.e. \\(E(X_{i1}\\varepsilon_i)=0\\)\n\n\\(Y_{i2}\\): \\((K_2\\times 1)\\) vector of endogenous regressors, i.e. \\(E(Y_{i2}\\varepsilon_i)\\neq 0\\)\n\nThe structural equation is then \\[\nY_{i1}=X_{i1}'\\beta_1 + Y_{i2}'\\beta_2 + \\varepsilon_i\n\\tag{7.5}\\]\n\n\\(Y_{i1}=Y_i\\): endogenous dependent variable, i.e. \\(E(Y_{i1}\\varepsilon_i)\\neq 0\\)\n\nThis notation clarifies which variables are exogenous and which endogenous."
  },
  {
    "objectID": "07-Instrumental-Variables.html#instruments",
    "href": "07-Instrumental-Variables.html#instruments",
    "title": "7  Instrumental Variables",
    "section": "7.4 Instruments",
    "text": "7.4 Instruments\nTo consistently estimate the structural parameter \\(\\beta\\) we need additional information. One type of information commonly used in econometrics are called instruments or instrumental variables.\n\nDefinition 7.1 (Instrumental Variable) The \\(\\ell\\times 1\\) random vector \\(Z_i\\) is called an instrumental variable for Equation 7.5 if \\[\nE(Z_i\\varepsilon_i)=0\n\\tag{7.6}\\] \\[\n\\operatorname{rank}\\Big(E(Z_iZ_i')\\Big)=\\ell  \n\\tag{7.7}\\] \\[\n\\operatorname{rank}\\Big(\\underbrace{E(Z_iX_i')}_{(\\ell\\times K)}\\Big)=K,\n\\tag{7.8}\\] where \\(\\underset{1\\times (K_1+K_2)}{X_i'}=(X_{i1}',Y_{i2}').\\)\n\nExplanation of Definition 7.1:\n\nEquation 7.6 states that the instruments are uncorrelated with the error term (exogeneity condition)\nEquation 7.7 excludes linearly redundant instruments\nEquation 7.8 is called the relevance condition and is essential for the identification of the structural parameter \\(\\beta\\); see Section 7.6.\n\nA necessary condition for Equation 7.8 is that \\(\\ell\\geq K.\\)\n\n\nNote that the exogenous regressors \\(X_{i1}\\) fulfill Equation 7.6 (they are valid instruments for themselves) and thus should be included as (additional) instrumental variables, i.e.  \\[\n\\underset{(\\ell\\times 1)}{Z_i}=\n\\underset{((K_1+\\ell_2)\\times 1)}{\\left(\\begin{matrix}Z_{i1}\\\\Z_{i2}\\end{matrix}\\right)} :=\n\\underset{((K_1+\\ell_2)\\times 1)}{\\left(\\begin{matrix}X_{i1}\\\\Z_{i2}\\end{matrix}\\right)}\n\\] with \\(\\ell=K_1+\\ell_2.\\)\nWith this notation we can also write the structural Equation 7.5 as \\[\nY_{i1} = Z_{i1}'\\beta_1 + Y_{i2}'\\beta_2 + \\varepsilon_i\n\\tag{7.9}\\]\n\n\\(Z_{i1}\\): (\\(K_1\\times 1\\)) vector of included exogenous variables \n\nuncorrelated with \\(\\varepsilon_i\\) and thus potentially usable\nincluded since they have potentially non-zero coefficients in Equation 7.9\n\n\\(Z_{i2}\\): (\\(\\ell_2\\times 1\\)) vector of excluded exogenous variables\n\nuncorrelated with \\(\\varepsilon_i\\) and thus potentially usable\nexcluded since they have zero coefficients1 in Equation 7.9\n\n\n\n\n\nWe say that the model is\n\njust-identified if \\(\\ell = K_1 + \\ell_2 = K\\)\nover-identified if \\(\\ell = K_1 + \\ell_2 > K\\)\n\n\nWhat variables can be used as Instrumental Variables (IVs)?\n\n\nInstrumental variables must be uncorrelated with the error term (Equation 7.6).\nInstrumental variables must be correlated with the endogenous variables \\(Y_{i2}\\) also after controlling for the other already included exogenous variables \\(Z_{i1}\\) (Equation 7.8).2 I.e. the IVs must contain additional information, and thus there must not be a perfect multicollinearity between the IVs, \\(Z_{i2},\\) and the already included exogenous regressors \\(Z_{i1}.\\)\n\n\nThese two requirements mean that the IVs, \\(Z_{i2},\\) …\n\n… are determined outside the systems for the endogenous variables \\(Y_{i1}\\) and \\(Y_{i2}.\\)\n… causally determine the endogenous variables \\(Y_{i2}.\\)\n… not causally determine the dependent variable \\(Y_{i1},\\) except indirectly through \\(Y_{i2}.\\)"
  },
  {
    "objectID": "07-Instrumental-Variables.html#reduced-form",
    "href": "07-Instrumental-Variables.html#reduced-form",
    "title": "7  Instrumental Variables",
    "section": "7.5 Reduced Form",
    "text": "7.5 Reduced Form\nThe reduced form models are projection models giving us the relationships\n\nbetween the \\((K_2\\times 1)\\) endogenous regressors \\(Y_{i2}\\) and the \\((\\ell\\times 1)\\) instruments \\(Z_{i}.\\)\nbetween the \\((1\\times 1)\\) endogenous regressors \\(Y_{i1}\\) and the \\((\\ell\\times 1)\\) instruments \\(Z_{i}.\\)\n\nThe linear reduced form model for \\(Y_{i2}\\) is given by the population projection model that regresses \\(Y_{i2}\\) on the exogenous instruments \\(Z_{i}:\\) \\[\n\\begin{align}\nY_{i2}\n&= \\Gamma'Z_i + u_{i2}\\\\\n&= \\Gamma_{12}'Z_{i1} + \\Gamma_{22}'Z_{i2} + u_{i2},\n\\end{align}\n\\tag{7.10}\\] which implies that \\(E(Z_iu_{i2}')=0.\\)\nThe \\((\\ell\\times K_2)\\) dimensional coefficient matrix \\(\\Gamma\\) is defined by the projection coefficient \\[\n\\underset{(\\ell\\times K_2)}{\\Gamma}=\\left(\\begin{matrix}\\underset{(K_1\\times K_2)}{\\Gamma_{12}}\\\\ \\underset{(\\ell_2\\times K_2)}{\\Gamma_{22}}\\end{matrix}\\right) = E(Z_iZ_i')^{-1}E(Z_iY_{i2}')\n\\tag{7.11}\\]\n\nNote: Equation 7.11 defines a matrix-valued regression/projection parameter and can be thought as lining up single ordinary vector-valued regression parameters \\([\\Gamma]_{\\cdot,l} = E(Z_iZ_i')^{-1}E(Z_iY_{i2,l}')\\), \\(l=1,\\dots,\\ell\\), where \\([\\Gamma]_{\\cdot,l}\\) is the \\(l\\)th column of \\(\\Gamma\\) and \\(Y_{i2,l}\\) the \\(l\\)th element of \\(Y_{i2}\\). More details can be found, for instance, in Chapter 11 of Hansen (2022).\n\nThe projection coefficient \\(\\Gamma\\) (Equation 7.11) is well defined and unique if Equation 7.7 holds since then \\(E(Z_iZ_i')\\) is invertible.\nThe linear reduced form projection model for \\(Y_{i1}\\) is derived from substituting the reduced form projection model for \\(Y_{i2}\\) (Equation 7.10) into the structural equation for \\(Y_{i1}\\) (Equation 7.9): \\[\n\\begin{align}\nY_{i1}\n&= Z_{i1}'\\beta_1 + Y_{i2}'\\beta_2 + \\varepsilon_i\\\\\n&= Z_{i1}'\\beta_1 + (\\Gamma_{12}'Z_{i1} + \\Gamma_{22}'Z_{i2} + u_{i2})'\\beta_2 + \\varepsilon_i\\\\\n&= Z_{i1}'\\beta_1 + Z_{i1}'\\Gamma_{12}\\beta_2 + Z_{i2}'\\Gamma_{22}\\beta_2  + u_{i2}'\\beta_2 + \\varepsilon_i\\\\\n&= Z_{i1}' (\\beta_1 + \\Gamma_{12}\\beta_2) + Z_{i2}' (\\Gamma_{22}\\beta_2)  + (u_{i2}'\\beta_2 + \\varepsilon_i)\\\\\n&= Z_{i1}' \\lambda_1 + Z_{i2}' \\lambda_2  + u_{i1}\\\\\n&= Z_{i}' \\lambda + u_{i1},\n\\end{align}\n\\tag{7.12}\\] where \\(u_{i1} = u_{i2}'\\beta_2 + \\varepsilon_i.\\) Since Equation 7.12 is a projection model, we have that \\(E(Z_iu_{i1})=0\\) such that the \\((\\ell\\times 1)\\) dimensional coefficient \\(\\lambda\\) is defined by the projection coefficient \\[\n\\underset{(\\ell\\times 1)}{\\lambda}=\\left(\\begin{matrix}\\underset{(K_1\\times 1)}{\\lambda_{1}}\\\\ \\underset{(\\ell_2\\times 1)}{\\lambda_{2}}\\end{matrix}\\right) = E(Z_iZ_i')^{-1}E(Z_iY_{i})\n\\tag{7.13}\\]\nNote that the thus estimable projection parameter \\(\\lambda\\) of the reduced form projection model for \\(Y_{i1}\\) (Equation 7.12) can written as a linear function \\(\\bar{\\Gamma}\\beta\\) of the (not directly estimable) structural parameter \\(\\beta\\): \\[\n\\begin{align*}\n\\underset{(\\ell\\times 1)}{\n\\lambda} =\n\\left(\n\\begin{matrix}\n\\lambda_1\\\\\n\\lambda_2\n\\end{matrix}\n\\right)\n&=\n\\left(\n\\begin{matrix}\n\\beta_1 + \\Gamma_{12}\\beta_2 \\\\\n\\Gamma_{22}\\beta_2\n\\end{matrix}\n\\right) \\\\\n&=\n\\underset{(\\ell\\times (K_1+K_2))}{\n\\left[\n\\begin{matrix}\nI_{K_1} &  \\Gamma_{12}\\\\\n0 &  \\Gamma_{22}\n\\end{matrix}\n\\right]}\n\\underset{((K_1+K_2)\\times 1)}{\n\\left(\n\\begin{matrix}\n\\beta_1\\\\\n\\beta_2\n\\end{matrix}\n\\right)} = \\bar{\\Gamma} \\beta.\n\\end{align*}\n\\tag{7.14}\\] Luckily, the unknown components \\(\\Gamma_{12}\\) and \\(\\Gamma_{12}\\) of \\(\\bar{\\Gamma}\\) are together the estimable projection parameter \\(\\Gamma\\) (Equation 7.11). Moreover, we can generalize Equation 7.11 for \\(\\Gamma\\) to an expression for \\(\\bar{\\Gamma}\\):  \\[\n\\underset{(\\ell\\times K )}{\\bar{\\Gamma}}=\\left[\\begin{matrix}I_{K_1}&\\underset{(K_1\\times K_2)}{\\Gamma_{12}}\\\\ \\underset{(\\ell_2\\times K_1)}{0}& \\underset{(\\ell_2\\times K_2)}{\\Gamma_{22}}\\end{matrix}\\right] = E(Z_iZ_i')^{-1}E\\big(Z_{i}\\;\\overbrace{\\underset{(1\\times (K_1+K_2))}{(Z_{i1}',Y_{i2}')}}^{=X_i'}\\;\\big)\n\\tag{7.15}\\]\n\nThe least squares estimators of the regression/projection parameters\n\n\\(\\Gamma\\) (Equation 7.11)\n\\(\\bar{\\Gamma}\\) (Equation 7.15)\n\\(\\lambda\\) (Equation 7.13)\n\nare given by the following moment estimators: \\[\n\\begin{align*}\n\\hat{\\Gamma}\n&= \\left(\\sum_{i=1}^nZ_iZ_i'\\right)^{-1}\\left(\\sum_{i=1}^nZ_iY_{i2}'\\right)\\\\\n&= \\left(Z'Z\\right)^{-1}(Z'Y_{2})\n\\end{align*}\n\\tag{7.16}\\] \\[\n\\begin{align*}\n\\widehat{\\bar{\\Gamma}}\n&= \\left(\\sum_{i=1}^nZ_iZ_i'\\right)^{-1}\\left(\\sum_{i=1}^nZ_i X_i'\\right)\\\\\n&= \\left(Z'Z\\right)^{-1}(Z'X)\n\\end{align*}\n\\tag{7.17}\\] \\[\n\\begin{align*}\n\\hat{\\lambda}\n&= \\left(\\sum_{i=1}^nZ_iZ_i'\\right)^{-1}\\left(\\sum_{i=1}^nZ_iY_{i1}\\right)\\\\\n&= \\left(Z'Z\\right)^{-1}(Z'Y_1)\n\\end{align*}\n\\tag{7.18}\\] where\n\n\\(Z\\) is a \\((n\\times \\ell)\\) matrix with \\(Z_i'=(Z_{i1}',Z_{i2}')\\) in the \\(i\\)th row\n\\(X\\) is a \\((n\\times K)\\) matrix with \\(X_i'=(X_{i1}',Y_{i2}')\\) in the \\(i\\)th row\n\\(Y_1\\) is a \\((n\\times 1)\\) vector with \\(Y_{i1}\\) in the \\(i\\)th element\n\\(Y_2\\) is a \\((n\\times K_2)\\) matrix with \\(Y_{i2}'\\) in the \\(i\\)th row\n\nwith \\(i=1,\\dots,n\\)"
  },
  {
    "objectID": "07-Instrumental-Variables.html#sec-identification",
    "href": "07-Instrumental-Variables.html#sec-identification",
    "title": "7  Instrumental Variables",
    "section": "7.6 Identification",
    "text": "7.6 Identification\nA parameter is identified if it is a unique function of the probability distribution of the observables.\nOne way to show that a parameter is identified is to write it as an explicit function of population moments. For example, the reduced form coefficient matrices \\(\\Gamma\\) and \\(\\lambda\\) are identified because they can be written as explicit functions of the moments of the variables \\((Y_{i1},Y_{i2},X_i,Z_i).\\) That is, \\[\n\\underset{(\\ell\\times K_2)}{\\Gamma}=\\left(\\begin{matrix}\\underset{(K_1\\times K_2)}{\\Gamma_{12}}\\\\ \\underset{(\\ell_2\\times K_2)}{\\Gamma_{22}}\\end{matrix}\\right) = E(Z_iZ_i')^{-1}E(Z_iY_{i2}')\n\\] and \\[\n\\underset{(\\ell\\times 1)}{\\lambda}=\\left(\\begin{matrix}\\underset{(K_1\\times 1)}{\\lambda_{1}}\\\\ \\underset{(\\ell_2\\times 1)}{\\lambda_{2}}\\end{matrix}\\right) = E(Z_iZ_i')^{-1}E(Z_iY_{i})\n\\] are uniquely determined by the probability distribution of \\((Y_{i1},Y_{i2},Z_i)\\) if Definition 7.1 holds because this definition includes the requirement that \\(E(Z_iZ_i')\\) has full rank (Equation 7.7).\nWe are interested in the structural parameter \\(\\beta,\\) which relates to \\(\\Gamma\\) and \\(\\lambda\\) through equation Equation 7.14; i.e. through \\[\n\\lambda = \\underset{(\\ell\\times K)}{\\bar{\\Gamma}}\\beta.\n\\]\nThe structural parameter \\(\\beta\\) is identified if it is uniquely determined by this relation, which is a set of \\(\\ell\\) equations with \\(K\\) unknown, where \\(\\ell\\geq K.\\) Form linear algebra we know that there is a unique solution if and only if \\(\\bar{\\Gamma}\\) has full column rank \\(K\\) \\[\n\\operatorname{rank}\\left(\\bar{\\Gamma}\\right)=K.\n\\tag{7.19}\\] \nFrom Equation 7.15 we know that we can write \\(\\bar{\\Gamma}\\) as \\[\n\\bar{\\Gamma}=\\left[\\begin{matrix}I_{K_1}&\\underset{(K_1\\times K_2)}{\\Gamma_{12}}\\\\ 0& \\underset{(\\ell_2\\times K_2)}{\\Gamma_{22}}\\end{matrix}\\right] = E(Z_iZ_i')^{-1}E(Z_iX_i').\n\\] Combining this with Equation 7.14, we obtain \\[\n\\begin{align*}\n\\overbrace{E(Z_iZ_i')^{-1}E(Z_iY_{i})}^{\\lambda} & = \\overbrace{E(Z_iZ_i')^{-1}E(Z_iX_i')}^{\\bar{\\Gamma}} \\beta \\\\\n\\Leftrightarrow \\underset{(\\ell\\times 1)}{E(Z_iY_{i})} & =  \\underset{(\\ell\\times K)}{E(Z_iX_i')} \\underset{(K\\times 1)}{\\beta}\n\\end{align*}\n\\] which is a set of \\(\\ell\\) equations in \\(K\\) unknowns. This system of linear equations has a unique solution if and only if \\[\n\\operatorname{rank}\\left(E(Z_iX_i')\\right)=K\n\\] which is the required identification/relevance condition (Equation 7.8) in the definition of instrumental variables (Definition 7.1).\nIt is useful to have explicit expressions for the solution \\(\\beta.\\) The easiest case is when \\(\\ell = K,\\) because then \\(\\beta=E(Z_iX_i')^{-1}E(Z_iY_{i})\\) and we can get an estimator for \\(\\beta\\) by substituting the population moments with sample moments (methods of moments estimator).\nThe two-state least squares estimator, discussed in the next section, allows us to consider the information of more instruments \\(\\ell\\geq K.\\)"
  },
  {
    "objectID": "07-Instrumental-Variables.html#two-stage-least-squares",
    "href": "07-Instrumental-Variables.html#two-stage-least-squares",
    "title": "7  Instrumental Variables",
    "section": "7.7 Two-Stage Least Squares",
    "text": "7.7 Two-Stage Least Squares\nFrom the reduced form projection Equation 7.12 and Equation 7.14 (i.e. \\(\\lambda = \\bar{\\Gamma}\\beta\\)), we have \\[\n\\begin{align*}\nY_i\n& = Z_i' \\lambda + u_{i1}  \\\\\n& = Z_i' \\bar{\\Gamma}\\beta + u_{i1}  \\\\\nE(Z_iu_{i1}) & = 0.   \n\\end{align*}\n\\] The parameter \\(\\bar{\\Gamma}\\) is unknown, but consistently estimable (see Equation 7.17).\nLet us, for a moment, assume we would know \\(\\bar{\\Gamma}\\) and let\n\\[\nW_{i}=\\bar{\\Gamma}'Z_i.\n\\] \\(W_{i}\\) is thus a specific linear combination of the \\(\\ell\\) instruments \\(Z_i\\) which allows us to identify the structural parameter \\(\\beta\\) in a regression/projection model \\[\n\\begin{align*}\nY_i & = W_i'\\beta + u_{i1}  \\\\\nE(W_iu_{i1}) & = 0.\n\\end{align*}\n\\]\nThus, if we would know \\(\\bar{\\Gamma}\\), we would estimate \\(\\beta\\) by \\[\n\\begin{align*}\n\\hat{\\beta}\n& = \\left(\\sum_{i=1}^nW_iW_i'\\right)^{-1}\\left(\\sum_{i=1}^nW_iY_{i1}\\right)\\\\\n& = \\left(W'W\\right)^{-1}\\left(WY_{1}\\right)\\\\\n& = \\left(\\bar{\\Gamma}'Z'Z\\bar{\\Gamma}\\right)^{-1}\\left(\\bar{\\Gamma}'Z'Y_{1}\\right)\\\\\n\\end{align*}\n\\] To make \\(\\hat{\\beta}\\) a feasible estimator, we simply replace the unknown \\(\\bar{\\Gamma}\\) by its estimator (Equation 7.17) \\[\n\\begin{align*}\n\\widehat{\\bar{\\Gamma}}\n& = \\left(\\sum_{i=1}^nZ_iZ_i'\\right)^{-1}\\left(\\sum_{i=1}^nZ_iX_{i}'\\right)\\\\\n& = \\left(Z'Z\\right)^{-1}\\left(Z'X\\right),\n\\end{align*}\n\\] where the \\((n\\times \\ell)\\) dimensional matrix \\(Z\\) contains in the \\(i\\)th row the \\((1\\times \\ell)\\) vector \\(Z_i',\\) and where the \\((n\\times K)\\) dimensional matrix \\(X\\) contains in the \\(i\\)th row the \\((1\\times K)\\) vector \\(X_i'.\\)\nThis then yields the Two Stage Lease Squares (2SLS) estimator: \\[\n\\begin{align*}\n\\hat{\\beta}_{2SLS}\n& = \\left(\\widehat{\\bar{\\Gamma}}'Z'Z\\widehat{\\bar{\\Gamma}}\\right)^{-1}\\left(\\widehat{\\bar{\\Gamma}}'Z'Y_{1}\\right)\\\\\n& = \\left(X'Z\\left(Z'Z\\right)^{-1}Z'Z\\left(Z'Z\\right)^{-1}Z'X\\right)^{-1}\\left(X'Z\\left(Z'Z\\right)^{-1}Z'Y_{1}\\right)\\\\\n& = \\left(X'Z\\left(Z'Z\\right)^{-1}Z'X\\right)^{-1}\\left(X'Z\\left(Z'Z\\right)^{-1}Z'Y_{1}\\right)\\\\\n& = \\left(X'P_ZX\\right)^{-1}\\left(X'P_ZY_{1}\\right),\\\\\n\\end{align*}\n\\] where \\(P_Z=Z\\left(Z'Z\\right)^{-1}Z'\\) is the projection matrix that projects into the vector spaced spanned by the columns of \\(Z.\\)\nThe projection matrix \\(P_Z\\) motivates the name “Two Stage Least Squares”, since computing \\(\\hat{\\beta}_{2SLS}\\) is equivalent to conduct the following two stage procedure:\n\nCompute the fitted values when regressing \\(X\\) on \\(Z\\): \\(\\hat{X}=P_ZX\\)\nRegress \\(Y_1\\) on \\(\\hat{X}:\\;\\) \\(\\hat{\\beta}_{2SLS}=\\left(\\hat{X}'\\hat{X}\\right)^{-1}\\hat{X}'Y_1\\)\n\nThis two stage approach is indeed equivalent to the above direct approach: \\[\n\\begin{align*}\n\\left(\\hat{X}'\\hat{X}\\right)^{-1}\\hat{X}'Y_1\n& = \\left((P_ZX)'P_ZX\\right)^{-1}\\left((P_ZX)'Y_{1}\\right)\\\\\n& = \\left(X'P_Z'P_ZX\\right)^{-1}\\left(X'P_Z'Y_{1}\\right)\\\\\n& = \\left(X'P_ZP_ZX\\right)^{-1}\\left(X'P_ZY_{1}\\right)\\\\\n& = \\left(X'P_ZX\\right)^{-1}\\left(X'P_ZY_{1}\\right) = \\hat{\\beta}_{2SLS}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "07-Instrumental-Variables.html#monte-carlo-simulation",
    "href": "07-Instrumental-Variables.html#monte-carlo-simulation",
    "title": "7  Instrumental Variables",
    "section": "7.8 Monte-Carlo Simulation",
    "text": "7.8 Monte-Carlo Simulation\nLet’s consider the following data generating process with endogenous regressor \\(X_{i3}\\): \\[\n\\begin{align*}\n\\overbrace{Y_i}^{=Y_{i1}}\n& = \\beta_1 + \\beta_2 X_{i2}  + \\beta_3 \\overbrace{X_{i3}}^{=Y_{i2}} + \\varepsilon_i\\\\\n& = X_i'\\beta + \\varepsilon_i,\n\\end{align*}\n\\] where \\(X_i'=(1,X_{i2},X_{i3})\\) with \\[\n\\begin{align*}\n\\beta         & = (\\beta_1,\\beta_2,\\beta_3)'=(2,2,2)'\\\\\n\\varepsilon_i & \\sim \\mathcal{N}(0,1)\\\\\nX_{i2}        & \\sim \\mathcal{N}(3,4)\\\\\nV_{i}         & \\sim \\mathcal{N}(1,9)\\\\\nX_{i3}        & = V_{i}  + 2 \\varepsilon_i\n\\end{align*}\n\\]\nThe following R code draws B=10000 many i.i.d. samples from this data generating process and checks the (un-)biasedness of the OLS estimators \\(\\hat\\beta_2\\) and \\(\\hat\\beta_2\\):\n\nn         <- 100      # Sample size \nbeta_true <- c(2,2,2) # Structural parameter\n\nB         <- 10000    # Number of MC replications\nbeta_hat  <- matrix(data = NA, # Container for \n                    nrow = 3,  # MC simulations\n                    ncol = B)\n\n## Monte Carlo (MC) simulation for generating \n## estimates when X3 is endogenous for beta3:\nfor(b in 1:B){\n  eps          <- rnorm(n)                # error term \n  X_2          <- rnorm(n, m = 3, sd = 2) # exogenous\n  V            <- rnorm(n, m = 1, sd = 3) # exogenous\n  X_3          <- V + 2 * eps             # endogenous\n  ## Dependent variable\n  Y            <- cbind(1, X_2, X_3) %*% beta_true + eps\n  ## Estimation\n  beta_hat[,b] <- coef(lm(Y ~ X_2 + X_3))\n}\n\n## MC means of \\hat{\\beta}_2 and \\hat{\\beta}_3 \nmean_beta_hat_2 <- round(mean(beta_hat[2,]), 2)\nmean_beta_hat_3 <- round(mean(beta_hat[3,]), 2)\n\n## Unbiased estimator \\hat{\\beta}_2:\nmean_beta_hat_2  - beta_true[2]\n\n[1] 0\n\n## Biased estimator \\hat{\\beta}_3:\nmean_beta_hat_3  - beta_true[3]\n\n[1] 0.15\n\n\n\nThe estimator \\(\\hat\\beta_3\\) is clearly biased due to the endogeneity of \\(X_{i3}.\\)\n\nInstrumental Variable\nLet’s say there is a real-valued instrumental variable \\[\nZ_{i2}=V_{i} + u_i,\\quad \\text{where}\\quad u_i\\sim\\mathcal{U}(0,1).\n\\] That is, \\(Z_{i2}\\) correlates with the endogenous \\(X_{i3}\\) through \\(V_i,\\) but is independent of \\(\\varepsilon_i.\\)\nThe \\(\\ell=3\\) dimensional instrumental variables vector \\(Z_i\\) collects all exogenous information, i.e. \\[\nZ_i =\n\\left(\\begin{matrix}\n1\\\\\nX_{i2}\\\\\nZ_{i2}\n\\end{matrix}\\right).\n\\]\nThe \\(Z_i\\) vector is a valid instrumental variables vector, if it fulfills the conditions of Equation 7.6 – Equation 7.8 of Definition 7.1.\n\nChecking Equation 7.6, i.e., \\(E(Z_i\\varepsilon_i)=0\\):  \\[\n\\begin{align*}\nE(Z_i\\varepsilon_i)\n&=\n\\left(\\begin{matrix}\nE(1\\varepsilon_i)\\\\\nE(X_{i2}\\varepsilon_i)\\\\\nE(Z_{i2}\\varepsilon_i)\n\\end{matrix}\\right)\n=\n\\left(\\begin{matrix}\n\\operatorname{Cov}(1,\\varepsilon_i)\\\\\n\\operatorname{Cov}(X_{i2},\\varepsilon_i)\\\\\n\\operatorname{Cov}(Z_{i2},\\varepsilon_i)\n\\end{matrix}\\right)\\\\\n&=\n\\left(\\begin{matrix}\n0\\\\\n0\\\\\n\\underbrace{\\operatorname{Cov}(V_{i},\\varepsilon_i)}_{=0} + \\underbrace{\\operatorname{Cov}(u_i,\\varepsilon_i)}_{=0}\n\\end{matrix}\\right)\n=\\underset{(\\ell\\times 1)}{0}\n\\end{align*}\n\\] Thus, the instruments do not correlate with the error term \\(\\varepsilon_i\\). \nChecking Equation 7.7, i.e., \\(\\operatorname{rank}(E(Z_iZ_i'))=\\ell=3\\):  \\[\n\\begin{align*}\n&\\operatorname{rank}(E(Z_iZ_i'))\\\\\n=&\\operatorname{rank}\\left(\n\\begin{matrix}\n1 \\cdot\\;1                   & E\\left(1\\;\\cdot X_{i2}\\right)  & E\\left(1\\;\\cdot Z_{i2}\\right) \\\\\nE\\left(X_{i2}\\cdot\\;1\\right) & E\\left(X_{i2} X_{i2}\\right)    & E\\left(X_{i2} Z_{i2}\\right) \\\\\nE\\left(Z_{i2}\\cdot\\;1\\right) & E\\left(X_{i2} Z_{i2}\\right)    & E\\left(Z_{i2} Z_{i2}\\right) \\\\\n\\end{matrix}\\right)\n\\end{align*}\n\\] A linear dependency would occur only …\n\nif \\(Z_{i2}= 1\\) (between 1st and 3rd column)\nif \\(Z_{i2}= X_{i2}\\) (between 2nd and 3rd column)\nif \\(Z_{i2}= 0\\) (between zero column)\n\n\nThus the “no redundancies” condition, \\(\\operatorname{rank}(E(Z_iZ_i'))=3\\), is fulfilled since \\[\n\\begin{align*}\nZ_{i2}=V_i + u_i\n& \\neq 1\\\\\n&\\neq X_{i2}\\\\\n&\\neq 0.\n\\end{align*}\n\\]\n\nChecking Equation 7.8, i.e., \\(\\operatorname{rank}(E(Z_iX_i'))=K=3\\):\n\\[\n\\begin{align*}\n&\\operatorname{rank}(E(Z_iX_i'))\\\\\n=&\\operatorname{rank}\\left(\n\\begin{matrix}\n1  \\cdot\\;1                  & E\\left(1\\;\\cdot X_{i2}\\right) & E\\left(1\\cdot X_{i3}\\right) \\\\\nE\\left(X_{i2}\\cdot\\;1\\right) & E\\left(X_{i2} X_{i2}\\right)    & E\\left(X_{i2} X_{i3}\\right) \\\\\nE\\left(Z_{i2}\\cdot\\;1\\right) & E\\left(Z_{i2} X_{i2}\\right)    & E\\left(Z_{i2} X_{i3}\\right) \\\\\n\\end{matrix}\\right)\n\\end{align*}\n\\]\nSince \\(X_{i2}\\) and \\(X_{i3}\\) are independent, \\(E(X_{i2} X_{i3})=E(X_{i2})E(X_{i3})\\), which yields \\[\n\\begin{align*}\n&\\operatorname{rank}(E(Z_iX_i'))\\\\\n=&\\operatorname{rank}\\left(\n\\begin{matrix}\n1\\cdot\\;1                    & E\\left(1\\;\\cdot X_{i2}\\right)& \\;\\;1\\;\\cdot\\;\\; E\\left(X_{i3}\\right) \\\\\nE\\left(X_{i2}\\right)\\cdot\\;1 & E\\left(X_{i2} X_{i2}\\right)  & E\\left(X_{i2}\\right)\\cdot E\\left(X_{i3}\\right) \\\\\nE\\left(Z_{i2}\\right)\\cdot\\;1 & E\\left(Z_{i2} X_{i2}\\right)  & E\\left(Z_{i2}\\;\\;\\cdot\\;\\;X_{i3}\\right) \\\\\n\\end{matrix}\\right)  \n\\end{align*}\n\\] Thus, to guarantee that \\(\\operatorname{rank}(E(Z_iX_i'))=3\\) we need that \\[\nE\\left(Z_{i2} X_{i3}\\right)\\neq E\\left(Z_{i2}\\right)E\\left(X_{i3}\\right)\n\\] otherwise the first and the third column are linearly dependent.  Note that \\[\n\\begin{align*}\nE(Z_{i2} X_{i3}) \\neq  E(Z_{i2}) E(X_{i3}) \\Leftrightarrow \\operatorname{Cov}(Z_{i2},X_{i3}) \\neq 0\n\\end{align*}\n\\] since \\[\n\\begin{align*}\n\\operatorname{Cov}(Z_{i2},X_{i3})&=E(Z_{i2} X_{i3}) - E(Z_{i2}) E(X_{i3})\\\\\n%\\Leftrightarrow\n%E(Z_{i2} X_{i3}) &= E(Z_{i2}) E(X_{i3}) + \\underbrace{\\operatorname{Cov}(Z_{i2},X_{i3})}_{\\neq 0\\;?}.\n\\end{align*}\n\\] That is, we need that the instrument \\(Z_{i3}\\) is correlated with the endogenous variable \\(X_{i3}\\), i.e. that \\(\\operatorname{Cov}(Z_{i2},X_{i3}) \\neq 0.\\) This is fulfilled here since \\[\n\\begin{align*}\n\\operatorname{Cov}(Z_{i2},X_{i3})\n&=\\operatorname{Cov}(V_{i}+u_i,X_{i3})\\\\\n&=\\operatorname{Cov}(V_{i},X_{i3})+\\operatorname{Cov}(u_i,X_{i3})\\\\\n&=\\operatorname{Cov}(V_{i},X_{i3})+0\\\\  \n&=\\operatorname{Cov}(V_{i},V_i+2\\varepsilon_{i})\\\\  \n&=\\operatorname{Cov}(V_{i},V_i) + 2\\operatorname{Cov}(V_{i},\\varepsilon_{i})\\\\  \n&=\\operatorname{Var}(V_{i}) + 0 \\\\\n&=9\n\\end{align*}\n\\]\n\nThat is, the instrumental variables vector \\(Z_i=(1,X_{i2},Z_{i2})'\\) is a valid instrument.\n\n\nR function ivreg()\nThe function ivreg() from the R package AER carries out 2SLS estimation. It is used similarly as the lm() function. Instruments can be added to the usual specification of the regression formula using a vertical bar separating the model equation from the instruments. Thus, for the regression at hand the correct formula is:\n\nivreg(Y ~ X_2 + X_3 | X_2 + Z_2)\n\n\nlibrary(\"AER\")             # contains ivreg()\n\nn              <- 100      # Sample size \nbeta_true      <- c(2,2,2) # Structural parameter\n\nB              <- 10000    # Number of MC replications\nbeta_hat_2SLS  <- matrix(data = NA, # Container for \n                         nrow = 3,  # MC simulations\n                         ncol = B)\n\n## Monte Carlo (MC) simulation for generating \n## estimates when X3 is endogenous for beta3:\nfor(b in 1:B){\n  eps          <- rnorm(n)                # error term \n  X_2          <- rnorm(n, m = 3, sd = 2) # exogenous\n  V            <- rnorm(n, m = 1, sd = 3) # exogenous\n  X_3          <- V + 2 * eps             # endogenous\n  ##\n  Z_2          <- V + runif(n, 0, 1)      # Instrument\n  ## Dependent variable\n  Y            <- cbind(1, X_2, X_3) %*% beta_true + eps\n  ## 2SLS Estimation\n  beta_hat_2SLS[,b] <- coef(ivreg(Y ~ X_2 + X_3 | X_2 + Z_2))\n}\n\n## MC means of \\hat{\\beta}_{2SLS,2} and \\hat{\\beta}_{2SLS,3} \nmean_beta_hat_2SLS_2 <- round(mean(beta_hat_2SLS[2,]), 2)\nmean_beta_hat_2SLS_3 <- round(mean(beta_hat_2SLS[3,]), 2)\n\n## Unbiased estimator \\hat{\\beta}_{2SLS,2}:\nmean_beta_hat_2SLS_2  - beta_true[2]\n\n[1] 0\n\n## Unbiased estimator \\hat{\\beta}_{2SLS,3}:\nmean_beta_hat_2SLS_3  - beta_true[3]\n\n[1] 0"
  },
  {
    "objectID": "07-Instrumental-Variables.html#references",
    "href": "07-Instrumental-Variables.html#references",
    "title": "7  Instrumental Variables",
    "section": "7.9 References",
    "text": "7.9 References\n\n\n\n\nHaavelmo, Trygve. 1944. “The Probability Approach in Econometrics.” Econometrica 12: iii-vi+1-115.\n\n\nHansen, Bruce. 2022. Econometrics. Princeton University Press."
  },
  {
    "objectID": "04-Monte-Carlo-Simulations.html#bias-of-theta",
    "href": "04-Monte-Carlo-Simulations.html#bias-of-theta",
    "title": "3  Monte Carlo Simulations",
    "section": "3.4 Bias of \\(\\theta\\)",
    "text": "3.4 Bias of \\(\\theta\\)\nThe bias of an estimator \\(\\hat\\theta_n\\) is defined as\n\\[\n\\operatorname{Bias}\\left(\\hat\\theta_n\\right) = E\\left(\\hat\\theta_n\\right) - \\theta.\n\\] ::: :::\nWe would like to have unbiased estimators \\(\\operatorname{Bias}\\left(\\hat\\theta_n\\right)=0\\) or at least asymptotically unbiased estimators \\(\\lim_{n\\to\\infty}\\operatorname{Bias}\\left(\\hat\\theta_n\\right)=0\\). If the bias of an estimator is small (or zero), we know that the estimator will have a distribution that is centered around the true (usually unknown) parameter \\(\\theta\\); however, such an estimator may still vary a lot around \\(\\theta\\). Therefore, is is also important to assess the variance of the estimator.\n\nDefinition 3.1 (Variance and Standard Error of \\(\\theta\\)) The variance of an estimator \\(\\hat\\theta_n\\) is defined equivalently to the variance of any other random variable\n\\[\nVar\\left(\\hat\\theta_n\\right) = E\\left[\\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)^2\\right].\n\\] The square root of the variance of an estimator is called standard error (not standard deviation) of \\(\\hat\\theta_n\\), \\[\n\\operatorname{SE}\\left(\\hat\\theta_n\\right) = \\sqrt{Var\\left(\\hat\\theta_n\\right)}.\n\\]\n\nWe would like to have estimators with a small as possible variance, and the variance should decline as the sample size increases, such that \\(\\lim_{n\\to\\infty}Var\\left(\\hat\\theta_n\\right)=0\\).\n\nDefinition 3.2 (Mean Squared Error of \\(\\theta\\)) The mean squared error of an estimator \\(\\hat\\theta_n\\) is defined as\n\\[\n\\operatorname{MSE}\\left(\\hat\\theta_n\\right) =  E\\left[\\left(\\hat\\theta_n - \\theta\\right)^2\\right].\n\\]\n\nWe would like to have estimators with a small as possible mean squared error, and the mean squared error should decline as the sample size increases, such that \\(\\lim_{n\\to\\infty}\\operatorname{MSE}\\left(\\hat\\theta_n\\right)=0\\).\nThe following holds true:\n\nThe mean squared error equals the sum of the squared bias and the variance:\n\n\\[\n\\operatorname{MSE}\\left(\\hat\\theta_n\\right) = \\left(\\operatorname{Bias}\\left(\\hat\\theta_n\\right)\\right)^2 +  Var\\left(\\hat\\theta_n\\right)\n\\]\n\nFor unbiased estimators (i.e. \\(E(\\hat\\theta_n)=\\theta\\)) the mean squared error equals the variance, i.e.\n\n\\[\n\\underbrace{E\\left[\\left(\\hat\\theta_n - \\theta\\right)^2\\right]}_{\\operatorname{MSE}\\left(\\hat\\theta_n\\right)} = \\underbrace{E\\left[\\left(\\hat\\theta_n - E\\left(\\hat\\theta_n\\right)\\right)^2\\right]}_{ Var\\left(\\hat\\theta_n\\right)}\n\\]\nUnfortunately, it is often difficult to derive the above assessment metrics for given sample sizes \\(n\\) and given data distributions \\(F_X\\). Monte Carlo simulations allow us to solve this issue."
  },
  {
    "objectID": "04-Monte-Carlo-Simulations.html#approximating-bias-variance-and-mse-using-mc-simulations",
    "href": "04-Monte-Carlo-Simulations.html#approximating-bias-variance-and-mse-using-mc-simulations",
    "title": "\n4  Estimation Theory and Monte Carlo Simulations\n",
    "section": "\n4.3 Approximating Bias, Variance, and MSE using MC Simulations",
    "text": "4.3 Approximating Bias, Variance, and MSE using MC Simulations\nAny of the the above assessment metrics requires us to compute means, i.e. \\(E(\\cdot)\\), of random variables:\n\nFor the \\(\\operatorname{Bias}\\left(\\hat\\theta_n\\right)\\) we need to compute \\(E\\left(\\hat\\theta_n\\right)-\\theta\\)\nFor the \\(Var\\left(\\hat\\theta_n\\right)\\) we need to compute \\(E\\left[\\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)^2\\right]\\).\nFor the \\(\\operatorname{MSE}\\left(\\hat\\theta_n\\right)\\) we need to compute \\(E\\left[\\left(\\hat\\theta_n - \\theta\\right)^2\\right]\\).\n\nWe can use a Monte Carlo (MC) simulation to approximate these means by sample means using the law of large numbers which states that a sample mean over iid random variables is able to approximate the population mean of these random variables as the number of random variables to average over gets large (see Theorem 7.5 in Chapter 7).\nThe core part of a MC-simulation generates a large number of \\(B\\) (e.g. \\(B=10,000\\)) many realizations of \\(\\hat{\\theta}_n\\) for\n\na given sample size \\(n\\) and\na given data distribution \\(F_X.\\)\n\n\n\n\n\n\n\n\nCore Part of the MC-Simulation Algorithm:\n\n\n\n\n1. Step: Use a (pseudo-)random number generator to draw a realization of the random sample \\(\\{X_1,\\dots,X_n\\}\\) for a given distribution \\(F_X\\) and a given sample size \\(n:\\) \\[\\begin{align*}\n&(X_{1,j,obs},\\dots,X_{n,j,obs})\\\\\n%&(X_{1,2,obs},\\dots,X_{n,2,obs})\\\\\n%& \\hspace{2cm}\\vdots \\\\\n%&(X_{1,B,obs},\\dots,X_{n,B,obs})\n\\end{align*}\\]\n2. Step: Compute the corresponding realization of \\(\\hat{\\theta}_n:\\) \\[\\begin{align*}\n\\hat\\theta_{n,j,obs} &= \\hat\\theta(X_{1,j,obs},\\dots,X_{n,j,obs})\\\\\n%\\hat\\theta_{n,2,obs} &= \\hat\\theta(X_{1,2,obs},\\dots,X_{n,2,obs})\\\\\n%&\\vdots\\\\\n%\\hat\\theta_{n,B,obs} &= \\hat\\theta(X_{1,B,obs},\\dots,X_{n,B,obs})\n\\end{align*}\\]\nRepeat the two steps above for \\(j=1,\\dots,B\\) with \\(B\\) being a large number (e.g. \\(B=10,000\\)) to generate \\(B\\) independent realizations of \\(\\hat\\theta_n\\) \\[\\begin{align*}\n\\hat{\\theta}_{n,1,obs} &= \\hat\\theta(X_{1,1,obs},\\dots,X_{n,1,obs})\\\\\n\\hat{\\theta}_{n,2,obs} &= \\hat\\theta(X_{1,2,obs},\\dots,X_{n,2,obs})\\\\\n&\\;\\vdots\\\\\n\\hat{\\theta}_{n,B,obs} &= \\hat\\theta(X_{1,B,obs},\\dots,X_{n,B,obs})\n\\end{align*}\\]\n\n\nWe can use the above core part of the MC-Simulation algorithm to generate \\(B\\) many realizations of the estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}_{n,1,obs},\\dots,\\hat{\\theta}_{n,B,obs}\n\\] which allow us to compute the following MC-approximations based on sample means over the generated \\(B\\) realizations of \\(\\hat\\theta_n\\):\n\nThe bias of \\(\\operatorname{Bias}\\left(\\hat\\theta_n\\right)=E\\left(\\hat\\theta_n\\right)-\\theta\\) can be approximated by\n\n\\[\n\\widehat{\\operatorname{Bias}}_{MC}\\left(\\hat\\theta_n\\right) = \\left(\\frac{1}{B}\\sum_{b=1}^B \\hat\\theta_{n,b,obs}\\right) - \\theta\n\\]\n\nThe variance \\(Var\\left(\\hat\\theta_n\\right)=E\\left[\\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)^2\\right]\\) can be approximated by \\[\n\\widehat{Var}_{MC}\\left(\\hat\\theta_n\\right) = \\frac{1}{B}\\sum_{b=1}^B \\left(\\hat\\theta_{n,b,obs} - \\left(\\frac{1}{B}\\sum_{b=1}^B \\hat\\theta_{n,b,obs}\\right)\\right)^2\n\\]\nThe mean squared error \\(\\operatorname{MSE}\\left(\\hat\\theta_n\\right)=E\\left[\\left(\\hat\\theta_n - \\theta\\right)^2\\right]\\) can be approximated by \\[\n\\widehat{\\operatorname{MSE}}_{MC}\\left(\\hat\\theta_n\\right) = \\frac{1}{B}\\sum_{b=1}^B \\left(\\hat\\theta_{n,b,obs} - \\theta\\right)^2\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nBy the law of large numbers (see Theorem 7.5 in Chapter 7) these approximations get arbitrarily precise as \\(B \\to \\infty,\\) i.e. \\[\n\\begin{align*}\n\\widehat{\\operatorname{Bias}}_{MC}\\left(\\hat\\theta_n\\right)&\\to_p\n\\operatorname{Bias}\\left(\\hat\\theta_n\\right)\\quad\\text{as}\\quad B\\to\\infty\\\\[2ex]\n\\widehat{Var}_{MC}\\left(\\hat\\theta_n\\right)&\\to_p\nVar\\left(\\hat\\theta_n\\right)\\quad\\text{as}\\quad B\\to\\infty\\\\[2ex]\n\\widehat{\\operatorname{MSE}}_{MC}\\left(\\hat\\theta_n\\right)&\\to_p\n\\operatorname{MSE}\\left(\\hat\\theta_n\\right)\\quad\\text{as}\\quad B\\to\\infty\n\\end{align*}\n\\]\nSo for large \\(B\\) (e.g. \\(B=10,000\\)) we can consider \\[\n\\widehat{\\operatorname{Bias}}_{MC}(\\hat\\theta_n),\n\\widehat{Var}_{MC}(\\hat\\theta_n),\\text{ and }\\;\\;  \n\\widehat{\\operatorname{MSE}}_{MC}(\\hat\\theta_n)\n\\] as roughly equal to \\[\n\\operatorname{Bias}(\\hat\\theta_n),\nVar(\\hat\\theta_n),\\text{ and }\n\\operatorname{MSE}(\\hat\\theta_n).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation Theory and Monte Carlo Simulations</span>"
    ]
  },
  {
    "objectID": "03-Monte-Carlo-Simulations.html",
    "href": "03-Monte-Carlo-Simulations.html",
    "title": "\n4  Estimation Theory and Monte Carlo Simulations\n",
    "section": "",
    "text": "4.1 Estimator vs. Estimate\nLet’s assume that we have an iid random sample \\(\\{X_1,\\dots,X_n\\}\\) with \\[\nX_i\\overset{iid}{\\sim} F_X\n\\] for all \\(i=1,\\dots,n\\), and let \\(\\theta\\in\\mathbb{R}\\) denote some parameter (e.g. the mean or the variance) of the distribution \\(F_X\\).\nAn estimator \\(\\hat\\theta_n\\) of \\(\\theta\\) is a function of the random sample \\(X_1,\\dots,X_n\\), \\[\n\\hat\\theta_n:=\\hat\\theta(X_1,\\dots,X_n).\n\\]\nSince \\(\\hat\\theta_n\\) is a function of the random variables \\(X_1,\\dots,X_n\\), the estimator \\(\\hat\\theta_n\\) is itself a random variable.\nThe observed data \\(X_{1,obs},\\dots,X_{n,obs}\\) is assumed to be a certain realization of the random sample \\(X_1,\\dots,X_n\\). The corresponding realization of the estimator is called an estimate of \\(\\theta\\) \\[\n\\hat\\theta_{n,obs}=\\hat\\theta(X_{1,obs},\\dots,X_{n,obs}).\n\\]\nExamples:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation Theory and Monte Carlo Simulations</span>"
    ]
  },
  {
    "objectID": "03-Monte-Carlo-Simulations.html#estimator-vs.-estimate",
    "href": "03-Monte-Carlo-Simulations.html#estimator-vs.-estimate",
    "title": "\n4  Estimation Theory and Monte Carlo Simulations\n",
    "section": "",
    "text": "The sample mean as an estimator of the population mean \\(E(X_i) =\\theta\\): \\[\n\\hat\\theta_n=\\bar{X}_n=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\]\nThe sample variance as an estimator of the population variance \\(Var(X_i) =\\theta\\): \\[\n\\hat\\theta_n=s_{UB}^2=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i - \\bar{X}_n\\right)^2\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nOften we do not use a distinguishing notation, but denote both the estimator and its realization as \\(\\hat\\theta_{n}\\). This ambiguity is often convenient since both points of views can make sense in a given context.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation Theory and Monte Carlo Simulations</span>"
    ]
  },
  {
    "objectID": "03-Monte-Carlo-Simulations.html#deriving-the-distribution-of-estimators",
    "href": "03-Monte-Carlo-Simulations.html#deriving-the-distribution-of-estimators",
    "title": "\n4  Estimation Theory and Monte Carlo Simulations\n",
    "section": "\n4.2 Deriving the Distribution of Estimators",
    "text": "4.2 Deriving the Distribution of Estimators\nUsually, we do not know the distribution \\(F_X\\) of the random sample \\(X_1,\\dots,X_n\\) and thus we neither know the value of \\(\\theta\\) nor the distribution of the estimator \\[\n\\hat\\theta_n=\\hat\\theta(X_1,\\dots,X_n).\n\\] This is the fundamental statistical problem that we need to overcome in statistical inference (estimating \\(\\theta\\), hypothesis testing about \\(\\theta\\), etc.).\nThere are (roughly) three different possibilities to derive/approximate the distribution of an estimator \\(\\hat\\theta_n:\\)\n\n\nOption 1: Mathematical derivation using a complete distributional assumption on \\(F_X\\). Assuming a certain distribution \\(F_X\\) for the random sample \\(X_1,\\dots,X_n\\) may allow us to derive mathematically the exact distribution of \\(\\hat\\theta_n\\) for given sample sizes \\(n.\\) ➡️ We consider this option in ?sec-ssinf.\n\nPro: If the distributional assumption is correct, one has exact inference for each sample size \\(n\\).\nCon: This option can fail miserably if the distributional assumption on \\(F_X\\) is wrong.\nCon: This option is often only possible for rather simple distributions \\(F_X\\) like the normal distribution.\n\n\n\nOption 2: Mathematical derivation using only an incomplete distributional assumption on \\(F_X\\) and asymptotic statistics. Large sample \\((n\\to\\infty)\\) approximations (i.e. laws of large numbers and central limit theorems) often allows us to derive the approximate distribution of \\(\\hat\\theta_n\\) for large sample sizes \\(n.\\) ➡️ We consider this option in ?sec-lsinf.\n\nPro: Only a few qualitative distributional assumptions are needed. (Typically: A random sample with finite variances.)\n\nCon: The derived asymptotic (\\(n\\to\\infty\\)) distribution is only exact for the practically impossible case where \\(n=\\infty\\) and thus can fail to approximate the exact distribution of \\(\\hat\\theta_n\\) for given (finite) sample sizes \\(n\\); particularly if \\(n\\) is small.\n\n\n\nOption 3: Monte Carlo (MC) Simulations using a complete distributional assumption on \\(F_X\\). Assuming a certain distribution \\(F_X\\) for the random sample \\(X_1,\\dots,X_n\\) we can approximate (with arbitrary precision) the exact distribution of \\(\\hat\\theta_n\\) for given sample sizes \\(n;\\) see the Algorithm “MC-Simulation”.  ➡️ We use this option to check the behavior of estimators under different scenarios for \\(F_X\\) and \\(n\\) throughout the rest of this script.\nPro: Works for a basically every distribution \\(F_X\\) and sample size \\(n.\\)\n\nCon: This option can fail miserably if the distributional assumption on \\(F_X\\) is wrong.\n\n\n\nAlgorithm “MC-Simulation”:1. Step: Generate realizations of \\(\\hat{\\theta}_n\\). Use a (pseudo-)random number generator to draw a large number of \\(B\\) (e.g. \\(B=10,000\\)) many realizations of the random sample \\(\\{X_1,\\dots,X_n\\}\\) for a given distribution \\(F_X\\) and a given sample size \\(n:\\) \\[\\begin{align*}\n&(X_{1,1,obs},\\dots,X_{n,1,obs})\\\\\n&(X_{1,2,obs},\\dots,X_{n,2,obs})\\\\\n& \\hspace{2cm}\\vdots \\\\\n&(X_{1,B,obs},\\dots,X_{n,B,obs})\n\\end{align*}\\] Compute for each realization of the random sample a realization of \\(\\hat{\\theta}_n:\\) \\[\\begin{align*}\n\\hat\\theta_{n,1,obs} &= \\hat\\theta(X_{1,1,obs},\\dots,X_{n,1,obs})\\\\\n\\hat\\theta_{n,2,obs} &= \\hat\\theta(X_{1,2,obs},\\dots,X_{n,2,obs})\\\\\n&\\vdots\\\\\n\\hat\\theta_{n,B,obs} &= \\hat\\theta(X_{1,B,obs},\\dots,X_{n,B,obs})\n\\end{align*}\\] 2. Step: Approximate the distribution of \\(\\hat{\\theta}_n\\) (or a features of it). Use the realizations \\(\\hat\\theta_{n,1,obs},\\dots,\\hat\\theta_{n,B,obs}\\) to approximate the exact distribution of \\(\\hat\\theta_n\\) for a given \\(F_X\\) and a give sample size \\(n.\\)\n\n\n\n\n\n\nNote\n\n\n\nStep 2 of the above algorithm works, since the empirical distribution function \\[\n\\hat{F}_{\\hat{\\theta}_n,B}(x)=\\frac{1}{B}\\sum_{j=1}^BI_{(\\hat\\theta_{n,j} \\leq x)}\n\\] approximates the true (unknown) distribution function of \\(\\hat{\\theta}_n\\) \\[\nF_{\\hat{\\theta}_n}(x)=P(\\hat\\theta_{n} \\leq x)\n\\] arbitrarily well as \\(B\\to\\infty.\\)\nThis hold true, since by the famous Glivenko–Cantelli theorem\\[\n\\sup_x\\left| \\hat{F}_{\\hat{\\theta}_n,B}(x) - F_{\\hat{\\theta}_n}(x)\\right|\\to 0\\quad\\text{as}\\quad B\\to\\infty.\n\\] almost surely as \\(B\\to\\infty.\\)\nInstead of approximating the whole distribution of \\(\\hat{\\theta}_n,\\) we may only be interested in approximating specific features of the this distribution, such as:\n\nthe bias of \\(\\hat{\\theta}_n\\)\n\nthe variance of \\(\\hat{\\theta}_n\\)\n\nthe standard error of \\(\\hat{\\theta}_n\\)\n\nthe mean squared error of \\(\\hat{\\theta}_n\\)\n\netc.\n\nNote: These features are simple functionals of \\(\\hat{F}_{\\hat{\\theta}_n,B},\\) and thus can also be approximated arbitrarily well as \\(B\\to\\infty.\\)\n\n\n\n4.2.1 Example: Sample Mean \\(\\bar{X}_n\\)\n\nLet \\(\\{X_1,\\dots,X_n\\}\\) be an iid random sample with \\[\nX_i\\overset{iid}{\\sim} F_X,\n\\] where\n\n\n\\(F_X\\) is a normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\) with\n\nmean \\((\\theta=)\\mu=10\\) and\nvariance \\(\\sigma^2=5\\).\n\n\n\nTo estimate the (usually unknown) mean value \\(\\mu=10,\\) we use the sample mean estimator \\[\n\\bar{X}_n =  \\frac{1}{n}\\sum_{i=1}^n X_i\n\\]\nWe consider two sample sizes \\(n=5\\) and \\(n=50.\\)\n\n\n\n\n\n\nMathematical Derivation using the Distributional Assumptions\n\n\n\nHere we have specified the distribution \\(F_X\\) completely by setting \\(F_X=\\mathcal{N}(\\mu=10,\\sigma^2=5).\\) This is such a simple case, that we can actually use mathematical derivations to derive the distribution of \\(\\bar{X}_n.\\) (Often, this is not possible.)\nObserve that since \\(X_i\\overset{\\text{iid}}{\\sim}\\mathcal{N}(\\mu,\\sigma^2),\\) \\[\n\\sum_{i=1}^nX_i\\sim\\mathcal{N}(n \\mu, n \\sigma^2).\n\\] Multiplying by \\(\\frac{1}{n}\\) yields \\[\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^nX_i = \\bar{X}_n\n&\\sim\\mathcal{N}\\left(\\frac{1}{n}n \\mu, \\frac{1}{n^2}n \\sigma^2\\right)\\\\[2ex]\n\\bar{X}_n &\\sim\\mathcal{N}\\left( \\mu, \\frac{1}{n} \\sigma^2\\right).\n\\end{align*}\\]\nSumming up: If \\(X_i\\overset{iid}{\\sim}\\mathcal{N}(\\mu,\\sigma^2),\\) then the exact (exact for each \\(n\\)) distribution of \\(\\bar{X}_n\\) is given by \\[\n\\bar{X}_n\\sim\\mathcal{N}\\left( \\mu, \\frac{1}{n} \\sigma^2\\right).\n\\]\n\nFor \\(\\mu=10,\\) \\(\\sigma=5,\\) \\(n=5\\): \\[\n\\bar{X}_n\\sim\\mathcal{N}\\left(10, 1\\right).\n\\]\n\nFor \\(\\mu=10,\\) \\(\\sigma=5,\\) \\(n=50\\): \\[\n\\bar{X}_n\\sim\\mathcal{N}\\left(10, 0.1\\right).\n\\]\n\n\n⚠️ Unfortunately, such a mathematical derivation works only for very simple estimators and only for simple (and completely specified) distributions \\(F_X.\\)\n🤓 But for this special case, we can now check, whether a Monte Carlo simulation is able to approximate the distribution of \\(\\bar{X}_n\\sim\\mathcal{N}\\left( \\mu, \\frac{1}{n} \\sigma^2\\right).\\)\n\n\nNext, we use a Monte Carlo simulation to approximate the distribution of the estimator \\[\n\\bar{X}_n =  \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\]\nThe following R code generates \\(B=10,000\\) many realizations of the random sample \\(X_i\\overset{iid}{\\sim}\\mathcal{N}(\\mu,\\sigma^2)\\) with \\(\\mu=10\\) and \\(\\sigma^2=5.\\)\n\\[\\begin{align*}\n&(X_{1,1,obs},\\dots,X_{n,1,obs})\\\\\n&(X_{1,2,obs},\\dots,X_{n,2,obs})\\\\\n& \\hspace{2cm}\\vdots \\\\\n&(X_{1,B,obs},\\dots,X_{n,B,obs})\n\\end{align*}\\] leading to \\(B\\) many realizations of the estimator \\(\\bar{X}_n\\) \\[\\begin{align*}\n\\bar{X}_{n,1,obs} &= \\hat\\theta(X_{1,1,obs},\\dots,X_{n,1,obs})\\\\\n\\bar{X}_{n,2,obs} &= \\hat\\theta(X_{1,2,obs},\\dots,X_{n,2,obs})\\\\\n&\\vdots\\\\\n\\bar{X}_{n,B,obs} &= \\hat\\theta(X_{1,B,obs},\\dots,X_{n,B,obs})\n\\end{align*}\\] These realizations are then used to approximate the true distribution of \\(\\bar{X}_n.\\)\n\n## True parameter value \nmu            &lt;- 10\n## Number of Monte Carlo repetitions:\nB             &lt;- 10000\n## Sequence of different sample sizes:\nn_seq         &lt;- c(5, 50)\n\n\n## #############################################\n## 1st Coding-Possibility: Using a for() loop ##\n## #############################################\n\n## Set seed for the random number generator to get reproducible results\nset.seed(3)\n\n# Container for the generated estimates:\nestimates_mat &lt;- matrix(NA, nrow = B, ncol = length(n_seq))\n\nfor(j in 1:length(n_seq)){\n  ## select the sample size\n  n &lt;- n_seq[j]\n  for(b in 1:B){\n    ## generate realization of the random sample \n    X_sample &lt;- rnorm(n = n, mean = mu, sd = sqrt(5))\n    ## compute the sample mean and safe it\n    estimates_mat[b,j] &lt;- mean(X_sample)\n  }\n}\n\n## ############################################\n## 2nd Coding-Possibility: Using replicate() ##\n## ############################################\n\n## Set seed for the random number generator to get reproducible results\nset.seed(3)\n\n## Function that generates estimator realizations \nmy_estimates_generator &lt;- function(n){\n  X_sample &lt;- rnorm(n = n, mean = mu, sd = sqrt(5))\n  ## compute the sample mean realization\n  return(mean(X_sample))\n}\n\nestimates_mat &lt;- cbind(\n  replicate(B, my_estimates_generator(n = n_seq[1])),\n  replicate(B, my_estimates_generator(n = n_seq[2]))\n)\n\nBased on the \\(B=10,000\\) realizations of the estimator \\(\\bar{X}_n\\), we can compute the empirical density functions \\(\\hat{F}_{X_n,B}\\) (see Figure 4.1) and histograms (see Figure 4.2) to get an idea about the true distribution of \\(\\bar{X}_n.\\)\nIn this simple case, we also know the theoretical distribution function \\(F_{X_n}\\) and density function which allows us to check the simulation results (see Figure 4.1 and Figure 4.2).\n\nlibrary(scales)\npar(mfrow=c(1,2))\nplot(ecdf(estimates_mat[,1]), main=\"n=5\", ylab=\"\", xlab=\"\", col = \"black\", xlim = range(estimates_mat[,1]), ylim=c(0,1.25))\nmtext(expression(mu==10), side = 1, at = 10, line = 2.5)\ncurve(pnorm(x, mean=10, sd=sqrt(5/5)), add=TRUE, col=\"red\", lty = 3, lwd=4)\nlegend(\"topleft\", legend = c(\"Empir. Distr.-Function\", \"True Distr.-Function\"), col = c(\"black\", \"red\"), lty = c(1, 3), lwd = c(1.3, 2), bty = \"n\")\n##      \nplot(ecdf(estimates_mat[,2]), main=\"n=50\", ylab=\"\", xlab=\"\", col = \"black\", xlim = range(estimates_mat[,1]), ylim=c(0,1.25))\nmtext(expression(mu==10), side = 1, at = 10, line = 2.5)\ncurve(pnorm(x, mean=10, sd=sqrt(5/50)), add=TRUE, col=\"red\", lty = 3, lwd=4)\nlegend(\"topleft\", legend = c(\"Empir. Distr.-Function\", \"True Distr.-Function\"), col = c(\"black\", \"red\"), lty = c(1, 3), lwd = c(1.3, 2), bty = \"n\")\n\n\n\n\n\n\nFigure 4.1: Empirical distribution functions \\(\\hat{F}_{X_{n},B}\\) computed from the \\((B=10000)\\) simulated realizations \\(\\bar{X}_{n,1,obs},\\dots,\\bar{X}_{n,B,obs}\\) and the theoretical distribution functions for \\(n=5,50.\\) The empirical and the theoretical distribution functions match perfectly.\n\n\n\n\n\nlibrary(scales)\npar(mfrow=c(1,2))\nhist(estimates_mat[,1], main=\"n=5\", xlab=\"\",  xlim = range(estimates_mat[,1]), prob = TRUE, ylim = c(0, 1.5))\nmtext(expression(mu==10), side = 1, at = 10, line = 2.5)\ncurve(dnorm(x, mean=10, sd=sqrt(5/5)), add=TRUE, lty = 3, lwd=4, col=\"red\")\nlegend(\"topleft\", legend = c(\"Histogram\",\"True Density\"), \n      col = c(\"black\", \"red\"), lty = c(1,3), lwd = c(1.3, 4), bty = \"n\")\n##\nhist(estimates_mat[,2], main=\"n=50\", xlab=\"\",  xlim = range(estimates_mat[,1]), prob = TRUE, ylim = c(0, 1.5))\nmtext(expression(mu==10), side = 1, at = 10, line = 2.5)\ncurve(dnorm(x, mean=10, sd=sqrt(5/50)), add=TRUE, lty = 3, lwd=4, col=\"red\")\nlegend(\"topleft\", legend = c(\"Histogram\",\"True Density\"), \n      col = c(\"black\", \"red\"), lty = c(1,3), lwd = c(1.3, 4), bty = \"n\")\n\n\n\n\n\n\nFigure 4.2: Histrograms of \\((B=10000)\\) simulated realizations \\(\\bar{X}_{n,1,obs},\\dots,\\bar{X}_{n,B,obs}\\) and true density functions for \\(n=5,50.\\) The empirical (simulation based) histrograms and the theoretical density functions match perfectly.\n\n\n\n\nObservations in Figure 4.1 and Figure 4.2: The empirical distribution functions and the histograms based on the simulated realizations \\[\n\\bar{X}_{n,1,obs},\\dots,\\bar{X}_{n,B,obs}\n\\] mach their theoretical counterparts almost perfectly since we chose a sufficient large number of \\(B=10000\\) simulations.\n\n\n\n\n\n\nTake away message\n\n\n\nWe can use Monte Carlo simulations to approximate the exact distribution of an estimator \\(\\hat{\\theta}_n\\) for given distributions \\(F_X\\) of the underlying random sample. These approximations become arbitrarily precise as \\(B\\to\\infty\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation Theory and Monte Carlo Simulations</span>"
    ]
  },
  {
    "objectID": "03-Monte-Carlo-Simulations.html#assessing-the-quality-of-estimators",
    "href": "03-Monte-Carlo-Simulations.html#assessing-the-quality-of-estimators",
    "title": "\n4  Estimation Theory and Monte Carlo Simulations\n",
    "section": "\n4.3 Assessing the Quality of Estimators",
    "text": "4.3 Assessing the Quality of Estimators\n\nAny reasonable estimator \\(\\hat\\theta_n\\) should be able to approximate the (usually unknown) parameter value \\(\\theta\\), \\[\n\\left(\\text{random quantity}\\right)\\quad\\hat\\theta_n\\approx\\theta\\quad\\left(\\text{deterministic parameter}\\right),\n\\] and the approximation should get better as the sample size increases, i.e. as \\(n\\to\\infty\\).\nThe simulation results shown in Figure 4.1 and Figure 4.2 show this desired behavior for the case of \\(\\hat{\\theta}_n=\\bar{X}_n.\\)\nTo check (via MC-Simulations) the quality of an estimator, one can look at the total distribution or density function of \\(\\hat{\\theta}_n\\); as done in Figure 4.1 and Figure 4.2. However, it is often more convenient to consider only the most relevant features of the distribution of an estimator.\nStatisticians/econometricians use different metrics to assess the quality of an estimator \\(\\hat\\theta_n\\). The most prominent metrics are:\n\nbias of an estimator \\(\\hat{\\theta}_n\\)\n\nvariance and standard error of an estimator \\(\\hat{\\theta}_n\\)\n\nmean squared error (mse) of an estimator \\(\\hat{\\theta}_n\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nDefinition 4.1 (Bias of \\(\\theta\\)) The bias of an estimator \\(\\hat\\theta_n\\) is defined as\n\\[\n\\operatorname{Bias}\\left(\\hat\\theta_n\\right) = E\\left(\\hat\\theta_n\\right) - \\theta.\n\\]\n\n\n\nIf an estimator \\(\\hat\\theta_n\\) has no bias \\[\n\\operatorname{Bias}\\left(\\hat\\theta_n\\right)=0\n\\] for all \\(\\theta\\) and all sample sizes \\(n,\\) we call it an unbiased estimator.\nMany modern estimators are not unbiased. However, every estimator should be at least asymptotically unbiased, i.e. \\[\n\\lim_{n\\to\\infty}\\operatorname{Bias}\\left(\\hat\\theta_n\\right)=0\n\\] for all \\(\\theta.\\)\nWe would like to have estimators with a small (or zero) bias.\nIf the bias of an estimator is small (or zero), we know that the distribution of the estimator is roughly (or exactly) centered around the true (usually unknown) parameter \\(\\theta.\\)\nHowever, also unbiased estimators \\(\\hat{\\theta}_n\\) may still vary a lot around the parameter \\(\\theta\\) to be estimated. Therefore, is is also important to assess the variance of the estimator.\n\n\n\n\n\n\nNote\n\n\n\n\nDefinition 4.2 (Variance and Standard Error of \\(\\theta\\)) The variance of an estimator \\(\\hat\\theta_n\\) is defined equivalently to the variance of any other random variable\n\\[\nVar\\left(\\hat\\theta_n\\right) = E\\left[\\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)^2\\right].\n\\] The square root of the variance of an estimator is called standard error (not standard deviation) of \\(\\hat\\theta_n\\), \\[\n\\operatorname{SE}\\left(\\hat\\theta_n\\right) = \\sqrt{Var\\left(\\hat\\theta_n\\right)}.\n\\]\n\n\n\nWe would like to have estimators with a small as possible variance, and the variance should decline as the sample size increases, such that \\(\\lim_{n\\to\\infty}Var\\left(\\hat\\theta_n\\right)=0\\).\n\n\n\n\n\n\nNote\n\n\n\n\nDefinition 4.3 (Mean Squared Error of \\(\\theta\\)) The mean squared error of an estimator \\(\\hat\\theta_n\\) is defined as\n\\[\n\\operatorname{MSE}\\left(\\hat\\theta_n\\right) =  E\\left[\\left(\\hat\\theta_n - \\theta\\right)^2\\right].\n\\]\n\n\n\nWe would like to have estimators with a small as possible mean squared error, and the mean squared error should decline as the sample size increases, such that \\(\\lim_{n\\to\\infty}\\operatorname{MSE}\\left(\\hat\\theta_n\\right)=0\\).\nThe following holds true:\n\nThe mean squared error equals the sum of the squared bias and the variance:\n\n\\[\n\\operatorname{MSE}\\left(\\hat\\theta_n\\right) = \\left(\\operatorname{Bias}\\left(\\hat\\theta_n\\right)\\right)^2 +  Var\\left(\\hat\\theta_n\\right)\n\\]\n\nFor unbiased estimators (i.e. \\(E(\\hat\\theta_n)=\\theta\\)) the mean squared error equals the variance, i.e.\n\n\\[\n\\underbrace{E\\left[\\left(\\hat\\theta_n - \\theta\\right)^2\\right]}_{\\operatorname{MSE}\\left(\\hat\\theta_n\\right)} = \\underbrace{E\\left[\\left(\\hat\\theta_n - E\\left(\\hat\\theta_n\\right)\\right)^2\\right]}_{ Var\\left(\\hat\\theta_n\\right)}\n\\]\nUnfortunately, it is often difficult to derive the above assessment metrics for given sample sizes \\(n\\) and given data distributions \\(F_X\\). Monte Carlo simulations allow us to solve this issue.\n\n4.3.1 Approximating Bias, Variance, and MSE using MC Simulations\nWe can use Monte Carlo simulations to approximate the assessment metrics \\(\\operatorname{Bias}\\left(\\hat\\theta_n\\right),\\) \\(Var\\left(\\hat\\theta_n\\right),\\) and \\(\\operatorname{MSE}\\left(\\hat\\theta_n\\right)\\) for given sample sizes \\(n\\) and given data distributions \\(F_X\\) with arbitrary precision.\nAny of the the above assessment metrics require us to compute means of random variables:\n\nFor the \\(\\operatorname{Bias}\\left(\\hat\\theta_n\\right)\\) we need to compute \\(E\\left(\\hat\\theta_n\\right)-\\theta\\)\nFor the \\(Var\\left(\\hat\\theta_n\\right)\\) we need to compute \\(E\\left[\\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)^2\\right]\\).\nFor the \\(\\operatorname{MSE}\\left(\\hat\\theta_n\\right)\\) we need to compute \\(E\\left[\\left(\\hat\\theta_n - \\theta\\right)^2\\right]\\).\n\nA Monte Carlo simulation can approximate these means by using the law of large numbers which states that a sample mean over iid random variables is able to approximate the population mean of these random variables as the number of random variables to average over get large.1\n1 See ?thm-SLLN1 in ?sec-lsinf.Thus, to compute a very precise approximation to \\(E\\left(\\hat\\theta_n\\right)-\\theta\\), we can use a computer to execute the following algorithm:\nStep 1. Generate \\(B\\) many (e.g. \\(B=10,000\\)) realizations of the iid random sample \\((X_1,\\dots,X_n)\\) \\[\\begin{align*}\n&(X_{1,1,obs},\\dots,X_{n,1,obs})\\\\\n&(X_{1,2,obs},\\dots,X_{n,2,obs})\\\\\n& \\hspace{2cm}\\vdots \\\\\n&(X_{1,B,obs},\\dots,X_{n,B,obs})\n\\end{align*}\\] leading to \\(B\\) many realizations of the estimator \\(\\hat{\\theta}_n\\) \\[\\begin{align*}\n\\hat{\\theta}_{n,1,obs} &= \\hat\\theta(X_{1,1,obs},\\dots,X_{n,1,obs})\\\\\n\\hat{\\theta}_{n,2,obs} &= \\hat\\theta(X_{1,2,obs},\\dots,X_{n,2,obs})\\\\\n&\\;\\vdots\\\\\n\\hat{\\theta}_{n,B,obs} &= \\hat\\theta(X_{1,B,obs},\\dots,X_{n,B,obs})\n\\end{align*}\\]\nStep 2. Use the simulated realizations \\(\\hat{\\theta}_{n,1,obs},\\dots,\\hat{\\theta}_{n,B,obs}\\) to approximate the bias, variance, and the mean squared error of the estimator \\(\\hat{\\theta}_n\\):\n\nThe bias of \\(\\operatorname{Bias}\\left(\\hat\\theta_n\\right)=E\\left(\\hat\\theta_n\\right)-\\theta\\) can be approximated by\n\n\\[\n\\widehat{\\operatorname{Bias}}_{MC}\\left(\\hat\\theta_n\\right) = \\left(\\frac{1}{B}\\sum_{b=1}^B \\hat\\theta_{n,b,obs}\\right) - \\theta\n\\]\n\nThe variance \\(Var\\left(\\hat\\theta_n\\right)=E\\left[\\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)^2\\right]\\) can be approximated by \\[\n\\widehat{Var}_{MC}\\left(\\hat\\theta_n\\right) = \\frac{1}{B}\\sum_{b=1}^B \\left(\\hat\\theta_{n,b,obs} - \\left(\\frac{1}{B}\\sum_{b=1}^B \\hat\\theta_{n,b,obs}\\right)\\right)^2\n\\]\nThe mean squared error \\(\\operatorname{MSE}\\left(\\hat\\theta_n\\right)=E\\left[\\left(\\hat\\theta_n - \\theta\\right)^2\\right]\\) can be approximated by \\[\n\\widehat{\\operatorname{MSE}}_{MC}\\left(\\hat\\theta_n\\right) = \\frac{1}{B}\\sum_{b=1}^B \\left(\\hat\\theta_{n,b,obs} - \\theta\\right)^2\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nBy the law of large numbers these approximations get arbitrarily precise as \\(B \\to \\infty,\\) i.e. \\[\\begin{align*}\n\\widehat{\\operatorname{Bias}}_{MC}\\left(\\hat\\theta_n\\right)&\\to\n\\operatorname{Bias}\\left(\\hat\\theta_n\\right)\\quad\\text{as}\\quad B\\to\\infty\\\\[2ex]\n\\widehat{Var}_{MC}\\left(\\hat\\theta_n\\right)&\\to\nVar\\left(\\hat\\theta_n\\right)\\quad\\text{as}\\quad B\\to\\infty\\\\[2ex]\n\\widehat{\\operatorname{MSE}}_{MC}\\left(\\hat\\theta_n\\right)&\\to\n\\operatorname{MSE}\\left(\\hat\\theta_n\\right)\\quad\\text{as}\\quad B\\to\\infty\n\\end{align*}\\]\nSo for large \\(B\\) (e.g. \\(B=10,000\\)) we can consider \\[\n\\widehat{\\operatorname{Bias}}_{MC}(\\hat\\theta_n),\n\\widehat{Var}_{MC}(\\hat\\theta_n),\\text{ and }\\;\\;  \n\\widehat{\\operatorname{MSE}}_{MC}(\\hat\\theta_n)\n\\] as roughly equal to \\[\n\\operatorname{Bias}(\\hat\\theta_n),\nVar(\\hat\\theta_n),\\text{ and }\n\\operatorname{MSE}(\\hat\\theta_n).\n\\]\n\n\nExample (revisited): Sample Mean\nThe following R code contains a Monte Carlo simulation with \\(B = 10000\\) replications to approximate the bias, the variance, and the mean squared error for the sample mean \\[\n(\\hat\\theta_n=)\\bar{X}_n=\\sum_{i=1}^nX_i\n\\] Setup:\n\n\n\\(X_i\\overset{iid}{\\sim}F_X\\), \\(i=1,\\dots,n\\), with \\(F_X=\\mathcal{N}(\\mu,\\sigma^2)\\)\n\nMean \\(\\mu=10\\) and variance \\(\\sigma^2=5\\)\n\nSample sizes \\(n\\in\\{5,15,50\\}\\) \n\n\n\n## Set seed for the random number generator to get reproducible results\nset.seed(3)\n## True parameter value ('theta' here 'mu')\nmu            &lt;- 10\n## Number of Monte Carlo repetitions:\nB             &lt;- 10000\n## Sequence of different sample sizes:\nn_seq         &lt;- c(5, 15, 50)\n\n## Function that generates estimator realizations \nmy_estimates_generator &lt;- function(n){\n  X_sample &lt;- rnorm(n = n, mean = mu, sd = sqrt(5))\n  ## compute the sample mean realization\n  return(mean(X_sample))\n}\n\nestimates_mat &lt;- cbind(\n  replicate(B, my_estimates_generator(n = n_seq[1])),\n  replicate(B, my_estimates_generator(n = n_seq[2])),\n  replicate(B, my_estimates_generator(n = n_seq[3]))\n)\n\n## Bias of the sample mean for different sample sizes n\nMC_Bias_n_seq &lt;- apply(estimates_mat, 2, mean) - mu\n\n## Variance of the sample mean for different sample sizes n\nMC_Var_n_seq  &lt;- apply(estimates_mat, 2, var)\n\n## Mean squared error of the sample mean for different sample sizes n\nMC_MSE_n_seq  &lt;- apply(estimates_mat, 2, function(x){mean((x - mu)^2)})\n\nTable 4.1 shows the Monte Carlo approximations for the bias, the variance, and the mean squared error of \\(\\bar{X}_n.\\)\n\n\n\nTable 4.1: Monte Carlo approximations for the true bias, true variance, and true mean squared error of sample mean.\n\n\n\n\nn\nBias (MC-Sim)\nVariance (MC-Sim)\nMSE (MC-Sim)\n\n\n\n5\n-0.001\n1.02\n1.02\n\n\n15\n0.000\n0.33\n0.33\n\n\n50\n0.001\n0.10\n0.10\n\n\n\n\n\n\n\n\nThese Monte Carlo approximations (Table 4.1) indicate that:\n\nThe true bias \\(\\operatorname{Bias}(\\bar{X}_n)\\) is very likely zero for all sample sizes \\(n\\in\\{5,15,50\\}\\)\n\n\n\nThe true mean squared error \\(\\operatorname{MSE}(\\bar{X}_n)\\) is very likely decreasing as the sample size \\(n\\) get larger.\n\nComparing the MC-Results with the theoretical bias, variance, and mse of the sample mean\nSince this example is so simple, we actually know the true bias, the true variance, and the true mean squared error of \\(\\bar{X}_n.\\) In Section 4.2.1, we have already derived the exact theoretical distribution of \\(\\bar{X}_n\\) under the assumed random sample with sampling distribution \\(F_X=\\mathcal{N}(\\mu,\\sigma^2):\\) \\[\n\\bar{X}_n\\sim\\mathcal{N}\\left(\\mu,\\frac{1}{n}\\sigma^2\\right).\n\\]\nUsing this, we can simply compute the true bias, variance and mean squared error of \\(\\bar{X}_n\\) for \\(n\\in\\{5,15,50\\}\\) and compare them with their Monte Carlo approximations:\n\nTrue bias of \\(\\bar{X}_n\\): \\[\\begin{align*}\n\\operatorname{Bias}\\left(\\bar{X}_n\\right)\n&=E\\left(\\bar{X}_n\\right) - \\mu \\\\[2ex]\n&=E\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right) - \\mu \\\\[2ex]\n&= \\left(\\frac{1}{n}\\sum_{i=1}^nE(X_i)\\right) -\\mu\\\\[2ex]\n&= \\frac{n}{n}\\mu-\\mu \\\\[2ex]\n&=0,\n\\end{align*}\\] thus \\(\\bar{X}_n\\) is unbiased for all \\(\\mu.\\)\nTrue variance of \\(\\bar{X}_n\\): \\[\\begin{align*}\nVar\\left(\\bar{X}_n\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right)\\\\[2ex]\n&= \\frac{1}{n^2} \\sum_{i=1}^nVar\\left(X_i\\right)\\\\[2ex]\n&= \\frac{n}{n^2}\\sigma^2 \\\\[2ex]\n&= \\frac{1}{n}\\sigma^2\n\\end{align*}\\]\nTrue MSE of \\(\\bar{X}_n\\): \\[\\begin{align*}\n\\operatorname{MSE}\\left(\\bar{X}_n\\right)\n&=\\left(\\operatorname{Bias}\\left(\\bar{X}_n\\right)\\right)^2 +\nVar\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right)\\\\[2ex]\n&= 0+\\frac{1}{n}\\sigma^2\n\\end{align*}\\]\n\nThe following table shows the true bias, true variance and true mean squared error values for \\(\\sigma^2=5\\) and \\(n\\in\\{5,15,50\\}\\):\n\n\n\nTable 4.2: True bias, true variance, and true mean squared error of sample mean. (Only computable in simple special cases.)\n\n\n\n\nn\nBias (true)\nVariance (true)\nMSE (true)\n\n\n\n5\n0\n1.00\n1.00\n\n\n15\n0\n0.33\n0.33\n\n\n50\n0\n0.10\n0.10\n\n\n\n\n\n\n\n\nObviously, the Monte Carlo approximations (Table 4.1) for these true values (Table 4.2) are very good.\nIf we would further increase the number of Monte Carlo repetitions \\(B,\\) the Monte Carlo approximations in Table 4.1 would get even more precise since we can make them arbitrarily precise by letting \\(B\\to\\infty\\) using the law of large numbers.\n\n\n\n\n\n\nTake away message (continued)\n\n\n\nWe can use Monte Carlo simulations to approximate the exact distribution and its features (e.g. bias, variance, mse) of an estimator \\(\\hat{\\theta}_n\\) for given distributions \\(F_X\\) of the underlying random sample. These approximations become arbitrarily precise as \\(B\\to\\infty\\).\nTherefore, Monte Carlo simulations can be used to check the theoretical properties of estimation procedures under specific data generating processes and sample sizes.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation Theory and Monte Carlo Simulations</span>"
    ]
  },
  {
    "objectID": "04-Multiple-Linear-Regression.html",
    "href": "04-Multiple-Linear-Regression.html",
    "title": "4  Multiple Linear Regression",
    "section": "",
    "text": "In the following we focus on the case of random designs \\(X\\) (i.e. \\(X\\) being a random variable), since\nCaution: A random \\(X\\) requires us to consider conditional means and variances given \\(X.\\) That is, if we were able to resample from the population (from the data-generating process), we would do so by fixing (conditioning on) the observed \\(X_{obs}\\) and generating new realizations of all other random quantities involved."
  },
  {
    "objectID": "04-Multiple-Linear-Regression.html#sec-LinModAssumptions",
    "href": "04-Multiple-Linear-Regression.html#sec-LinModAssumptions",
    "title": "4  Multiple Linear Regression",
    "section": "4.1 Assumptions",
    "text": "4.1 Assumptions\nThe multiple linear regression model is defined by the following assumptions which together describe the relevant theoretical aspects of the underlying data generating process:\nAssumption 1: Model and Sampling\nPart (a): Linear Model\n\\[\n\\begin{align}\n  Y_i=\\sum_{k=1}^K\\beta_k X_{ik}+\\varepsilon_i, \\quad i=1,\\dots,n.\n\\end{align}\n\\tag{4.1}\\] Usually, a constant (intercept) is included, in this case \\(X_{i1}=1\\) for all \\(i=1,\\dots,n.\\) In the following we will always assume that \\(X_{i1}=1\\) for all \\(i\\), unless otherwise stated.\n\n\\(Y_i\\) is called “dependent variable” or “outcome variable” or “regressand”\n\\(X_{ik}\\) is called the \\(k\\)th “predictor variable” or “regressor” or “explanatory variable” or “control variable.” Each of these names emphasizes a slightly different perspective on \\(X_{ik}.\\)\n\nIt is convenient to write Equation 4.1 using matrix notation \\[\n\\begin{eqnarray*}\n  Y_i&=&\\underset{(1\\times K)}{X_i'}\\underset{(K\\times 1)}{\\beta} +\\varepsilon_i, \\quad i=1,\\dots,n,\n\\end{eqnarray*}\n\\] where \\[\n  X_i=\\left(\\begin{matrix}X_{i1}\\\\ \\vdots\\\\  X_{iK}\\end{matrix}\\right)\n  \\quad\\text{and}\\quad\n\\beta=\\left(\\begin{matrix}\\beta_1\\\\ \\vdots\\\\ \\beta_K\\end{matrix}\\right).\n\\] Stacking all individual rows \\(i=1,\\dots,n\\) leads to \\[\n\\begin{eqnarray*}\\label{LM}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\n\\] where \\[\n\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\n\\]\n\n\n\n\n\n\nSimple Linear Regression and Polynomial Regression Model\n\n\n\nThe special case of \\(K=2\\) \\[\nY_i = \\beta_1 + \\beta_2 X_{i2} + \\varepsilon_i\n\\] is called the simple linear regression model. With the simple linear regression model, only straight line fits are possible.\nBy contrast, with the multiple linear regression model, we can also fit polynomials. For instance, we can define \\[\nX_{i3} := X_{i2}^2\n\\] which leads to a quadratic regression model (often used for life-cycle analyses that include the predictor Age\\(_i=X_{i2}\\)) \\[\nY_i = \\beta_1 + \\beta_2 X_{i2} + \\beta_3 X_{i2}^2 + \\varepsilon_i.\n\\] Of course, further predictor variables \\(X_{i3},\\dots,X_{iK}\\) can (and should) be added to this model.\nThe same logic applies to polynomials with higher polynomial degrees \\((\\geq 2).\\) Large polynomial degrees, however, can lead to unstable estimation results.\n\n\nPart (b): Random Sample\nMoreover, we assume that the observed (“obs”) data points \\[\n((Y_{1,obs},X_{11,obs},\\dots,X_{1K,obs}),(Y_{2,obs},X_{21,obs},\\dots,X_{2K,obs}),\\dots,(Y_{n,obs},X_{n1,obs},\\dots,X_{nK,obs}))\n\\] are a realization of the random sample \\[\n((Y_{1},X_{11},\\dots,X_{1K}),(Y_{2},X_{21},\\dots,X_{2K}),\\dots,(Y_{n},X_{n1},\\dots,X_{nK})).\n\\]\nThat is, the \\(i\\)th observed \\(K+1\\) dimensional data point \\[(Y_{i,obs},X_{i1,obs},\\dots,X_{iK,obs})\\in\\mathbb{R}^{K+1}\n\\] is a realization of a \\(K+1\\) dimensional random variable \\[\n(Y_{i},X_{i1},\\dots,X_{iK})\\in\\mathbb{R}^{K+1},\n\\] where\n\n\\((Y_{i},X_{i1},\\dots,X_{iK})\\) has the identical \\(K+1\\) dimensional distribution for all \\(i=1,\\dots,n.\\)\n\\((Y_{i},X_{i1},\\dots,X_{iK})\\) is independent of \\((Y_{j},X_{j1},\\dots,X_{jK})\\) for all \\(i\\neq j=1,\\dots,n.\\)\n\n\n\n\n\n\n\nNote\n\n\n\nDue to Equation 4.1, this i.i.d. assumption is equivalent to assuming that the multivariate random variables \\[\n(\\varepsilon_i,X_{i1},\\dots,X_{iK})\\in\\mathbb{R}^{K+1}\n\\] are i.i.d. across \\(i=1,\\dots,n\\).\n\n\n\n\n\n\n\n\nDanger\n\n\n\nRemark: Usually, we do not use a different notation for observed realizations \\((Y_{i,obs},X_{i1,obs},\\dots,X_{iK,obs})\\in\\mathbb{R}^{K+1}\\) and for the corresponding random variable \\((Y_{i},X_{i1},\\dots,X_{iK})\\in\\mathbb{R}^{K+1}\\) since often both interpretations (random variable and its realizations) can make sense in the same statement and then it depends on the considered context whether the random variables point if view or the realization point of view applies.\n\n\n\n\n\nAssumption 2: Exogeneity \\[\nE(\\varepsilon_i|X_i)=0,\\quad i=1,\\dots,n\n\\tag{4.2}\\] This assumption demands that the mean of the random error term \\(\\varepsilon_i\\) is zero irrespective of the realizations of \\(X_i\\). This exogeneity assumption is also called\n\n“orthogonality assumption” or\n“mean independence assumption.”\n\n\n\n\n\n\n\nNote\n\n\n\nTogether with the random sample assumption (Assumption 1, Part (b)) Equation 4.2 even implies strict exogeneity \\[\nE(\\varepsilon|X) = \\underset{(n\\times 1)}{0}\n\\] since we have independence across \\(i=1,\\dots,n\\). Under strict exogeneity, the mean of the random vector \\(\\varepsilon\\) is zero irrespective of the realizations of the \\((n\\times K)\\)-dimensional random predictor matrix \\(X.\\)\n\n\n\n\nAssumption 3: Rank Condition (no perfect multicollinearity)\n\\[\n\\begin{align*}\n\\operatorname{rank}(X)&=K\\quad\\text{a.s.}\\\\\n\\Leftrightarrow P\\big(\\operatorname{rank}(X)&=K\\big)=1\n\\end{align*}\n\\] This assumption demands that, with probability one, no predictor variable \\(X_{k}\\in\\mathbb{R}^n\\) is linearly dependent of the others. (This is the literal translation of the “almost surely (a.s.)” concept.)\nNote: The assumption implies that \\(n\\geq K,\\) since \\[\n\\operatorname{rank}(X)\\leq \\min\\{n,K\\}\\quad(a.s.)\n\\] see the crash course on ranks below.\nThis rank assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: dummy variable trap, multicollinearity, variance inflation). The violation of this assumption harms any economic interpretation since we cannot disentangle the explanatory variables’ individual effects on \\(Y\\). Therefore, this assumption is also often called an identification assumption.\n\n\n\n\n\n\nCrash Course on Ranks and Matrix Inverse:\n\n\n\nLet \\(A\\) be a \\((n\\times m)\\)-dimensional matrix.\n\nThe rank of a matrix \\(A\\) can be defined as the maximal number of linearly independent columns (or rows) of the matrix \\(A.\\)\nA \\((n\\times 1)\\)-dimensional column vector \\(A_k\\) of \\(A\\) is said to be linearly dependent of the other column vectors \\(A_{1},\\dots,A_{k-1},A_{k+1},\\dots,A_m\\) if \\(A_k\\) can be written as a linear combination of the other column vectors, i.e., if there are linear coefficients \\(c_1,\\dots,c_{k-1},c_{k+1},\\dots,c_m\\in\\mathbb{R}\\) such that \\[\nA_k = c_1 A_1 + \\dots +c_{k-1} A_{k-1} + c_{k+1} A_{k+1}+ \\dots +c_m A_{m}\n\\] Conversely, the column vectors \\(A_1, \\dots, A_m\\) are linearly independent of each other if the equation\n\\[\n\\begin{align*}\n0   & = c_1 A_1 + \\dots +c_{m} A_{m}\n\\end{align*}\n\\] has only the trivial solution \\(c_1=\\dots=c_m=0.\\)\nIf \\(A\\) is a \\((n\\times m)\\) dimensional matrix, then \\[\n\\operatorname{rank}(A)\\leq \\min\\{m,n\\}.\n\\]\nThe column rank and the row rank are always equal, therefore \\[\n\\operatorname{rank}(A)=\\operatorname{rank}(A').\\] So, it is sufficient to check either compute the column rank or the row rank.\nA \\((n\\times m)\\) matrix \\(A\\) has full row-rank if \\(\\operatorname{rank}(A)=n\\)\nA \\((n\\times m)\\) matrix \\(A\\) has full column-rank if \\(\\operatorname{rank}(A)=m\\)\nA \\((m\\times m)\\) matrix \\(B\\) has full rank if \\(\\operatorname{rank}(B)=m\\)\nLet \\(B\\) a \\((m\\times m)\\)-dimensional matrix. The \\((m\\times m)\\)-dimensional matrix \\(B^{-1}\\) is called a matrix inverse of \\(B\\) if \\[\nB^{-1}B=I_m\\quad\\text{and}\\quad BB^{-1}=I_m,\n\\] where \\(I_m\\) is the \\((m\\times m)\\) dimensional identity matrix.\nIf a \\((m\\times m)\\) matrix \\(B\\) has full rank, i.e. \\(\\operatorname{rank}(B)=m,\\) then there exists an inverse matrix \\(B^{-1}.\\)\nThe inverse of a \\((2\\times 2)\\) matrix is given by \\[\n\\begin{bmatrix}\na & b\\\\\nc & d\\\\\n\\end{bmatrix}^{-1}\n= \\frac{1}{ad - bc}\n\\begin{bmatrix}\nd  & -b\\\\\n-c & a\\\\\n\\end{bmatrix},\n\\] where \\(ad - bc\\) is the determinant of the matrix, which has to be non-zero, otherwise the matrix is not invertible.\n\nThe above crash course on linear algebra contains all concepts relevant for this course. Further, way more extensive material on linear algebra can be found, for instance:\n\nin Appendix A of Marno Verbeek’s “A Guide to Modern Econometrics”\nin Frank Pinter’s lecture notes: LINK\nin Cesar Aguilar’s lecture notes: Link\nin the free book: Linear Algebra and Optimization with Applications to Machine Learning\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nUnder Assumption 3, we have that \\(\\operatorname{rank}(X)=K\\) (a.s.)\nThis implies that the \\((K\\times K)\\)-dimensional matrix \\(X'X\\) has full rank, i.e. that \\[\n\\operatorname{rank}(X'X)=K\\quad\\text{(a.s.)}\n\\]\nThus \\((X'X)\\) is invertible; i.e. there exists a \\((K\\times K)\\)-dimensional matrix \\((X'X)^{-1}\\) such that \\[\n(X'X)(X'X)^{-1} = (X'X)^{-1}(X'X) = I_k.\n\\]\n\n\n\nAssumption 4: Error distribution\nDepending on the context (i.e., parameter estimation vs. hypothesis testing and small \\(n\\) vs. large \\(n\\)) there are different more or less restrictive assumptions. Some of the most common ones are the following:\n\nConditional distribution: \\[\n\\varepsilon_i|X_i \\sim f_{\\varepsilon|X}\n\\] for all \\(i=1,\\dots,n\\) and for any distribution \\(f_{\\varepsilon|X}\\) with two (or more) finite moments.\nConditional normal distribution: \\[\n\\varepsilon_i|X_i \\sim \\mathcal{N}(0,\\sigma^2(X_i))\n\\] for all \\(i=1,\\dots,n\\).\nIndependence between error and predictors: \\(\\varepsilon_i\\sim f_\\varepsilon\\) for all \\(i=1,\\dots,n\\) such that \\(f_\\varepsilon=f_{\\varepsilon|X}\\) and such that \\(f_\\varepsilon\\) has two (or more) finite moments.\n\nIndependence between error and predictors and normal: As above, but with \\(f_\\varepsilon=\\mathcal{N}(0,\\sigma^2)\\).\nSpherical errors (“Gauss-Markov assumptions”): The conditional distributions of \\(\\varepsilon_i|X_i\\) may generally depend on \\(X_i\\) for all \\(i=1,\\dots,n,\\) but only such that \\[\nE(\\varepsilon|X)=\\underset{(n\\times 1)}{0}\n\\] and \\[\n\\begin{align*}\n&\\underset{(n\\times n)}{Var\\left(\\varepsilon|X\\right)}=\\\\[2ex]\n& = \\left(\\begin{matrix}\nVar(\\varepsilon_1|X)&Cov(\\varepsilon_1,\\varepsilon_2|X)&\\dots&Cov(\\varepsilon_1,\\varepsilon_n|X)\\\\\nCov(\\varepsilon_2,\\varepsilon_1|X)&Var(\\varepsilon_2|X)&\\dots&Cov(\\varepsilon_2,\\varepsilon_n|X)\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nCov(\\varepsilon_n,\\varepsilon_1|X)&Cov(\\varepsilon_n,\\varepsilon_2|X)&\\dots&Var(\\varepsilon_n|X)\n\\end{matrix}\\right)\\\\[2ex]\n& = \\left(\\begin{matrix}\n\\sigma^2&0&\\dots&0\\\\\n0&\\sigma^2&\\dots&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n0&0&\\dots&\\sigma^2\n\\end{matrix}\\right)\n= \\sigma^2 I_n,\n\\end{align*}\n\\] where \\(I_n\\) denotes the \\((n\\times n)\\) identity matrix with ones on the diagonal and zeros else. Thus, under the spherical errors assumption, one has, for all possible realizations of \\(X\\), that:\n\nuncorrelated: \\(Cov(\\varepsilon_i,\\varepsilon_j|X)=0\\) for all \\(i=1,\\dots,n\\) and all \\(j=1,\\dots,n\\) such that \\(i\\neq j\\)\nhomoskedastic: \\(Var(\\varepsilon_i|X)=\\sigma^2\\) for all \\(i=1,\\dots,n\\)\n\n\n\n\nHomoskedastic versus Heteroskedastic Error Terms\nThe i.i.d. assumption is not as restrictive as it may seem on first sight. It allows for dependence between \\(\\varepsilon_i\\) and \\((X_{i1},\\dots,X_{iK})\\in\\mathbb{R}^K\\). That is, the error term \\(\\varepsilon_i\\) can have a conditional distribution which depends on \\((X_{i1},\\dots,X_{iK})\\); see Section 2.2.2.5.\n\n\nThe exogeneity assumption (Assumption 2: Exogeneity) requires that the conditional mean of \\(\\varepsilon_i\\) is independent of \\(X_i\\). Besides this, dependencies between \\(\\varepsilon_i\\) and \\(X_{i1},\\dots,X_{iK}\\) are allowed. For instance, the variance of \\(\\varepsilon_i\\) can be a function of \\(X_{i1},\\dots,X_{iK}\\). If this is the case, \\(\\varepsilon_i\\) is said to be “heteroskedastic.”\n\nHeteroskedastic error terms: The conditional variances \\(Var(\\varepsilon_i|X_i=x_i)=\\sigma^2(x_i)\\) are equal to a non-constant variance-function \\(\\sigma^2(x_i)>0.\\)\n\nExample: \\[\n\\varepsilon_i|X_i\\sim U[-0.5|X_{i2}|, 0.5|X_{i2}|],\n\\] with \\[\nX_{i2}\\sim U[-4,4]\n\\] for all \\(i=1,\\dots,n.\\) This error term is mean independent of \\(X_i\\) since \\(E(\\varepsilon_i|X_i)=0\\), but it has a heteroskedastic conditional variance since \\[\nVar(\\varepsilon_i|X_i)=\\frac{1}{12}X_{i2}^2\n\\] depends on \\(X_{i2}.\\)\nSometimes, we need to be more restrictive by assuming that also the variances of the error terms \\(\\varepsilon_i\\) are independent from \\(X_i\\). For instance, to do small sample inference (see Chapter 5), we will assume that the error terms are homoskedastik. (Higher moments may still depend on \\(X_i\\).) This assumption leads to “homoskedastic” error terms.\n\nHomoskedastic error terms: The conditional variances \\(Var(\\varepsilon_i|X_i=x_i)=\\sigma^2\\) are equal to some constant \\(\\sigma^2>0\\) for every possible realization \\(X_i=x_i.\\)\n\n\n\n\n\n\n\n\nExample: \\[\n\\varepsilon_i\\sim{\\mathcal N} (0, \\sigma^2)\n\\] for all \\(i=1,\\dots,n.\\) Here, the conditional variance of the error terms \\(\\varepsilon_i\\) given \\(X_i\\) \\[\nVar(\\varepsilon_i|X_i)=Var(\\varepsilon_i)=\\sigma^2\n\\] are equal to the constant \\(\\sigma^2>0\\) for all \\(i=1,\\dots,n\\) and for every possible realization of \\(X_i.\\)\n\n\n4.1.1 Some Implications of the Exogeneity Assumption\n\n\n\n\n\n\n\n\n\n\n\nTheorem 4.1 (Unconditional Mean) If \\(E(\\varepsilon_i|X_i)=0\\) for all \\(i=1,\\dots,n,\\) then the also the unconditional mean of the error term is zero, i.e. \\[\nE(\\varepsilon_i)=0,\\quad i=1,\\dots,n.\n\\]\n\n\n\n\nProof. Using the Law of Total Expectations (i.e., \\(E[E(Z|X)]=E(Z)\\)) we can rewrite \\(E(\\varepsilon_i)\\) as \\[\nE(\\varepsilon_i)=E[E(\\varepsilon_i|X_i)]\n\\] for all \\(i=1,\\dots,n.\\) But the exogeneity assumption yields \\[\nE[E(\\varepsilon_i|X_i)]=E[0]=0\n\\] for all \\(i=1,\\dots,n,\\) which completes the proof. \\(\\square\\)\n\nGenerally, two random variables \\(X\\) and \\(Y\\) are said to be orthogonal if their cross moment is zero, i.e. if \\[\nE(XY)=0.\n\\] The exogeneity assumption (Assumption 2) is sometimes also called “orthogonality” assumption, due to the following result:\n\n\n\n\n\n\n\n\n\n\n\nTheorem 4.2 (Orthogonality) If \\(E(\\varepsilon_i|X_{i})=0\\) for all \\(i=1,\\dots,n,\\) then the regressors and the error term are orthogonal to each other, i.e, \\[\nE(X_{ik}\\varepsilon_i)=0\n\\] for all \\(i=1,\\dots,n\\) and all \\(k=1,\\dots,K.\\)\n\n\n\n\nProof. \\[\\begin{align*}\nE(X_{ik}\\varepsilon_i)\n&=E(E(X_{ik}\\varepsilon_i|X_{ik}))\\quad{\\small\\text{(By the Law of Total Expectations)}}\\\\\n&=E(X_{ik}E(\\varepsilon_i|X_{ik}))\\quad{\\small\\text{(By the linearity of cond.~expectations)}}\n\\end{align*}\\] Now, to show that \\(E(X_{ik}\\varepsilon_i)=0\\), we need to show that \\(E(\\varepsilon_i|X_{ik})=0,\\) which is done in the following:\nSince \\(X_{ik}\\) is an element of \\(X_i,\\) a slightly more sophisticated use of the Law of Total Expectations (i.e., \\(E(Y|X)=E(E(Y|X,Z)|X)\\)) implies that \\[\nE(\\varepsilon_i|X_{ik})=E(E(\\varepsilon_i|X_i)|X_{ik}).\n\\] So, the exogeneity assumption, \\(E(\\varepsilon_i|X_i)=0\\) yields \\[\nE(\\varepsilon_i|X_{ik})=E(\\underbrace{E(\\varepsilon_i|X_i)}_{=0}|X_{ik})=E(0|X_{ik})=0.\n\\] I.e., we have that \\(E(\\varepsilon_i|X_{ik})=0\\) which allows us to conclude that \\[\nE(X_{ik}\\varepsilon_i)=E(X_{ik}E(\\varepsilon_i|X_{ik}))=E(X_{ik}0)=0\n\\] which completes the proof. \\(\\square\\)\n\nBecause the mean of the error term is zero (\\(E(\\varepsilon_i)=0\\) for all \\(i\\) (see Theorem 4.1), it follows that the orthogonality property, \\(E(X_{ik}\\varepsilon_i)=0,\\) is equivalent to a zero correlation property.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 4.3 (No Correlation) If \\(E(\\varepsilon_i|X_{i})=0\\) for all \\(i=1,\\dots,n,\\) then \\[\\begin{eqnarray*}\n  Cov(\\varepsilon_i,X_{ik})&=&0\\quad\\text{for all}\\quad i=1,\\dots,n\\quad\\text{and all}\\quad k=1,\\dots,K.\n\\end{eqnarray*}\\]\n\n\n\n\nProof. \\[\\begin{eqnarray*}\n  Cov(\\varepsilon_i,X_{ik})&=&E(X_{ik}\\varepsilon_i)-E(X_{ik})\\,E(\\varepsilon_i)\\quad{\\small\\text{(Def. of Cov)}}\\\\\n  &=&E(X_{ik}\\varepsilon_i)\\quad{\\small\\text{(By point (a): $E(\\varepsilon_i)=0$)}}\\\\\n  &=&0\\quad{\\small\\text{(By orthogonality result in point (b))}}\\quad\\square\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "04-Multiple-Linear-Regression.html#deriving-the-expression-of-the-ols-estimator",
    "href": "04-Multiple-Linear-Regression.html#deriving-the-expression-of-the-ols-estimator",
    "title": "4  Multiple Linear Regression",
    "section": "4.2 Deriving the Expression of the OLS Estimator",
    "text": "4.2 Deriving the Expression of the OLS Estimator\nWe derive the expression for the OLS estimator \\[\n\\hat\\beta=(\\hat\\beta_1,\\dots,\\hat\\beta_K)'\\in\\mathbb{R}^K\n\\] as the vector-valued minimizing argument of the sum of squared residuals, \\[\nS_n(b)=\\sum_{i=1}^n\\big(\\underbrace{Y_i-X_i'b}_{\\text{$i$th residual}}\\big)^2\n\\] with \\(b\\in\\mathbb{R}^K\\), for a given sample \\[\n((Y_1,X_1),\\dots,(Y_n,X_n)).\n\\]\nUsing matrix/vector notation we can write \\(S(b)\\) as \\[\\begin{align*}\nS_n(b)\n&=\\sum_{i=1}^n(Y_i-X_i'b)^2\\\\[2ex]\n&=(Y-X b)^{\\prime}(Y-X b)\\\\[2ex]\n&=Y^{\\prime}Y-2 Y^{\\prime} X b+b^{\\prime} X^{\\prime} X b.\n\\end{align*}\\] To find the minimizing argument \\[\n\\hat\\beta=\\arg\\min_{b\\in\\mathbb{R}^K}S_n(b)\n\\] we compute the vector containing all partial derivatives \\[\n\\begin{aligned}\n\\underset{(K\\times 1)}{\\frac{\\partial S(b)}{\\partial b}} &=-2\\left(X^{\\prime}Y -X^{\\prime} Xb\\right).\n\\end{aligned}\n\\] Setting each partial derivative to zero leads to \\(K\\) linear equations (“normal equations”) in \\(K\\) unknowns. This linear system of equations defines the OLS estimates, \\(\\hat{\\beta}\\), for a given dataset: \\[\n\\begin{align*}\n-2\\left(X^{\\prime}Y -X^{\\prime} X\\hat{\\beta}\\right)\n&=\\underset{(K\\times 1)}{0}\\\\[2ex]\nX^{\\prime} X\\hat{\\beta}\n&=\\underset{(K\\times 1)}{X^{\\prime}Y}.\n\\end{align*}\n\\] From our rank assumption (Assumption 3) it follows that \\(X^{\\prime}X\\) is an invertible \\((K\\times K)\\)-dimensional matrix which allows us to solve the equation system by \\[\n\\begin{aligned}\n\\underset{(K\\times 1)}{\\hat{\\beta}} &=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} Y.\n\\end{aligned}\n\\]\nThe following codes computes the estimate \\(\\hat{\\beta}\\) for a given dataset with \\(X_i\\in\\mathbb{R}^K\\), \\(K=3\\).\n\n# Some given data\nX_2     <- c(1.9,0.8,1.1,0.1,-0.1,4.4,4.6,1.6,5.5,3.4)\nX_3     <- c(66, 62, 64, 61, 63, 70, 68, 62, 68, 66)\nY       <- c(0.7,-1.0,-0.2,-1.2,-0.1,3.4,0.0,0.8,3.7,2.0)\ndataset <-  data.frame(\"X_2\" = X_2, \"X_3\" = X_3, \"Y\" = Y)\n## Compute the OLS estimation\nlmobj   <- lm(Y ~ X_2 + X_3, data = dataset)\n## Plot sample regression surface\nlibrary(\"scatterplot3d\") # library for 3d plots\nplot3d  <- scatterplot3d(x = X_2, y = X_3, z = Y,\n            angle = 33, scale.y = 0.8, pch = 16,\n            color =\"red\", \n            xlab = expression(X[2]),\n            ylab = expression(X[3]),\n            main =\"OLS Regression Surface\")\nplot3d$plane3d(lmobj, lty.box = \"solid\", col=gray(.5), draw_polygon=TRUE)"
  },
  {
    "objectID": "04-Multiple-Linear-Regression.html#some-quantities-of-interest",
    "href": "04-Multiple-Linear-Regression.html#some-quantities-of-interest",
    "title": "4  Multiple Linear Regression",
    "section": "4.3 Some Quantities of Interest",
    "text": "4.3 Some Quantities of Interest\nPredicted values and residuals.\n\nThe (OLS) predicted values: \\[\n\\hat{Y}_i=X_i'\\hat\\beta, \\quad i=1,\\dots,n\n\\] The \\((n\\times 1)\\) vector of predicted values \\[\\begin{align*}\n\\hat{Y} = \\left(\\begin{matrix}\\hat{Y}_1\\\\\\hat{Y}_2\\\\ \\vdots\\\\ \\hat{Y}_n\\end{matrix}\\right)\n&=X\\hat{\\beta}\\\\[-2ex]\n&=\\underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\\\[2ex]\n&=P_X Y\n\\end{align*}\\]\nThe (OLS) residuals: \\[\n\\hat\\varepsilon_i=Y_i-\\hat{Y}_i, \\quad i=1,\\dots,n\n\\] The \\((n\\times 1)\\) vector of residuals \\[\\begin{align*}\n\\hat{\\varepsilon} =\n\\left(\\begin{matrix}\\hat{\\varepsilon}_1\\\\\\hat{\\varepsilon}_2\\\\ \\vdots\\\\ \\hat{\\varepsilon}_n\\end{matrix}\\right)\n&=\n\\left(\\begin{matrix}Y_1\\\\[.5ex]Y_2\\\\[.5ex] \\vdots\\\\[.5ex] Y_n\\end{matrix}\\right)-\n\\left(\\begin{matrix}\\hat{Y}_1\\\\\\hat{Y}_2\\\\ \\vdots\\\\ \\hat{Y}_n\\end{matrix}\\right)\\\\[2ex]\n&=Y - \\hat{Y}\\\\[2ex]\n%&=Y - X\\hat{\\beta}\\\\[-2ex]\n%&=Y - \\underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\\\[2ex]\n&=Y - P_X Y\\\\[2ex]\n&=\\underbrace{(I_n - P_X)}_{=M_X} Y\\\\[2ex]\n&=M_XY\n\\end{align*}\\]\n\nProjection matrices.\nThe matrix \\[\nP_X=X(X'X)^{-1}X'\n\\] is the \\((n\\times n)\\) projection matrix that projects any vector from \\(\\mathbb{R}^n\\) into the column space spanned by the column vectors of \\(X\\) and \\[\nM_X=I_n-X(X'X)^{-1}X'=I_n-P_X\n\\] is the associated \\((n\\times n)\\) orthogonal projection matrix that projects any vector from \\(\\mathbb{R}^n\\) into the vector space that is orthogonal to that spanned by the column vectors of \\(X.\\)\nThe projection matrices \\(P_X\\) and \\(M_X\\) have some nice properties:\n\n\\(P_X\\) and \\(M_X\\) are symmetric, i.e.  \\[\nP_X=P_X'\\quad\\text{ and }\\quad M_X=M_X'\n\\]\n\\(P_X\\) and \\(M_X\\) are idempotent, i.e.  \\[\nP_XP_X=P_X\\quad\\text{ and }\\quad M_X M_X=M_X\n\\]\nMoreover, we have that\n\n\\(X'P_X=X'\\) and \\(P_X X=X\\)\n\\(X'M_X=0\\) and \\(M_XX=0\\)\n\\(P_XM_X=0\\) and \\(M_XP_X=0\\)\n\n\nThe properties (a)-(c) follow directly from the definitions of \\(P_X\\) and \\(M_X\\) (check it out). Using these properties one can show that the residual vector \\(\\hat\\varepsilon=(\\hat\\varepsilon_1,\\dots,\\hat\\varepsilon_n)'\\) is orthogonal to each of the column vectors in \\(X\\), i.e \\[\\begin{eqnarray}\nX'\\hat\\varepsilon&=&X'M_XY\\quad\\text{\\small(By Def.~of $M_X$)}\\\\\n\\Leftrightarrow X'\\hat\\varepsilon&=&\\underset{(K\\times n)}{0}\\underset{(n\\times 1)}{Y}\\quad\\text{\\small(since $X'M_X=0$)}\\\\\n\\Leftrightarrow X'\\hat\\varepsilon&=&\\underset{(K\\times 1)}{0}.\n\\end{eqnarray}\\]    Note that, in the case with intercept, the result \\(X'\\hat\\varepsilon=0\\) implies that \\(\\sum_{i=1}^n\\hat\\varepsilon_i=0\\).\nMoreover, the equation \\(X'\\hat\\varepsilon=0\\) implies also that the residual vector \\(\\hat{\\varepsilon}\\) is orthogonal to the predicted values vector, since \\[\\begin{align*}\nX'\\hat\\varepsilon&=0\\\\\n\\Rightarrow\\;\\hat\\beta'X'\\hat\\varepsilon&=\\hat\\beta'0\\\\\n\\Leftrightarrow\\;\\hat Y'\\hat\\varepsilon&=0.\n\\end{align*}\\]\nAnother insight from equation \\(X'\\hat{\\varepsilon}=0\\) is that the vector \\(\\hat\\varepsilon\\) has to satisfy \\(K\\) linear restrictions which means we loose \\(K\\) degrees of freedom in the data. Consequently, the vector of residuals \\(\\hat\\varepsilon\\) has only \\(n-K\\) so-called degrees of freedom. This loss of \\(K\\) degrees of freedom also appears in the definition of the unbiased variance estimator \\[\ns_{UB}^2=\\frac{1}{n-K}\\sum_{i=1}^n\\hat\\varepsilon_i^2.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe \\(K\\) linear restrictions follow from the fact that \\[\nX'\\hat\\varepsilon=\\underset{(K\\times 1)}{0}\n\\] is a linear system of \\(K\\) equations. That is, for each \\(k=1,\\dots,K\\) the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat\\varepsilon_n\\) need to fulfill the equation\n\\[\n\\sum_{i=1}^nX_{ik}\\hat\\varepsilon_i=0.\n\\]\n\n\nVariance decomposition: A further useful result that can be shown using the properties of \\(P_X\\) and \\(M_X\\) is that \\(Y'Y=\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\\), i.e. \\[\\begin{eqnarray*}\nY'Y&=&(\\hat Y+\\hat\\varepsilon)'(\\hat Y+\\hat\\varepsilon)\\notag\\\\\n  &=&(P_XY+M_XY)'(P_XY+M_XY)\\notag\\\\\n  &=&(Y'P_X'+Y'M_X')(P_XY+M_XY)\\notag\\\\\n  &=&Y'P_X'P_XY+Y'M_X'M_XY+0\\notag\\\\\n  &=&\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\n\\end{eqnarray*}\\] The decomposition \\[\n\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\n\\] is the basis for the well-known variance decomposition result for OLS regressions.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 4.4 For the linear regression model with intercept (Equation 4.1), the total sample variance of the dependent variable \\(Y_1,\\dots,Y_n\\) can be decomposed as following: \\[\\begin{eqnarray}\n\\underset{\\text{total sample variance}}{\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2}&=&\\underset{\\text{explained sample variance}}{\\frac{1}{n}\\sum_{i=1}^n\\left(\\hat{Y}_i-\\bar{\\hat{Y}}\\right)^2}+\\underset{\\text{unexplained sample variance}}{\\frac{1}{n}\\sum_{i=1}^n\\hat\\varepsilon_i^2,}\\label{VarDecomp}\n\\end{eqnarray}\\] where \\(\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^nY_i\\) and \\(\\bar{\\hat{Y}}=\\frac{1}{n}\\sum_{i=1}^n\\hat{Y}_i\\).\n\n\n\n\nProof. From equation \\(X'\\hat\\varepsilon=0\\) we have for regressions with intercept that \\(\\sum_{i=1}^n\\hat\\varepsilon_i=0\\). Hence, from \\(Y_i=\\hat{Y}_i+\\hat\\varepsilon_i\\) it follows that \\[\\begin{eqnarray*}\n  \\frac{1}{n}\\sum_{i=1}^n Y_i&=&\\frac{1}{n}\\sum_{i=1}^n \\hat{Y}_i+\\frac{1}{n}\\sum_{i=1}^n \\hat\\varepsilon_i\\\\\n  \\bar{Y}&=&\\bar{\\hat{Y}}+0\n\\end{eqnarray*}\\]\nUsing the decomposition \\(Y'Y=\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\\), we can now derive the result: \\[\\begin{eqnarray*}\n   Y'Y&=&\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\\\\\n   Y'Y-n\\bar{Y}^2&=&\\hat{Y}'\\hat{Y}-n\\bar{Y}^2+\\hat\\varepsilon'\\hat\\varepsilon\\\\\n   Y'Y-n\\bar{Y}^2&=&\\hat{Y}'\\hat{Y}-n\\bar{\\hat{Y}}^2+\\hat\\varepsilon'\\hat\\varepsilon\\quad\\text{(by $\\bar{Y}=\\bar{\\hat{Y}}$)}\\\\\n   \\sum_{i=1}^nY_i^2-n\\bar{Y}^2&=&\\sum_{i=1}^n\\hat{Y}_i^2-n\\bar{\\hat{Y}}^2+\\sum_{i=1}^n\\hat\\varepsilon_i^2\\\\\n   \\sum_{i=1}^n(Y_i-\\bar{Y})^2&=&\\sum_{i=1}^n(\\hat{Y}_i-\\bar{\\hat{Y}})^2+\\sum_{i=1}^n\\hat\\varepsilon_i^2\\quad\\square\\\\\n\\end{eqnarray*}\\]\n\n\nCoefficients of determination: \\(R^2\\) and \\(\\overline{R}^2\\)\nThe larger the proportion of the explained variance, the better is the fit of the model. This motivates the definition of the so-called \\(R^2\\) coefficient of determination: \\[\\begin{eqnarray*}\nR^2=\\frac{\\sum_{i=1}^n\\left(\\hat{Y}_i-\\bar{\\hat{Y}}\\right)^2}{\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2}\\;=\\;1-\\frac{\\sum_{i=1}^n\\hat{\\varepsilon}_i^2}{\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2}\n\\end{eqnarray*}\\]\n\nObviously, we have that \\(0\\leq R^2\\leq 1\\).\nThe closer \\(R^2\\) lies to \\(1\\), the better is the fit of the model to the observed data.\n\n\n\n\n\n\n\nDanger\n\n\n\n\nA high/low \\(R^2\\) value only means that the predictors have high/low predictive power.\nA high/low \\(R^2\\) does not mean a validation/falsification of the estimated model. Any econometric model needs a plausible explanation from relevant economic theory.\nThe most often criticized disadvantage of the \\(R^2\\) is that additional regressors (relevant or not) will increase the \\(R^2\\). The below R-codes demonstrates this problem.\n\n\n\n\nset.seed(123)\nn     <- 100                  # Sample size\nX     <- runif(n, 0, 10)      # Relevant X variable\nX_ir  <- runif(n, 5, 20)      # Irrelevant X variable\nerror <- rt(n, df = 10)*10    # True error\nY     <- 1 + 5 * X + error    # Y variable\nlm1   <- summary(lm(Y~X))     # Correct OLS regression \nlm2   <- summary(lm(Y~X+X_ir))# OLS regression with X_ir \nlm1$r.squared < lm2$r.squared\n\n[1] TRUE\n\n\nSo, \\(R^2\\) increases here even though X_ir is a completely irrelevant explanatory variable.\nBecause of this, the \\(R^2\\) cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called adjusted \\(R^2\\), \\(\\overline{R}^2,\\) defined as \\[\\begin{eqnarray*}\n  \\overline{R}^2&=&1-\\frac{\\frac{1}{n-K}\\sum_{i=1}^n\\hat{\\varepsilon}^2_i}{\\frac{1}{n-1}\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2}\\leq R^2%\\\\\n  %=\\dots=\n  %&=&1-\\frac{n-1}{n-K}\\left(1-R^2\\right)\\quad{\\small\\text{(since $1-R^2=(\\sum_i\\hat\\varepsilon_i^2)/(\\sum_i(Y_i-\\bar{Y}))$)}}\\\\\n  %&=&1-\\frac{n-1}{n-K}+\\frac{n-1}{n-K}R^2\\quad+\\frac{K-1}{n-K}R^2-\\frac{K-1}{n-K}R^2\\\\\n  %&=&1-\\frac{n-1}{n-K}+R^2\\quad+\\frac{K-1}{n-K}R^2\\\\\n  %&=&-\\frac{K-1}{n-K}+R^2\\quad+\\frac{K-1}{n-K}R^2\\\\\n  %&=&R^2-\\underbrace{\\frac{K-1}{n-K}\\left(1-R^2\\right)}_{\\geq 0\\;\\text{and}\\;\\leq(K-1)/(n-K)}\\;\\leq\\;R^2\n\\end{eqnarray*}\\] The adjustment is in terms of the degrees of freedom \\(n-K\\)."
  },
  {
    "objectID": "04-Multiple-Linear-Regression.html#sec-MMEstimator",
    "href": "04-Multiple-Linear-Regression.html#sec-MMEstimator",
    "title": "4  Multiple Linear Regression",
    "section": "4.4 Method of Moments Estimator",
    "text": "4.4 Method of Moments Estimator\nRemember that the exogeneity assumption (Assumption 2), \\(E(\\varepsilon_i|X_i)=0,\\) implies that \\(E(X_{ik}\\varepsilon_i)=0\\) for all \\(k=1,\\dots,K\\). Thus, the exogeneity assumption gives us a system of \\(K\\) linear equations: \\[\n\\left.\n  \\begin{array}{c}\n  E(\\varepsilon_i)=0\\\\\n  E(X_{i2}\\varepsilon_i)=0\\\\\n  \\vdots\\\\\n  E(X_{iK}\\varepsilon_i)=0\n  \\end{array}\n\\right\\}\\Leftrightarrow \\underset{(K\\times 1)}{E(X_i\\varepsilon_i)}=\\underset{(K\\times 1)}{0}\n\\]\nThe linear equation system \\[\n\\underset{(K\\times 1)}{E(X_i\\varepsilon_i)}=\\underset{(K\\times 1)}{0}\n\\] allows us to identify the unknown parameter vector \\(\\beta\\in\\mathbb{R}^K\\) in terms of population moments: \\[\n\\begin{align*}\nE(X_i(\\overbrace{Y_i - X_i'\\beta}^{=\\varepsilon_i})) &= 0\\\\\n%\\Leftrightarrow \\hspace{1.5cm}\nE(X_iY_i) - E(X_iX_i')\\beta & = 0\\\\\nE(X_iX_i')\\beta & =  E(X_iY_i)  \\\\\n\\beta & = \\left(E(X_iX_i')\\right)^{-1} E(X_iY_i)  \\\\\n\\end{align*}\n\\] The fundamental idea behind method of moments estimation is to define an estimator by substituting population moments by sample moment analogues (sample means): \\[\n\\begin{align*}\n\\hat\\beta_{mm}\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\frac{1}{n}\\sum_{i=1}^n X_iY_i,\n\\end{align*}\n\\] where \\(\\hat\\beta_{mm}\\) can be simplified as following: \\[\n\\begin{align*}\n\\hat\\beta_{mm}\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\frac{1}{n}\\sum_{i=1}^n X_iY_i  \\\\\n& = \\left(\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\sum_{i=1}^n X_iY_i\\\\\n& = \\left(X'X\\right)^{-1} X'Y.\\\\\n\\end{align*}\n\\]\nThus the method of moments estimator, \\(\\hat\\beta_{mm},\\) coincides with the OLS estimator."
  },
  {
    "objectID": "04-Multiple-Linear-Regression.html#sec-GMTheorem",
    "href": "04-Multiple-Linear-Regression.html#sec-GMTheorem",
    "title": "4  Multiple Linear Regression",
    "section": "4.5 The Gauss-Markov Theorem",
    "text": "4.5 The Gauss-Markov Theorem\n\n\n\n\n\n\n\n\n\n\nThe OLS estimator \\(\\hat\\beta\\) is\n\na linear (in \\(Y\\)) and\nan unbiased estimator\n\nof \\(\\beta\\).\n\n\nShowing linearity: A function \\(f(Y)\\) is called linear in \\(Y\\in\\mathbb{R}^n\\) if for any two vectors \\(Y_1\\in\\mathbb{R}^n\\) and \\(Y_2\\in\\mathbb{R}^n\\) such that \\(Y=Y_1+Y_2\\) and for any scalar \\(a\\in\\mathbb{R}\\) \\[\\begin{align*}\nf(Y_1+Y_2)&=f(Y_1)+f(Y_2)\\\\[2ex]\nf(aY)&=af(Y).\n\\end{align*}\\] This property applies to the OLS estimator \\[\n\\hat\\beta=f(Y)=(X'X)^{-1}X'Y\n\\] since \\[\\begin{align*}\n(X'X)^{-1}X'(Y_1+Y_2)\n& = (X'X)^{-1}X'Y_1 + (X'X)^{-1}X'Y_2\n\\end{align*}\\] and \\[\\begin{align*}\n(X'X)^{-1}X'aY & = a (X'X)^{-1}X'Y\n\\end{align*}\\] for any \\(Y_1\\) and \\(Y_2\\) such that \\(Y=Y_1+Y_2\\) and for any \\(a\\in\\mathbb{R}.\\)\nShowing unbiasedness: The OLS estimator is unbiased if \\[\n\\operatorname{Bias}(\\hat\\beta) = E(\\hat\\beta) - \\beta = 0\n\\] This can be shown as following: Observe that \\[\n\\hat\\beta=(X'X)^{-1}X'Y\n\\] consists of two multivariate random variables \\(X\\) and \\(Y.\\) Thus one needs to show first the conditional unbiasedness of \\(\\hat\\beta\\) given \\(X\\) which effectively allows us to focus on randomness due to \\(\\varepsilon,\\)\n\\[\n\\begin{align*}\n\\operatorname{Bias}(\\hat\\beta|X)\n&= E(\\hat\\beta|X)                                        - \\beta \\\\[2ex]\n&= E((X'X)^{-1}X'\\underbrace{Y}_{=X\\beta+\\varepsilon}|X) - \\beta \\\\[2ex]\n&= E((X'X)^{-1}X'(X\\beta+\\varepsilon)|X)                 - \\beta \\\\[2ex]\n&= E(\\underbrace{(X'X)^{-1}X'X}_{=I_K}\\beta|X) + E((X'X)^{-1}X'\\varepsilon|X) - \\beta \\\\[2ex]\n&= \\underbrace{E(\\beta|X)}_{=\\beta} + \\underbrace{E((X'X)^{-1}X'\\varepsilon|X)}_{=(X'X)^{-1}X'E(\\varepsilon|X)} - \\beta \\\\[2ex]\n&=  (X'X)^{-1}X'\\underbrace{E(\\varepsilon|X)}_{=0} =\\underset{(K\\times 1)}{0}  \n\\end{align*}\n\\] Thus \\(\\hat\\beta\\) is unbiased conditionally on \\(X.\\) From this if follows, by the iterated law of expectations, that the OLS estimator is also unconditionally unbiased, i.e.\n\\[\n\\operatorname{Bias}(\\hat\\beta) = E\\left(\\operatorname{Bias}(\\hat\\beta|X)\\right) = E(0) = 0.\n\\]\nFor the Gauss-Markov Theorem, we also need the conditional variance of \\(\\hat\\beta\\) given \\(X,\\) which can be derived as following: \\[\n\\begin{align*}\nVar(\\hat\\beta|X)\n&=Var(\\hat\\beta - \\beta|X)\\\\\n&=Var((X'X)^{-1}X'\\varepsilon|X),\n\\end{align*}\n\\] where we used that \\[\n\\begin{align*}\n\\hat\\beta\n&=(X'X)^{-1}X'Y\\\\\n&=(X'X)^{-1}X'(X\\beta+\\varepsilon)\\\\\n&=\\beta+(X'X)^{-1}X'\\varepsilon\\\\\n\\Leftrightarrow \\hat\\beta - \\beta\n& = (X'X)^{-1}X'\\varepsilon.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 4.1 (Conditional Variance of a Multivariate Random Variable) The conditional variance of a multivariate random variable \\(Z\\in\\mathbb{R}^K,\\) given \\(X,\\) is defined as \\[\\begin{align*}\nVar(Z|X)\n&=\\underbrace{E\\Big[\\overbrace{(Z-E(Z|X))}^{(K\\times 1)}\\overbrace{(Z-E(Z|X))'}^{(1\\times K)}|X\\Big]}_{K\\times K}\\\\[2ex]\n&=\\underbrace{E\\big[ZZ'|X\\big]}_{K\\times K} - \\underbrace{E\\big[Z|X\\big]E\\big[Z'|X\\big]}_{K\\times K}\n\\end{align*}\\]\n\n\n\nUsing the definition of the conditional variance (Definition 4.1) for multivariate random variables \\((Z=(X'X)^{-1}X'\\varepsilon)\\) we have that \\[\n\\begin{align*}\n&Var(\\hat\\beta|X)=\\\\[2ex]\n%&=Var(\\hat\\beta - \\beta|X)\\\\[2ex]\n&=Var((X'X)^{-1}X'\\varepsilon|X)\\\\[2ex]\n&=E\\Big[\\big((X'X)^{-1}X'\\varepsilon-\\underbrace{E((X'X)^{-1}X'\\varepsilon|X)}_{=0}\\big)\\times\\\\[2ex]\n&\\phantom{=\\Big(}\\,\\times\\big((X'X)^{-1}X'\\varepsilon-\\underbrace{E((X'X)^{-1}X'\\varepsilon|X)}_{=0}\\big)'|X\\Big]\\\\[2ex]\n&=E\\left[((X'X)^{-1}X'\\varepsilon)((X'X)^{-1}X'\\varepsilon)'|X\\right]\\\\[2ex]\n&=E\\left[(X'X)^{-1}X'\\varepsilon\\varepsilon' X(X'X)^{-1}|X\\right]\\\\[2ex]\n&=\\;\\;\\;(X'X)^{-1}X'\\underbrace{E\\left(\\varepsilon\\varepsilon'|X\\right)}_{=Var(\\varepsilon|X)}X(X'X)^{-1}\n\\end{align*}\n\\]\nUnder the assumption of spherical errors (see Assumption 4), we have that \\[\\begin{align*}\nVar\\left(\\varepsilon|X\\right)\n& = \\left(\\begin{matrix}\nVar(\\varepsilon_1|X)&Cov(\\varepsilon_1,\\varepsilon_2|X)&\\dots&Cov(\\varepsilon_1,\\varepsilon_n|X)\\\\\nCov(\\varepsilon_2,\\varepsilon_1|X)&Var(\\varepsilon_2|X)&\\dots&Cov(\\varepsilon_2,\\varepsilon_n|X)\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nCov(\\varepsilon_n,\\varepsilon_1|X)&Cov(\\varepsilon_n,\\varepsilon_2|X)&\\dots&Var(\\varepsilon_n|X)\n\\end{matrix}\\right)\\\\[2ex]\n& = \\left(\\begin{matrix}\n\\sigma^2&0&\\dots&0\\\\\n0&\\sigma^2&\\dots&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n0&0&\\dots&\\sigma^2\n\\end{matrix}\\right)\n= \\sigma^2 I_n,\n%& = E\\left(\\varepsilon\\varepsilon'|X\\right) - E\\left(\\varepsilon|X\\right) E\\left(\\varepsilon'|X\\right)\\\\[2ex]\n%& = E\\left(\\varepsilon\\varepsilon'|X\\right) - \\underset{(K\\times K)}{0}\\\\[2ex]\n\\end{align*}\\] where \\(I_n\\) is the \\((n\\times n)\\) dimensional identity matrix with ones on the diagonal and zeros everywhere else.\nThus under the assumption of spherical errors, we have that \\[\n\\begin{align*}\nVar(\\hat\\beta|X)\n&=(X'X)^{-1}X' \\left(\\sigma^2I_n\\right)X(X'X)^{-1}\\\\[2ex]\n&=\\sigma^2(X'X)^{-1}X'X(X'X)^{-1}\\\\[2ex]\n&=\\sigma^2(X'X)^{-1}.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\nSummary:\n\nThe OLS estimator belongs to the large family of linear (in \\(Y\\)) and unbiased estimators of \\(\\beta.\\)\nUnder the assumption of spherical errors, the conditional variance of \\(\\hat\\beta,\\) given \\(X,\\) is \\[\nVar(\\hat\\beta|X)=\\sigma^2(X'X)^{-1}.\n\\]\n\n\n\nThe famous Gauss-Markov Theorem states that the OLS estimator is the “best” (smallest conditional variance) estimator within the family of linear (in \\(Y\\)) and unbiased estimators.\n\n\n\n\n\n\n\n\n\n\n\nTheorem 4.5 (Gauss-Markov Theorem) If it holds true that\n\n\\(Y = X\\beta + \\varepsilon\\)\n\\(\\operatorname{rank}(X)=K\\) a.s.\n\\(E(\\varepsilon|X) = \\underset{(n\\times 1)}{0}\\)\n\\(Var(\\varepsilon|X) = \\sigma^2I_n\\)\n\nthen the OLS estimator \\[\n\\underset{(K\\times 1)}{\\hat\\beta}=(X'X)^{-1}X'Y\n\\] has the smallest conditional variance (given \\(X\\)) among all linear and unbiased estimators. That is, for any alternative linear and unbiased estimator \\(\\tilde{\\beta}\\) we have that \\[\n\\begin{align*}\n&Var(\\tilde\\beta|X)\\geq Var(\\hat\\beta|X)\\quad\\text{(in the matrix sense)}\\\\\n\\Leftrightarrow&Var(\\tilde\\beta|X)-Var(\\hat\\beta|X)=\\underset{(K\\times K)}{D},\n\\end{align*}\n\\] where \\(D\\) is a positive semidefinite \\((K\\times K)\\) matrix which implies that \\[\nVar(\\tilde{\\beta}_k|X) \\geq Var(\\hat\\beta_k | X)\n\\] for all \\(k=1,\\dots,K\\).\n\n\n\n\n\n\nTip\n\n\n\nA \\((K\\times K)\\) matrix \\(D\\) is called “positive semidefinite” if \\[\na'Da\\geq 0\n\\] for any \\(K\\)-dimensional vector \\(a\\in\\mathbb{R}^K\\).\n\n\n\n\n\n\nProof. Since \\(\\tilde{\\beta}\\) is assumed to be linear in \\(Y\\), we can write \\[\n\\tilde{\\beta}=CY,\n\\] where \\(C\\) is some \\((K\\times n)\\) matrix, which is a function of \\(X\\) and/or nonrandom components. Adding a \\((K\\times n)\\) zero matrix \\(0\\) yields \\[\n\\tilde{\\beta}=\\Big(C\\overbrace{-\\left(X'X\\right)^{-1}X'+\\left(X'X\\right)^{-1}X'}^{=0}\\Big)Y.\n\\] Let now \\(D=C-\\left(X'X\\right)^{-1}X'\\), then \\[\n\\begin{align*}\n\\tilde{\\beta}&=\\left(D+\\left(X'X\\right)^{-1}X'\\right)Y\\\\\n\\tilde{\\beta}&=DY + \\left(X'X\\right)^{-1}X'Y\\\\\n\\tilde{\\beta}&=D\\left(X{\\beta}+{\\varepsilon}\\right) + \\left(X'X\\right)^{-1}X'Y\n\\end{align*}\n\\] Thus \\[\n\\tilde{\\beta}=DX{\\beta}+D{\\varepsilon} + \\hat{\\beta}.\n\\tag{4.3}\\] Moreover, \\[\nE(\\tilde{\\beta}|X)=\\underbrace{E(DX{\\beta}|X)}_{=DX\\beta}+\\underbrace{E(D\\varepsilon|X)}_{=DE(\\varepsilon|X)=0}+\\underbrace{E(\\hat{\\beta}|X)}_{=\\beta}\n\\] and thus \\[\nE(\\tilde{\\beta}|X)=DX{\\beta}+0+{\\beta}.\n\\tag{4.4}\\]\nSince \\(\\tilde{\\beta}\\) is (by assumption) unbiased, we have that \\(E(\\tilde{\\beta}|X)={\\beta}\\). Therefore, Equation 4.4 implies that \\(DX=0_{(K\\times K)}\\) since we must have that \\[\nE(\\tilde{\\beta}|X)=DX{\\beta}+0+{\\beta}=\\beta.\n\\] Plugging \\(DX=0\\) into Equation 4.3 yields, \\[\n\\begin{align*}\n\\tilde{\\beta}&=D{\\varepsilon} + \\hat{\\beta}\\\\\n\\tilde{\\beta}-{\\beta}&=D{\\varepsilon} + (\\hat{\\beta}-{\\beta})\\\\\n\\tilde{\\beta}-{\\beta}&=D{\\varepsilon} + \\left(X'X\\right)^{-1}X'{\\varepsilon}\n\\end{align*}\n\\] such that \\[\n\\tilde{\\beta}-{\\beta}=\\left(D + \\left(X'X\\right)^{-1}X'\\right){\\varepsilon},\n\\tag{4.5}\\] where we used that \\[\n\\begin{align*}\n\\hat\\beta-\\beta&=(X'X)^{-1}X'Y-\\beta\\\\\n&=(X'X)^{-1}X'(X\\beta+\\varepsilon)-\\beta\\\\\n&=(X'X)^{-1}X'\\varepsilon.\n\\end{align*}\n\\]\nUsing that \\(Var(\\tilde{\\beta}|X)= Var(\\tilde{\\beta}-{\\beta}|X)\\) since \\(\\beta\\) is not random and using Equation 4.5 yields \\[\n\\begin{align*}\nVar(\\tilde{\\beta}|X)\n&= Var((D + (X'X)^{-1}X'){\\varepsilon}|X)\\\\\n&= (D + (X'X)^{-1}X')Var({\\varepsilon}|X)(D' + X(X'X)^{-1})\\\\\n&= \\sigma^2(D + (X'X)^{-1}X')I_n(D' + X(X'X)^{-1})\\\\\n&= \\sigma^2\\left(DD'+(X'X)^{-1}\\right)\\quad \\text{(using that $DX=0$)} \\\\\n&\\geq\\sigma^2(X'X)^{-1} \\quad \\text{(using that $DD'\\geq 0$)}\\\\\n&= Var(\\hat{\\beta}|X).\n\\end{align*}\n\\] Finally, we need to show that \\(DD'\\) is really positive semidefinite (i.e. \\(DD'\\geq 0\\) in matrix sense): \\[\n\\begin{align*}\na'DD'a=(D'a)'(D'a)=\\tilde{a}'\\tilde{a}\\geq 0,\n\\end{align*}\n\\] where \\(\\tilde{a}\\) is a \\(K\\) dimensional column-vector."
  },
  {
    "objectID": "04-Multiple-Linear-Regression.html#practice",
    "href": "04-Multiple-Linear-Regression.html#practice",
    "title": "4  Multiple Linear Regression",
    "section": "4.6 Practice",
    "text": "4.6 Practice\n\n4.6.1 Factor Variables and the Dummy-Variable Trap\nIn the following, we consider a simple linear regression model \\[\nY_i = \\beta_1 + \\beta_2 X_{i2} + \\varepsilon_i\n\\] that aims to predict wages \\(Y_i\\in\\mathbb{R}\\) in the year 2008 using gender \\(X_{i2}\\in\\{\\texttt{male},\\texttt{female}\\}\\) as the only predictor.\nHere \\[\nX_{i2}\\in\\{\\texttt{male},\\texttt{female}\\}\n\\] is a categorical variable also called factor variable with two categories (two factor levels).\nWe use data provided in the accompanying materials of Stock and Watson’s Introduction to Econometrics textbook (Stock and Watson 2015). You can download the data stored as an xlsx-file cps_ch3.xlsx HERE.\nLet us first prepare the dataset:\n\n## load the 'tidyverse' package\nsuppressPackageStartupMessages(library(\"tidyverse\"))\n## load the 'readxl' package\nlibrary(\"readxl\")\n\n## import the data into R\ncps <- read_excel(path = \"data/cps_ch3.xlsx\")\n\n# names(cps)\n# range(cps$year)\n# range(cps$a_sex) # 1 = male, 2 = female\n\n## Data wrangling\ncps_2008 <- cps %>% \n  mutate(\n    wage   = ahe08,      # rename \"ahe08\" as \"wage\"    \n    gender = fct_recode( # rename factor \"a_sex\" as \"gender\"\n      as_factor(a_sex),     \n                \"male\" = \"1\",  # rename factor level \"1\" to \"male\"\n                \"female\" = \"2\" # rename factor level \"2\" to \"female\"\n             ) \n  ) %>%  \n  filter(year == 2008) %>%     # Only data from year 2008\n  select(wage, gender)         # Select only the variables \"wage\" and \"gender\"\n\nThe first six lines of the dataset cps_2008 look as following:\n\n\n\n\n \n  \n    wage \n    gender \n  \n \n\n  \n    38.46154 \n    male \n  \n  \n    12.50000 \n    male \n  \n  \n    17.78846 \n    male \n  \n  \n    30.38461 \n    male \n  \n  \n    23.66864 \n    male \n  \n  \n    12.01923 \n    female \n  \n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAbove in our theoretical part, we have not mentioned the possibility of categorical, i.e., factor variables.\n\nHow can one compute the estimators from non-numeric factor levels like female and male?\n\nWell, we need to transform the non-numeric factor level to numeric data. R and other statistical software-packages do this for us in the background.\n\n\nComputing the estimation results:\n\nlm_obj <- lm(wage ~ gender, data = cps_2008)\n\ncoef(lm_obj)\n\n\n\n\ngives\n\n\\(\\hat\\beta_1=\\) 24.98\n\\(\\hat\\beta_2=\\) -4.1\n\nTo compute these estimation results, R assigns to each factor level (male and female) a numeric value. To see the numeric values used by R one can take a look at model.matrix(lm_obj):\n\n# this is the internally used numeric X-matrix\nX <- model.matrix(lm_obj) \nX[1:6,]\n\n  (Intercept) genderfemale\n1           1            0\n2           1            0\n3           1            0\n4           1            0\n5           1            0\n6           1            1\n\n\nCompare this with the factor variable gender in the dataset cps_2008:\n\n# Factor variable 'gender' in the dataset cps_2008\ncps_2008$gender[1:6]\n\n[1] male   male   male   male   male   female\nLevels: male female\n\n\nThus R internally recodes \\(X_{i2}\\in\\{\\texttt{male}, \\texttt{female}\\}\\) as a dummy variable \\(X_{i2}\\in\\{0,1\\}\\) such that \\[\nX_{i2}=\n\\left\\{\\begin{array}{ll}\n0 &\\text{ if subject $i$ is male}\\\\\n1 &\\text{ if subject $i$ is female}.\n\\end{array} \\right.\n\\] Therefore, we can interpret the estimation result as \\[\n\\hat\\beta_1  + \\hat\\beta_2 X_{i2}=\n\\left\\{\\begin{array}{ll}\n\\hat\\beta_1              &\\text{ if subject $i$ is male}\\\\\n\\hat\\beta_1 + \\hat\\beta_2&\\text{ if subject $i$ is female}\n\\end{array} \\right.\n\\]\nInterpretation:\n\nThe average wage of male workers in 2008 was \\(\\hat{\\beta}_1=\\) 24.98 (USD/Hour).\nThe average wage of female workers in 2008 was \\(\\hat{\\beta}_1 + \\hat{\\beta}_2=\\) 20.87 (USD/Hour).\nThe difference in the earnings between male and female works in 2008 is \\(\\hat{\\beta}_2=\\) -4.1 (USD/Hour).\n\n\n\nDummy-Variable Trap\nAbove, we used R’s automatic recoding of categorical factor variables into numeric dummy variable which is very convenient.\nOf course, we can also construct a dummy variable for each of the levels of a factor yourself. But be careful, and do not step into the dummy variable trap!\nLet’s construct the \\(Y\\) variable and the numeric \\(X\\) variable “by hand”:\n\n## Dependent variable\nY        <- cps_2008$wage\n\n## Intercept variable\nX_1      <- rep(1, times = nrow(cps_2008))\n\n## 1. Dummy variable for 'female' (indicates female subjects)\nX_female <- ifelse(cps_2008$gender == \"female\", 1, 0)\n\n## 2. Dummy variable for 'male' (indicates male subjects)\nX_male   <- ifelse(cps_2008$gender == \"male\", 1, 0)\n\n## Construct the numeric model matrix 'X'\nX        <- cbind(X_1, X_female, X_male)\n\nComputing the estimation result for \\(\\beta\\) “by hand” yields\n\n## Computing the estimator\nbeta_hat <- solve(t(X) %*% X) %*% t(X) %*% Y\n\n\nError in solve.default(X %*% t(X)) : \n  Lapack routine dgesv: system is exactly singular: U[3,3] = 0\nCalls: .main ... eval_with_user_handlers -> eval -> eval -> solve -> solve.default\nExecution halted\n\n\n\n\n\n\n\nAn error message! 🤬\n\n\n\nWe stepped into the dummy variable trap!\nThat is, we constructed a \\((n\\times K)\\)-dimensional matrix \\(X\\) for which Assumption 3 \\((\\operatorname{rank}(X)=K)\\) is violated. Therefore \\((X'X)\\) does not have full rank and is thus not invertible.\n\n\nThe estimation result is not computable since \\((X'X)\\) is not invertible due to the perfect multicollinearity between the intercept and the two dummy variables X_female and X_male, i.e. \n\nX_1 = X_female \\(+\\) X_male\n\n which violates Assumption 3 (no perfect multicollinearity).\n\n\n\n\n\n\nSolution\n\n\n\nUse one dummy-variable less than factor levels. I.e., in this example you can either drop X_female or X_male.\n\n\n\n## New model matrix after dropping X_male:\nX        <- cbind(X_1, X_female)\n\n## Computing the estimator\nbeta_hat <- solve(t(X) %*% X) %*% t(X) %*% Y\nbeta_hat\n\n              [,1]\nX_1      24.978404\nX_female -4.103626\n\n\nThis give the same result as computed by R’s lm() function using the factor variable gender.\n\n\n4.6.2 Detecting Heteroskedasticity\nA very simple, but highly effective way to check for heteroskedastic error terms is to plot the residuals. If we have the correct model (hopefully we have), then (and only then) the residuals \\(\\hat\\varepsilon_i\\) are good approximations to the realizations of the error terms \\(\\varepsilon_i\\). Thus a plot of the residuals can be used to check for heteroskedasticity.\nYou can use R’s internal diagnostic plots that can be called using the plot() method for lm-objects:\n\n# install.packages(\"devtools\")\n# library(devtools)\n# install_git(\"https://github.com/ccolonescu/PoEdata\")\n\nlibrary(PoEdata) # for the \"food\" dataset contained in this package\n\ndata(\"food\")     # makes the dataset \"food\" usable\n\nlm_object <- lm(food_exp ~ income, data = food)\n\n## Diagnostic scatter plot of residuals vs fitted values \nplot(lm_object, which = 1)\n\n\n\n\nInterpretation:\n\nThe plot shows the residuals \\(\\hat{\\varepsilon}_i\\) plotted against the fitted values \\(\\hat{Y}_i\\).\nThe diagnostic plot indicates that the variance increases with \\(\\hat{Y}_i=X_i'\\hat{\\beta}.\\)\n\n\n\n\n\n\n\nTip\n\n\n\nNote: Plotting against the fitted values is actually a quite smart idea since this also works for multiple predictors \\(X_{i1},\\dots,X_{iK}\\) with \\(K\\geq 3\\).\n\n\n\n\n4.6.3 Checking (Non-)Linearity of the Regression Line\nTo check whether there are non-linear relationships between the outcome and the predictors, one should take a look at the data using a pairs-plot function that procures scatter plots for all pairs of variables in a dataset.\n\nBase RTidyverse R\n\n\n\ncar_data <- read.csv(file = \"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv\")\n\nmy_car_df <- data.frame(\n    \"MPG\"   = car_data$MPG.city,\n    \"HP\"    = car_data$Horsepower\n    )\n\npairs(my_car_df)\n\n\n\n\n\n\n\n## install.packages(\"tidyverse\")\n## install.packages(\"GGally\")\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nsuppressPackageStartupMessages(library(\"GGally\")) # nice pairs plot \n\ncar_data <- readr::read_csv(\n  file = \"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv\",\n  show_col_types = FALSE)\n\nmy_car_df <- car_data %>% \n dplyr::mutate(\n  \"MPG\"   = MPG.city, \n  \"HP\"    = Horsepower) %>%\n select(\"MPG\", \"HP\")\n\nggpairs(my_car_df) + theme_bw()\n\n\n\n\n\n\n\nThe plots indicate a positive, but non-linear relationship between the outcome variable MPG (gasoline consumption in miles per gallon) and the predictor HP (power of the mashing in horsepower).\nLet’s begin with a simple linear regression model \\[\nY_i = \\beta_1 + \\beta_2 X_{i2} +  \\varepsilon_i\n\\] with\n\n\\(Y_i\\) denoting MPG of car \\(i\\) and\n\\(X_{i2}\\) denoting HP of car \\(i.\\)\n\nSuch a simple model is not able to take into account the non-linear relationship between \\(Y_i\\) and \\(X_{i2}\\). Therefore, we get bad model fits which can be spotted\n\nin the scatter plot of MPG vs. HP with added linear regression line fit and\nin the diagnostic plot of the residuals \\(\\hat{\\varepsilon}_i\\) vs. the fitted values \\(\\hat{Y}_i=\\hat\\beta_1 + \\hat\\beta_2 X_{i2}.\\)\n\n\nBase RTidyverse R\n\n\n\nlm_obj_1 <- lm(MPG ~ HP, data = my_car_df)\n\npar(mfrow=c(2,1))\nplot(y=my_car_df$MPG, x=my_car_df$HP, \n     main = \"Simple Linear Regression\",\n     xlab = \"Horse Power\", \n     ylab = \"Miles per Gallon\")\nabline(lm_obj_1)\n##\nplot(lm_obj_1, which=1, \n       main = \"Simple Linear Regression\")\n\n\n\n\n\n\n\n## install.packages(\"tidyverse\")\n## install.packages(\"ggfortify\")\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nlibrary(\"ggfortify\") # tidy diagnostic plots\n\nlm_obj_1 <- lm(MPG ~ HP, data = my_car_df)\n\n## Plot: Simple Linear Regression\nmy_car_df %>%\nggplot(aes(x=HP, y=MPG)) + \n    geom_point(alpha=0.8, size=3)+\n    stat_smooth(method='lm', formula = y~x) +\n    theme_classic() + \n    labs(x = \"Horse Power\", y = \"Miles per Gallon\", \n         title = \"Simple Linear Regression\")\n\n\n\n## Diagnostic Plot: Simple Linear Regression \nautoplot(lm_obj_1, which = 1)\n\n\n\n\n\n\n\nTo model the non-linear, more or less quadratic relationship between \\(Y_i\\) and \\(X_{i2}\\), we can use a a polynomial regression model with polynomial degree 2: \\[\nY_i = \\beta_1 + \\beta_2 X_{i2} + \\beta_3 X_{i2}^2 +  \\varepsilon_i\n\\] with\n\n\\(Y_i\\) denoting MPG of car \\(i,\\)\n\\(X_{i2}\\) denoting HP of car \\(i,\\) and\n\\(X_{i2}^2\\) denoting the squared value of HP of car \\(i.\\)\n\nThis more flexible model is indeed better in taking into account the non-linear relationship between \\(Y_i\\) and \\(X_{i2}.\\) The improved model fits can be spotted\n\nin the scatter plot of MPG vs. HP with added non-linear regression line fit and\nin the diagnostic plot of the residuals \\(\\hat{\\varepsilon}_i\\) vs. the fitted values \\(\\hat{Y}_i=\\hat\\beta_1 + \\hat\\beta_2 X_{i2} + \\hat\\beta_3 X_{i2}^2.\\)\n\n\nBase RTidyverse R\n\n\n\n## Adding new variable HP squared:\nmy_car_df$HP_sq <- car_data$Horsepower^2\n\nlm_obj_2 <- lm(MPG ~ HP + HP_sq, data = my_car_df)\n\npar(mfrow=c(2,1))\nplot(y=my_car_df$MPG, x=my_car_df$HP,\n     main = \"Polynomial Regression (Degree: 2)\",\n     xlab = \"Horse Power\", \n     ylab = \"Miles per Gallon\")\nX_seq <- seq(from = min(my_car_df$HP), \n             to   = max(my_car_df$HP), len = 25)\nlines(y = predict(lm_obj_2, \n                  newdata=data.frame(\"HP\"    = X_seq, \n                                     \"HP_sq\" = X_seq^2)), \n      x = X_seq)\nplot(lm_obj_2, which=1, \n       main = \"Polynomial Regression (Degree: 2)\")\n\n\n\n\n\n\n\n## install.packages(\"tidyverse\")\n## install.packages(\"ggfortify\")\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nlibrary(\"ggfortify\") # tidy diagnostic plots\n\n## Adding new variable HP squared:\nmy_car_df <- my_car_df %>% \n dplyr::mutate(\n  \"HP_sq\" = HP^2\n)\n\n## Polynomial Regression\n\nlm_obj_2 <- lm(MPG ~ HP + HP_sq, data = my_car_df)\n\n## Plot: Polynomial Regression\nmy_car_df %>%\nggplot(aes(x=HP, y=MPG)) + \n    geom_point(alpha=0.8, size=3)+\n    stat_smooth(method='lm', formula = y~poly(x, 2, raw = TRUE)) +\n    theme_classic() + \n    labs(x = \"Horse Power\", y = \"Miles per Gallon\", \n         title = \"Polynomial Regression\", \n         subtitle = \"Polynomial Degree: 2\")\n\n\n\n## Diagnostic Plot: Polynomial Regression\nautoplot(lm_obj_2, which = 1)     \n\n\n\n\n\n\n\n\n\n4.6.4 Behavior of the OLS Estimates for Resampled Data\nUsually, we only observe one estimate \\(\\hat{\\beta}\\) of \\(\\beta\\) computed based on one given dataset (one realization of the random sample).\nHowever, in order to understand the statistical properties of the estimators \\(\\hat{\\beta}\\) we need to view them as random variables which yield different realizations in repeated realizations of the random sample from Model (Equation 4.1).\nThis view allows us then to think about questions like:\n\nIs the estimator able to estimate the unknown parameter-value correctly on average?\n\nAre the estimation results more precise (small variance) if we have more data?\n\nA first idea about the statistical properties of the estimator \\(\\hat{\\beta}\\) can be gained using Monte Carlo simulations as following.\n\n## Sample sizes\nn_small      <-  30 # smallish sample size\nn_large      <- 100 # largish sample size\n\n## True parameter values\nbeta0 <- 1\nbeta1 <- 1\n\n## Monte-Carlo (MC) Simulation \n## 1. Generate data\n## 2. Compute and store estimates\n## Repeat steps 1. and 2. many times\nset.seed(3)\n## Number of Monte Carlo repetitions\n## How many samples to draw from the models\nB          <- 1000\n\n## Containers to store the estimation results\nbeta0_estimates_n_small <- numeric(B)\nbeta1_estimates_n_small <- numeric(B)\nbeta0_estimates_n_large <- numeric(B)\nbeta1_estimates_n_large <- numeric(B)\n\nfor(b in 1:B){\n## Generate artificial samples (n_small)\nerror_n_small     <- rnorm(n_small, mean = 0, sd = 5)\nX_n_small         <- runif(n_small, min = 1, max = 10)\nY_n_small         <- beta0 + beta1 * X_n_small + error_n_small\nlm_obj            <- lm(Y_n_small ~ X_n_small) \n## Save estimation results \nbeta0_estimates_n_small[b] <- lm_obj$coefficients[1]\nbeta1_estimates_n_small[b] <- lm_obj$coefficients[2]\n\n## Generate artificial samples (n_large)\nerror_n_large     <- rnorm(n_large, mean = 0, sd = 5)\nX_n_large         <- runif(n_large, min = 1, max = 10)\nY_n_large         <- beta0 + beta1 * X_n_large + error_n_large\nlm_obj            <- lm(Y_n_large ~ X_n_large)\n## Save estimation results \nbeta0_estimates_n_large[b] <- lm_obj$coefficients[1]\nbeta1_estimates_n_large[b] <- lm_obj$coefficients[2] \n}\n\nNow, we have produced B=1000 realizations of the estimators \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) and saved these realizations in the vectors\n\nbeta0_estimates_n_small\nbeta1_estimates_n_small\nbeta0_estimates_n_large\nbeta1_estimates_n_large\n\nThese artificially generated realizations allow us to visualize the behavior of the OLS estimates for the repeatedly sampled data.\n\n\n\n\n\n\n\n\n\nThis are promising plots:\n\nThe realizations of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are scattered around the true (unknown) parameter values \\(\\beta_0\\) and \\(\\beta_1\\). This indicates unbiasdness of the estimators.\nIn the case of a larger sample size, the realizations of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) concentrate stronger around the true (unknown) parameter values \\(\\beta_0\\) and \\(\\beta_1.\\) This indicates consistency of the estimators.\n\n\n\n\n\n\n\nImportant\n\n\n\nHowever, this was only a simulation for one specific data generating process. Such a Monte Carlo simulation does not allow us to generalize the observed properties to other data generating processes.\nIn the following chapters, we use theoretical arguments to investigate under which assumptions we can make general statements about the distributional properties of the estimator \\(\\hat\\beta.\\)"
  },
  {
    "objectID": "04-Multiple-Linear-Regression.html#references",
    "href": "04-Multiple-Linear-Regression.html#references",
    "title": "4  Multiple Linear Regression",
    "section": "References",
    "text": "References\n\n\n\n\nStock, J. H., and M. W. Watson. 2015. Introduction to Econometrics. Pearson Education."
  },
  {
    "objectID": "03-Monte-Carlo-Simulations.html#exercises",
    "href": "03-Monte-Carlo-Simulations.html#exercises",
    "title": "\n4  Estimation Theory and Monte Carlo Simulations\n",
    "section": "\n4.4 Exercises",
    "text": "4.4 Exercises",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation Theory and Monte Carlo Simulations</span>"
    ]
  },
  {
    "objectID": "04-Multiple-Linear-Regression.html#exercises",
    "href": "04-Multiple-Linear-Regression.html#exercises",
    "title": "4  Multiple Linear Regression",
    "section": "4.7 Exercises",
    "text": "4.7 Exercises\n\nExercises for Chapter 4\nExercises of Chapter 4 with Solutions"
  },
  {
    "objectID": "05-Small-Sample-Inference.html#sec-testingsinglep",
    "href": "05-Small-Sample-Inference.html#sec-testingsinglep",
    "title": "5  Small Sample Inference",
    "section": "5.2 Tests about One Parameter (t-Tests)",
    "text": "5.2 Tests about One Parameter (t-Tests)\nA hypothesis about only one parameter \\[\\begin{equation*}\n\\begin{array}{ll}\n& H_0: \\beta_k=\\beta_k^{(0)}\\\\\n\\text{versus}\\quad & H_1: \\beta_k\\ne \\beta_k^{(0)}\\\\\n\\end{array}\n\\end{equation*}\\] is simply a special case of the general null hypothesis \\(H_0:R\\beta =r^{(0)},\\) where\n\n\\(R\\) is a \\((1\\times K)\\) row-vector of zeros, but with a one as the \\(k\\)th element, and where\nwe write \\(r^{(0)}=\\beta_k^{(0)}\\) since we make a hypothesis only about \\(\\beta_k.\\)\n\nThus the \\(F\\)-test statistic simplifies to \\[\nF=\\frac{\\left(\\hat{\\beta}_k-\\beta_k^{(0)}\\right)^2}{\\widehat{Var}(\\hat{\\beta}_k|X)}\\overset{H_0}{\\sim}F_{(1,n-K)},\n\\] where \\[\n\\widehat{Var}(\\hat{\\beta}_k|X)=s^2_{UB}[(X'X)^{-1}]_{kk}.\n\\] Taking square roots yields the \\(t\\) test statistic \\[\nT=\\frac{\\hat{\\beta}_k-\\beta_k^{(0)}}{\\widehat{\\operatorname{SE}}(\\hat{\\beta}_k|X)}\\overset{H_0}{\\sim}t_{(n-K)},\n\\] where \\[\n\\widehat{\\operatorname{SE}}(\\hat{\\beta}_k|X)\n=\\sqrt{\\widehat{Var}(\\hat{\\beta}_k|X)}=s_{UB}[(X'X)^{-1/2}]_{kk},\n\\] and where \\(t_{(n-K)}\\) denotes the \\(t\\)-distribution with \\(n-K\\) degrees of freedom.\nThus the \\(t\\)-distribution with \\(n-K\\) degrees of freedom is the appropriate distribution to judge whether or not an observed value \\(T_{\\text{obs}}\\) of the test statistic is “unusually large” under the null hypothesis.\n\n\n\n\n\n\nTip\n\n\n\nAll commonly used statistical software packages report \\(t\\)-tests testing the null hypothesis \\[\nH_0:\\beta_k=0\n\\] for each \\(k=1,\\dots,K.\\) This means to test the null hypothesis that \\(X_k\\) has “no (linear) effect” on the conditional mean of \\(Y\\) given \\(X.\\)\n\n\nThe \\(t\\) distribution. The following plot illustrates that as the degrees of freedom increase, the shape of the \\(t\\) distribution comes closer to that of a standard normal bell curve. Already for \\(25\\) degrees of freedom we find little difference to the standard normal density. In case of small degrees of freedom values, we find the distribution to have heavier tails than a standard normal. See Section 2.2.10.4 for more information about the \\(t\\)-distribution."
  },
  {
    "objectID": "02b-Matrix-Algebra.html",
    "href": "02b-Matrix-Algebra.html",
    "title": "\n3  Matrix Algebra\n",
    "section": "",
    "text": "3.1 Basic definitions\nLet’s start with some basic definitions and specific examples.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "02b-Matrix-Algebra.html#scalar-vector-and-matrix",
    "href": "02b-Matrix-Algebra.html#scalar-vector-and-matrix",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n4.1 Scalar, vector, and matrix",
    "text": "4.1 Scalar, vector, and matrix\nA scalar \\(a\\) is a single real number. We write \\(a \\in \\mathbb R\\).\nA vector \\(\\boldsymbol a\\) of length \\(k\\) is a \\(k \\times 1\\) list of real numbers \\[\n\\boldsymbol a = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_k \\end{pmatrix}.\n\\] By default, when we refer to a vector, we always mean a column vector. We write \\(\\boldsymbol a \\in \\mathbb R^k\\). The value \\(a_i\\) is called \\(i\\)-th entry or \\(i\\)-th component of \\(\\boldsymbol a\\). A scalar is a vector of length 1. A row vector of length \\(k\\) is written as \\(\\boldsymbol b = (b_1, \\ldots, b_k)\\).\nA matrix \\(\\boldsymbol A\\) of order \\(k \\times m\\) is a rectangular array of real numbers \\[\\boldsymbol A =\\begin{pmatrix}\n             a_{11} & a_{12} & \\cdots & a_{1m}\\\\\n             a_{21} & a_{22} & \\cdots & a_{2m}\\\\\n             \\vdots & \\vdots &       & \\vdots\\\\\n             a_{k1} & a_{k2} & \\cdots & a_{km}\n\\end{pmatrix}\\] with \\(k\\) rows and \\(m\\) columns. We write \\(\\boldsymbol A \\in \\mathbb R^{k \\times m}\\). The value \\(a_{ij}\\) is called \\((i,j)\\)-th entry or \\((i,j)\\)-th component of \\(\\boldsymbol A\\). We also use the notation \\((\\boldsymbol A)_{i,j}\\) to denote the \\((i,j)\\)-th entry. A vector of length \\(k\\) is a \\(k \\times 1\\) matrix. A row vector of length \\(k\\) is a \\(1 \\times k\\) matrix. A scalar is a matrix of order \\(1 \\times 1\\).\nWe may describe a matrix \\(\\boldsymbol A\\) by its column or row vectors as \\[\n\\boldsymbol A = \\begin{pmatrix} \\boldsymbol a_1 & \\boldsymbol a_2 & \\ldots & \\boldsymbol a_m \\end{pmatrix}\n= \\begin{pmatrix} \\boldsymbol \\alpha_1 \\\\ \\vdots \\\\ \\boldsymbol \\alpha_k \\end{pmatrix},\n\\] where \\[\n\\boldsymbol a_i = \\begin{pmatrix} a_{1i} \\\\ \\vdots \\\\ a_{ki} \\end{pmatrix}\n\\] is the \\(i\\)-th column of \\(\\boldsymbol A\\) and \\(\\boldsymbol \\alpha_i = (a_{i1}, \\ldots, a_{im})\\) is the \\(i\\)-th row.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "02b-Matrix-Algebra.html#some-specific-matrices",
    "href": "02b-Matrix-Algebra.html#some-specific-matrices",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n4.2 Some specific matrices",
    "text": "4.2 Some specific matrices\nA matrix is called square matrix if the numbers of rows and columns coincide (i.e., \\(k=m\\)). \\[\\boldsymbol{B} = \\begin{pmatrix}1 & 2 \\\\ 3 & 4 \\end{pmatrix}\\] is a square matrix. A square matrix is called diagonal matrix if all off-diagonal elements are zero. \\[\\boldsymbol{C} = \\begin{pmatrix}1 & 0 \\\\ 0 & 4 \\end{pmatrix}\\] is a diagonal matrix. We also write \\(\\boldsymbol C = \\mathop{\\mathrm{diag}}(1,4)\\). A square matrix is called upper triangular if all elements below the main diagonal are zero, and lower triangular if all elements above the main diagonal are zero. Examples of an upper triangular matrix \\(\\boldsymbol D\\) and a lower triangular matrix \\(\\boldsymbol E\\) are \\[\\boldsymbol{D} = \\begin{pmatrix}1 & 2 \\\\ 0 & 4 \\end{pmatrix},\n\\quad\n\\boldsymbol{E} = \\begin{pmatrix}1 & 0 \\\\ 3 & 4 \\end{pmatrix}.\n\\] The \\(k \\times k\\) diagonal matrix \\[\\boldsymbol{I}_k=\n      \\begin{pmatrix}\n      1      & 0      & \\cdots & 0\\\\\n      0      & 1      & \\cdots & 0\\\\\n      \\vdots & \\vdots & \\ddots & \\vdots\\\\\n      0      & 0      & \\cdots & 1\n      \\end{pmatrix} = \\mathop{\\mathrm{diag}}(1, \\ldots, 1)\\] is called identity matrix of order \\(k\\). The \\(k \\times m\\) matrix \\[\\boldsymbol{0}_{k\\times m}\n       =\\begin{pmatrix}\n             0      & \\cdots & 0\\\\\n            \\vdots &  \\ddots      & \\vdots\\\\\n            0      & \\cdots & 0\n       \\end{pmatrix}\n      \\] is called zero matrix. The zero vector of length \\(k\\) is \\[\\boldsymbol{0}_{k}\n       =\\begin{pmatrix}\n             0   \\\\\n            \\vdots \\\\\n            0\n       \\end{pmatrix}.\n      \\] If the order becomes clear from the context, we omit the indices and write \\(\\boldsymbol I\\) for the identity matrix and \\(\\boldsymbol 0\\) for the zero matrix or zero vector.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "02b-Matrix-Algebra.html#transposition",
    "href": "02b-Matrix-Algebra.html#transposition",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n4.3 Transposition",
    "text": "4.3 Transposition\nThe transpose \\(\\boldsymbol A'\\) of the matrix \\(\\boldsymbol A\\) is obtained by flipping rows and columns on the main diagonal: \\[\\boldsymbol{A}'=\\begin{pmatrix}\n    a_{11} & a_{21} & \\cdots & a_{k1}\\\\\n    a_{12} & a_{22} & \\cdots & a_{k2}\\\\\n    \\vdots & \\vdots &       & \\vdots\\\\\n    a_{1m} & a_{2m} & \\cdots & a_{km}\n\\end{pmatrix}.\\] If \\(\\boldsymbol A\\) is a matrix of order \\(k\\times m\\), then \\(\\boldsymbol A'\\) is a matrix of order \\(m \\times k\\). Example: \\[\\boldsymbol{A}=\\begin{pmatrix}\n        1 & 2\\\\\n        4 & 5\\\\\n        7 & 8\n\\end{pmatrix}\\quad\\Rightarrow\\quad\n\\boldsymbol{A}'=\\begin{pmatrix}\n        1 & 4 & 7\\\\\n        2 & 5 & 8\n\\end{pmatrix}\\] The definition implies that transposing twice produces the original matrix: \\[\n(\\boldsymbol A')' = \\boldsymbol A.\n\\] The transpose of a (column) vector is a row vector: \\[\n\\boldsymbol a' = (a_1, \\ldots, a_k)\n\\]\nA symmetric matrix is a square matrix \\(\\boldsymbol A\\) with \\(\\boldsymbol{A}'=\\boldsymbol{A}\\). An example of a symmetric matrix is \\[\\boldsymbol A = \\begin{aligned}         \\left(\\begin{matrix}1 & 2 \\\\ 2 & 4 \\end{matrix}\\right)\\end{aligned}.\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "02b-Matrix-Algebra.html#matrices-in-r",
    "href": "02b-Matrix-Algebra.html#matrices-in-r",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n4.4 Matrices in R\n",
    "text": "4.4 Matrices in R\n\nLet’s define some matrices in R:\n\nA = matrix(c(1,4,7,2,5,8), nrow = 3, ncol = 2)\nA\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    4    5\n[3,]    7    8\n\nt(A) #transpose of A\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n\nA[3,2] #the (3,2)-entry of A\n\n[1] 8\n\nB = matrix(c(1,2,2,4), nrow = 2, ncol = 2) # another matrix\nall(B == t(B)) #check whether B is symmetric\n\n[1] TRUE\n\ndiag(c(1,4)) #diagonal matrix\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    4\n\ndiag(1, nrow = 3) #identity matrix\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\nmatrix(0, nrow=2, ncol=5) #matrix of zeros\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    0    0    0    0    0\n\ndim(A) #number of rows and columns\n\n[1] 3 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "02b-Matrix-Algebra.html#basic-definitions",
    "href": "02b-Matrix-Algebra.html#basic-definitions",
    "title": "\n3  Matrix Algebra\n",
    "section": "",
    "text": "3.1.1 Scalar, vector, and matrix\nA scalar \\(a\\) is a single real number. We write \\(a \\in \\mathbb R\\).\nA vector \\(\\boldsymbol a\\) of length \\(k\\) is a \\(k \\times 1\\) list of real numbers \\[\n\\boldsymbol a = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_k \\end{pmatrix}.\n\\] By default, when we refer to a vector, we always mean a column vector. We write \\(\\boldsymbol a \\in \\mathbb R^k\\). The value \\(a_i\\) is called \\(i\\)-th entry or \\(i\\)-th component of \\(\\boldsymbol a\\). A scalar is a vector of length 1. A row vector of length \\(k\\) is written as \\(\\boldsymbol b = (b_1, \\ldots, b_k)\\).\nA matrix \\(\\boldsymbol A\\) of order \\(k \\times m\\) is a rectangular array of real numbers \\[\\boldsymbol A =\\begin{pmatrix}\n             a_{11} & a_{12} & \\cdots & a_{1m}\\\\\n             a_{21} & a_{22} & \\cdots & a_{2m}\\\\\n             \\vdots & \\vdots &       & \\vdots\\\\\n             a_{k1} & a_{k2} & \\cdots & a_{km}\n\\end{pmatrix}\\] with \\(k\\) rows and \\(m\\) columns. We write \\(\\boldsymbol A \\in \\mathbb R^{k \\times m}\\). The value \\(a_{ij}\\) is called \\((i,j)\\)-th entry or \\((i,j)\\)-th component of \\(\\boldsymbol A\\). We also use the notation \\((\\boldsymbol A)_{i,j}\\) to denote the \\((i,j)\\)-th entry. A vector of length \\(k\\) is a \\(k \\times 1\\) matrix. A row vector of length \\(k\\) is a \\(1 \\times k\\) matrix. A scalar is a matrix of order \\(1 \\times 1\\).\nWe may describe a matrix \\(\\boldsymbol A\\) by its column or row vectors as \\[\n\\boldsymbol A = \\begin{pmatrix} \\boldsymbol a_1 & \\boldsymbol a_2 & \\ldots & \\boldsymbol a_m \\end{pmatrix}\n= \\begin{pmatrix} \\boldsymbol \\alpha_1 \\\\ \\vdots \\\\ \\boldsymbol \\alpha_k \\end{pmatrix},\n\\] where \\[\n\\boldsymbol a_i = \\begin{pmatrix} a_{1i} \\\\ \\vdots \\\\ a_{ki} \\end{pmatrix}\n\\] is the \\(i\\)-th column of \\(\\boldsymbol A\\) and \\(\\boldsymbol \\alpha_i = (a_{i1}, \\ldots, a_{im})\\) is the \\(i\\)-th row.\n\n3.1.2 Some specific matrices\nA matrix is called square matrix if the numbers of rows and columns coincide (i.e., \\(k=m\\)). \\[\\boldsymbol{B} = \\begin{pmatrix}1 & 2 \\\\ 3 & 4 \\end{pmatrix}\\] is a square matrix. A square matrix is called diagonal matrix if all off-diagonal elements are zero. \\[\\boldsymbol{C} = \\begin{pmatrix}1 & 0 \\\\ 0 & 4 \\end{pmatrix}\\] is a diagonal matrix. We also write \\(\\boldsymbol C = \\mathop{\\mathrm{diag}}(1,4)\\). A square matrix is called upper triangular if all elements below the main diagonal are zero, and lower triangular if all elements above the main diagonal are zero. Examples of an upper triangular matrix \\(\\boldsymbol D\\) and a lower triangular matrix \\(\\boldsymbol E\\) are \\[\\boldsymbol{D} = \\begin{pmatrix}1 & 2 \\\\ 0 & 4 \\end{pmatrix},\n\\quad\n\\boldsymbol{E} = \\begin{pmatrix}1 & 0 \\\\ 3 & 4 \\end{pmatrix}.\n\\] The \\(k \\times k\\) diagonal matrix \\[\\boldsymbol{I}_k=\n      \\begin{pmatrix}\n      1      & 0      & \\cdots & 0\\\\\n      0      & 1      & \\cdots & 0\\\\\n      \\vdots & \\vdots & \\ddots & \\vdots\\\\\n      0      & 0      & \\cdots & 1\n      \\end{pmatrix} = \\mathop{\\mathrm{diag}}(1, \\ldots, 1)\\] is called identity matrix of order \\(k\\). The \\(k \\times m\\) matrix \\[\\boldsymbol{0}_{k\\times m}\n       =\\begin{pmatrix}\n             0      & \\cdots & 0\\\\\n            \\vdots &  \\ddots      & \\vdots\\\\\n            0      & \\cdots & 0\n       \\end{pmatrix}\n      \\] is called zero matrix. The zero vector of length \\(k\\) is \\[\\boldsymbol{0}_{k}\n       =\\begin{pmatrix}\n             0   \\\\\n            \\vdots \\\\\n            0\n       \\end{pmatrix}.\n      \\] If the order becomes clear from the context, we omit the indices and write \\(\\boldsymbol I\\) for the identity matrix and \\(\\boldsymbol 0\\) for the zero matrix or zero vector.\n\n3.1.3 Transposition\nThe transpose \\(\\boldsymbol A'\\) of the matrix \\(\\boldsymbol A\\) is obtained by flipping rows and columns on the main diagonal: \\[\\boldsymbol{A}'=\\begin{pmatrix}\n    a_{11} & a_{21} & \\cdots & a_{k1}\\\\\n    a_{12} & a_{22} & \\cdots & a_{k2}\\\\\n    \\vdots & \\vdots &       & \\vdots\\\\\n    a_{1m} & a_{2m} & \\cdots & a_{km}\n\\end{pmatrix}.\\] If \\(\\boldsymbol A\\) is a matrix of order \\(k\\times m\\), then \\(\\boldsymbol A'\\) is a matrix of order \\(m \\times k\\). Example: \\[\\boldsymbol{A}=\\begin{pmatrix}\n        1 & 2\\\\\n        4 & 5\\\\\n        7 & 8\n\\end{pmatrix}\\quad\\Rightarrow\\quad\n\\boldsymbol{A}'=\\begin{pmatrix}\n        1 & 4 & 7\\\\\n        2 & 5 & 8\n\\end{pmatrix}\\] The definition implies that transposing twice produces the original matrix: \\[\n(\\boldsymbol A')' = \\boldsymbol A.\n\\] The transpose of a (column) vector is a row vector: \\[\n\\boldsymbol a' = (a_1, \\ldots, a_k)\n\\]\nA symmetric matrix is a square matrix \\(\\boldsymbol A\\) with \\(\\boldsymbol{A}'=\\boldsymbol{A}\\). An example of a symmetric matrix is \\[\\boldsymbol A = \\begin{aligned}         \\left(\\begin{matrix}1 & 2 \\\\ 2 & 4 \\end{matrix}\\right)\\end{aligned}.\\]\n\n3.1.4 Matrices in R\n\nLet’s define some matrices in R:\n\nA = matrix(c(1,4,7,2,5,8), nrow = 3, ncol = 2)\nA\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    4    5\n[3,]    7    8\n\nt(A) #transpose of A\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n\nA[3,2] #the (3,2)-entry of A\n\n[1] 8\n\nB = matrix(c(1,2,2,4), nrow = 2, ncol = 2) # another matrix\nall(B == t(B)) #check whether B is symmetric\n\n[1] TRUE\n\ndiag(c(1,4)) #diagonal matrix\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    4\n\ndiag(1, nrow = 3) #identity matrix\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\nmatrix(0, nrow=2, ncol=5) #matrix of zeros\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    0    0    0    0    0\n\ndim(A) #number of rows and columns\n\n[1] 3 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "02b-Matrix-Algebra.html#sums-and-products",
    "href": "02b-Matrix-Algebra.html#sums-and-products",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n3.2 Sums and Products",
    "text": "3.2 Sums and Products\n\n3.2.1 Matrix summation\nLet \\(\\boldsymbol A\\) and \\(\\boldsymbol B\\) both be matrices of order \\(k \\times m\\). Their sum is defined componentwise: \\[\\boldsymbol{A} + \\boldsymbol{B}\n=\\begin{pmatrix}\na_{11}+ b_{11} & a_{12}+ b_{12} & \\cdots & a_{1m}+ b_{1m} \\\\\na_{21}+ b_{21} & a_{22}+ b_{22} & \\cdots & a_{2m}+ b_{2m} \\\\\n\\vdots        & \\vdots        &       & \\vdots        \\\\\na_{k1}+ b_{k1} & a_{k2}+ b_{k2} & \\cdots & a_{km}+ b_{km}\n\\end{pmatrix}.\\] Only two matrices of the same order can be added. Example: \\[\\boldsymbol{A}=\\begin{pmatrix}2&0\\\\1&5\\\\3&2\\end{pmatrix}\\,,\\quad\n\\boldsymbol{B}=\\begin{pmatrix}-1&1\\\\7&1\\\\-5&2\\end{pmatrix}\\,,\\quad\n\\boldsymbol{A}+\\boldsymbol{B}=\\begin{pmatrix}1&1\\\\8&6\\\\-2&4\\end{pmatrix}\\,.\\]\nThe matrix summation satisfies the following rules: \\[\\begin{array}{@{}rr@{\\ }c@{\\ }l@{}r@{}}\n\\text{(i)}    & \\boldsymbol{A}+\\boldsymbol{B}         &=& \\boldsymbol{B}+\\boldsymbol{A}\\,               &  \\text{(commutativity)} \\\\\n\\text{(ii)}   & (\\boldsymbol{A}+\\boldsymbol{B})+\\boldsymbol{C}  &=& \\boldsymbol{A}+(\\boldsymbol{B}+\\boldsymbol{C})\\,        &  \\text{(associativity)} \\\\\n\\text{(iii)}    & \\boldsymbol A + \\boldsymbol 0 &=& \\boldsymbol A &   {\\text{(identity element)}}     \\\\\n\\text{(iv)}   & (\\boldsymbol A + \\boldsymbol B)' &=& \\boldsymbol A' + \\boldsymbol B'     &\n{\\text{(transposition)}}          \n\\end{array}\\]\n\n3.2.2 Scalar-matrix multiplication\nThe product of a \\(k \\times m\\) matrix \\(\\boldsymbol{A}\\) with a scalar \\(\\lambda\\in\\mathbb{R}\\) is defined componentwise: \\[\\lambda \\boldsymbol{A} =\n\\begin{pmatrix}\n   \\lambda a_{11}   & \\lambda a_{12} & \\cdots  & \\lambda a_{1n} \\\\\n   \\lambda a_{21}   & \\lambda a_{22} & \\cdots  & \\lambda a_{2n} \\\\\n   \\vdots           & \\vdots &                & \\vdots         \\\\\n   \\lambda a_{m1}   & \\lambda a_{m2} & \\cdots  & \\lambda a_{mn}\n\\end{pmatrix}.\\] Example: \\[\\lambda=2, \\quad \\boldsymbol{A}=\\begin{pmatrix}2&0\\\\1&5\\\\3&2\\end{pmatrix},\\quad\n\\lambda\\boldsymbol{A}=\\begin{pmatrix}4&0\\\\2&10\\\\6&4\\end{pmatrix}.\\] Scalar-matrix multiplication satisfies the distributivity law: \\[\\begin{array}{@{}rr@{\\ }c@{\\ }l@{}r@{}}\n\\text{(i)}    & \\lambda(\\boldsymbol{A}+\\boldsymbol{B})&=& \\lambda\\boldsymbol{A}+\\lambda\\boldsymbol{B}\\, &    \\\\\n\\text{(ii)}   & (\\lambda+\\mu)\\boldsymbol{A} &=& \\lambda\\boldsymbol{A}+\\mu\\boldsymbol{A}\\,     &\n\\end{array}\\]\n\n3.2.3 Element-by-element operations in R\nBasic arithmetic operations work on an element-by-element basis in R:\n\nA = matrix(c(2,1,3,0,5,2), ncol=2)\nB = matrix(c(-1,7,-5,1,1,2), ncol=2)\nA+B #matrix summation\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    8    6\n[3,]   -2    4\n\nA-B #matrix subtraction\n\n     [,1] [,2]\n[1,]    3   -1\n[2,]   -6    4\n[3,]    8    0\n\n2*A #scalar-matrix product\n\n     [,1] [,2]\n[1,]    4    0\n[2,]    2   10\n[3,]    6    4\n\nA/2 #division of entries by 2\n\n     [,1] [,2]\n[1,]  1.0  0.0\n[2,]  0.5  2.5\n[3,]  1.5  1.0\n\nA*B #element-wise multiplication\n\n     [,1] [,2]\n[1,]   -2    0\n[2,]    7    5\n[3,]  -15    4\n\n\n\n3.2.4 Vector-vector multiplication\n\n3.2.4.1 Inner product\nThe inner product (also known as dot product) of two vectors \\(\\boldsymbol{a},\\boldsymbol{b}\\in\\mathbb{R}^k\\) is \\[\\boldsymbol{a}'\\boldsymbol{b} = a_1 b_1+a_2b_2+\\ldots+a_kb_k=\\sum_{i=1}^k a_ib_i\\in\\mathbb{R}.\\] Example: \\[\\boldsymbol{a}=\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix},\\quad\n\\boldsymbol{b}=\\begin{pmatrix}-2\\\\0\\\\2\\end{pmatrix},\\quad\n\\boldsymbol{a}'\\boldsymbol{b}=1\\cdot(-2)+2\\cdot0+3\\cdot2=4.\\]\nThe inner product is commutative: \\[\\begin{align*}\n  \\boldsymbol a' \\boldsymbol b = \\boldsymbol b' \\boldsymbol a.\n\\end{align*}\\] Two vectors \\(\\boldsymbol a\\) and \\(\\boldsymbol b\\) are called orthogonal if \\(\\boldsymbol a' \\boldsymbol b = 0\\). The vectors \\(\\boldsymbol a\\) and \\(\\boldsymbol b\\) are called orthonormal if, in addition to \\(\\boldsymbol a'\\boldsymbol b\\), we have \\(\\boldsymbol a' \\boldsymbol a = 1\\) and \\(\\boldsymbol b' \\boldsymbol b=1\\).\n\n3.2.4.2 Outer product\nThe outer product (also known as dyadic product) of two vectors \\(\\boldsymbol{x} \\in \\mathbb R^k\\) and \\(\\boldsymbol{y}\\in\\mathbb{R}^m\\) is \\[\\boldsymbol{x}\\boldsymbol{y}' =\n\\left(\\begin{matrix}\nx_1 y_1 & x_1 y_2  &\\ldots & x_1 y_m \\\\\nx_2 y_1 & x_2 y_2 & \\ldots & x_2 y_m \\\\\n\\vdots   & \\vdots &      & \\vdots    \\\\\nx_k y_1 & x_k y_2 & \\ldots & x_k y_m\n\\end{matrix}\\right)\\in  \\mathbb{R}^{k \\times m}.\\] Example: \\[\\boldsymbol{x}=\\begin{pmatrix}1\\\\2\\end{pmatrix}\\,,\\quad\n\\boldsymbol{y}=\\begin{pmatrix}-2\\\\0\\\\2\\end{pmatrix}\\,,\\quad\n\\boldsymbol{x}\\boldsymbol{y}'=\\left(\\begin{matrix}\n-2 & 0 & 2 \\\\\n-4 & 0 & 4\n\\end{matrix}\\right).\\]\n\n3.2.4.3 Vector multiplication in R\n\nFor vector multiplication in R, we use the operator %*% (recall that * is already reserved for element-wise multiplication). Let’s implement some multiplications.\n\ny = c(2,7,4,1) #y is treated as a column vector\nt(y) %*% y #the inner product of y with itself\n\n     [,1]\n[1,]   70\n\ny %*% t(y) #the outer product of y with itself\n\n     [,1] [,2] [,3] [,4]\n[1,]    4   14    8    2\n[2,]   14   49   28    7\n[3,]    8   28   16    4\n[4,]    2    7    4    1\n\nc(1,2) %*% t(c(-2,0,2)) #the example from above\n\n     [,1] [,2] [,3]\n[1,]   -2    0    2\n[2,]   -4    0    4\n\n\n\n3.2.5 Matrix-matrix multiplication\nThe matrix product of a \\(k \\times m\\) matrix \\(\\boldsymbol{A}\\) and a \\(m \\times n\\) matrix \\(\\boldsymbol{B}\\) is the \\(k\\times n\\) matrix \\(\\boldsymbol C = \\boldsymbol{A}\\boldsymbol{B}\\) with the components \\[c_{ij} = a_{i1}b_{1j}+a_{i2}b_{2j}+\\ldots+a_{im}b_{mj}=\\sum_{l=1}^m a_{il}b_{lj} = \\boldsymbol a_i' \\boldsymbol b_j,\\] where \\(\\boldsymbol a_i = (a_{i1}, \\ldots, a_{im})'\\) is the \\(i\\)-th row of \\(\\boldsymbol A\\) written as a column vector, and \\(\\boldsymbol b_j = (b_{1j}, \\ldots, b_{mj})'\\) is the \\(j\\)-th column of \\(\\boldsymbol B\\). The full matrix product can be written as \\[\n\\boldsymbol A \\boldsymbol B = \\begin{pmatrix} \\boldsymbol a_1' \\\\ \\vdots \\\\ \\boldsymbol a_k' \\end{pmatrix}\n\\begin{pmatrix} \\boldsymbol b_1 & \\ldots & \\boldsymbol b_n \\end{pmatrix}\n= \\begin{pmatrix} \\boldsymbol a_1' \\boldsymbol b_1 & \\ldots & \\boldsymbol a_1' \\boldsymbol b_n \\\\ \\vdots & & \\vdots \\\\ \\boldsymbol a_k' \\boldsymbol b_1 & \\ldots & \\boldsymbol a_k' \\boldsymbol b_n \\end{pmatrix}.\n\\] The matrix product is only defined if the number of columns of the first matrix equals the number of rows of the second matrix. Therefore, we say that the \\(k \\times m\\) matrix \\(\\boldsymbol A\\) and the \\(m \\times n\\) matrix \\(\\boldsymbol B\\) are conformable for matrix multiplication.\nExample: Let \\[\\begin{aligned}\n    \\boldsymbol{A}=\\begin{pmatrix}\n    1 & 0\\\\\n    0 & 1\\\\\n    2 & 1\n\\end{pmatrix}, \\quad\n\\boldsymbol{B}=\\begin{pmatrix}\n    -1 & 2\\\\\n    -3 & 0\n\\end{pmatrix}.\\end{aligned}\\] Their matrix product is \\[\\begin{aligned}\n\\boldsymbol{A} \\boldsymbol{B} &= \\begin{pmatrix}\n    1 & 0\\\\\n    0 & 1\\\\\n    2 & 1\n\\end{pmatrix} \\begin{pmatrix}\n    -1 & 2\\\\\n    -3 & 0\n\\end{pmatrix} \\\\ &= \\left(\\begin{matrix}1 \\cdot (-1) + 0 \\cdot (-3) & 1 \\cdot 2 + 0 \\cdot 0 \\\\ 0 \\cdot (-1) + 1 \\cdot (-3) & 0 \\cdot 2 + 1 \\cdot 0 \\\\ 2 \\cdot (-1) + 1 \\cdot (-3) & 2 \\cdot 2 + 1 \\cdot 0 \\end{matrix}\\right)\n= \\left(\\begin{matrix}-1 & 2 \\\\ -3 & 0 \\\\ -5 & 4 \\end{matrix}\\right).\\end{aligned}\\]\nThe %*% operator is used in R for matrix-matrix multiplications:\n\nA = matrix(c(1,0,2,0,1,1), ncol=2)\nB = matrix(c(-1,-3,2,0), ncol=2)\nA %*% B\n\n     [,1] [,2]\n[1,]   -1    2\n[2,]   -3    0\n[3,]   -5    4\n\n\nMatrix multiplication is not commutative. In general, we have \\(\\boldsymbol A \\boldsymbol B \\neq \\boldsymbol B \\boldsymbol A\\). Example: \\[\\begin{aligned}\n    \\boldsymbol{A}\\boldsymbol{B} = \\begin{pmatrix} 1 & 2\\\\ 3 & 4\\end{pmatrix}\n    \\begin{pmatrix} 1 & 1\\\\ 1 & 2\\end{pmatrix}\n    &=\n    \\begin{pmatrix} 3 & 5\\\\ 7 & 11\\end{pmatrix}\\,,\\\\\n    \\boldsymbol{B}\\boldsymbol{A} = \\begin{pmatrix} 1 & 1\\\\ 1 & 2\\end{pmatrix}\n    \\begin{pmatrix} 1 & 2\\\\ 3 & 4\\end{pmatrix}\n    &=\n    \\begin{pmatrix} 4 & 6\\\\ 7 & 10\\end{pmatrix}\\,.\\end{aligned}\\] Even if neither of the two matrices contains zeros, the matrix product can give the zero matrix: \\[\n\\boldsymbol{A}\\boldsymbol{B} = \\begin{pmatrix} 1 & 2\\\\ 2 & 4\\end{pmatrix}\n    \\begin{pmatrix} 2 & -4\\\\ -1 & 2\\end{pmatrix}\n    =\n    \\begin{pmatrix} 0 & 0\\\\ 0 & 0\\end{pmatrix}=\\boldsymbol{0}.\n\\]\nThe following rules of calculation apply (provided the matrices are conformable): \\[\n\\begin{array}{rrcl@{}r@{}}\n\\text{(i)}   & \\boldsymbol{A}(\\boldsymbol{B}\\boldsymbol{C})       & = & (\\boldsymbol{A}\\boldsymbol{B})\\boldsymbol{C}\\,     &\\text{(associativity)} \\\\\n\\text{(ii)}  &   \\boldsymbol{A}(\\boldsymbol{B}+\\boldsymbol{D})    & = & \\boldsymbol{A}\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{D}\\,  & \\text{(distributivity)} \\\\\n\\text{(iii)}   &   (\\boldsymbol{B}+\\boldsymbol{D})\\boldsymbol{C}    & = & \\boldsymbol{B}\\boldsymbol{C}+\\boldsymbol{D}\\boldsymbol{C}\\,  & \\text{(distributivity)} \\\\\n\\text{(iv)}  &   \\boldsymbol{A}(\\lambda \\boldsymbol{B}) & = & \\lambda(\\boldsymbol{A}\\boldsymbol{B})\\,  & \\text{(scalar commutativity)}\\\\\n\\text{(v)}  & \\boldsymbol{A}\\boldsymbol{I}_{n} & = & \\boldsymbol{A}\\,,              & \\text{(identity element)}\\\\\n\\text{(vi)} & \\boldsymbol{I}_{m}\\boldsymbol{A} & = & \\boldsymbol{A}\\,               &\n\\text{(identity element)}     \\\\\n\\text{(vii)} &   (\\boldsymbol{A}\\boldsymbol{B})'  & = & \\boldsymbol{B}'\\boldsymbol{A}'\\,         & \\text{(product transposition)} \\\\\n\\text{(viii)} &   (\\boldsymbol{A}\\boldsymbol{B} \\boldsymbol C)'  & = & \\boldsymbol C' \\boldsymbol{B}'\\boldsymbol{A}'\\,  & \\text{(product transposition)}\n\\end{array}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "02b-Matrix-Algebra.html#rank-and-inverse",
    "href": "02b-Matrix-Algebra.html#rank-and-inverse",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n3.3 Rank and inverse",
    "text": "3.3 Rank and inverse\n\n3.3.1 Linear combination\nLet \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) be vectors of the same order, and let \\(\\lambda_1, \\ldots, \\lambda_n\\) be scalars. The vector \\[ \\lambda_1 \\boldsymbol x_1 + \\lambda_2 \\boldsymbol x_2 + \\ldots + \\lambda_n \\boldsymbol x_n \\]\nis called linear combination of \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\). A linear combination can also be written as a matrix-vector product. Let \\(\\boldsymbol X =\\begin{pmatrix} \\boldsymbol x_1 & \\ldots & \\boldsymbol x_n \\end{pmatrix}\\) be the matrix with columns \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\), and let \\(\\boldsymbol \\lambda = (\\lambda_1, \\ldots, \\lambda_n)'\\). Then, \\[ \\lambda_1 \\boldsymbol x_1 + \\lambda_2 \\boldsymbol x_2 + \\ldots + \\lambda_n \\boldsymbol x_n = \\boldsymbol X \\boldsymbol \\lambda.\\] The vectors \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) are called linearly dependent if at least one can be written as a linear combination of the others. That is, there exists a nonzero vector \\(\\boldsymbol \\lambda\\) with \\[\n\\boldsymbol X \\boldsymbol \\lambda = \\lambda_1 \\boldsymbol x_1 + \\ldots + \\lambda_n \\boldsymbol x_n = \\boldsymbol 0.\n\\] The vectors \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) are called linearly independent if \\[\n\\boldsymbol X \\boldsymbol \\lambda = \\lambda_1 \\boldsymbol x_1 + \\ldots + \\lambda_n \\boldsymbol x_n \\neq \\boldsymbol 0\n\\] for all nonzero vectors \\(\\boldsymbol \\lambda\\).\nTo check whether the vectors are linearly independent, we can solve the system of equations \\(\\boldsymbol X \\boldsymbol \\lambda = \\boldsymbol 0\\) by Gaussian elimination. If \\(\\boldsymbol \\lambda = \\boldsymbol 0\\) is the only solution, then the columns of \\(\\boldsymbol X\\) are linearly independent. If there is a solution \\(\\boldsymbol \\lambda\\) with \\(\\boldsymbol \\lambda \\neq \\boldsymbol 0\\), then the columns of \\(\\boldsymbol X\\) are linearly dependent.\n\n3.3.2 Column rank\nThe rank of a \\(k \\times m\\) matrix \\(\\boldsymbol A = \\begin{pmatrix} \\boldsymbol a_1 & \\ldots & \\boldsymbol a_m \\end{pmatrix}\\), written as \\(\\mathop{\\mathrm{rank}}(\\boldsymbol A)\\), is the number of linearly independent columns \\(\\boldsymbol a_i\\). We say that \\(\\boldsymbol A\\) has full column rank if \\(\\mathop{\\mathrm{rank}}(\\boldsymbol X) = m\\).\nThe identity matrix \\(\\boldsymbol I_k\\) has full column rank (i.e., \\(\\mathop{\\mathrm{rank}}(\\boldsymbol I_n) = k\\)). As another example, consider \\[\n\\boldsymbol X = \\begin{pmatrix} 2 & 1 & 4 \\\\ 0 & 1 & 2 \\end{pmatrix},\n\\] which has linearly dependent columns since the third column is a linear combination of the first two columns: \\[\n\\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix} = 1 \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n\\] The first two columns are linearly independent since \\(\\lambda_1 = 0\\) and \\(\\lambda_2 = 0\\) are the only solutions to the equation \\[\n\\lambda_1 \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\lambda_2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n\\] Therefore, we have \\(\\mathop{\\mathrm{rank}}(\\boldsymbol X) = 2\\), i.e., \\(\\boldsymbol X\\) does not have a full column rank.\nSome useful properties are\n\n\\(\\mathop{\\mathrm{rank}}(\\boldsymbol A) \\leq \\min(k,m)\\)\n\\(\\mathop{\\mathrm{rank}}(\\boldsymbol A) = \\mathop{\\mathrm{rank}}(\\boldsymbol A')\\)\n\\(\\mathop{\\mathrm{rank}}(\\boldsymbol A \\boldsymbol B) = \\min(\\mathop{\\mathrm{rank}}(\\boldsymbol A), \\mathop{\\mathrm{rank}}(\\boldsymbol B))\\)\n\n\\(\\mathop{\\mathrm{rank}}(\\boldsymbol A) = \\mathop{\\mathrm{rank}}(\\boldsymbol A' \\boldsymbol A) = \\mathop{\\mathrm{rank}}(\\boldsymbol A \\boldsymbol A')\\).\n\nWe can use the qr() function to extract the rank in R. Let’s compute the rank of the matrices \\[\n\\boldsymbol A = \\begin{pmatrix}\n1 & 2 & 3 \\\\ 3 & 9 & 1 \\\\ 0 & 11 & 5\n\\end{pmatrix},\n\\] \\(\\boldsymbol B = \\boldsymbol I_3\\), and \\(\\boldsymbol X\\) from the example above:\n\nA = matrix(c(1,3,0,2,9,11,3,1,5), nrow=3)\nqr(A)$rank\n\n[1] 3\n\nB = matrix(c(1,1,1,1,1,1,1,1,1), nrow=3)\nqr(B)$rank\n\n[1] 1\n\nX = matrix(c(2,0,1,1,4,2), ncol=3)\nqr(X)$rank\n\n[1] 2\n\n\n\n3.3.3 Nonsingular matrix\nA square \\(k \\times k\\) matrix \\(\\boldsymbol A\\) is called nonsingular if it has full rank, i.e., \\(\\mathop{\\mathrm{rank}}(\\boldsymbol A) = k\\). Conversely, \\(\\boldsymbol A\\) is called singular if it does not have full rank, i.e., \\(\\mathop{\\mathrm{rank}}(\\boldsymbol A) &lt; k\\).\n\n3.3.4 Determinant\nConsider a square \\(k \\times k\\) matrix \\(\\boldsymbol A\\). The determinant \\(\\det(\\boldsymbol A)\\) is a measure of the volume of the geometric object formed by the columns of \\(\\boldsymbol A\\) (a parallelogram for \\(k=2\\), a parallelepiped for \\(k=3\\), a hyper-parallelepiped for \\(k&gt;3\\)). For \\(2 \\times 2\\) matrices, the determinant is easy to calculate: \\[\n\\boldsymbol A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}, \\quad\n\\det(\\boldsymbol A) = ad - bc.\n\\]\nIf \\(\\boldsymbol A\\) is triangular (upper or lower), the determinant is the product of the diagonal entries, i.e., \\(\\det(\\boldsymbol A) = \\prod_{i=1}^k a_{ii}\\). Hence, Gaussian elimination can be used to compute the determinant by transforming the matrix to a triangular one. The exact definition of the determinant is technical and of little importance to us. A useful relation is the following: \\[\\begin{align*}\n  \\det(\\boldsymbol A) = 0 \\quad &\\Leftrightarrow \\quad \\boldsymbol A \\ \\text{is singular} \\\\\n   \\det(\\boldsymbol A) \\neq 0 \\quad &\\Leftrightarrow \\quad \\boldsymbol A \\ \\text{is nonsingular}.\n\\end{align*}\\]\nIn R, we have the det() function to compute the determinant:\n\ndet(A)\n\n[1] 103\n\ndet(B)\n\n[1] 0\n\n\nSince \\(\\det(\\boldsymbol A) \\neq 0\\) and \\(\\det(\\boldsymbol B) = 0\\), we conclude that \\(\\boldsymbol A\\) is nonsingular and \\(\\boldsymbol B\\) is singular.\n\n3.3.5 Inverse matrix\nThe inverse \\(\\boldsymbol{A}^{-1}\\) of a square \\(k \\times k\\) matrix \\(\\boldsymbol A\\) is defined by the property \\[\\boldsymbol{A} \\boldsymbol{A}^{-1} = \\boldsymbol{A}^{-1} \\boldsymbol{A} =\\boldsymbol{I}_k.\\] When multiplied from the left or the right, the inverse matrix produces the identity matrix. The inverse exists if and only if \\(\\boldsymbol{A}\\) is nonsingular, i.e., \\(\\det(\\boldsymbol A) \\neq 0\\). Therefore, a nonsingular matrix is also called invertible matrix. Note that only square matrices can be inverted.\nFor \\(2 \\times 2\\) matrices, there exists a simple formula: \\[\\boldsymbol{A}^{-1} = \\frac{1}{\\det(\\boldsymbol{A})} \\begin{pmatrix}d&-b\\\\-c&a\\end{pmatrix}\\,,\\] where \\(\\det(\\boldsymbol{A}) = ad - bc\\). We swap the main diagonal elements, reverse the sign of the off-diagonal elements, and divide all entries by the determinant. Example: \\[\\displaystyle\\boldsymbol{A}=\\begin{pmatrix}5&6\\\\1&2\\end{pmatrix}\\] We have \\(\\det(\\boldsymbol{A}) = ad-bc=5\\cdot2-6\\cdot1=4\\), and \\[\\boldsymbol{A}^{-1}= \\frac{1}{4} \\cdot \\begin{pmatrix}2&-6\\\\-1&5\\end{pmatrix}.\\] Indeed, \\(\\boldsymbol A^{-1}\\) is the inverse of \\(\\boldsymbol A\\) since \\[\\boldsymbol{A}\\boldsymbol{A}^{-1}\n=\\begin{pmatrix}5&6\\\\1&2\\end{pmatrix} \\cdot \\frac{1}{4} \\cdot \\begin{pmatrix}2&-6\\\\-1&5\\end{pmatrix}\n=\\frac{1}{4} \\cdot \\begin{pmatrix}4&0\\\\0&4\\end{pmatrix}\n= \\left(\\begin{matrix}1 & 0 \\\\ 0 & 1 \\end{matrix}\\right)\n= \\boldsymbol{I}_2.\\]\nOne way to calculate the inverse of higher order square matrices is to solve equation \\(\\boldsymbol A \\boldsymbol A^{-1} = \\boldsymbol I\\) with Gaussian elimination. R can compute the inverse matrix quickly using the function solve():\n\nsolve(A) #inverse if A\n\n           [,1]        [,2]        [,3]\n[1,]  0.3300971  0.22330097 -0.24271845\n[2,] -0.1456311  0.04854369  0.07766990\n[3,]  0.3203883 -0.10679612  0.02912621\n\n\nWe have the following relationship between invertibility, rank, and determinant of a square matrix \\(\\boldsymbol A\\):\n\\[\\begin{align*}\n  &\\boldsymbol A \\ \\text{is nonsingular} \\\\\n  \\Leftrightarrow \\quad &\\text{all columns of} \\ \\boldsymbol A \\ \\text{are linearly independent} \\\\\n  \\Leftrightarrow \\quad &\\boldsymbol A \\ \\text{has full column rank} \\\\\n  \\Leftrightarrow \\quad &\\text{the determinant is nonzero} \\ (\\det(\\boldsymbol A) \\neq 0).\n\\end{align*}\\]\nSimilarly, \\[\\begin{align*}\n  &\\boldsymbol A \\ \\text{is singular} \\\\\n  \\Leftrightarrow \\quad &\\boldsymbol A \\ \\text{has linearly dependent columns} \\\\\n  \\Leftrightarrow \\quad &\\boldsymbol A \\ \\text{does not have full rank} \\\\\n  \\Leftrightarrow \\quad &\\text{the determinant is zero} \\ (\\det(\\boldsymbol A) = 0).\n\\end{align*}\\]\nBelow you will find some important properties for nonsingular matrices:\n\n\\((\\boldsymbol{A}^{-1})^{-1} = \\boldsymbol{A}\\)\n\\((\\boldsymbol{A}')^{-1} = (\\boldsymbol{A}^{-1})'\\)\n\n\\((\\lambda\\boldsymbol{A})^{-1} = \\frac{1}{\\lambda}\\boldsymbol{A}^{-1}\\) for any \\(\\lambda \\neq 0\\)\n\n\\(\\det(\\boldsymbol A^{-1}) = \\frac{1}{\\det(\\boldsymbol A)}\\)\n\\((\\boldsymbol{A} \\boldsymbol{B})^{-1} = \\boldsymbol{B}^{-1} \\boldsymbol{A}^{-1}\\)\n\\((\\boldsymbol A \\boldsymbol B \\boldsymbol C)^{-1} = \\boldsymbol C^{-1} \\boldsymbol B^{-1} \\boldsymbol A^{-1}\\)\nIf \\(\\boldsymbol{A}\\) is symmetric, then \\(\\boldsymbol{A}^{-1}\\) is symmetric.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "02b-Matrix-Algebra.html#advanced-concepts",
    "href": "02b-Matrix-Algebra.html#advanced-concepts",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n3.4 Advanced concepts",
    "text": "3.4 Advanced concepts\n\n3.4.1 Trace\nThe trace of a \\(k \\times k\\) square matrix \\(\\boldsymbol A\\) is the sum of the diagonal entries: \\[\\mathop{\\mathrm{tr}}(\\boldsymbol A) = \\sum_{i=1}^n a_{ii}\\] Example: \\[\n\\boldsymbol A = \\begin{pmatrix}\n1 & 2 & 3 \\\\ 3 & 9 & 1 \\\\ 0 & 11 & 5\n\\end{pmatrix} \\quad \\Rightarrow \\quad \\mathop{\\mathrm{tr}}(\\boldsymbol A) = 1+9+5 = 15\n\\] In Rwe have\n\nA = matrix(c(1,3,0,2,9,11,3,1,5), nrow=3)\nsum(diag(A))  #trace = sum of diagonal entries\n\n[1] 15\n\n\nThe following properties hold for square matrices \\(\\boldsymbol A\\) and \\(\\boldsymbol B\\) and scalars \\(\\lambda\\):\n\n\\(\\mathop{\\mathrm{tr}}(\\lambda \\boldsymbol A) = \\lambda \\mathop{\\mathrm{tr}}(\\boldsymbol A)\\)\n\\(\\mathop{\\mathrm{tr}}(\\boldsymbol A + \\boldsymbol B) = \\mathop{\\mathrm{tr}}(\\boldsymbol A) + \\mathop{\\mathrm{tr}}(\\boldsymbol B)\\)\n\\(\\mathop{\\mathrm{tr}}(\\boldsymbol A') = \\mathop{\\mathrm{tr}}(\\boldsymbol A)\\)\n\\(\\mathop{\\mathrm{tr}}(\\boldsymbol I_k) = k\\)\n\nFor \\(\\boldsymbol A\\in \\mathbb R^{k \\times m}\\) and \\(\\boldsymbol B \\in \\mathbb R^{m \\times k}\\) we have \\[\n\\mathop{\\mathrm{tr}}(\\boldsymbol A \\boldsymbol B) = \\mathop{\\mathrm{tr}}(\\boldsymbol B \\boldsymbol A).\n\\]\n\n3.4.2 Idempotent matrix\nThe square matrix \\(\\boldsymbol A\\) is called idempotent if \\(\\boldsymbol A \\boldsymbol A = \\boldsymbol A\\). The identity matrix is idempotent: \\(\\boldsymbol I_n \\boldsymbol I_n = \\boldsymbol I_n\\). Another example is the matrix \\[\n\\boldsymbol A = \\begin{pmatrix}\n4 & -1 \\\\ 12 & -3\n\\end{pmatrix}.\n\\] We have \\[\\begin{align*}\n\\boldsymbol A \\boldsymbol A\n&= \\begin{pmatrix}\n4 & -1 \\\\ 12 & -3\n\\end{pmatrix}\n\\begin{pmatrix}\n4 & -1 \\\\ 12 & -3\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n16-12 & -4+3 \\\\ 48-36 & -12+9\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n4 & -1 \\\\ 12 & -3\n\\end{pmatrix}\n= \\boldsymbol A.\n\\end{align*}\\]\n\n3.4.3 Eigendecomposition\n\n3.4.3.1 Eigenvalues\nAn eigenvalue \\(\\lambda\\) of a \\(k \\times k\\) square matrix is a solution to the equation \\[\n  \\det(\\lambda \\boldsymbol I_k - \\boldsymbol A) = 0.\n\\] The function \\(f(\\lambda) = \\det(\\lambda \\boldsymbol I_k - \\boldsymbol A)\\) has exactly \\(k\\) roots so that \\(\\det(\\lambda \\boldsymbol I_k - \\boldsymbol A) = 0\\) has exactly \\(k\\) solutions. The solutions \\(\\lambda_1, \\ldots, \\lambda_k\\) are the \\(k\\) eigenvalues of \\(\\boldsymbol A\\).\nMost applications of eigenvalues in econometrics concern symmetric matrices. In this case, all eigenvalues are real-valued. In the case of non-symmetric matrices, some eigenvalues may be complex-valued.\nUseful properties of the eigenvalues of a symmetric \\(k \\times k\\) matrix are:\n\n\\(\\det(\\boldsymbol A) = \\lambda_1 \\cdot \\ldots \\cdot \\lambda_k\\)\n\\(\\mathop{\\mathrm{tr}}(\\boldsymbol A) = \\lambda_1 + \\ldots + \\lambda_k\\)\n\n\\(\\boldsymbol A\\) is nonsingular if and only if all eigenvalues are nonzero\n\n\\(\\boldsymbol A \\boldsymbol B\\) and \\(\\boldsymbol B \\boldsymbol A\\) have the same eigenvalues.\n\n3.4.3.2 Eigenvectors\nIf \\(\\lambda_i\\) is an eigenvalue of \\(\\boldsymbol A\\), then \\(\\lambda_i \\boldsymbol I_k - \\boldsymbol A\\) is singular, which implies that there exists a linear combination vector \\(\\boldsymbol v_i\\) with \\((\\lambda_i \\boldsymbol I_k - \\boldsymbol A) \\boldsymbol v_i = \\boldsymbol 0\\). Equivalently, \\[\n  \\boldsymbol A \\boldsymbol v_i = \\lambda_i \\boldsymbol v_i,\n\\]\nwhich can be solved by Gaussian elimination. It is convenient to normalize any solution such that \\(\\boldsymbol v_i'\\boldsymbol v_i = 1\\). The solutions \\(\\boldsymbol v_1, \\ldots, \\boldsymbol v_k\\) are called eigenvectors of \\(\\boldsymbol A\\) to corresponding eigenvalues \\(\\lambda_1, \\ldots, \\lambda_k\\).\n\n3.4.3.3 Spectral decomposition\nIf \\(\\boldsymbol A\\) is symmetric, then \\(\\boldsymbol v_1, \\ldots, \\boldsymbol v_k\\) are pairwise orthogonal (i.e., \\(\\boldsymbol v_i' \\boldsymbol v_j = 0\\) for \\(i \\neq j\\)). Let \\(\\boldsymbol V = \\begin{pmatrix} \\boldsymbol v_1 & \\ldots & \\boldsymbol v_k \\end{pmatrix}\\) be the \\(k \\times k\\) matrix of eigenvectors and let \\(\\boldsymbol \\Lambda = \\mathop{\\mathrm{diag}}(\\lambda_1, \\ldots, \\lambda_k)\\) be the \\(k \\times k\\) diagonal matrix with the eigenvalues on the main diagonal. Then, we can write \\[\n  \\boldsymbol A = \\boldsymbol V \\boldsymbol \\Lambda \\boldsymbol V',\n\\]\nwhich is called the spectral decomposition of \\(\\boldsymbol A\\). The matrix of eigenvalues can be written as \\(\\boldsymbol \\Lambda = \\boldsymbol V' \\boldsymbol A \\boldsymbol V\\).\n\n3.4.3.4 Eigendecomposition in R\n\nThe function eigen() computes the eigenvalues and corresponding eigenvectors.\n\nB=t(A)%*%A \nB #A'A is symmetric\n\n     [,1] [,2] [,3]\n[1,]   10   29    6\n[2,]   29  206   70\n[3,]    6   70   35\n\neigen(B) #eigenvalues and eigenvector matrix\n\neigen() decomposition\n$values\n[1] 234.827160  12.582227   3.590613\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.1293953 -0.5312592  0.8372697\n[2,] -0.9346164 -0.2167553 -0.2819739\n[3,] -0.3312839  0.8190121  0.4684764\n\n\n\n3.4.4 Definite matrix\nThe \\(k \\times k\\) square matrix \\(\\boldsymbol{A}\\) is called positive definite if \\[\\boldsymbol{c}'\\boldsymbol{Ac}&gt;0\\] holds for all nonzero vectors \\(\\boldsymbol{c}\\in \\mathbb{R}^k\\). If \\[\\boldsymbol{c}'\\boldsymbol{Ac}\\geq 0\\]\nfor all vectors \\(\\boldsymbol{c}\\in \\mathbb{R}^k\\), the matrix is called positive semi-definite. Analogously, \\(\\boldsymbol A\\) is called negative definite if \\(\\boldsymbol{c}'\\boldsymbol{Ac}&lt;0\\) and negative semi-definite if \\(\\boldsymbol{c}'\\boldsymbol{Ac}\\leq 0\\) for all nonzero vectors \\(\\boldsymbol c \\in \\mathbb R^k\\). A matrix that is neither positive semi-definite nor negative semi-definite is called indefinite\nThe definiteness property of a symmetric matrix \\(\\boldsymbol A\\) can be determined using its eigenvalues:\n\n\n\\(\\boldsymbol A\\) is positive definite  \\(\\Leftrightarrow\\)  all eigenvalues of \\(\\boldsymbol A\\) are strictly positive\n\n\\(\\boldsymbol A\\) is negative definite  \\(\\Leftrightarrow\\)   all eigenvalues of \\(\\boldsymbol A\\) are strictly negative\n\n\\(\\boldsymbol A\\) is positive semi-definite  \\(\\Leftrightarrow\\)   all eigenvalues of \\(\\boldsymbol A\\) are non-negative\n\n\\(\\boldsymbol A\\) is negative semi-definite  \\(\\Leftrightarrow\\)   all eigenvalues of \\(\\boldsymbol A\\) are non-positive\n\n\neigen(B)$values #B is positive definite (all eigenvalues positive)\n\n[1] 234.827160  12.582227   3.590613\n\n\nThe matrix analog of a positive or negative number (scalar) is a positive definite or negative definite matrix. Therefore, we use the notation\n\n\n\\(\\boldsymbol A &gt; 0\\)  if \\(\\boldsymbol A\\) is positive definite\n\n\\(\\boldsymbol A &lt; 0\\)  if \\(\\boldsymbol A\\) is negative definite\n\n\\(\\boldsymbol A \\geq 0\\)  if \\(\\boldsymbol A\\) is positive semi-definite\n\n\\(\\boldsymbol A \\leq 0\\)  if \\(\\boldsymbol A\\) is negative semi-definite\n\nThe notation \\(\\boldsymbol A &gt; \\boldsymbol B\\) means that the matrix \\(\\boldsymbol A - \\boldsymbol B\\) is positive definite.\n\n3.4.5 Cholesky decomposition\nAny positive definite and symmetric matrix \\(\\boldsymbol B\\) can be written as \\[\n  \\boldsymbol B = \\boldsymbol P \\boldsymbol P',\n\\] where \\(P\\) is a lower triangular matrix with strictly positive diagonal entries \\(p_{jj} &gt; 0\\). This representation is called Cholesky decomposition. The matrix \\(\\boldsymbol P\\) is unique. For a \\(2 \\times 2\\) matrix \\(\\boldsymbol B\\) we have \\[\\begin{align*}\n\\begin{pmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{pmatrix}\n&= \\begin{pmatrix} p_{11} & 0 \\\\ p_{21} & p_{22} \\end{pmatrix}\n\\begin{pmatrix} p_{11} & p_{21} \\\\ 0 & p_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} p_{11}^2 & p_{11} p_{21} \\\\ p_{11} p_{21} & p_{21}^2 + p_{22}^2 \\end{pmatrix},\n\\end{align*}\\] which implies \\(p_{11} = \\sqrt{b_{11}}\\), \\(p_{21} = b_{21}/p_{11}\\), and \\(p_{22} = \\sqrt{b_{22} - p_{21}^2}\\). For a \\(3 \\times 3\\) matrix we obtain\n\\[\\begin{align*}\n\\begin{pmatrix} b_{11} & b_{12} & b_{31} \\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{pmatrix}\n= \\begin{pmatrix} p_{11} & 0 & 0 \\\\ p_{21} & p_{22} & 0 \\\\ p_{31} & p_{32} & p_{33} \\end{pmatrix}\n\\begin{pmatrix} p_{11} & p_{21} & p_{31} \\\\ 0 & p_{22} & p_{32} \\\\ 0 & 0 & p_{33}\\end{pmatrix} \\\\\n= \\begin{pmatrix} p_{11}^2 & p_{11} p_{21} & p_{11} p_{31} \\\\ p_{11} p_{21} & p_{21}^2 + p_{22}^2 & p_{21} p_{31} + p_{22} p_{32} \\\\ p_{11}p_{31} & p_{21}p_{31} + p_{22}p_{32} & p_{31}^2 + p_{32}^2  + p_{33}^2\\end{pmatrix},\n\\end{align*}\\] which implies\n\\[\\begin{gather*}\np_{11}=\\sqrt{b_{11}}, \\ \\ p_{21} = \\frac{b_{21}}{p_{11}}, \\ \\ p_{31} = \\frac{b_{31}}{p_{11}}, \\ \\ p_{22} = \\sqrt{b_{22}-p_{21}^2}, \\\\\np_{32}= \\frac{b_{32}-p_{21}p_{31}}{p_{22}}, \\ \\ p_{33} = \\sqrt{b_{33} - p_{31}^2 - p_{32}^2}.\n\\end{gather*}\\]\nLet’s compute the Cholesky decomposition of \\[\n\\boldsymbol B = \\begin{pmatrix} 1 & -0.5 & 0.6 \\\\ -0.5 & 1 & 0.25 \\\\ 0.6 & 0.25 & 1 \\end{pmatrix}\n\\] using the R function chol():\n\nB = matrix(c(1, -0.5, 0.6, -0.5, 1, 0.25, 0.6, 0.25, 1), ncol=3)\nchol(B)\n\n     [,1]       [,2]      [,3]\n[1,]    1 -0.5000000 0.6000000\n[2,]    0  0.8660254 0.6350853\n[3,]    0  0.0000000 0.4864840\n\n\n\n3.4.6 Vectorization\nThe vectorization operator \\(\\mathop{\\mathrm{vec}}()\\) stacks the matrix entries column-wise into a large vector. The vectorized \\(k \\times m\\) matrix \\(\\boldsymbol A\\) is the \\(km \\times 1\\) vector \\[\n\\mathop{\\mathrm{vec}}(\\boldsymbol A) = (a_{11}, \\ldots, a_{k1}, a_{12}, \\ldots, a_{k2}, \\ldots, a_{1m}, \\ldots, a_{km})'.\n\\]\n\nc(A) #vectorize the matrix A\n\n[1]  1  3  0  2  9 11  3  1  5\n\n\n\n3.4.7 Kronecker product\nThe Kronecker product \\(\\otimes\\) multiplies each element of the left-hand side matrix with the entire matrix on the right-hand side. For a \\(k \\times m\\) matrix \\(\\boldsymbol A\\) and a \\(r \\times s\\) matrix \\(\\boldsymbol B\\), we get the \\(kr\\times ms\\) matrix \\[\nA \\otimes B = \\begin{pmatrix} a_{11}\\boldsymbol B & \\ldots & a_{1m}\\boldsymbol B \\\\ \\vdots & & \\vdots \\\\ a_{k1}\\boldsymbol B & \\ldots & a_{km}\\boldsymbol B \\end{pmatrix},\n\\] where each entry \\(a_{ij} \\boldsymbol B\\) is a \\(r \\times s\\) matrix.\n\nA %x% B #Kronecker product in R\n\n      [,1]  [,2] [,3] [,4]  [,5]  [,6] [,7]  [,8] [,9]\n [1,]  1.0 -0.50 0.60  2.0 -1.00  1.20  3.0 -1.50 1.80\n [2,] -0.5  1.00 0.25 -1.0  2.00  0.50 -1.5  3.00 0.75\n [3,]  0.6  0.25 1.00  1.2  0.50  2.00  1.8  0.75 3.00\n [4,]  3.0 -1.50 1.80  9.0 -4.50  5.40  1.0 -0.50 0.60\n [5,] -1.5  3.00 0.75 -4.5  9.00  2.25 -0.5  1.00 0.25\n [6,]  1.8  0.75 3.00  5.4  2.25  9.00  0.6  0.25 1.00\n [7,]  0.0  0.00 0.00 11.0 -5.50  6.60  5.0 -2.50 3.00\n [8,]  0.0  0.00 0.00 -5.5 11.00  2.75 -2.5  5.00 1.25\n [9,]  0.0  0.00 0.00  6.6  2.75 11.00  3.0  1.25 5.00\n\n\n\n3.4.8 Vector and matrix norm\nA norm \\(\\|\\cdot\\|\\) of a vector or a matrix is a measure of distance from the origin. The most commonly used norms are the Euclidean vector norm \\[\n  \\|\\boldsymbol a\\| = \\sqrt{\\boldsymbol a' \\boldsymbol a} = \\sqrt{\\sum_{i=1}^k a_i^2}\n\\] for \\(\\boldsymbol a \\in \\mathbb R^k\\), and the Frobenius matrix norm \\[\n  \\|\\boldsymbol A \\| = \\sqrt{\\sum_{i=1}^k \\sum_{j=1}^m a_{ij}^2}\n\\] for \\(\\boldsymbol A \\in \\mathbb R^{k \\times m}\\).\nA norm satisfies the following properties:\n\n\n\\(\\|\\lambda \\boldsymbol A\\| = |\\lambda| \\|\\boldsymbol A\\|\\) for any scalar \\(\\lambda\\) (absolute homogeneity)\n\n\\(\\|\\boldsymbol A + \\boldsymbol B\\| \\leq \\|\\boldsymbol A\\| + \\|\\boldsymbol B\\|\\) (triangle inequality)\n\n\\(\\|\\boldsymbol A\\| = 0\\) implies \\(\\boldsymbol A = \\boldsymbol 0\\) (definiteness)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "02b-Matrix-Algebra.html#matrix-calculus",
    "href": "02b-Matrix-Algebra.html#matrix-calculus",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n3.5 Matrix calculus",
    "text": "3.5 Matrix calculus\nLet \\(f(\\beta_1, \\ldots, \\beta_k) = f(\\boldsymbol{\\beta})\\) be a twice-differential real-valued function that depends on some vector \\(\\boldsymbol \\beta = (\\beta_1, \\ldots, \\beta_k)'\\). Examples that frequently appear in econometrics are functions of the inner product form \\(f(\\boldsymbol \\beta) = \\boldsymbol a' \\boldsymbol \\beta\\), where \\(\\boldsymbol a \\in \\mathbb R^k\\), and functions of the sandwich form \\(f(\\boldsymbol \\beta) = \\boldsymbol \\beta' \\boldsymbol A \\boldsymbol \\beta\\), where \\(\\boldsymbol A \\in \\mathbb R^{k \\times k}\\).\n\n3.5.1 Gradient\nThe first derivatives vector or gradient is \\[\n\\frac{\\partial f(\\boldsymbol \\beta)}{\\partial\\boldsymbol{\\beta}}  = \\begin{pmatrix}\\frac{\\partial f(\\boldsymbol \\beta)}{\\partial \\beta_1} \\\\ \\vdots \\\\ \\frac{\\partial f(\\boldsymbol \\beta)}{\\partial \\beta_k} \\end{pmatrix}\n\\] If the gradient is evaluated at some particular value \\(\\boldsymbol \\beta = \\boldsymbol b\\), we write \\[\n\\frac{\\partial f}{\\partial\\boldsymbol{\\beta}}(\\boldsymbol b)\n\\] Useful properties for inner product and sandwich forms are \\[\\begin{align*}\n(i)& \\quad &&\\frac{\\partial (\\boldsymbol a' \\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}  = \\boldsymbol a \\\\\n(ii)& \\quad &&\\frac{\\partial ( \\boldsymbol \\beta' \\boldsymbol A \\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}   = (\\boldsymbol A + \\boldsymbol A') \\boldsymbol \\beta.\n\\end{align*}\\]\n\n3.5.2 Hessian\nThe second derivatives matrix or Hessian is the \\(k \\times k\\) matrix \\[\n    \\frac{\\partial^2 f(\\boldsymbol \\beta)}{\\partial\\boldsymbol{\\beta }\\partial \\boldsymbol{\\beta}'}\n    =  \\begin{pmatrix}\\frac{\\partial^2 f(\\boldsymbol \\beta)}{\\partial \\beta_1 \\partial \\beta_1} & \\ldots & \\frac{\\partial^2 f(\\boldsymbol \\beta)}{\\partial \\beta_k \\partial \\beta_1} \\\\\n    \\vdots & & \\vdots \\\\\n    \\frac{\\partial^2 f(\\boldsymbol \\beta)}{\\partial \\beta_1 \\partial \\beta_k} & \\ldots & \\frac{\\partial^2 f(\\boldsymbol \\beta)}{\\partial \\beta_k \\partial \\beta_k}\n    \\end{pmatrix}.\\]\nIf the Hessian is evaluated at some particular value \\(\\boldsymbol \\beta = \\boldsymbol b\\), we write \\[\n\\frac{\\partial^2 f}{\\partial\\boldsymbol{\\beta }\\partial \\boldsymbol{\\beta}'}(\\boldsymbol b)\n\\]\nThe Hessian is symmetric. Each column of the Hessian is the derivative of the components of the gradient for the corresponding variable in \\(\\boldsymbol \\beta'\\):\n\\[\\begin{align*}\n\\frac{\\partial^2 f(\\boldsymbol \\beta)}{\\partial\\boldsymbol{\\beta }\\partial \\boldsymbol{\\beta}'}\n    &= \\frac{\\partial(\\partial f(\\boldsymbol \\beta)/\\partial \\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta'} \\\\\n    &= \\Bigg[ \\frac{\\partial(\\partial f(\\boldsymbol \\beta)/\\partial \\boldsymbol \\beta)}{\\partial \\beta_1} \\ \\frac{\\partial(\\partial f(\\boldsymbol \\beta)/\\partial \\boldsymbol \\beta)}{\\partial \\beta_2} \\ \\ldots \\ \\frac{\\partial(\\partial f(\\boldsymbol \\beta)/\\partial \\boldsymbol \\beta)}{\\partial \\beta_n} \\Bigg]\n\\end{align*}\\]\nThe Hessian of a sandwich form function is \\[\n  \\frac{\\partial^2 ( \\boldsymbol \\beta' \\boldsymbol A \\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta \\partial \\boldsymbol \\beta'}  = \\boldsymbol A + \\boldsymbol A'.\n\\]\n\n3.5.3 Optimization\nRecall the first-order (necessary) and second-order (sufficient) conditions for optimum (maximum or minimum) in the univariate case:\n\n\nFirst-order condition: the first derivative evaluated at the optimum is zero.\n\nSecond-order condition: the second derivative at the optimum is negative for a maximum and positive for a minimum.\n\nSimilarly, we formulate first and second-order conditions for a function \\(f(\\boldsymbol \\beta)\\). The first-order condition for an optimum (maximum or minimum) at \\(\\boldsymbol b\\) is \\[\n  \\frac{\\partial f}{\\partial\\boldsymbol{\\beta}}(\\boldsymbol b)  = \\boldsymbol 0.\n\\] The second-order condition is \\[\\begin{align*}\n  &\\frac{\\partial^2 f}{\\partial\\boldsymbol{\\beta }\\partial \\boldsymbol{\\beta}'}(\\boldsymbol b) &gt; 0 \\quad \\text{for a minimum at} \\ \\boldsymbol b, \\\\\n  &\\frac{\\partial^2 f}{\\partial\\boldsymbol{\\beta }\\partial \\boldsymbol{\\beta}'}(\\boldsymbol b) &lt; 0 \\quad \\text{for a maximum at} \\ \\boldsymbol b.\n\\end{align*}\\] Recall that, in the context of matrices, the notation “\\(&gt; 0\\)” means positive definite, and “\\(&lt; 0\\)” means negative definite.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "06-Small-Sample-Inference.html",
    "href": "06-Small-Sample-Inference.html",
    "title": "6  Small Sample Inference",
    "section": "",
    "text": "6.1 Normality of the OLS-Estimator\nAssumptions: In Chapter 5, we did not impose a complete distributional assumption on \\(\\varepsilon\\) (see Assumption 4). For instance, the i.i.d. normal case in Assumption 4 was only one possible option. However, to do inference in small samples, the normality Assumption on the error terms is not a mere option, but a necessity.\nTherefore, in this chapter we assume that Assumptions 1-3 from Chapter 5 hold and that additionally the following assumption holds:\nAssumption 4\\(^\\boldsymbol{\\ast}\\): Conditional Gaussian error distribution: The error terms are Gaussian and homoskedastik, i.e., \\[\n\\varepsilon_i|X_i\\sim\\mathcal{N}(0,\\sigma^2)\n\\] for all \\(i=1,\\dots,n.\\)\nAssumption 4\\(^\\boldsymbol{\\ast}\\) together with the random sample assumption of Assumption 1, part (b), leads to Gaussian spherical errors, \\[\n\\varepsilon|X\\sim\\mathcal{N}_n\\left(0,\\sigma^2I_n\\right),\n\\] where \\(\\varepsilon=(\\varepsilon_1,\\dots,\\varepsilon_n)',\\) and where \\(\\mathcal{N}_n\\left(0,\\sigma^2I_n\\right)\\) denotes the \\(n\\)-dimensional normal distribution with \\((n\\times 1)\\)-dimensional mean zero vector \\(0\\) and \\((n\\times n)\\)-dimensional covariance matrix \\(\\sigma^2I_n.\\)\nRemark: The subscript \\(n\\) in \\(\\hat\\beta_n\\) is here only to emphasize that the distribution of \\(\\hat\\beta_n\\) depends on \\(n\\); we will, however, often simply write \\(\\hat\\beta\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Small Sample Inference</span>"
    ]
  },
  {
    "objectID": "06-Small-Sample-Inference.html#sec-testmultp",
    "href": "06-Small-Sample-Inference.html#sec-testmultp",
    "title": "6  Small Sample Inference",
    "section": "\n6.2 \\(F\\)-Tests: Hypothesis Tests about Multiple Parameters",
    "text": "6.2 \\(F\\)-Tests: Hypothesis Tests about Multiple Parameters\nLet us consider the following system of \\(q\\)-many null hypotheses: \\[\n\\begin{align*}\nH_0: \\underset{(q\\times K)}{R}\\underset{(K\\times 1)}{\\beta}  = \\underset{(q\\times 1)}{r^{(0)}},\n\\end{align*}\n\\] where\n\nthe \\((q \\times K)\\) matrix \\(R,\\) which describes the considered linear combinations of the unknown \\(\\beta=(\\beta_1,\\dots,\\beta_K)'\\) values, and\nthe \\((q\\times 1)\\) vector \\(r^{(0)}=(r^{(0)}_{1},\\dots,r^{(0)}_{q})'\\) of null hypothetical values\n\nare chosen by the statistician to specify the null hypothesis about the unknown true parameter vector \\(\\beta\\).\nTo make sure that there are no redundant equations, it is required that \\(\\operatorname{rank}(R)=q\\).\nWe must also specify the alternative against which we are testing the null hypothesis, for instance \\[\n\\begin{equation*}\nH_1: R\\beta  \\neq r^{(0)}\n\\end{equation*}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe above multiple parameter hypotheses cover also the special case of single parameter hypothesis; for instance, by setting\n\n\n\\(R=(0,1,0\\dots,0)\\) and\n\\(r^{(0)}=0\\)\n\nwe get the classic single parameter \\(H_0\\) and \\(H_1\\) that allows us to test the hypothesis that “\\(X_{i2}\\) has no effect”\\[\n\\begin{equation*}\n\\begin{array}{ll}\n&H_0:  \\beta_{2}=0 \\\\\n\\text{versus}\\quad &H_1:  \\beta_{2} \\ne 0 \\\\\n\\end{array}\n\\end{equation*}\n\\] We come back to this in Section 6.3.\n\n\nUnder our assumptions (Assumptions 1 to 4\\(^\\ast\\)), we have that \\[\n\\begin{align*}\n(R\\hat\\beta_n-r^{(0)})|X&\\sim\\mathcal{N}_q\\left(R\\beta -r^{(0)}, RVar(\\hat\\beta_n|X)R'\\right)%\\\\[2ex]\n%\\Leftrightarrow\\quad \\left(RVar(\\hat\\beta_n|X)R'\\right)^{-1/2}(R\\hat\\beta_n-r^{(0)})|X&\\sim\\\\[2ex]\n%\\hspace{2cm}\\sim\\mathcal{N}_q\\left(\\left(RVar(\\hat\\beta_n|X)R'\\right)^{-1/2}(R\\beta -r^{(0)}),I_q\\right)\n\\end{align*}\n\\]\nThus, under the scenario that the null-hypothesis is true, we have that \\[\n\\begin{align*}\n(R\\hat\\beta_n-r^{(0)})|X&\\overset{{\\color{red}{H_0}}}{\\sim}\\mathcal{N}_q\\left(\\,{\\color{red}0}\\,,RVar(\\hat\\beta_n|X)R'\\right)\\\\[2ex]\n\\Leftrightarrow\\quad \\left(RVar(\\hat\\beta_n|X)R'\\right)^{-1/2}(R\\hat\\beta_n-r^{(0)})\\highlight{|X}&\\overset{{\\color{red}{H_0}}}{\\sim}\\underbrace{\\mathcal{N}_q\\left(\\,{\\color{red}0}\\,,I_q\\right)}_{\\highlight{\\text{doesn't depend on $X$}}}\\\\[2ex]\n\\highlight{\\Rightarrow}\\quad \\left(RVar(\\hat\\beta_n|X)R'\\right)^{-1/2}(R\\hat\\beta_n-r^{(0)})&\\overset{{\\color{red}{H_0}}}{\\sim}\\mathcal{N}_q\\left(\\,{\\color{red}0}\\,,I_q\\right),\n\\end{align*}\n\\] where the last implication follows, since the standardized normal distribution does not depend on \\(X.\\)\nThat is, if \\(H_0\\) is correct (i.e., if \\(R\\beta-r^{(0)}=0\\)), the realizations of \\[\n\\left(RVar(\\hat\\beta_n|X)R'\\right)^{-1/2}(R\\hat\\beta_n-r^{(0)})\n\\] will scatter around the \\((q\\times 1)\\) vector \\(0\\) in a Gaussian fashion.\nWe use a test statistic to detect a systematic location shift away from the \\((q\\times 1)\\) vector \\(0.\\)\n\n\n\n6.2.1 The Test Statistic and its Null Distribution\nThe fact that \\[\n(R\\hat\\beta_n-r^{(0)})\\in\\mathbb{R}^q\n\\] is a \\(q\\)-dimensional random variable makes it a little bothersome to use as a test-statistic. Fortunately, we can turn \\((R\\hat\\beta_n-r^{(0)})\\) into a scalar-valued test statistic using the following quadratic form: \\[\n\\begin{align*}\nW\n& = \\overbrace{\\left(\\left(RVar(\\hat\\beta_n|X)R'\\right)^{-1/2}(R\\hat\\beta_n-r^{(0)})\\right)'\\;\\;}^{(1\\times q)}\\\\\n&\\;\\;\\;\\;\\;\\;\\underbrace{\\left(\\left(RVar(\\hat\\beta_n|X)R'\\right)^{-1/2}(R\\hat\\beta_n-r^{(0)})\\right)}_{(q \\times 1)}\\\\[2ex]\n& =\\underbrace{(R\\hat\\beta_n -r^{(0)})'}_{(1\\times q)}\\underbrace{[RVar(\\hat\\beta_n|X)R']^{-1}}_{(q\\times q)}\\underbrace{(R\\hat\\beta_n -r^{(0)})}_{(q\\times 1)}\n\\end{align*}\n\\qquad(6.2)\\]\n\n\n\n\n\n\nNote\n\n\n\nNote that the test statistic \\(W\\) is simply measuring the distance (it’s a weighted, squared Euclidean distance) between the \\((q\\times 1)\\) vectors \\(R\\hat\\beta_n\\) and \\(r^{(0)}.\\)\n\n\nUnder the null hypothesis (i.e., if \\(H_0\\) is true), \\(W\\) is a sum of \\(q\\)-many independent, squared standard normal \\(\\mathcal{N}(0,1)\\) random variables. Therefore, under the null hypothesis, \\(W\\) is chi-square distributed with \\(q\\) degrees of freedom (see definition of the chi-square distribution in Section 2.2.10.3), \\[\n\\begin{align*}\nW&\\overset{H_0}{\\sim} \\chi^2_{(q)}\n\\end{align*}\n\\] Note that the distribution of \\(W\\) does not depend on \\(X.\\) I.e. \\(W\\) follows a \\(\\chi^2_{(q)}\\)-distribution no matter the realization of \\(X.\\)\nUsually, however, we do not know \\(Var(\\hat\\beta_n|X)=\\sigma^2 (X'X)^{-1},\\) since we usually do not know the value of \\(\\sigma^2.\\) Thus, we need to substitute \\(\\sigma^2\\) by an estimator.\nFor exact finite sample inference, we need a variance estimator of \\(\\sigma^2\\) for which we can derive its exact small sample distribution. Therefore, we require Assumption 4\\(^*\\) of spherical errors, i.e. \\(Var(\\varepsilon|X)=\\sigma^2I_n,\\) which implies that \\(Var(\\hat\\beta_n|X)=\\sigma^2(X'X)^{-1}\\), and where \\(\\sigma^2\\) can be estimated by the unbiased (\\(UB\\)) variance estimator\\[\ns_{UB}^2=(n-K)^{-1}\\sum_{i=1}^n\\hat\\varepsilon_i^2.\n\\]\nFrom the normality assumption in Assumption 4\\(^*\\), it follows then that \\[\n\\frac{(n-K)}{\\sigma^{2}}s_{UB}^2\\sim\\chi^2_{(n-K)}.\n\\qquad(6.3)\\]\nSubstituting the unknown \\[\nVar(\\hat\\beta_n|X)=\\sigma^2 (X'X)^{-1}\n\\] in Equation 6.2 by its estimator \\[\n\\widehat{Var}(\\hat\\beta_n|X)=s_{UB}^2 (X'X)^{-1}\n\\] leads to the \\(F\\)-test statistic \\[\nF=(R\\hat\\beta_n -r^{(0)})'[R(s_{UB}^2(X'X)^{-1})R']^{-1}(R\\hat\\beta_n -r^{(0)})/q\n\\] and takes into account the additional randomness (estimation errors) due to estimating \\(\\sigma^2\\) by \\(s_{UB}^2.\\)\nUnder the null-hypothesis, one can show that \\[\nF\\overset{H_0}{\\sim} F_{(q,n-K)},\n\\qquad(6.4)\\] where \\(F_{(q,n-K)}\\) denotes the \\(F\\)-distribution with \\(q\\) numerator and \\(n-K\\) denominator degrees of freedom.\nAs in the case of \\(W\\), the distribution of \\(F\\) does not depend on \\(X.\\)\n\nNote: The distributional statements in Equation 6.3 and Equation 6.4 are a little cumbersome to derive and we do not go into details here, but in case you’re interested you can find some more details, for instance, in Chapter 1 of Hayashi (2000).\n\nBy contrast to \\(W,\\) \\(F\\) is now a practically useful test statistic, and we can use the observed value \\(F_{\\text{obs}}\\) to measure the distance of our observed estimate \\(R\\hat\\beta_n\\) from its null-hypothetical value \\(r^{(0)}.\\)\nObserved values, \\(F_{\\text{obs}}\\), that are “unusually large” under the null hypothesis, lead to a rejection of the null hypothesis. The null distribution \\(F_{(q,n-K)}\\) of \\(F\\) is used to judge what’s “unusually large” under the null hypothesis.\nThe F distribution. The F distribution is a ratio of two \\(\\chi^2\\) distributions. It has two parameters: the numerator degrees of freedom, and the denominator degrees of freedom. Each combination of the parameters yields a different F distribution (Figure 6.1). See Section 2.2.10.6 for more information on the \\(F\\) distribution.\n\n\n\n\n\n\n\nFigure 6.1: Graphs of the density function of the \\(F\\)-distribution for different numerator degrees of freedom and different denominator degrees of freedom.\n\n\n\n\n\n6.2.2 Test Decision using the Rejection Region\nLet \\(0&lt;\\alpha&lt;1\\) denote the significance level and let \\(c_{1-\\alpha}\\) denote the \\((1-\\alpha)\\) quantile of the \\(F\\)-distribution with \\((q,n-K)\\) degrees of freedom.\nThis quantile \\(c_{1-\\alpha}\\) is the critical value that defines the rejection region: \\[\n\\mathcal{R}=\\; ]c_{1-\\alpha},\\infty[\n\\] \n\nWe can rejection \\(H_0\\) if \\[\nF_{obs}\\in \\mathcal{R}=\\; ]c_{1-\\alpha},\\infty[\n\\]\n\nWe cannot rejection \\(H_0\\) if \\[\nF_{obs}\\not \\in \\mathcal{R}=\\; ]c_{1-\\alpha},\\infty[\n\\]\n\n\n\n\n\n\n\n\n\nFigure 6.2: Graph of the density function of the \\(F\\)-distribution with \\(9\\) numerator degrees of freedom and \\(120\\) denominator degrees of freedom. The rejection region \\(\\mathcal{R}\\) is shown in red.\n\n\n\n\nThe rejection region: The rejection region describes a range of values of the test statistic \\(F\\) which we rarely see if the null hypothesis is true (only in at most \\(\\alpha \\cdot 100\\%\\) cases). If the observed value of the test statistic, \\(F_{\\text{obs}}\\), falls in this region, we will reject the null hypothesis and acknowledge a type-I-error rate of at most \\(\\alpha\\).\nThe non-rejection region: The non-rejection region describes a range of values of the test statistic \\(F\\) which we expect to see (in \\((1-\\alpha) \\cdot 100\\%\\) cases) if the null hypothesis is true. If the observed value of the test statistic, \\(F_{\\text{obs}}\\) falls in this region, we cannot reject the null hypothesis.\nTo find the critical value \\(c_{1-\\alpha}\\) we can use R as following:\n\nalpha &lt;- 0.05 # chosen significance level\ndf1   &lt;- 9    # numerator df\ndf2   &lt;- 120  # denominator df\n\n## Critical value:\ncrit_value &lt;- qf(p = 1-alpha, df1 = df1, df2 = df2)\ncrit_value\n\n[1] 1.958763\n\n\nChanging the significance level from \\(\\alpha=0.05\\) to \\(\\alpha=0.01\\) makes the critical value \\(c_{1-\\alpha}\\) larger and, therefore, the rejection region smaller (smaller probability of type-I-errors).\n\nalpha &lt;- 0.01 # chosen significance level\n## Critical value:\ncrit_value &lt;- qf(p = 1-alpha, df1 = df1, df2 = df2)\ncrit_value\n\n[1] 2.558574\n\n\n\n6.2.3 Test Decision using the \\(p\\)-Value\nRemember that (under Assumption 4\\(^\\boldsymbol{\\ast}\\)) \\[\nF\\overset{H_0}{\\sim}F_{q,n-K}.\n\\]\nThe \\(p\\)-value of the \\(F\\)-test is the probability of seeing realizations of \\(F\\) that are equal to or larger than the observed value \\(F_{\\text{obs}}\\) given that the null hypothesis is true \\[\np_{\\text{obs}}=P(F\\geq F_{\\text{obs}}\\;|\\;H_0 \\text{ is true})\n\\]\n\nWe reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} &lt; \\alpha\n\\]\n\nWe cannot reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} \\geq \\alpha,\n\\] where \\(0&lt;\\alpha&lt;1\\) denotes the chosen significance level. Typical choices are\n\n\\(\\alpha = 0.05\\)\n\\(\\alpha = 0.01\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Small Sample Inference</span>"
    ]
  },
  {
    "objectID": "06-Small-Sample-Inference.html#sec-testingsinglep",
    "href": "06-Small-Sample-Inference.html#sec-testingsinglep",
    "title": "6  Small Sample Inference",
    "section": "\n6.3 \\(t\\)-Tests: Hypothesis Tests about One Parameter",
    "text": "6.3 \\(t\\)-Tests: Hypothesis Tests about One Parameter\nA hypothesis about only one parameter \\[\n\\begin{equation*}\n\\begin{array}{ll}\n& H_0: \\beta_k=\\beta_k^{(0)}\\\\\n\\text{versus}\\quad & H_1: \\beta_k\\ne \\beta_k^{(0)}\\\\\n\\end{array}\n\\end{equation*}\n\\] is simply a special case of the general null hypothesis \\(H_0:R\\beta =r^{(0)},\\) where\n\n\n\\(R\\) is a \\((1\\times K)\\) row-vector of zeros, but with a one as the \\(k\\)th element, and where\nwe write \\(r^{(0)}=\\beta_k^{(0)}\\) since we make a hypothesis only about \\(\\beta_k.\\)\n\n\nThus the \\(F\\)-test statistic simplifies to \\[\nF=\\frac{\\left(\\hat{\\beta}_k-\\beta_k^{(0)}\\right)^2}{\\widehat{Var}(\\hat{\\beta}_k|X)}\\overset{H_0}{\\sim}F_{(1,n-K)},\n\\] where \\[\n\\widehat{Var}(\\hat{\\beta}_k|X)=s^2_{UB}[(X'X)^{-1}]_{(k,k)},\n\\] and where \\([(X'X)^{-1}]_{(k,k)}\\) denotes the element in the \\(k\\)th row and \\(k\\)th column of the \\((K\\times K)\\) matrix \\((X'X)^{-1}.\\)\n\n6.3.1 The Test Statistic and its Null Distribution\nTaking square roots yields the \\(t\\) test statistic \\[\nT=\\frac{\\hat{\\beta}_k-\\beta_k^{(0)}}{\\widehat{\\operatorname{SE}}(\\hat{\\beta}_k|X)}\\overset{H_0}{\\sim}t_{(n-K)},\n\\] where \\[\n\\begin{align*}\n\\widehat{\\operatorname{SE}}(\\hat{\\beta}_k|X)\n&=\\sqrt{\\widehat{Var}(\\hat{\\beta}_k|X)}\\\\[2ex]\n&=s_{UB}[(X'X)^{-1/2}]_{(k,k)},\n\\end{align*}\n\\] and where \\(t_{(n-K)}\\) denotes the \\(t\\)-distribution with \\(n-K\\) degrees of freedom.\nThus the \\(t\\)-distribution with \\(n-K\\) degrees of freedom is the appropriate distribution to judge whether an observed value \\(T_{\\text{obs}}\\) of the test statistic is “unusually large” under the null hypothesis.\n\n\n\n\n\n\nTip\n\n\n\nAll commonly used statistical software packages report in their regression output tables \\(t\\)-tests testing the “no (linear) effect” null hypothesis \\[\nH_0:\\beta_k=0\n\\] for each \\(k=1,\\dots,K.\\)\nThis means to test the null hypothesis that \\(X_k\\) has on average no (linear) effect on the outcome variable \\(Y.\\)\n\n\nThe \\(t\\) distribution. Figure 6.3 illustrates that as the degrees of freedom increase, the shape of the \\(t\\) distribution comes closer to that of a standard normal bell curve. Already for \\(25\\) degrees of freedom we find little difference to the standard normal density. In case of small degrees of freedom values, we find the distribution to have heavier tails than a standard normal. See Section 2.2.10.4 for more information about the \\(t\\)-distribution.\n\n\n\n\n\n\n\nFigure 6.3: Graphs of the density function of the \\(t\\)-distribution with \\(2,\\,4,\\) and \\(25\\) degrees of freedom and of the density of the standard normal distribution.\n\n\n\n\n\n6.3.2 Test Decision using the Rejection Region (One-Sided 1/2)\nThe right-sided version of the one-sided hypothesis is given by \\[\n\\begin{align*}\n&H_0: \\beta_k \\leq \\beta_k^{(0)}\\\\\n\\text{versus}\\quad\n& H_1: \\beta_k &gt; \\beta_k^{(0)},\n\\end{align*}\n\\] where \\(\\beta_k\\) denotes the true (unknown) parameter value and \\(\\beta_k^{(0)}\\) the null hypothetical value specified by the statistician (e.g. \\(\\beta_k^{(0)}=0\\)).\nThe rejection region is \\[\n\\mathcal{R}=\\;]c_{1-\\alpha}, \\infty[,\n\\] where \\(0&lt;\\alpha&lt;1\\) denotes the significance level and \\(c_{1-\\alpha}\\) denotes the \\((1-\\alpha)\\) quantile of the \\(t\\)-distribution with \\((n-K)\\) degrees of freedom.\n\nWe can reject \\(H_0\\) if \\[\n\\begin{align*}\nT_{obs}\n& \\in \\mathcal{R} = \\;]c_{1-\\alpha}, \\infty[\n\\end{align*}\n\\]\n\nWe cannot reject \\(H_0\\) if \\[\n\\begin{align*}\nT_{obs}\n&\\not \\in \\mathcal{R} = \\;]c_{1-\\alpha}, \\infty[\n\\end{align*}\n\\]\n\n\nFigure 6.4 shows an example of the rejection region for the case of a significance level \\(\\alpha=0.05\\) and \\(n-K=12\\) degrees of freedom.\n\n\n\n\n\n\n\nFigure 6.4: \\(t\\)-distribution with \\(n-K=12\\) degrees of freedom and critical value \\(c_{1-\\alpha}=1.78\\) for the significance level \\(\\alpha=0.05\\).\n\n\n\n\nTo find the \\(c_{1-\\alpha}\\) critical value we can use R as following:\n\nalpha &lt;- 0.05 # chosen significance level \ndf    &lt;- 12   # degrees of freedom \n\n## One-sided critical value (1-alpha) quantile:\nc_oneSided &lt;- qt(p = 1-alpha, df = df)\nc_oneSided\n\n[1] 1.782288\n\n\n\n6.3.3 Test Decision using the Rejection Region (One-Sided 2/2)\nThe left-sided version of the one-sided hypothesis is given by \\[\n\\begin{align*}\n&H_0:  \\beta_k \\geq \\beta_k^{(0)}\\\\\n\\text{versus}\\quad\n&H_1:  \\beta_k &lt;  \\beta_k^{(0)},\n\\end{align*}\n\\] where \\(\\beta_k\\) denotes the true (unknown) parameter value and \\(\\beta_k^{(0)}\\) the null hypothetical value specified by the statistician (e.g. \\(\\beta_k^{(0)}=0\\)).\nThe rejection region is \\[\n\\mathcal{R}=\\;]-\\infty,c_{\\alpha}[,\n\\] where \\(0&lt;\\alpha&lt;1\\) denotes the significance level and \\(c_{1-\\alpha}\\) denotes the \\((1-\\alpha)\\) quantile of the \\(t\\)-distribution with \\((n-K)\\) degrees of freedom.\n\nWe can reject \\(H_0\\) if \\[\n\\begin{align*}\nT_{obs}\n& \\in \\mathcal{R} = \\;]-\\infty,c_{\\alpha}[\n\\end{align*}\n\\]\n\nWe cannot reject \\(H_0\\) if \\[\n\\begin{align*}\nT_{obs}\n&\\not \\in \\mathcal{R} = \\;]-\\infty,c_{\\alpha}[\n\\end{align*}\n\\]\n\n\nFigure 6.5 shows an example of the rejection region for the case of a significance level \\(\\alpha=0.05\\) and \\(n-K=12\\) degrees of freedom.\n\n\n\n\n\n\n\nFigure 6.5: \\(t\\)-distribution with \\(n-K=12\\) degrees of freedom and critical value \\(c_{\\alpha}=-1.78\\) for the significance level \\(\\alpha=0.05\\).\n\n\n\n\nTo find the \\(c_{\\alpha}\\) critical value we can use R as following:\n\nalpha &lt;- 0.05 # chosen significance level \ndf    &lt;- 12   # degrees of freedom \n\n## One-sided critical value (alpha) quantile:\nc_oneSided &lt;- qt(p = alpha, df = df)\nc_oneSided\n\n[1] -1.782288\n\n\n\n6.3.4 Test Decision using the Rejection Region (Two-Sided)\nThe two-sided \\(t\\)-test allows us to test \\[\n\\begin{align*}\n& H_0: \\beta_k=\\beta_k^{(0)}\\\\\n\\text{versus}\\quad\n& H_1: \\beta_k\\ne \\beta_k^{(0)},\n\\end{align*}\n\\] where \\(\\beta_k\\) denotes the true (unknown) parameter value and \\(\\beta_k^{(0)}\\) the null hypothetical value specified by the statistician (e.g. \\(\\beta_k^{(0)}=0\\)).\nThe rejection region is \\[\n\\mathcal{R}=\\;]-\\infty,c_{\\alpha/2}[\\;\\;\\cup\\;\\;]c_{1-\\alpha/2}, \\infty[,\n\\] where \\(0&lt;\\alpha&lt;1\\) denotes the significance level and \\(c_{\\alpha/2}\\) and \\(c_{1-\\alpha/2}\\) denote the \\(\\alpha/2\\) and the \\((1-\\alpha/2)\\) quantiles of the \\(t\\)-distribution with \\((n-K)\\) degrees of freedom.\n\nWe can reject \\(H_0\\) if \\[\n\\begin{align*}\nT_{obs}\n& \\in \\mathcal{R}%\\\\[2ex]\n%& \\in \\;]-\\infty,c_{\\alpha/2}[\\;\\;\\cup\\;\\;]c_{1-\\alpha/2}, \\infty[\n\\end{align*}\n\\]\n\nWe cannot reject \\(H_0\\) if \\[\n\\begin{align*}\nT_{obs}\n&\\not \\in \\mathcal{R}%\\\\[2ex]\n%&\\not \\in \\;]-\\infty,c_{\\alpha/2}[\\;\\;\\cup\\;\\;]c_{1-\\alpha/2}, \\infty[\n\\end{align*}\n\\]\n\n\nFigure 6.6 shows an example of the rejection region for the case of a significance level \\(\\alpha=0.05\\) and \\(n-K=12\\) degrees of freedom.\n\n\n\n\n\n\n\nFigure 6.6: \\(t\\)-distribution with \\(n-K=12\\) degrees of freedom and critical values \\(c_{\\alpha/2}=-2.18\\) and \\(c_{1-\\alpha/2}=2.18\\) for the significance level \\(\\alpha=0.05\\).\n\n\n\n\nTo find the \\(c_{\\alpha/2}\\) and \\(c_{1-\\alpha/2}\\) critical values we can use R as following:\n\nalpha &lt;- 0.05 # chosen signficance level \ndf    &lt;- 12   # degrees of freedom \n\n## Two-sided critical value (= (1-alpha/2) quantile):\nc_twoSided &lt;- qt(p = 1-alpha/2, df = df)\n\n## lower critical value\n-c_twoSided\n\n[1] -2.178813\n\n## upper critical value\nc_twoSided\n\n[1] 2.178813\n\n\n\n6.3.5 Test Decision using the \\(p\\)-Value (One-Sided 1/2)\nRight-sided version of the one-sided hypothesis: \\[\n\\begin{align*}\n&H_0: \\beta_k \\leq \\beta_k^{(0)}\\\\\n\\text{versus}\\quad & H_1:\\beta_k &gt;    \\beta_k^{(0)}\\\\\n\\end{align*}\n\\]\n\nWe know that \\[\nT\\overset{H_0}{\\sim}t_{n-K}.\n\\]\nThe \\(p\\)-value is the probability of seeing realizations of \\(T\\) that are equal to or larger than the observed value \\(T_{\\text{obs}}\\) given that the null hypothesis is true \\[\np_{\\text{obs}}=P(T\\geq T_{\\text{obs}}\\;|\\;H_0 \\text{ is true}).\n\\] * We reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} &lt; \\alpha\n\\]\n\nWe cannot reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} \\geq \\alpha,\n\\] where \\(0&lt;\\alpha&lt;1\\) denotes the chosen significance level. Typical choices are\n\n\\(\\alpha = 0.05\\)\n\\(\\alpha = 0.01\\)\n\n\n\n6.3.6 Test Decision using the \\(p\\)-Value (One-Sided 2/2)\nLeft-sided version of the one-sided hypothesis: \\[\n\\begin{align*}\n&H_0: \\beta_k \\geq \\beta_k^{(0)}\\\\\n\\text{versus}\\quad & H_1: \\beta_k &lt;  \\beta_k^{(0)}\\\\\n\\end{align*}\n\\]\nThe \\(p\\)-value is the probability of seeing realizations of \\(T\\) that are equal to or smaller than the observed value \\(T_{\\text{obs}}\\) given that the null hypothesis is true \\[\np_{\\text{obs}}=P(T\\leq T_{\\text{obs}}\\;|\\;H_0 \\text{ is true}).\n\\]\n\nWe reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} &lt; \\alpha\n\\]\n\nWe cannot reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} \\geq \\alpha,\n\\] where \\(0&lt;\\alpha&lt;1\\) denotes the chosen significance level. Typical choices are\n\n\\(\\alpha = 0.05\\)\n\\(\\alpha = 0.01\\)\n\n\n\n6.3.7 Test Decision using the \\(p\\)-Value (Two-Sided)\nThe two-sided \\(t\\)-test allows us to test \\[\n\\begin{align*}\n& H_0: \\beta_k=\\beta_k^{(0)}\\\\\n\\text{versus}\\quad\n& H_1: \\beta_k\\ne \\beta_k^{(0)}\n\\end{align*}\n\\] where \\(\\beta_k\\) denotes the true (unknown) parameter value and \\(\\beta_k^{(0)}\\) the null hypothetical value specified by the statisticaian (e.g. \\(\\beta_k^{(0)}=0\\)).\nWe know that \\[\nT\\overset{H_0}{\\sim}t_{n-K}.\n\\]\nThe \\(p\\)-value of the two-sided \\(t\\)-test is the probability of seeing realizations of \\(T\\) that are equal to or more extreme than the observed value \\(T_{\\text{obs}}\\) given that the null hypothesis is true \\[\n\\begin{align*}\np_{\\text{obs}}\n&=P(|T|\\geq |T_{\\text{obs}}|\\;|\\;H_0 \\text{ is true})\\\\[2ex]\n&=2\\cdot\\min\\{P(T\\leq T_{\\text{obs}}\\;|\\;H_0 \\text{ is true}),\n            P(T\\geq T_{\\text{obs}}\\;|\\;H_0 \\text{ is true})\\}\n\\end{align*}\n\\]\n\nWe reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} &lt; \\alpha\n\\]\n\nWe cannot reject the null hypothesis \\(H_0\\) if \\[\np_{\\text{obs}} \\geq \\alpha,\n\\] where \\(0&lt;\\alpha&lt;1\\) denotes the chosen significance level. Typical choices are\n\n\\(\\alpha = 0.05\\)\n\\(\\alpha = 0.01\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Small Sample Inference</span>"
    ]
  },
  {
    "objectID": "06-Small-Sample-Inference.html#testtheory",
    "href": "06-Small-Sample-Inference.html#testtheory",
    "title": "6  Small Sample Inference",
    "section": "\n6.5 Testtheory",
    "text": "6.5 Testtheory\nEvery statistical test statistic is a function of the random sample, i.e. \\[\nT_n\\equiv T((X_1,Y_1),\\dots,(X_n,Y_n))\n\\] and is thus a random variable.\nCaution: In this section, \\(T_n\\) denotes any test statistic. Specific examples for \\(T_n\\) are, for instance,\n\nthe \\(F\\)-test statistic (Section 6.2) and\nthe \\(t\\)-test statistic (Section 6.3).\n\nGenerally, we can only derive the distribution of \\(T_n\\) under \\(H_0;\\) i.e. under the scenario that \\(H_0\\) is true, \\[\n\\begin{equation*}\n\\begin{array}{ll}\nT_n & \\overset{{H_0}}\\sim f_{T_n}\\quad ✅\n\\end{array}\n\\end{equation*}\n\\] For instance, in case of the \\(F\\)-test, \\(f_{T_n}\\) denotes the \\(F\\)-distribution with \\(q\\) and \\(n-K\\) degrees of freedom and in case of the \\(t\\)-test, \\(f_{T_n}\\) denotes the \\(t\\)-distribution with \\(n-K\\) degrees of freedom.\nHowever, the distribution of \\(T_n\\) under \\(H_1\\) is generally unknown. \\[\n\\begin{equation*}\n\\begin{array}{ll}\nT_n & \\overset{{H_1}}\\sim ❌\n\\end{array}\n\\end{equation*}\n\\]\n\n\n6.5.1 Simple versus Composite Hypotheses\nWe use the observed realization \\[\nT_{n,\\text{obs}}\\equiv T((X_{1,\\text{obs}},Y_{1,\\text{obs}}),\\dots,(X_{n,\\text{obs}},Y_{n,\\text{obs}}))\n\\] of the random test statistic \\(T_n\\) to decide:\n\nwhether we cannot reject a null hypothesis \\(H_0\\) about some parameter \\(\\theta\\) \\[\nH_0: \\theta\\in\\Theta_0,\n\\]\n\nor wether we can reject \\(H_0\\) in favor of the alternative hypothesis \\(H_1\\) \\[\nH_1: \\theta\\in\\Theta_1.\n\\] The test decision is made using the rejection region, the \\(p\\)-value, or a confidence interval.\n\nNotation:\n\n\n\\(\\Theta_0\\) denotes the set of parameter values \\(\\theta\\) under \\(H_0\\).\n\n\\(\\Theta_1\\) denotes the set of parameter values \\(\\theta\\) under \\(H_1\\).\n\nIt is required that \\[\n\\Theta_0 \\cap \\Theta_1 = \\emptyset.\n\\]\n\nIf \\(\\Theta_j,\\) \\(j=1,2,\\) contains only one value \\[\n\\Theta_j=\\theta^{(j)},\n\\] it is called a simple hypothesis.\nIf \\(\\Theta_j,\\) \\(j=1,2,\\) contains multiple values, it is called a composite hypothesis.\n\nThe idea of a composite null hypothesis \\[\nH_0:\\theta\\in\\Theta_0\n\\] is to collect all hypotheses which we do not care to detect by the statistical test. This way, the set of alternative hypotheses \\(H_1:\\theta\\in\\Theta_1\\) becomes smaller which leads to more powerful tests.\n\n\n\n\n\n\nExample: Two-Sided Test with \\(\\theta\\in\\mathbb{R}\\)\n\n\n\n\\[\n\\begin{equation*}\n\\begin{array}{ll}\n& H_0: \\theta = \\theta^{(0)}\\\\\n\\text{versus}\\quad\n& H_1: \\theta\\neq \\theta^{(0)}\\\\\n\\end{array}\n\\end{equation*}\n\\]\nHere we have a simple null hypothesis \\[\n\\Theta_0=\\theta^{(0)}\n\\] and a composite alternative hypothesis \\[\n\\Theta_1=\\mathbb{R}\\setminus\\theta^{(0)}.\n\\]\n\n\n\n\n\n\n\n\nExample: One-Sided Test with \\(\\theta\\in\\mathbb{R}\\)\n\n\n\n\\[\n\\begin{equation*}\n\\begin{array}{ll}\n& H_0: \\theta \\geq  \\theta^{(0)}\\\\\n\\text{versus}\\quad\n& H_1: \\theta  &lt; \\theta^{(0)}\\\\\n\\end{array}\n\\end{equation*}\n\\]\nHere we have a composite null hypothesis \\[\n\\Theta_0=\\;[\\theta^{(0)},\\infty[\n\\] and a composite alternative hypothesis \\[\n\\Theta_1=\\;]-\\infty,\\theta^{(0)}[.\n\\]\n(Likewise for the other direction of the one-sided test.)\n\n\n\n6.5.2 Decisions Errors\nHypothesis testing is like a legal trial. We assume someone is innocent unless the evidence strongly suggests that he is guilty. Similarly, we retain \\(H_0\\) unless there is strong evidence to reject \\(H_0.\\)\nWe differentiate two decision errors events:\n\n\ntype-I-error (or false positive): Rejecting \\(H_0\\) even though \\(H_0\\) is true.\n\ntype-II-error: Not rejecting \\(H_0\\) even though \\(H_1\\) is true.\n\n\n\n\n\n\n\nCaution\n\n\n\nNot being able to reject \\(H_0,\\) does not mean a validation/confirmation of \\(H_0,\\) but only reflects that we do not have sufficient evidence against a possibly false \\(H_0.\\)\nIndeed, a given violation of \\(H_0\\) may only be too small to stand out from the estimation errors; i.e. we may do a type-II-error. Problem is, we generally cannot control the probability of type-II-errors since we do not know the distribution of the test statistic under \\(H_1.\\) We can only control the probability of type-I-errors since we know the distribution of the test statistic under \\(H_0.\\)\nTherefore, if you are not able to reject \\(H_0,\\) never ever state something like: “I conclude \\(H_0\\) is true.”\n\n\n\n\n\n\nTip\n\n\n\nFor the special case of a “no-effect” null hypothesis \\[\nH_0:\\beta_k=0,\n\\] there’s a famous sentence which goes back to Altman and Bland (1995) :\n\n“Absence of evidence is not evidence of absence.”\n\n\n\n\n\n\n6.5.3 Size\nThe probability of a type-I-error event is called size of \\(T_n.\\)\n\n\n\n\n\n\n\nDefinition 6.1 (Size) The size of a test statistic \\(T_n\\) is the largest probability of rejecting \\(H_0\\) over all possible null-hypothetical parameter values \\(\\theta\\in\\Theta_0\\) \\[\n\\begin{align*}\n\\text{Size}_{n,\\alpha}\n=& \\sup_{\\theta\\in\\Theta_0} P(T_n \\in \\mathcal{R}_{n,\\alpha} | \\theta\\in \\Theta_0 ).\n\\end{align*}\n\\]\n\n\n\n\nA statistical hypothesis test procedure is called valid test if its size can be bounded from above by the chosen significance level (nominal size) \\(\\alpha\\), i.e. if\\[\n\\begin{align*}\n\\text{Size}_{n,\\alpha}\\; \\leq\\; \\alpha = \\text{Nominal Size}\n\\end{align*}\n\\]\nSince we want to keep the probability of falsely rejecting \\(H_0\\) small, we choose small singificance levels such as\n\n\n\\(\\alpha=0.05\\),\n\n\\(\\alpha=0.01\\), or\n\\(\\alpha=0.001.\\)\n\n\n\n\n\n\n\nExact vs conservative vs invalid\n\n\n\nA test statistic \\(T_n\\) is called\n\n\nexact if \\[\n\\text{Size}_{n,\\alpha}=\\sup_{\\theta\\in\\Theta_0} P(T_n \\in \\mathcal{R}_{n,\\alpha}| \\theta\\in \\Theta_0 ) =\\alpha,\n\\]\n\n\nconservative if \\[\n\\text{Size}_{n,\\alpha}= \\sup_{\\theta\\in\\Theta_0} P(T_n \\in \\mathcal{R}_{n,\\alpha}| \\theta\\in \\Theta_0 ) &lt;\\alpha,\n\\]\n\n\ninvalid if \\[\n\\text{Size}_{n,\\alpha}=\\sup_{\\theta\\in\\Theta_0} P(T_n \\in \\mathcal{R}_{n,\\alpha}| \\theta\\in \\Theta_0 ) &gt; \\alpha.\n\\]\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUnder Assumptions 1-4\\(^\\ast,\\) the \\(F\\)-test and the \\(t\\)-test are exact test statistics.\n\n\nShowing Exactness of the \\(F\\)-Test\n\\[\n\\begin{equation*}\n\\begin{array}{lll}\n& H_0: R\\beta = r^{(0)}   & (\\text{simple hypothesis})\\\\\n\\text{versus}\\quad\n& H_1: R\\beta \\neq r^{(0)}& (\\text{composite hypothesis})\\\\\n\\end{array}\n\\end{equation*}\n\\]\nUnder Assumptions 1-4\\(^\\ast\\) (see Section 6.2), we have that \\[\nF_n\\overset{H_0}{\\sim}F_{q,n-K}.\n\\qquad(6.6)\\]\nEquation 6.6 allows us to show that the \\(F_n\\)-test is an exact test. Since the null hypothesis is a simple hypothesis we do not need to consider the supremum \\((\\sup_{\\theta\\in\\Theta_0}),\\) but simply have \\[\n\\begin{align*}\n\\text{Size}_{n,\\alpha}\n= & P(F_n \\in \\mathcal{R}_{n,\\alpha} \\;|\\; R\\beta = r^{(0)}).\n\\end{align*}\n\\] Thus, \\[\n\\begin{align*}\n\\text{Size}_{n,\\alpha}\n= & P(F_n \\in \\mathcal{R}_{n,\\alpha} \\;|\\; R\\beta = r^{(0)})\\\\[2ex]\n= & P\\Big(F_n &gt; c_{n,1-\\alpha}\\;|\\;R\\beta = r^{(0)}\\Big)\\\\[2ex]\n= & 1 - P\\Big(F \\leq c_{n,1-\\alpha}\\;|\\;R\\beta = r^{(0)}\\Big)=\\alpha,\n\\end{align*}\n\\] for all sample sizes \\(n&gt;K.\\)\nShowing Exactness of the Two-Sided \\(t\\)-Test:\n\\[\n\\begin{equation*}\n\\begin{array}{lll}\n& H_0: \\beta_k =  \\beta_k^{(0)}   & (\\text{simple hypothesis})\\\\\n\\text{versus}\\quad\n& H_1: \\beta_k \\neq \\beta_k^{(0)}& (\\text{composite hypothesis})\\\\\n\\end{array}\n\\end{equation*}\n\\]\nUnder Assumptions 1-4\\(^\\ast\\) (see Section 6.3), we have that \\[\nT_n\\overset{H_0}{\\sim}t_{n-K}.\n\\qquad(6.7)\\]\nEquation 6.7 allows us to show that the \\(t\\)-test is an exact test. Since the null hypothesis is a simple hypothesis we do not need to consider the supremum \\((\\sup_{\\theta\\in\\Theta_0}),\\) but simply have \\[\n\\begin{align*}\n\\text{Size}_{n,\\alpha}\n=&P(T_n \\in \\mathcal{R}_{n,\\alpha} \\;|\\; \\beta_k = \\beta_k^{(0)}).\n\\end{align*}\n\\] Thus, \\[\n\\begin{align*}\n\\text{Size}_{n,\\alpha}\n=&P(T_n \\in \\mathcal{R}_{n,\\alpha} \\;|\\; \\beta_k = \\beta_k^{(0)})\\\\[2ex]\n=&P\\Big(T_n &lt; c_{n,\\alpha/2}\\quad\\text{or}\\quad T_n&gt;c_{n,1-\\alpha/2} \\;\\Big|\\; \\beta_k = \\beta_k^{(0)}\\Big)\\\\[2ex]\n=&P\\Big(T_n &lt; c_{n,\\alpha/2} \\;\\Big|\\; \\beta_k = \\beta_k^{(0)}\\Big) +\n  P\\Big(T_n &gt; c_{n,1-\\alpha/2} \\;\\Big|\\; \\beta_k = \\beta_k^{(0)}\\Big)\\\\[2ex]\n&=\\frac{\\alpha}{2}+\\frac{\\alpha}{2}=\\alpha.\n\\end{align*}\n\\] for all sample sizes \\(n&gt;K.\\)\nShowing Exactness of the One-Sided \\(t\\)-Test:\nConsider, without loss of generality, the following right-side version of the one-sided hypothesis:\n\\[\n\\begin{equation*}\n\\begin{array}{lll}\n& H_0: \\beta_k \\leq \\beta_k^{(0)}  & (\\text{composite hypothesis})\\\\\n\\text{versus}\\quad\n& H_1: \\beta_k &gt; \\beta_k^{(0)}     & (\\text{composite hypothesis})\\\\\n\\end{array}\n\\end{equation*}\n\\]\n\nConsidering the right-side version is without loss of generality, since effectively the same arguments apply to the case of the left-side version.\n\nThe case of a one-sided hypothesis is slightly more involved since the null hypothesis is a composite hypothesis; here: \\[\nH_0: \\beta_k \\in \\;\\big]-\\infty,\\beta_k^{(0)}\\big].\n\\]\nTo conduct the \\(t\\)-test we need to take one null hypothetical value \\[\n\\tilde\\beta_k^{(0)}\\in ]-\\infty,\\beta_k^{(0)}]\n\\] which then leads to a \\(\\tilde\\beta_k^{(0)}\\) specific \\(t\\)-test \\[\n\\begin{align*}\nT_n(\\tilde\\beta_k^{(0)}) :=\n\\frac{\\hat\\beta_{n,k} - \\tilde\\beta_k^{(0)}}{\\widehat{SE}(\\hat\\beta_{n,k}|X)}\n\\end{align*}\n\\]\nUnder Assumptions 1-4\\(^\\ast,\\) we have that \\[\n\\begin{align*}\nT_n(\\tilde\\beta_{n,k}^{(0)})\n&=\n\\frac{\\hat\\beta_{n,k} \\overbrace{- \\beta_k + \\beta_k}^{=0} - \\tilde\\beta_k^{(0)}}{\\widehat{SE}(\\hat\\beta_{n,k}|X)}\\\\[2ex]\n&=\n\\underbrace{\\frac{\\hat\\beta_{n,k} - \\beta_k}{\\widehat{SE}(\\hat\\beta_{n,k}|X)}}_{\\sim t_{(n-K)}} +\n\\underbrace{\\frac{\\beta_k - \\tilde\\beta_k^{(0)}}{\\widehat{SE}(\\hat\\beta_{n,k}|X)}}_{❓},\n\\end{align*}\n\\qquad(6.8)\\] where under \\(H_0:\\beta_k \\in ]-\\infty,\\beta_k^{(0)}].\\)\nThe second term in Equation 6.8 is challenging, since generally, we do not know its value and thus also not its sign.\nSolution:\nThe solution of this problem is to set (as done in Section 6.3.1) \\[\n\\tilde\\beta_k^{(0)} = \\beta_k^{(0)}\n\\] and to use the \\(t\\)-test statistic \\[\nT_n(\\beta_k^{(0)})\\equiv T_n\n=\\frac{\\hat\\beta_{n,k} - \\beta_k^{(0)}}{\\widehat{SE}(\\hat\\beta_{n,k}|X)}.\n\\] Under \\(H_0,\\)\\[\n\\beta_k \\leq \\beta_k^{(0)} \\Leftrightarrow \\beta_k - \\beta_k^{(0)}\\highlight{\\leq 0},\n\\] which implies that \\[\n\\begin{align*}\nT_n\n&=\n\\frac{\\hat\\beta_{n,k} - \\beta_k^{(0)}}{\\widehat{SE}(\\hat\\beta_{n,k}|X)}\\\\[2ex]\n&=\n\\underbrace{\\frac{\\hat\\beta_{n,k} - \\beta_k}{\\widehat{SE}(\\hat\\beta_{n,k}|X)}}_{\\sim t_{(n-K)}} +\n\\underbrace{\\frac{\\beta_k - \\beta_k^{(0)}}{\\widehat{SE}(\\hat\\beta_{n,k}|X)}}_{\\highlight{\\leq 0}}\\\\[2ex]\n&\\highlight{\\leq}\n\\frac{\\hat\\beta_{n,k} - \\beta_k}{\\widehat{SE}(\\hat\\beta_{n,k}|X)} \\overset{H_0}{\\sim} t_{(n-K)},\n\\end{align*}\n\\qquad(6.9)\\] where the inequality holds with probability one (i.e. for any possible realization).\nThe inequality in Equation 6.9 implies that \\[\n\\begin{align*}\nP(T_n\\in\\mathcal{R}_{n,\\alpha}|\\beta_k \\leq \\beta_k^{(0)}) \\highlight{\\leq}  P(T_n\\in\\mathcal{R}_{n,\\alpha} \\;|\\;\\beta_k = \\beta_k^{(0)}) & = \\alpha\\\\[2ex]\n\\Leftrightarrow\\quad  \n\\text{Size}_{n,\\alpha} = \\sup_{\\beta_k \\in ]-\\infty,\\beta_k^{(0)}]} P(T_n\\in\\mathcal{R}_{n,\\alpha}\\;|\\;\\beta_k \\in ]-\\infty,\\beta_k^{(0)}]) & = \\alpha,\n\\end{align*}\n\\] for all sample sizes \\(n&gt;K,\\) where \\[\n\\mathcal{R}_{n,\\alpha} = ]c_{n,1-\\alpha},\\infty[,\n\\] with \\(c_{n,1-\\alpha}\\) denoting the \\((1-\\alpha)\\)-quantile of the \\(t\\)-distribution with \\((n-K)\\) degrees of freedom.\n\n\n\n\n\n\n\n\n6.5.4 Power\n\n\n\n\n\n\n\nDefinition 6.2 (Power) The power of a test statistic \\(T_n\\) is the probability of correctly rejecting \\(H_0\\) for a given \\(\\theta\\in \\Theta_1\\) \\[\n\\begin{align*}\n\\text{Power}_{n,\\theta,\\alpha}\n& = P(T_n \\in \\mathcal{R}_{n,\\alpha} | \\theta\\in \\Theta_1)\\\\[2ex]\n& = 1-P(\\text{type-II-error}| \\theta\\in \\Theta_1)\n\\end{align*}\n\\]\n\n\n\n\nSince we want to detect a given violations of \\(H_0,\\) we want test statistics with a large power.\n\n\n\n\n\n\nPower is a function of \\(\\alpha,\\) \\(\\theta,\\) and \\(n\\)\n\n\n\n\nLarge violations of \\(H_0\\): \\[\n\\begin{align*}\n&\\text{Power}_{n,\\theta,\\alpha}\n\\to 1 \\quad \\text{as}\\quad |\\theta - \\theta^{(0)}|\\to\\infty,\n\\end{align*}\n\\] while keeping \\(0&lt;\\alpha&lt;1\\) and \\(n\\) fix.\nLarge sample sizes \\(n\\): \\[\n\\begin{align*}\n&\\text{Power}_{n,\\theta,\\alpha}\n\\to 1 \\quad \\text{as}\\quad n\\to\\infty,\n\\end{align*}\n\\] while keeping \\(0&lt;\\alpha&lt;1\\) and \\(|\\theta - \\theta^{(0)}|&gt;0\\) fix.\nSmall significance levels \\(\\alpha\\): \\[\n\\begin{align*}\n&\\text{Power}_{n,\\theta,\\alpha}\n\\to 0 \\quad \\text{as}\\quad \\alpha\\to 0,\n\\end{align*}\n\\] while keeping \\(n\\) and \\(|\\theta - \\theta^{(0)}|&gt;0\\) fix.\n\n\n\n\n\n\n\n\n\n\nDefinition 6.3 (Consistency of a Statistical Test) A statistical test is called consistent if its power increases as the sample size gets large \\[\n\\begin{align*}\n&\\text{Power}_{n,\\theta,\\alpha}\n\\to 1 \\quad \\text{as}\\quad n\\to\\infty\\\\[2ex]\n\\end{align*}\n\\] for any fixed \\(0&lt;\\alpha&lt;1\\) and any fixed \\(|\\theta - \\theta^{(0)}|&gt;0.\\)\n\n\n\n\nCaution: Consistency of a statistical test is an important proporty, however, this property implies that even small, practically irrelevant violations of \\(H_0\\) will be detected if the sample size is sufficiently large.\nPower of the One-Sided \\(t\\)-Test\n\n\n\nUnfortunately, computing the power of a statistical test is usually impossible, since this requires knowing the distribution of the test statistic under the alternative hypothesis \\(H_1.\\) The distribution of a test statistic under \\(H_1\\) can only be derived under quite restrictive setups.\nIn the following, we consider such a restrictive setup for the \\(t\\)-test statistic:\n\nLet’s consider the right-sided version of the one-sided hypothesis \\[\n\\begin{align*}\n&H_0: \\beta_k\\leq \\beta_{k}^{(0)}\\\\\n&H_1: \\beta_k &gt; \\beta_{k}^{(0)}\n\\end{align*}\n\\]\nLet \\(X\\) be deterministic.\nLet the true standard error of \\(\\hat\\beta_{n,k}\\) be\\[\n\\operatorname{SE}(\\hat\\beta_{n,k}|X)=\\frac{1}{\\sqrt{n}}4.5.\n\\]\n\n\n\n\n\n\n\nStandard error of \\(\\hat\\beta_{nk}\\) is proportional to \\(1/\\sqrt{n}\\)\n\n\n\nOf course, usually we do not know the standard error of the estimator, but have to estimate it. However, it is true that the standard error of the OLS estimator \\(\\hat{\\beta}_{n,k}\\) is proportional to \\(\\boldsymbol{1/\\sqrt{n}},\\) \\[\n\\operatorname{SE}(\\hat\\beta_{n,k}|X) = C \\cdot \\frac{1}{\\sqrt{n}},\n\\] where \\(C&gt;0\\) denotes a constant that does not depend on \\(n.\\)\n\n\nUnder this simplified setup and under Assumptions 1-4\\(^\\ast,\\) the \\(t\\)-test statistic is normally distributed.\nIf \\(H_0\\) is true with \\(\\beta_k=\\beta_k^{(0)},\\) then \\[\n\\begin{align*}\nT_n\n&=\\frac{\\hat\\beta_{n,k}-\\beta_k^{(0)}}{\\frac{1}{\\sqrt{n}}4.5}\\\\[2ex]\n&\\overset{H_0}{=}\\frac{\\sqrt{n}(\\hat\\beta_{n,k}-\\beta_k)}{4.5} \\overset{H_0}{\\sim} \\mathcal{N}(0,1).\n\\end{align*}\n\\]\n\nIf \\(H_1: \\beta_k-\\beta_k^{(0)}&gt;0\\) is true, then \\[\n\\begin{align*}\nT_n&=\\frac{\\hat\\beta_{n,k}-\\beta_k^{(0)}}{\\frac{1}{\\sqrt{n}}4.5}\n=\\frac{\\hat\\beta_{n,k}\\overbrace{-\\beta_k+\\beta_k}^{=0}-\\beta_k^{(0)}}{\\frac{1}{\\sqrt{n}}4.5}\\\\[2ex]\n&=\\underbrace{\\frac{\\sqrt{n}(\\hat\\beta_{n,k}-\\beta_k)}{4.5}}_{\\sim \\mathcal{N}(0,1)}+\\underbrace{\\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}}_{=\\text{ mean shift }&gt;0}\\\\[2ex]\n&\\overset{H_1}{\\sim} \\mathcal{N}\\Bigg(\\;\\underbrace{\\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}}_{=\\text{ mean shift }&gt;0}\\;,\\;1\\Bigg)\n\\end{align*}\n\\]\nThus \\[\n\\begin{align*}\n\\text{Power}_{n,\\theta,\\alpha}\n& = P\\Big(\\;\\underbrace{T_n &gt; z_{1-\\alpha}}_{T_n\\in\\mathcal{R}_{n,\\alpha}}\\;|\\; \\underbrace{\\beta_k &gt; \\beta_k^{(0)}}_{H_1\\text{ is true}}\\;\\Big),\n\\end{align*}\n\\] where \\(z_{1-\\alpha}\\) denotes the \\((1-\\alpha)\\) quantile of the standard normal distribution \\(\\mathcal{N}(0,1),\\) and where \\[\nT_n\\sim \\mathcal{N}\\left(\\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5},1\\right).\n\\]\nThis allows us to compute the power as following: \\[\n\\begin{align*}\n& \\text{Power}_{n,\\theta,\\alpha}=\\\\[2ex]  \n& = P(T_n &gt; z_{1-\\alpha}\\;|\\; \\beta_k &gt; \\beta_k^{(0)}),\n\\\\[2ex]\n& = P\\Bigg(\\;\\overbrace{T_n - \\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}}^{=Z\\sim\\mathcal{N}(0,1)} &gt; z_{1-\\alpha} - \\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}\\;\\Bigg|\\;\\beta_k &gt; \\beta_k^{(0)}\\Bigg)\\\\[2ex]\n& = P\\left(Z &gt; z_{1-\\alpha} - \\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}\\Bigg|\\;\\beta_k &gt; \\beta_k^{(0)}\\right)\\\\[2ex]\n&=1-P\\left(Z \\leq z_{1-\\alpha} - \\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}\\Bigg|\\;\\beta_k &gt; \\beta_k^{(0)}\\right)\\\\[2ex]\n&=1-\\Phi\\left(z_{1-\\alpha} - \\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}\\right)\\quad\\text{with}\\quad \\beta_k &gt; \\beta_k^{(0)},\n\\end{align*}\n\\] where \\(\\Phi\\) denotes the cumulative distribution function of the standard normal distribution \\(\\mathcal{N}(0,1).\\)\nFigure 6.7 illustrates the probability of a type-II-error and the power for the case where\n\n\\(\\alpha = 0.05\\)\n\\(n=9\\)\n\\(\\beta_k - \\beta_k^{(0)}=3\\)\n\nsuch that \\[\n\\begin{align*}\n\\text{Power}\n&=1-\\Phi\\Bigg(z_{1-\\alpha} - \\overbrace{\\frac{\\sqrt{n}(\\beta_k-\\beta_k^{(0)})}{4.5}}^{=\\frac{3\\cdot 3}{4.5} = 2}\\Bigg)\\\\[2ex]\n&=1-\\Phi\\left(1.64  - 2 \\right)\\\\[2ex]\n&=1-0.359=0.641\n\\end{align*}\n\\] That is, we expect to detect the violation of the null hypothesis \\((\\beta_k - \\beta_k^{(0)}=3)\\) in \\(64\\%\\) of resamplings from the random sample (data generating process).\n\n\n\n\n\n\n\nFigure 6.7: Probability of a type-II-error and the power for a one-sided \\(t\\)-test at significance level \\(\\alpha = 0.05,\\) sample size \\(n=9,\\) and violation of the null hypothesis \\(\\beta_k - \\beta_k^{(0)}=3.\\)\n\n\n\n\n\n6.5.5 \\(p\\)-Value\nThe \\(p\\)-value of a test-statistic \\(T_n\\) is defined as the probability of observing realizations of \\(T_n\\) that are equal to or more extreme (in direction of the alternative) than the observed value \\(T_{n,\\text{obs}}\\) given that the null hypothesis is true.\n\\(p\\)-value for a left-sided version of a one-sided hypothesis test: \\[\np_{\\text{left},\\,\\text{obs}} = P(T_n\\leq T_{n,\\text{obs}}\\;|\\;H_0 \\text{ is true})\n\\]\n\\(p\\)-value for a right-sided version of a one-sided hypothesis test: \\[\np_{\\text{right},\\,\\text{obs}} = P(T_n\\geq T_{n,\\text{obs}}\\;|\\;H_0 \\text{ is true})\n\\]\n\\(p\\)-value for a two-sided hypothesis test: \\[\n\\begin{align*}\np_{\\text{obs}}\n&=P(|T_n|\\geq |T_{n,\\text{obs}}|\\;|\\;H_0 \\text{ is true})\\\\[2ex]\n&=2\\cdot\\min\\{p_{\\text{left},\\,\\text{obs}},\n              p_{\\text{right},\\,\\text{obs}}\\}\n\\end{align*}\n\\]\n\n\n\n\n\n\nMarginal Significance Value\n\n\n\nThe \\(p\\)-value equals the significance level \\(\\alpha\\) for which we just fail to reject the null. Therefore, the \\(p\\)-value is sometimes also called “marginal significance value”.\n\n\nSince the \\(p\\)-value is defined under the hypothetical scenario that the null-hypothesis is true, …\n\n❗… the \\(p\\)-value cannot be the probability that the null-hypothesis is true. (A common misinterpretation of the \\(p\\)-value.)❗\n\n\n\nThe \\(p\\)-value is a Random Variable\nLet’s consider the two-sided \\(t\\)-test for testing the hypotheses \\[\nH_0: \\beta_k   =  \\beta_k^{(0)}\\\\\nH_1: \\beta_k \\neq \\beta_k^{(0)}.\n\\] Given an observed value of the test statistik \\(T_{n,\\operatorname{obs}},\\) the observed \\(p\\)-value is \\[\n\\begin{align*}\np_{\\text{obs}}\n&= P(|T_n|\\geq |T_{n,\\text{obs}}|\\;|\\;H_0 \\text{ is true})\\\\[2ex]\n&= 1-P(|T_n|\\leq |T_{n,\\text{obs}}|\\;|\\;H_0 \\text{ is true})\\\\[2ex]\n&= 1-F_{H_0,|T_n|}(|T_{n,\\text{obs}}|),\n\\end{align*}\n\\] where \\(F_{H_0,|T_n|}\\) denotes the cumulative distribution function of \\(|T_n|\\) under \\(H_0.\\)\nGenerally, the \\(p\\)-value is a random variable, since it depends on the data. For the random \\(p\\)-value, we have that \\[\n\\begin{align*}\np\n&= P(|T_n|\\geq |T_{n}|\\;|\\;H_0 \\text{ is true})\\\\[2ex]\n&= 1-P(|T_n|\\leq |T_{n}|\\;|\\;H_0 \\text{ is true})\\\\[2ex]\n&= 1-F_{H_0,|T_n|}(|T_{n}|).\n\\end{align*}\n\\]\nUnder \\(H_0,\\) the \\(p\\)-value of the two-sided \\(t\\)-test is a uniformly distributed random variable \\[\np \\overset{H_0}{\\sim}\\mathcal{U}[0,1].\n\\qquad(6.10)\\]\nDerivation of the above result:\n\nLet \\[\nF_{H_0,p}(x) = P(p \\leq x\\;|\\;H_0 \\text{ is true})\n\\] denote the cdf of the random \\(p\\)-value under \\(H_0.\\) If the claim in Equation 6.10 is correct, then the random \\(p\\)-value has, under \\(H_0,\\) the cumulative distribution function of \\(\\mathcal{U}[0,1],\\) that is \\[\nF_{H_0,p}(x) = x\\quad\\text{for}\\quad x\\in[0,1].\n\\]\nThis is indeed true since, under \\(H_0,\\) \\[\n\\begin{align*}\nF_{H_0,p}(x)\n&=   P(p                       \\leq                      x \\;|\\;H_0 \\text{ is true}) \\\\[2ex]\n&[\\text{using that }\\;p= 1-F_{H_0,|T_n|}(|T_{n}|)]\\\\[2ex]\n&=   P(1-F_{H_0,|T_n|}(|T_{n}|)\\leq                      x \\;|\\;H_0 \\text{ is true}) \\\\[2ex]\n&=   P(  F_{H_0,|T_n|}(|T_{n}|)\\geq                    1-x \\;|\\;H_0 \\text{ is true}) \\\\[2ex]\n&= 1-P(  F_{H_0,|T_n|}(|T_{n}|)\\leq                    1-x \\;|\\;H_0 \\text{ is true}) \\\\[2ex]\n&[\\text{using that }\\;F_{H_0,|T_n|}\\;\\text{ is invertible}]\\\\[2ex]\n&= 1-P(                |T_{n}| \\leq F^{-1}_{H_0,|T_n|}(1-x)\\;|\\;H_0 \\text{ is true}) \\\\[2ex]\n&= 1-F_{H_0,|T_n|}( F^{-1}_{H_0,|T_n|}(1-x) ) \\\\[2ex]\n&= 1-(1-x)\\\\[2ex]\n&= x,\n\\end{align*}\n\\] where \\(F_{H_0,|T_n|}\\) is invertible, since \\(|T_n|\\) is a continuously distribued random variable.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Small Sample Inference</span>"
    ]
  },
  {
    "objectID": "06-Small-Sample-Inference.html#sec-CIsmallsample",
    "href": "06-Small-Sample-Inference.html#sec-CIsmallsample",
    "title": "6  Small Sample Inference",
    "section": "\n6.4 Confidence Intervals",
    "text": "6.4 Confidence Intervals\nWe define a two-sided \\((1-\\alpha)\\cdot 100\\%\\) percent confidence interval for the deterministic (unknown) true \\(\\beta_k\\) as the random interval \\(\\operatorname{CI}_{k,n,1-\\alpha}\\) for which \\[\nP\\Big(\\beta_k\\in\\operatorname{CI}_{k,n,1-\\alpha}\\Big)\\geq 1-\\alpha.\n\\]\nDerivation of the random interval \\(\\operatorname{CI}_{k,n,1-\\alpha}\\)\nObserve that (under Ass 1-4\\(^\\ast\\)) \\[\n\\frac{\\hat\\beta_{n,k}-\\beta_k}{\\widehat{\\operatorname{SE}}(\\hat\\beta_{n,k}|X)}\\sim t_{(n-K)}\n\\qquad(6.5)\\] Therefore, \\[\n\\begin{align*}\nP\\left(-c_{n,1-\\alpha/2}\\leq\\frac{\\hat\\beta_{n,k}-\\beta_k}{\\widehat{\\operatorname{SE}}(\\hat\\beta_{n,k}|X)}\\leq c_{n,1-\\alpha/2}\\right)=1-\\alpha,\n\\end{align*}\n\\] where \\(c_{n,1-\\alpha/2}\\) denotes the \\((1-\\alpha/2)\\) quantile of the \\(t\\)-distribution with \\((n-K)\\) degrees of freedom.\nNext, we can do the following equivalent transformations \\[\n\\begin{align*}\nP\\left(-c_{n,1-\\alpha/2}\\leq\\frac{\\hat\\beta_{n,k}-\\beta_k}{\\widehat{\\operatorname{SE}}(\\hat\\beta_{n,k}|X)}\\leq c_{n,1-\\alpha/2}\\right)&=1-\\alpha\\\\[2ex]\n\\Leftrightarrow P\\left(\\hat\\beta_{n,k}-c_{n,1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_{n,k}|X)\\leq \\beta_k\\leq\\hat\\beta_{n,k} +c_{n,1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_{n,k}|X)\\right)&=1-\\alpha\\\\[2ex]\n\\Leftrightarrow P\\left(\\beta_k\\in\\underbrace{\\left[\\hat\\beta_{n,k}-c_{n,1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_{n,k}|X),\\;\\hat\\beta_{n,k} +c_{n,1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_{n,k}|X)\\right]}_{=:\\operatorname{CI}_{k,n,1-\\alpha}}\\right)&=1-\\alpha\n\\end{align*}\n\\] That is, the random interval \\[\n\\begin{align*}\n\\operatorname{CI}_{k,n,1-\\alpha}\n&=\\left[\\hat\\beta_{n,k}-c_{n,1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_{n,k}|X),\\;\\hat\\beta_{n,k} + c_{n,1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_{n,k}|X)\\right]\\\\[2ex]\n&=\\left[\\hat\\beta_{n,k}\\pm c_{n,1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_{n,k}|X)\\right]\n\\end{align*}\n\\] is our \\((1-\\alpha)\\cdot 100\\%\\) confidence interval for \\(\\beta_k\\).\nSince the confidence interval is based on the exact distribution (under Assumptions 1-4\\(^\\ast\\)) in Equation 6.5, the confidence interval has an exact coverage probability \\[\n\\begin{align*}\nP\\left(\\beta_k\\in\\operatorname{CI}_{k,n,1-\\alpha}\\right)&=1-\\alpha\n\\end{align*}\n\\] provided the Assumptions 1-4\\(^\\ast\\) are true.\n\n\n\n\n\n\nInterpretation of Confidence Intervals\n\n\n\nThe random interval \\(\\operatorname{CI}_{k,n,1-\\alpha}\\) for \\(\\beta_k\\) contains the true parameter value \\(\\beta_k\\) with probability \\(1-\\alpha;\\) i.e. we expect that \\(\\operatorname{CI}_{k,n,1-\\alpha}\\) covers \\(\\beta_k\\) in \\((1-\\alpha)\\cdot 100\\%\\) of resamplings from the random sample.\nIt’s best to take a look at dynamic visualizations like this one:\n\nhttps://rpsychologist.com/d3/ci/\n\nUnfortunately, this “frequentist” interpretation is not a statement about a single given \\(\\operatorname{CI}_{k,n,1-\\alpha,\\text{obs}}\\) realization computed for an observed realization of the random sample. A given, realized \\(\\operatorname{CI}_{k,n,1-\\alpha,\\text{obs}}\\) will either contain the true parameter \\(\\beta_k\\) or not, and usually we do not know the answer. So, confidence intervals are quite hard to interpret. However, they are very well suited as a tool to visualize estimation uncertainties in \\(\\hat\\beta_{n,k}\\), \\(k=1,\\dots,K\\).\n\n\n\n\n\nPoint Estimator versus Interval Estimator\nOften, \\(\\hat\\beta_{n,k}\\) is called a point estimator of \\(\\beta_k\\) and the confidence interval \\[\n\\begin{align*}\n\\operatorname{CI}_{k,n,1-\\alpha}\n&=\\left[\\hat\\beta_{n,k}\\pm c_{n,1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_{n,k}|X)\\right]\n\\end{align*}\n\\] is called an interval estimator of \\(\\beta_k,\\) where the width of \\(\\operatorname{CI}_{k,n,1-\\alpha}\\) quantifies the estimation uncertainties.\n\n6.4.1 Confidence Intervals for Statistical Hypothesis Testing\nWe can use the \\((1-\\alpha)\\cdot 100\\%\\) confidence interval \\(\\operatorname{CI}_{k,n,1-\\alpha}\\) to do statistical hypothesis testing at the significance level \\(0&lt;\\alpha&lt;1.\\) Typical significance levels:\nLet us consider the following two-sided statistical hypotheses \\[\n\\begin{align*}\nH_0:&\\;\\beta_k=\\beta^{(0)}_{k}\\\\\nH_1:&\\;\\beta_k\\neq \\beta^{(0)}_{k}\n\\end{align*}\n\\]\nTesting-Procedure:\n\nIf the observed (obs) realization of the confidence interval, \\(\\operatorname{CI}_{k,n,1-\\alpha,\\text{obs}},\\) contains the null-hypothetical value \\(\\beta^{(0)}_{k},\\) i.e. \\[\n\\begin{align*}\n\\beta^{(0)}_{k}&\\in\\operatorname{CI}_{k,n,1-\\alpha,\\text{obs}}\\\\[2ex]\n\\Leftrightarrow\\beta^{(0)}_{k}&\\in\n\\left[\\hat\\beta_{n,k,\\text{obs}}\\pm c_{n,1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_{n,k}|X)_{\\text{obs}}\\right],\n\\end{align*}\n\\] then we cannot reject the null hypothesis.\nIf, however, the observed (obs) realization of the confidence interval, \\(\\operatorname{CI}_{k,n,1-\\alpha,\\text{obs}},\\) does not contain the null-hypothetical value \\(\\beta^{(0)}_{k},\\) i.e. \\[\n\\begin{align*}\n\\beta^{(0)}_{k}&\\not\\in\\operatorname{CI}_{k,n,1-\\alpha,\\text{obs}}\\\\[2ex]\n\\Leftrightarrow\\beta^{(0)}_{k}&\\not\\in\n\\left[\\hat\\beta_{n,k,\\text{obs}}\\pm c_{n,1-\\alpha/2}\\widehat{\\operatorname{SE}}(\\hat\\beta_{n,k}|X)_{\\text{obs}}\\right],\n\\end{align*}\n\\] then we reject the null hypothesis.\n\nThe test decisions are then perfectly equivalent to those based on the two-sided \\(t\\)-test.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Small Sample Inference</span>"
    ]
  },
  {
    "objectID": "06-Small-Sample-Inference.html#sec-PSSI",
    "href": "06-Small-Sample-Inference.html#sec-PSSI",
    "title": "6  Small Sample Inference",
    "section": "\n6.6 Monte Carlo Simulations",
    "text": "6.6 Monte Carlo Simulations\nLet’s check the above exact inference results using Monte Carlo simulations. The check, of course, applies only to the considered special case and generally does not generalize to other data generating processes.\nFirst, we program a function myDataGenerator() which allows us to generate data from the following model, i.e., from the following fully specified data generating process: \\[\n\\begin{align*}\nY_i &=\\beta_1+\\beta_2X_{i2}+\\beta_3X_{i3}+\\varepsilon_i,\\qquad i=1,\\dots,n\\\\\n\\beta &=(\\beta_1,\\beta_2,\\beta_3)'=(2,3,4)'\\\\\nX_{i2}&\\sim U[2,10]\\\\\nX_{i3}&\\sim U[12,22]\\\\\n\\varepsilon_i|X&\\sim\\mathcal{N}(0,3^2),\n\\end{align*}\n\\] where \\((Y_i,X_i)\\) is i.i.d. across \\(i=1,\\dots,n\\).\nLet us consider a small sample size of \\(n=7\\).\nThe below function myDataGenerator() allows to sample new realizations of the random sample \\[\n((X_1,Y_1),\\dots,(X_n,Y_n))\n\\] You can provide your own values for the sample size \\(n\\) and for the parameter vector \\(\\beta=(\\beta_1,\\beta_2,\\beta_3)'\\).\n\n## Function to generate artificial data\n## If the user provides 'X_cond' data, \n## the sampling of new Y variables is \n## conditionally on the given X_cond variables.\n## If X_cond = NULL, sampling is done unconditionally. \n\nmyDataGenerator &lt;- function(n, beta){\n\n## sampling predictors X:\nX   &lt;- cbind(rep(1, n), \n            runif(n, 2, 10), \n            runif(n,12, 22))\n## sampling error terms: \neps  &lt;- rnorm(n, sd = 3)\n## generate realizations of Y:\nY    &lt;- X %*% beta + eps\n## safe X and Y as a data frame:\ndata &lt;- data.frame(\"Y\"   = Y, \n                   \"X_1\" = X[,1], \n                   \"X_2\" = X[,2], \n                   \"X_3\" = X[,3])\n## return the data frame\nreturn(data)\n}\n\n## Small sample size\nn             &lt;- 7        \n\n## Define the true beta vector\nbeta_true     &lt;- c(2,3,4)\n\n## Generate Y and X data \ntest_data     &lt;- myDataGenerator(n = n, beta=beta_true)\n\n## Look at the first six lines of the data frame\nround(head(test_data,     3), 2) \n\n      Y X_1  X_2   X_3\n1 97.53   1 4.23 20.50\n2 75.37   1 8.06 12.35\n3 70.92   1 4.70 14.95\n\n\n\n6.6.1 Check: Testing Multiple Parameters\nIn the following, we do inference about multiple parameters. We test \\[\\begin{align*}\nH_0:\\;&\\beta_2=3\\quad\\text{and}\\quad\\beta_3=4\\\\\n\\text{versus}\\quad H_1:\\;&\\beta_2\\neq 3\\quad\\text{and/or}\\quad\\beta_3\\neq 4.\n\\end{align*}\\] Or equivalently \\[\\begin{align*}\nH_0:\\;&R\\beta  = r^{(0)} \\\\\nH_1:\\;&R\\beta  \\neq r^{(0)},\n\\end{align*}\\] where \\[\nR=\\left(\n\\begin{matrix}\n0&1&0\\\\\n0&0&1\\\\\n\\end{matrix}\\right)\\quad\\text{ and }\\quad\nr^{(0)}=\\left(\\begin{matrix}3\\\\4\\\\\\end{matrix}\\right).\n\\] The following R code can be used to test this hypothesis:\n\n## Library containing the function 'linearHyothesis()' \n## for testing multiple parameters \nsuppressMessages(library(\"car\")) \n## See ?linearHypothesis\n\n## Generate one Monte Carlo sample (under H0)\ndata   &lt;- myDataGenerator(n = n, beta = beta_true)\n\n## Estimate the linear regression model parameters\nlm_obj &lt;- lm(Y ~ X_2 + X_3, data = data)\n\n## Option 1:\ntest_result &lt;- car::linearHypothesis(\n      model = lm_obj, \n      hypothesis.matrix = c(\"X_2=3\", \"X_3=4\"))   \ntest_result        \n\nLinear hypothesis test\n\nHypothesis:\nX_2 = 3\nX_3 = 4\n\nModel 1: restricted model\nModel 2: Y ~ X_2 + X_3\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1      6 46.527                           \n2      4 32.573  2    13.954 0.8568 0.4901\n\n\nThe \\(p\\)-value\n\n\\(p_{\\text{obs}}=\\) 0.4901 \\(\\;&gt;\\alpha=0.05\\)\n\nis larger than the chosen significance level \\(\\alpha=0.05.\\) Thus we cannot reject the null hypothesis \\[\nH_0:\\;\\beta_2=3\\quad\\text{and}\\quad \\beta_3=4.\n\\]\nThe following codes gives an alternative, equivalent way to compute the test result:\n\n## Option 2:\nR &lt;- rbind(c(0,1,0),\n           c(0,0,1))\ncar::linearHypothesis(model = lm_obj, \n                      hypothesis.matrix = R, \n                      rhs = c(3,4))\n\nWe simulated data under \\(H_0\\) and thus it is not surprising that we cannot reject \\(H_0.\\)\nHowever, in repeated samples we should nevertheless observe \\(\\alpha\\cdot 100\\%\\) type I errors (false rejections of \\(H_0\\)) under \\(H_0.\\) Let’s check the type-I-error rate using the following Monte Carlo simulation:\n\n## Let's generate 5000 F-test decisions and check \n## whether the empirical rate of type I errors is \n## close to the theoretical significance level. \nB               &lt;- 5000 # MC replications\nF_test_pvalues  &lt;- rep(NA, times=B)\n##\nfor(r in 1:B){\n  ## generate new data (under H0)\n  MC_data &lt;- myDataGenerator(n = n, beta = beta_true)\n  ## estimate \n  lm_obj  &lt;- lm(Y ~ X_2 + X_3, data = MC_data)\n  ## compute test and p-value\n  p       &lt;- linearHypothesis(lm_obj, c(\"X_2=3\", \"X_3=4\"))$`Pr(&gt;F)`[2]\n  ## save the p-value\n  F_test_pvalues[r] &lt;- p\n}\n\nUsing the collection of \\(p\\)-value realizations (under \\(H_0\\)) we can check whether the size equals the nominal size (significance level) \\(\\alpha.\\)\nFor \\(\\alpha = 0.05:\\)\n\nalpha        &lt;-  0.05          # signif level\nrejections   &lt;- F_test_pvalues[F_test_pvalues &lt; alpha]\nround(length(rejections)/B, 4) # actual type-I-error rate \n\n[1] 0.0524\n\n\nFor \\(\\alpha = 0.01:\\)\n\nalpha        &lt;-  0.01  # signif level\nrejections   &lt;- F_test_pvalues[F_test_pvalues &lt; alpha]\nround(length(rejections)/B, 4) # actual type-I-error rate \n\n[1] 0.0092\n\n\nObservations:\n\nWe correctly control for the type-I-error rate since the empirical type-I-error rate is not larger than the chosen significance level \\(\\alpha.\\)\n\nThe \\(F\\) test is not conservative since the empirical type-I-error rates essentially matches the chosen significance levels \\(\\alpha.\\)  In fact, if we would increase the number of Monte Carlo repetitions, the empirical type-I-error rate would converge to the nominal type-I-error rate \\(\\alpha\\) due to the law of large numbers.\nLast but not least: All this works unconditionally on \\(X\\) since the distribution of the \\(F\\) statistic (Equation 6.4) does not depend on \\(X\\).\n\nNext, we check how well the \\(F\\) test detects certain violations of the null hypothesis. We do this by using the same data generating process, but by testing the following incorrect null hypothesis: \\[\\begin{align*}\nH_0:\\;&{\\color{red}\\beta_2=4}\\quad\\text{and}\\quad\\beta_3=4\\\\\nH_1:\\;&\\beta_2\\neq 4\\quad\\text{and/or}\\quad\\beta_3\\neq 4\n\\end{align*}\\]\n\nB               &lt;- 5000 # MC replications\nF_test_pvalues  &lt;- rep(NA, times=B)\n##\nfor(r in 1:B){\n  ## generate new data \n  MC_data &lt;- myDataGenerator(n    = n, beta = beta_true)\n  ## estimate \n  lm_obj  &lt;- lm(Y ~ X_2 + X_3, data = MC_data)\n  ## compute test and p-value (for a false H0)\n  p       &lt;- linearHypothesis(lm_obj, c(\"X_2=4\", \"X_3=4\"))$`Pr(&gt;F)`[2]\n  ## save the p-value\n  F_test_pvalues[r] &lt;- p\n}\n\n## Checking the power of the F test \n\nalpha       &lt;-  0.05  # signif_level\nrejections  &lt;- F_test_pvalues[F_test_pvalues &lt; alpha]\nlength(rejections)/B  # power \n\n[1] 0.204\n\n\nWe can now correctly reject the false null hypothesis in approximately 20.4 % of all Monte Carlo replications.\nCaution: This means that we are not able to detect the violation of the null hypothesis in 79.6 % of cases. Therefore, we can never use an insignificant test result (\\(p\\)-value \\(\\geq\\alpha\\)) as a confirmation of the null hypothesis. Obviously, there are type-II-error events (not rejecting a false \\(H_0\\)), but since we typically do not know the distribution of the test statistic under the alternative hypothesis, we cannot control the type-II-error rate. We can only control the type-I-error rate by using a small significance level \\(\\alpha\\).\nMoreover, note that the \\(F\\) test is not informative about which part of the null hypothesis (\\(\\beta_2=4\\) and/or \\(\\beta_3=4\\)) is violated. We only get the information that at least one of the multiple parameter hypotheses is violated. Test statistics with this property are called omnibus tests.\n\n6.6.2 Check: Dualty of Confidence Intervals and Hypothesis Tests\nConfidence intervals can be computed using R as following:\n\n## Significance level\nalpha        &lt;- 0.05\n## Confidence level\nconf_level   &lt;- 1 - alpha\n\n## 95% CI for beta_2\nconfint(lm_obj, parm = \"X_2\", level = conf_level)\n\n       2.5 %  97.5 %\nX_2 2.984806 4.18956\n\n## 95% CI for beta_3 \nconfint(lm_obj, parm = \"X_3\", level = conf_level)\n\n       2.5 %   97.5 %\nX_3 3.877389 4.489954\n\n\nWe can use these two-sided confidence intervals to conduct hypotheses tests. This property of confidence intervals is called the duality of confidence intervals and hypothesis tests.\nFor instance, when testing the null hypothesis \\[\\begin{align*}\nH_0:&\\;\\beta_2=3\\\\\n\\text{versus}\\quad H_1: &\\;\\beta_2\\neq 3\n\\end{align*}\\] we can either use a \\(t\\)-test or equivalently check whether the confidence interval \\(\\operatorname{CI}_{2,1-\\alpha}\\) for \\(\\beta_2\\) contains the hypothetical value \\(4\\) or not.\n\nIn case of \\(3    \\in\\operatorname{CI}_{2,1-\\alpha}\\), we cannot reject the null hypothesis \\(H_0\\): \\(\\beta_2=3.\\)\n\nIn case of \\(3\\not\\in\\operatorname{CI}_{2,1-\\alpha}\\), we can reject the null hypothesis \\(H_0\\): \\(\\beta_2=3.\\)\n\n\nIf the Assumptions 1-4\\(^\\ast\\) hold true, then \\(\\operatorname{CI}_{2,1-\\alpha}\\) is an exact confidence interval. That is, under the null hypothesis, it falsely rejects the null hypothesis in only \\(\\alpha\\cdot 100\\%\\) of resamplings. Let’s check this in the following Monte Carlo simulation:\n\n## Significance level\nalpha        &lt;- 0.05\n\n## beta_2 safed separately\nbeta_true_2  &lt;- beta_true[2]\n\n## Container to save all CI realizations\nconfint_m  &lt;- matrix(NA, nrow=2, ncol=B)\n##\nfor(r in 1:B){\n  ## generate new data \n  MC_data &lt;- myDataGenerator(n = n, beta = beta_true)\n  ## estimate\n  lm_obj  &lt;- lm(Y ~ X_2 + X_3, data = MC_data)\n  ## compute confidence interval \n  CI &lt;- confint(lm_obj, parm=\"X_2\", level = 1 - alpha)\n  ## save confidence interval\n  confint_m[,r] &lt;- CI\n}\n## check whether true parameter is inside the CI\ninside_CI  &lt;- confint_m[1,] &lt;= beta_true_2 & \n                beta_true_2 &lt;= confint_m[2,]\n\n## CI-lower, CI-upper, beta_true_2 inside?\nhead(cbind(t(confint_m), inside_CI))\n\n                       inside_CI\n[1,] 2.507509 4.505450         1\n[2,] 1.479413 4.468068         1\n[3,] 2.540762 5.533256         1\n[4,] 1.129250 6.473386         1\n[5,] 1.167703 4.362476         1\n[6,] 1.430087 4.107289         1\n\n\nThe following code computes the relative frequency of confidence intervals not containing the true parameter value \\((\\beta_2=3)\\):\n\nround(length(inside_CI[inside_CI == FALSE])/B, 4)\n\n[1] 0.0442\n\n\nThat’s good! The relative frequency is basically equal to the chosen \\(\\alpha=0.05\\) value.\nNext, we visualize a subsample of 100 confidence intervals from the total sample of 5000 generated confidence interval realizations:\n\nnCIs &lt;- 100\nplot(x=0, y=0,type=\"n\", xlim=c(0,nCIs), ylim=range(confint_m[,1:nCIs]),\n     ylab=\"\", xlab=\"Resamplings\", main=\"Confidence Intervals\")\nfor(r in 1:nCIs){\n  if(inside_CI[r]==TRUE){\n      lines(x=c(r,r), y=c(confint_m[1,r], confint_m[2,r]), lwd=2, col=gray(.5,.5))\n  }else{\n      lines(x=c(r,r), y=c(confint_m[1,r], confint_m[2,r]), lwd=2, col=\"darkred\")\n    }\n}\naxis(4, at=beta_true_2, labels = expression(beta[2]))\nabline(h=beta_true_2)\n\n\n\n\n\n\n\nAs expected, only about \\(\\alpha\\cdot 100\\%=5\\%\\) of all confidence intervals do not contain the true parameter value \\(\\beta_2=3\\), but about \\((1-\\alpha)\\cdot 100\\%=95\\%\\) of all confidence intervals contain the true parameter value \\(\\beta_2=3\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Small Sample Inference</span>"
    ]
  },
  {
    "objectID": "06-Small-Sample-Inference.html#sec-RDSSInf",
    "href": "06-Small-Sample-Inference.html#sec-RDSSInf",
    "title": "6  Small Sample Inference",
    "section": "\n6.7 Real Data Example",
    "text": "6.7 Real Data Example\n\n## The AER package contains a lot of datasets \nsuppressPackageStartupMessages(library(AER))\n\n## Attach the DoctorVisits data to make it usable\ndata(\"DoctorVisits\")\n\nlm_obj &lt;- lm(visits ~ gender + age + income, data = DoctorVisits)\n\nThe above R codes estimate the following regression model \\[\nY_i = \\beta_1 + \\beta_{gender} X_{gender,i}\n              + \\beta_{age} X_{age,i}\n              + \\beta_{income} X_{income,i} + \\varepsilon_i,\n\\] where \\(i=1,\\dots,n\\) and\n\n\n\\(X_{gender,i}=1\\) if the \\(i\\)th subject is a woman and \\(X_{gender,i}=0\\) if the \\(i\\)th subject is a man\n\n\\(X_{age,i}\\) is the age of subject \\(i\\) measured in years divided by \\(100\\)\n\n\n\\(X_{income,i}\\) is the annual income of subject \\(i\\) in tens of thousands of dollars\n\nThe following R codes produces the classic regression output table (simular tables are produced by all statistical/econometric software packages):\n\nlm_obj_summary &lt;- summary(lm_obj)\nlm_obj_summary\n\n\nCall:\nlm(formula = visits ~ gender + age + income, data = DoctorVisits)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5009 -0.3435 -0.2306 -0.1682  8.6174 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.15371    0.03607   4.262 2.07e-05 ***\ngenderfemale  0.06245    0.02345   2.662  0.00778 ** \nage           0.40235    0.05713   7.043 2.13e-12 ***\nincome       -0.08231    0.03167  -2.599  0.00938 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7908 on 5186 degrees of freedom\nMultiple R-squared:  0.01885,   Adjusted R-squared:  0.01829 \nF-statistic: 33.22 on 3 and 5186 DF,  p-value: &lt; 2.2e-16\n\n\nThe above regression output table contains the following information:\n\nEstimate: The column “Estimate” containes the estimates \\[\n\\hat\\beta_{j},\\quad j\\in\\{1,gender, age, income\\}\n\\] You can extract them using coef(lm_obj).\n\nStd. Error: The column “Std. Error” containes the estimates \\[\n\\widehat{\\operatorname{SE}}(\\hat\\beta_{j}|X),\\quad j\\in\\{1,gender, age, income\\}\n\\]\n\nYou can extract the total \\((K\\times K)=(4\\times 4)\\) variance-covariance matrix estimate \\(\\widehat{Var}(\\hat\\beta|X)\\) using vcov(lm_obj).\nThe diagonal diag(vcov(lm_obj)) contains the variance estimates \\(\\widehat{Var}(\\hat\\beta_j|X)\\), \\(j\\in\\{1,gender, age, income\\}\\).\nThe square root of the diagonal sqrt(diag(vcov(lm_obj))) allows you to compute the estimated standard errors shown in the regression table.\n\n\nt value: The column “t value” contains the observed \\(t\\) test statistics \\[\nT_{obs,j}=\\frac{\\hat\\beta_{j}-0}{\\widehat{\\operatorname{SE}}(\\hat\\beta_{j}|X)},\\quad j\\in\\{1,gender, age, income\\}\n\\] You can extract the values using lm_obj_summary$coefficients[,3].\nPr(&gt;|t|): The column “Pr(&gt;|t|)” contains the \\(p\\) values \\[\nP_{H_0}(|t|&gt;t_{obs,j}),\\quad j\\in\\{1,gender, age, income\\}\n\\] You can extract the values using lm_obj_summary$coefficients[,4].\nResidual standard error \\(\\sqrt{\\frac{1}{n-K}\\sum_{i=1}^n\\hat\\varepsilon^2_i}=\\) sqrt(sum(resid(lm_obj)^2)/(n-4)) \\(=\\) 0.7908\nMultiple R-squared: \\(R^2=\\) lm_obj_summary$r.squared \\(=\\) 0.01885\nAdjusted R-squared: \\(\\bar{R}^2=\\) lm_obj_summary$adj.r.squared \\(=\\) 0.01829\nF-statistic: This is a standard \\(F\\) test that tests the null hypothesis that all parameters except the intercept are zero; i.e.\\(H_0\\): \\(\\beta_{gender}=\\beta_{age}=\\beta_{income}=0\\) versus\\(H_1\\): At least one parameter is not zero. R’s summary() functions reports an observed \\(F\\) statistic value of \\(33.22\\) which needs to be evaluated for an \\(F\\) distribution with \\(3\\) and \\(5186\\) degrees of freedom leading to a \\(p\\)-value \\(p&lt; 0.00001.\\)  You can replicate this \\(F\\)-test result using the following R code:\n\n\ncar::linearHypothesis(\n      model = lm_obj, \n      hypothesis.matrix = c(\"genderfemale=0\", \"age=0\", \"income=0\"))  \n\nLinear hypothesis test\n\nHypothesis:\ngenderfemale = 0\nage = 0\nincome = 0\n\nModel 1: restricted model\nModel 2: visits ~ gender + age + income\n\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   5189 3305.5                                  \n2   5186 3243.2  3     62.32 33.218 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nR Package Stargazer\nBeautiful and “publication ready” regression outputs can be produced using the R package stargazer and its function stargazer():\n\n\n## Hint: use type = \"latex\" \n## to produce a latex table\nstargazer(lm_obj, type=\"html\")\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nvisits\n\n\n\n\n\n\n\n\ngenderfemale\n\n\n0.062***\n\n\n\n\n\n\n(0.023)\n\n\n\n\n\n\n\n\n\n\nage\n\n\n0.402***\n\n\n\n\n\n\n(0.057)\n\n\n\n\n\n\n\n\n\n\nincome\n\n\n-0.082***\n\n\n\n\n\n\n(0.032)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.154***\n\n\n\n\n\n\n(0.036)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n5,190\n\n\n\n\nR2\n\n\n0.019\n\n\n\n\nAdjusted R2\n\n\n0.018\n\n\n\n\nResidual Std. Error\n\n\n0.791 (df = 5186)\n\n\n\n\nF Statistic\n\n\n33.218*** (df = 3; 5186)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\nCritical Discussion of the Regression above Results\nThe above real data analysis does not fit into the small sample inference framework we introduced in this chapter.\n\nThe dependent variable \\(Y_i\\) visits is a categorial variable taking finitely many discrete values, indeed\n\nunique(DoctorVisits$visits) = 1, 2, 3, 4, 8, 5, 7, 6, 9, 0.\n\nConsequently, the error term \\(\\varepsilon_i\\) cannot be normal distributed.\nThe diagnostic plot (“Residuals versus Fitted”) indicates a possible issue violation of the homoskedasticity assumption. In case of homokedastic variances, the data points \\((\\hat\\varepsilon_i,\\hat{Y}_i)\\), \\(i=1,\\dots,n\\) should roughly show a homogenous scattering across the fitted values \\(\\hat{Y}_i=X\\hat\\beta\\). This seems not to be the case here.\n\n\n## Diagonstic Plot \n## Residuals versus fitted values\nplot(lm_obj, which = 1)\n\n\n\n\n\n\n\nLukily, the data set DoctorVisits actually has a large sample size of \\(n=\\) 5190 and thus there is a way out of this problem: The large sample inference framework introduced in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Small Sample Inference</span>"
    ]
  },
  {
    "objectID": "06-Small-Sample-Inference.html#exercises",
    "href": "06-Small-Sample-Inference.html#exercises",
    "title": "6  Small Sample Inference",
    "section": "\n6.8 Exercises",
    "text": "6.8 Exercises\n\nExercises for Chapter 6\nExercises of Chapter 6 with Solutions",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Small Sample Inference</span>"
    ]
  },
  {
    "objectID": "06-Small-Sample-Inference.html#references",
    "href": "06-Small-Sample-Inference.html#references",
    "title": "6  Small Sample Inference",
    "section": "References",
    "text": "References\n\n\n\n\nAltman, Douglas G, and J Martin Bland. 1995. “Statistics Notes: Absence of Evidence Is Not Evidence of Absence.” British Medical Journal 311 (7003): 485.\n\n\nHayashi, Fumio. 2000. Econometrics. Princeton University Press.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Small Sample Inference</span>"
    ]
  },
  {
    "objectID": "07-Asymptotics.html",
    "href": "07-Asymptotics.html",
    "title": "7  Large Sample Inference",
    "section": "",
    "text": "7.1 Tools for Asymptotic Statistics\nBasically every modern econometric method is justified using the toolbox of asymptotic statistics. The following core concepts from asymptotic statistics will allow us to drop the restrictive normality assumption of Chapter 6 and to introduce robust standard errors:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Large Sample Inference</span>"
    ]
  },
  {
    "objectID": "07-Asymptotics.html#tools-for-asymptotic-statistics",
    "href": "07-Asymptotics.html#tools-for-asymptotic-statistics",
    "title": "7  Large Sample Inference",
    "section": "",
    "text": "Concepts on stochastic convergence\nContinuous mapping theorem\nSlutsky’s theorem\nLaw of large numbers\nCentral limit theorems\nCramér-Wold device\n\n\n7.1.1 Modes of Convergence\nIn the following we will discuss the four most important convergence concepts for sequences of random variables \\[\n\\{Z_n\\}:=(Z_1,Z_2,\\dots,Z_n),\n\\] where \\[\nZ_i,\\quad i=1,\\dots,n,\n\\] is a uni-variate or multivariate random variable.\nNon-random quantities (scalars, vectors or matrices) will be denoted by Greek letters such as \\(\\alpha\\).\n\n\n\nFour Important Modes of Convergence\nProbably the most often used mode of convergence is convergence in probability. It states that a stochastic sequence \\(\\{Z_n\\}\\) concentrates around its limit \\(\\alpha\\) such that deviations \\(|Z_n-\\epsilon|\\) larger than some small \\(\\epsilon &gt;0\\) occur eventually (as \\(n\\to\\infty\\)) with probability zero.\nIn order to show that some stochastic sequence converges in probability to its limit, one typically uses a (weak) law of large numbers.\n\n\n\n\n\n\n\nDefinition 7.1 (Convergence in Probability) \nA sequence of univariate real valued random variables \\(\\{Z_n\\}\\) converges in probability to a deterministic \\(\\alpha\\in\\mathbb{R}\\) if for any (arbitrarily small) \\(\\varepsilon&gt;0\\) \\[\n\\begin{align*}\n  \\lim_{n\\to\\infty} P\\left(|Z_n-\\alpha|&lt;\\epsilon\\right)&=1\\\\\n  \\Leftrightarrow\\quad \\lim_{n\\to\\infty} P\\left(|Z_n-\\alpha|&gt;\\epsilon\\right)&=0.\n\\end{align*}\n\\] We write: \\[\n\\operatorname{plim}_{n\\to\\infty}Z_n=\\alpha\n\\] or more shortly \\[\nZ_n\\to_{p}\\alpha,\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nNote:\n\nConvergence in probability of a sequence of random vectors or matrices \\(\\{Z_n\\}\\) to a deterministic vector or matrix \\(\\alpha\\) requires element-wise convergence in probability.\nThe above definition can also be stated for random limits; i.e., for \\(\\alpha\\) being a random variable.\n\n\n\n\n\nA stricter mode of convergence is almost sure convergence. Almost sure convergence is (usually) rather hard to derive, since the probability is about an event concerning an infinite sequence. Fortunately, however, there are established strong laws of large numbers that we can use to argue that some stochastic sequence converges almost surely to its limit.\nEvery sequence that converges almost surely, also converges in probability.\n\n\n\n\n\n\n\nDefinition 7.2 (Almost Sure Convergence) \nA sequence of univariate real valued random variables \\(\\{Z_n\\}\\) converges almost surely to a deterministic \\(\\alpha\\in\\mathbb{R}\\) if \\[\\begin{eqnarray*}\nP\\left(\\lim_{n\\to\\infty}Z_n=\\alpha\\right)=1.\n\\end{eqnarray*}\\] We write: \\[\nZ_n\\to_{as}\\alpha,\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nNote:\n\nAlmost sure convergence of a sequence of random vectors (or matrices) \\(\\{Z_n\\}\\) to a deterministic vector (or matrix) \\(\\alpha\\) requires element-wise almost sure convergence.\nThe above definition can also be stated for random limits; i.e., for \\(\\alpha\\) being a random variable.\n\n\n\n\n\nConvergence in mean square is typically the most intuitive mode of convergence. If a stochastic sequence \\(\\{Z_n\\}\\) converges to a certain limit \\(\\alpha,\\) then\n\nthe mean of the stochastic sequence \\(E(Z_n)\\) must converge to the limit \\(\\alpha,\\) and\nthe variance of the stochastic sequence \\(E(Z_n)\\) must converge to zero.\n\nConvergence in mean square is typically easy to show.\n\n\n\n\n\n\n\nDefinition 7.3 (Convergence in Mean Square) \nA sequence of univariate real valued random variables \\(\\{z_n\\}\\) converges in mean square (or in quadratic mean) to a deterministic \\(\\alpha\\in\\mathbb{R}\\) if \\[\n\\begin{align*}\n  \\lim_{n\\to\\infty}E\\left((Z_n-\\alpha)^2\\right)&=0.\n\\end{align*}\n\\] We write: \\[\nZ_n\\to_{ms}\\alpha,\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nNote:\n\nMean square convergence of a sequence of random vectors (or matrices) \\(\\{Z_n\\}\\) to a deterministic vector (or matrix) \\(\\alpha\\) requires element-wise mean square convergence.\nThe above definition can also be stated for random limits; i.e., for \\(\\alpha\\) being a random variable.\n\n\n\n\n\n\nConvergence in distribution is the weakest and at the same time most important mode of convergence.\n\n\n\n\n\n\n\nDefinition 7.4 (Convergence in Distribution)  \n\nLet \\(F_n\\) denote the cumulative distribution function (cdf) of the univariate real valued random variable \\(Z_n\\in\\mathbb{R},\\) and\nlet \\(F\\) denote the cdf of the univariate real valued random variable \\(Z\\in\\mathbb{R}.\\)\n\n\nA sequence of univariate real valued random variables \\(\\{Z_n\\}\\) converges in distribution to a the univariate real valued random variable \\(Z\\) if \\[\n\\begin{align*}\n  \\lim_{n\\to\\infty}F_n(x)=F(x)\n\\end{align*}\n\\] for all \\(x\\in\\mathbb{R}\\) at which \\(F(x)\\) is continuous.\nWe write: \\[\nZ_n\\to_{d}Z,\\quad\\text{as}\\quad n\\to\\infty\n\\] We call \\(F\\) the asymptotic (or limit) distribution of \\(Z_n\\).\n\n\n\n\nRemarks on Definition 7.4:\n\nOften you will see statements like \\[\nZ_n\\to_{d} \\mathcal{N}(0,1)\n\\] or \\[\nZ_n\\overset{a}{\\sim}\\mathcal{N}(0,1),\n\\] which should be read as \\[\n\\lim_{n\\to\\infty}F_n(x) = \\Phi(x)\\quad\\text{for all}\\quad x\\in\\mathbb{R},\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\nA stochastic sequence \\(\\{Z_n\\}\\) can also convergence in distribution to a constant \\(\\alpha\\). In this case \\(\\alpha\\) is treated as a degenerated random variable with cdf \\[\nF_\\alpha(x)=\\left\\{\n  \\begin{matrix}\n  0&\\text{if}\\;\\;x   &lt;\\alpha\\\\\n  1&\\text{if}\\;\\;x\\geq\\alpha.\n  \\end{matrix}\\right.\n\\]\n\n\nBy contrast to all other modes of convergence, Definition 7.4 only addresses the univariate case \\((Z_n\\in\\mathbb{R}).\\) The reason for this is that Definition 7.4 cannot simply applied element-wise since this would ignore the possible dependencies between the different uni-variate random variables elements of a random vector.\nTo handle multivariate convergence in distribution, we need the following theorem known as the Cramér-Wold device.\n\n\n\n\n\n\n\nTheorem 7.1 (Cramér-Wold) \nLet \\(Z_n\\in\\mathbb{R}^K\\) and \\(Z\\in\\mathbb{R}^K\\) be \\((K\\times 1)\\)-dimensional random vectors, then \\[\nZ_n\\to_{d} Z\\quad\\text{if and only if}\\quad\\lambda'Z_n\\to_{d}\\lambda'Z\n\\] for any \\(\\lambda\\in\\mathbb{R}^K\\).\n\n\n\n\nA proof of Theorem 7.1 can be found, e.g., in Billingsley (2008) (p. 383).\nThe Cramér-Wold theorem (Theorem 7.1) is needed since element-wise convergence in distribution generally does not imply convergence of the joint distribution of \\(Z_n\\) to the joint distribution of \\(Z\\); except, if all elements in the random vectors \\(Z_n\\) and \\(Z\\) are independent from each other.\n\nRelations among Modes of Convergence\n\n\n\n\n\n\n\nLemma 7.1 (Relations among Modes of Convergence) \nThe following relationships hold:\n\nMean square convergence implies convergence in probability: \\[\nZ_n\\to_{ms}\\alpha\\quad \\Rightarrow\\quad Z_n\\to_{p}\\alpha\n\\]\n\nAlmost sure convergence implies convergence in probability: \\[\nZ_n\\to_{as}\\alpha\\quad \\Rightarrow\\quad Z_n\\to_{p}\\alpha\n\\]\n\nConvergence in distribution to a constant is equivalent to convergence in probability to the same constant: \\[\nZ_n\\to_{d}\\alpha\\quad \\Leftrightarrow\\quad Z_n\\to_{p}\\alpha\n\\]\n\n\n\n\n\n\nProofs of the result in Lemma 7.1 can be found, e.g., here: https://www.statlect.com/asymptotic-theory/relations-among-modes-of-convergence\n\n7.1.2 Continuous Mapping Theorem (CMT)\n\n\n\n\n\n\n\nTheorem 7.2 (Preservation of Convergence for Continuous Transformations (or “Continuous Mapping Theorem (CMT)”)) \nLet \\(\\{Z_n\\}\\) denote a stochastic sequence of univariate (or multivariate) random variables and let \\(f\\) denote a continuous function that does not depended on \\(n\\). Then \\[\n\\begin{align*}\nZ_n\\to_{p}  \\alpha \\quad & \\Rightarrow\\quad f(Z_n)\\to_{p} f(\\alpha)\\\\[2ex]\nZ_n\\to_{as} \\alpha \\quad & \\Rightarrow\\quad f(Z_n)\\to_{as} f(\\alpha)\\\\[2ex]\nZ_n\\to_{d}  \\alpha \\quad & \\Rightarrow\\quad f(Z_n)\\to_{d} f(\\alpha)\n\\end{align*}\n\\]\n\n\n\n\nProofs of Theorem 7.2 can be found, e.g., in Van der Vaart (2000) (see Theorem 2.3) or here: https://www.statlect.com/asymptotic-theory/continuous-mapping-theorem\nNote: The CMT does not hold for m.s.-convergence except for the case where \\(f\\) is a linear function.\nExamples: As a consequence of the CMT (Theorem 7.2) we have that the usual arithmetic operations preserve convergence in probability (and equivalently for almost sure convergence and convergence in distribution):\n\nIf \\(X_n\\to_{p} \\beta\\) and \\(Y_n\\to_{p} \\gamma,\\) then \\[\nX_n+Y_n\\to_{p} \\beta+\\gamma\n\\]\nIf \\(X_n\\to_{p} \\beta\\) and \\(Y_n\\to_{p} \\gamma,\\) then \\[\nX_n\\cdot Y_n\\to_{p} \\beta\\cdot\\gamma\n\\]\nIf \\(X_n\\to_{p} \\beta\\) and \\(Y_n\\to_{p} \\gamma,\\) then \\[\nX_n/Y_n\\to_{p} \\beta/\\gamma,\n\\] provided that \\(\\gamma\\neq 0\\)\nIf \\(\\frac{1}{n}\\sum_{i=1}^nX_iX_i'=\\frac{1}{n}X_n'X_n\\to_{p} \\Sigma_{X'X},\\) then \\[\n\\left(\\frac{1}{n}X_n'X_n\\right)^{-1}\\to_{p} \\Sigma_{X'X}^{-1},\n\\] provided \\(\\Sigma_{X'X}\\) is a nonsingular/invertible matrix.\n\n\n7.1.3 Slutsky’s Theorem\nSlutsky’s Theorem is a collection of results concerned with combinations of\n\nconvergence in probability and\nconvergence in distribution.\n\nThese results are particularly important for the derivation of the asymptotic distribution of estimators.\n\n\n\n\n\n\n\nTheorem 7.3 (Slutsky’s Theorem) \nLet \\(X_n\\) and \\(Y_n\\) denote sequences of random scalars or vectors and let \\(A_n\\) denote a sequences of random matrices. Moreover, \\(\\alpha\\) and \\(A\\) are deterministic limits of appropriate dimensions and \\(X\\) is a random limit of appropriate dimension.\n\nIf \\(X_n\\to_{d} X\\quad\\) and \\(\\quad Y_n\\to_{p} \\alpha,\\quad\\) then \\[\nX_n+Y_n\\to_{d} X + \\alpha.\n\\]\nIf \\(X_n\\to_{d} X\\quad\\) and \\(\\quad Y_n\\to_{p} 0,\\quad\\) then \\[\nX_n'Y_n\\to_{p} 0.\n\\]\nIf \\(X_n\\to_{d} X\\quad\\) and \\(\\quad A_n\\to_{p} A,\\quad\\) then \\[\nA_n X_n\\to_{d} AX.\n\\]\n\nAbove, we assume that \\(X_n,\\) \\(Y_n,\\) and \\(A_n\\) are “conformable” (i.e., the matrix- and vector-dimensions fit to each other).\n\n\n\n\nProofs of Theorem 7.3 can be found, e.g., in Van der Vaart (2000) (see Theorem 2.8) or here: https://www.statlect.com/asymptotic-theory/Slutsky-theorem\nRemark: Sometimes, only the first two points in Theorem 7.3 are called “Slutsky’s theorem.”\nImportant special case of Theorem 7.3:\nLet \\(\\{X_n\\}\\) and \\(\\{A_n\\}\\) be two sequences of real valued \\((K\\times 1)\\)-dimensional random vectors. If \\[\nX_n\\to_{d}\\mathcal{N}_K(0,\\Sigma)\\quad\\text{and}\\quad A_n\\to_{p} A\n\\] then \\[\nA_nX_n\\to_{d}\\mathcal{N}_K(0,A\\Sigma A').\n\\]\n\n7.1.4 Law of Large Numbers and Central Limit Theorems\nSo far, we discussed the definitions of the four most important convergence modes, their relations among each other, and basic theorems about functionals of stochastic sequences (CMT and Slutsky). Though, we still lack of tools that allow us to actually show that a stochastic sequence convergences (in some of the discussed modes) to some limit.\nIn the following we consider the stochastic sequences \\[\n\\bar{Z}_1,\\bar{Z}_2,\\dots,\\bar{Z}_n,\\quad\\text{as}\\quad n \\to\\infty,\n\\] of sample means \\[\n\\bar{Z}_n:=n^{-1}\\sum_{i=1}^nZ_i,\n\\] where \\(Z_i\\), \\(i=1,\\dots,n\\), are (scalar, vector, or matrix-valued) random variables.\nRemember: The sample mean \\(\\bar{Z}_n\\) is an estimator of the deterministic population mean \\(\\mu.\\)\nWeak Law of Large Numbers (WLLNs), Strong LLNs (SLLNs), and Central Limit Theorems (CLTs) tell us conditions under which arithmetic means \\[\n\\bar{Z}_n=n^{-1}\\sum_{i=1}^nZ_i\n\\] converge in probability, almost surely, and in distribution, respectively:\n\nWeak LLN: \\(\\bar{Z}_n \\to_{p}\\mu\\)\nStrong LLN: \\(\\bar{Z}_n\\to_{as}\\mu\\)\nCLT: \\(\\sqrt{n}(\\bar{Z}_n-\\mu)\\to_{d}\\mathcal{N}(0,\\sigma^2)\\)\n\nIn the following we introduce the most well-known versions of a WLLN, SLLN, and a CLT.\n\n\n\n\n\n\n\nTheorem 7.4 (Weak LLN (by Chebychev)) \nIf\n\n\n\\(\\lim_{n\\to\\infty} E(\\bar{Z}_n)=\\mu\\) and\n\n\\(\\lim_{n\\to\\infty}Var(\\bar{Z}_n)=0,\\) then \\[\n\\bar{Z}_n\\to_{p}\\mu.\n\\]\n\n\n\n\n\n\nA proof of Theorem 7.4 can be found, for instance, here: https://www.statlect.com/asymptotic-theory/law-of-large-numbers\n\n\n\n\n\n\n\nTheorem 7.5 (Strong LLN (by Kolmogorov)) \nIf \\(\\{Z_i\\}\\) is an\n\niid sequence with\n\n\\(E(Z_i)=\\mu,\\) then \\[\n\\bar{Z}_n\\to_{as}\\mu.\n\\]\n\n\n\n\n\n\nA proof of Theorem 7.5 can be found, e.g., in Linear Statistical Inference and Its Applications, Rao (1973), pp. 112-114.\n\nNote: The WLLN and the SLLN for random vectors follow from applying the the theorem separately for each element of the random vectors.\n\n\n\n\n\n\n\nTheorem 7.6 (CLT (Lindeberg-Levy)) \nIf \\(\\{Z_i\\}\\) is\n\nan iid sequence with\n\n\n\\(E(Z_i)=\\mu\\) for all \\(i=1,\\dots,n\\) and\n\n\\(Var(Z_i)=\\sigma^2\\) for all \\(i=1,\\dots,n,\\)\n\n\nthen \\[\n\\sqrt{n}(\\bar{Z}_n-\\mu)\\to_{d} \\mathcal{N}(0,\\sigma^2),\\quad\\text{as}\\quad n\\to\\infty\n\\]\n\n\n\n\nA proof of Theorem 7.6 can be found, e.g., in Van der Vaart (2000) (see Theorem 2.17).\n\nUsing the Cramér-Wold device (Theorem 7.1), the Lindeberg-Levy CLT (Theorem 7.6) can also be applied to \\(K\\)-dimensional estimators \\[\n\\bar{Z}_n \\in\\mathbb{R}^K.\n\\]\nTo show that \\[\n\\sqrt{n}(\\bar{Z}_n-\\mu)\\to_d\\mathcal{N}_K(0,\\Sigma)\n\\] converges to the multivariate, \\(K\\)-dimensional, normal distribution \\(\\mathcal{N}_K(0,\\Sigma)\\) as \\(n\\to\\infty\\), we need to check whether for any \\(\\lambda\\in\\mathbb{R}^K\\):\n\nthe univariate stochastic sequence \\(\\{\\lambda'Z_i\\}\\) is i.i.d. with\n\n\\(E(\\lambda'Z_i)=\\lambda'\\mu\\) for all \\(i=1,\\dots,n,\\) where \\(E(Z_i)=\\mu\\in\\mathbb{R}^K\\) denotes the \\(K\\) dimensional mean vector, and\n\n\\(Var(\\lambda'Z_i)=\\lambda'\\Sigma\\lambda\\) for all \\(i=1,\\dots,n,\\) where \\(Var(Z_i)=\\Sigma\\) denotes the \\((K\\times K)\\) dimensional variance-covariance matrix.\n\nThese points are fulfilled if the multivariate stochastic sequence \\(\\{Z_i\\}\\) is an i.i.d. sequence with \\(E(Z_i)=\\mu\\) and \\(Var(Z_i)=\\Sigma.\\)\n\n\n\n\n\n\nTip\n\n\n\nThe LLNs and the CLT are stated with respect to sequences of sample means \\(\\{\\bar{Z}_n\\}\\); i.e., the simplest estimators you probably can think of. We will see, however, that this is all we need in order to analyze also more complicated estimators such as the OLS estimator.\n\n\n\n7.1.5 Estimators: Sequences of Random Variables\nThe concepts introduced above readily apply to univariate or multivariate (\\(K\\)-dimensional) estimators \\[\n\\hat\\theta_n\\equiv\\hat\\theta((X_1,Y_1),\\dots,(X_n,Y_n))\n\\] which are functions of the random sample with sample size \\(n.\\)\nAn increasing sample size \\[\nn\\to\\infty\n\\] makes an estimator \\[\n\\{\\hat\\theta_n\\}\n\\] nothing but a sequence of random variables converging (hopefully) to the correct limit; namely, to the parameter value \\(\\theta\\) we aim to estimate.\nIf an estimator \\(\\hat\\theta_n\\) converges in probability to its limit \\(\\theta\\), we call the estimator weakly consistent or simply consistent. If it converges almost surely to \\(\\theta,\\) we call the estimator strongly consistent.\n\n\n\n\n\n\n\n\nDefinition 7.5 ((Weak) Consistency) \nWe say that an estimator \\(\\hat\\theta_n\\) is (weakly) consistent for \\(\\theta\\) if \\[\n\\hat\\theta_n\\to_{p}\\theta,\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 7.6 (Strong Consistency) \nWe say that an estimator \\(\\hat\\theta_n\\) is strongly consistent for \\(\\theta\\) if \\[\n\\hat\\theta_n\\to_{as}\\theta,\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\nA necessary requirement for weak and strong consistency is that the estimator is asymptotically unbiased.\n\n\n\n\n\n\n\nDefinition 7.7 (Asymptotic Bias) \nThe asymptotic bias of an estimator \\(\\hat\\theta_n\\) of some parameter \\(\\theta\\) is defined as \\[\n\\begin{align*}\n\\operatorname{ABias}(\\hat\\theta_n,\\theta)\n&=\\lim_{n\\to\\infty}\\operatorname{Bias}(\\hat\\theta_n,\\theta)\\\\\n&=\\lim_{n\\to\\infty}E(\\hat\\theta_n)-\\theta.\n\\end{align*}\n\\] If \\[\n\\operatorname{ABias}(\\hat\\theta_n,\\theta)=0,\n\\] then \\(\\hat\\theta_n\\) is called an asymptotically unbiased.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 7.8 (Asymptotic Normality, Asymptotic Variance, \\(\\sqrt{n}\\)-Consistent) \nAn estimator \\(\\hat\\theta_n\\) is called asymptotically normal if \\[\n\\sqrt{n}(\\hat\\theta_n-\\theta)\\to_{d} \\mathcal{N}(0,\\operatorname{AVar}(\\hat\\theta_n)),\\quad\\text{as}\\quad n\\to\\infty,\n\\] where \\[\n\\begin{align}\n\\operatorname{AVar}(\\hat\\theta_n)\n%\\sigma^2\n& = \\lim_{n\\to\\infty}Var(\\sqrt{n}(\\hat\\theta_n-\\theta))\\\\\n& = \\lim_{n\\to\\infty}n Var(\\hat\\theta_n-\\theta)\\\\\n& = \\lim_{n\\to\\infty}n Var(\\hat\\theta_n)\\\\\n\\end{align}\n\\] is called the asymptotic variance of \\(\\hat\\theta_n.\\)\nDue to the \\(\\sqrt{n}\\)-scaling, \\(\\theta_n\\) is called \\(\\sqrt{n}\\)-consistent.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 7.9 (Mean Squared Error (MSE)) \\[\n\\operatorname{MSE}(\\hat\\theta_n) = E\\left((\\hat\\theta_n - \\theta)^2\\right)\n\\] is called the mean squared error (MSE) of the estimator \\(\\hat\\theta_n.\\)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf we can show that the MSE of \\(\\hat\\theta_n\\) converges to zero, \\[\n\\operatorname{MSE}(\\hat\\theta_n) \\to 0,\\quad n\\to\\infty,\n\\] then we have shown that \\(\\hat\\theta_n\\) converges to \\(\\theta\\) in the mean square sense \\[\n\\hat\\theta_n  \\to_{ms} \\theta,\\quad n\\to\\infty,\n\\] which implies that \\(\\hat\\theta_n\\) is weakly consistent, i.e. \\[\n\\hat\\theta_n \\to_{p} \\theta,\\quad n\\to\\infty.\n\\]\n\n\nThe MSE of \\(\\hat\\theta_n\\) can be decomposed into the squared bias of \\(\\hat\\theta_n\\) and the variance of \\(\\hat\\theta_n:\\) \\[\n\\begin{align*}\n\\operatorname{MSE}(\\hat\\theta_n)\n&=E\\Big[(\\hat\\theta_n-\\theta)^2\\Big]\\\\[2ex]\n&=E\\Big[\\Big(\\overbrace{E(\\hat\\theta_n) - E(\\hat\\theta_n)}^{=0} + \\hat\\theta_n-\\theta\\Big)^2\\Big]\\\\[2ex]\n&=E\\left[\\left((E(\\hat\\theta_n) -\\theta) + (\\hat\\theta_n - E(\\hat\\theta_n))\\right)^2\\right]\\\\[2ex]\n&=E\\Big[\\left(E(\\hat\\theta_n) -\\theta\\right)^2 + \\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)^2\\\\[2ex]\n&\\quad +2\\left(E(\\hat\\theta_n) -\\theta\\right)\\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)\\Big]\\\\[2ex]\n&=\\left(E(\\hat\\theta_n) -\\theta\\right)^2 + E\\Big[\\left(\\hat\\theta_n - E(\\hat\\theta_n)\\right)^2\\Big]\\\\[2ex]\n&\\quad +2\\left(E(\\hat\\theta_n) -\\theta\\right)\\overbrace{\\Big(E(\\hat\\theta_n) - E(\\hat\\theta_n)\\Big)}^{=0}\\\\[2ex]\n%&=\\left(E(\\hat\\theta_n)-\\theta\\right)^2 + Var(\\hat\\theta_n)\\\\\n&=\\left(\\operatorname{Bias}(\\hat\\theta_n)\\right)^2 + Var(\\hat\\theta_n).\n\\end{align*}\n\\] Thus, to show that an estimator \\(\\hat\\theta_n\\) converges in the mean square sense to \\(\\theta,\\) we need to show that:\n\nThe estimator is asymptotically unbiased\\[\n\\operatorname{Bias}(\\hat\\theta_n)\\to 0,\\quad n\\to\\infty\n\\]\n\nThe variance of the estimator converges to zero \\[\nVar(\\hat\\theta_n)\\to 0,\\quad n\\to\\infty.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Large Sample Inference</span>"
    ]
  },
  {
    "objectID": "07-Asymptotics.html#asymptotics-under-the-classic-regression-model",
    "href": "07-Asymptotics.html#asymptotics-under-the-classic-regression-model",
    "title": "7  Large Sample Inference",
    "section": "\n7.2 Asymptotics under the Classic Regression Model",
    "text": "7.2 Asymptotics under the Classic Regression Model\nGiven the above introduced machinery, we can now proof that the two OLS estimators \\[\n\\hat\\beta_n=(X'X)^{-1}X'Y\n\\] and \\[\ns^2_{ub,n}=\\frac{1}{n-K}\\sum_{i=1}^n\\hat\\epsilon_i^2\n\\] are both consistent, and that \\(\\hat\\beta_n\\) is asymptotically normal distributed.\n\nUsing asymptotic statistics, allows us to drop the unrealistic normality and spherical errors assumption (Assumption 4\\(^\\ast\\)) of Chapter 6, but still use our inference tools (\\(t\\)-tests, \\(F\\)-tests) from Chapter 6; as long as the sample size \\(n\\) is “large.”\n\n\nFor the following, it will be useful to introduce some notation that allows us to consider the different parts of the OLS estimator \\(\\hat\\beta_n\\) separately.\\[\n\\begin{align*}\n\\hat\\beta_n\n&=\\left(X'X\\right)^{-1}X'Y\\\\[2ex]\n&=\\left(\\frac{1}{n}X'X\\right)^{-1}\\frac{1}{n} X'Y\\\\[2ex]\n&=\\;\\;\\;\\;S_{X'X}^{-1}\\;\\;\\;\\;\\frac{1}{n} X'Y,\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n\\underset{(K\\times K)}{S_{X'X}}\n&=\\frac{1}{n}X'X\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^nX_iX_i',\n\\end{align*}\n\\] and where the mean of the \\((K\\times K)\\) matrix \\(S_{X'X}\\) will be denoted as \\[\n\\begin{align*}\n\\Sigma_{X'X}\n&=E\\left(S_{X'X}\\right)\\\\[2ex]\n&=E\\left(\\frac{1}{n}X'X\\right)\\\\[2ex]\n&=E\\left(\\frac{1}{n}\\sum_{i=1}^nX_iX_i'\\right)\\\\[2ex]\n&=\\frac{n}{n}E\\left(X_iX_i'\\right)\\\\[2ex]\n&=E\\left(X_iX_i'\\right).\n\\end{align*}\n\\] For the below results, we need to restate the full rank assumption (Assumption 3 of Chapter 5) with respect to \\(\\Sigma_{X'X}.\\)\nAssumption 3\\(^\\ast\\): (Population) Rank Condition  The \\((K\\times K)\\) matrix \\[\n\\Sigma_{X'X}=E(S_{X'X})\n\\]    has full rank \\(K\\). I.e., \\(\\Sigma_{X'X}\\) is nonsingular and invertible. \n\n\n\n\n\n\n\n\n\n\n\nTheorem 7.7 (Consistency of \\(S_{X'X}^{-1}\\)) Under Assumptions 1, 2, 3\\(^\\ast\\), and 4, and under the assumption that \\(X_i\\) has finite second moments, we have that \\[\nS_{X'X}^{-1} = \\left(\\frac{1}{n}X'X\\right)^{-1}\\quad\\to_{p}\\quad\\Sigma_{X'X}^{-1},\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\nThe proof of Theorem 7.7 is done in the lecture.\n\n\n\n\n\n\n\n\nTheorem 7.8 (Consistency of \\(\\hat\\beta\\)) Under Assumptions 1, 2, 3\\(^\\ast\\), and 4, and under the assumption that \\(X_i\\) has finite second moments, and \\(\\varepsilon_i\\) has finite first moments, we have that \\[\n\\hat\\beta_n\\to_{p}\\beta\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\nThe proof of Theorem 7.8 is done in the lecture.\n\nMoreover, we can show that the appropriately scaled OLS estimator is asymptotically normal distributed. The following theorem is stated for the simpler homoskedastic case, the heteroskedastic case is presented in Section 7.2.1.\n\n\n\n\n\n\n\nTheorem 7.9 (Asymptotic Normality of \\(\\hat{\\beta}\\) (Homoskedastic Case)) Under Assumption 1, 2, 3\\(^\\ast\\), and 4, and under the assumption that \\(X_i\\) and \\(\\varepsilon_i\\) have finite second moments, and under the simplifying assumption of spherical errors \\[\nVar(\\varepsilon|X)=\\sigma^2I_n,\n\\] we have that \\[\n\\sqrt{n}(\\hat\\beta_n-\\beta)\\to_{d} \\mathcal{N}_K\\left(0,\\sigma^2 \\Sigma^{-1}_{X'X}\\right),\\quad\\text{as}\\quad n\\to\\infty,\n\\] where \\[\n\\begin{align*}\n\\sigma^2 \\Sigma^{-1}_{X'X}\n& = \\lim_{n\\to\\infty} Var\\left(\\sqrt{n}(\\hat\\beta_n-\\beta)\\right)\\\\[2ex]\n& = \\lim_{n\\to\\infty} n Var\\left(\\hat\\beta_n\\right)\\\\[2ex]\n& = \\operatorname{AVar}(\\hat\\beta_n)\n\\end{align*}\n\\] is the asymptotic variance of \\(\\hat\\beta_n.\\)\n\n\n\n\nThe proof of Theorem 7.9 is done in the lecture.\nTheorem 7.9 implies that for spherical errors and for largish sample sizes \\(n,\\) the OLS estimator \\(\\beta_n\\) is approximately normal distributed, i.e. \\[\n\\begin{align*}\n\\sqrt{n}(\\hat\\beta_n-\\beta)&\\overset{a}{\\sim}\\mathcal{N}_K\\left(0,\\sigma^2 \\Sigma^{-1}_{X'X}\\right)\\\\[2ex]\n\\Rightarrow\\qquad\\hat\\beta_n &\\overset{a}{\\sim}\\mathcal{N}_K\\left(\\beta, n^{-1}\\sigma^2 \\Sigma^{-1}_{X'X}\\right),\n\\end{align*}\n\\] where the approximation error becomes arbitrarily small as \\(n\\to\\infty.\\)\nUsually, we do not know the asymptotic variance \\[\n\\operatorname{AVar}(\\hat\\beta_n) = \\sigma^2\\Sigma_{X'X}^{-1},\n\\] and thus have to plug-in the (consistent) estimator\\[\n\\widehat{\\operatorname{AVar}}(\\hat\\beta_n) = s_{ub}^2 S_{X'X}^{-1}.\n\\]\n\nNote: Consistency of \\(S_{X'X}^{-1}\\) is provided by Theorem 7.7 and consistency of \\(s_{ub}^2\\) is provided by Theorem 7.10.\n\n\n\n\n\n\n\n\nTheorem 7.10 (Consistency of \\(s^2_{UB}\\)) Under the assumptions of Theorem 7.9, but with the additional requirement that \\(\\varepsilon_i\\) has finite fourth moments, we have that \\[\ns_{ub}^2\\to_{p}\\sigma^2,\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\nThe proof of Theorem 7.10 is skipped, but a detailed proof can be found here: https://www.statlect.com/fundamentals-of-statistics/OLS-estimator-properties\n\n7.2.1 The Case of Heteroskedasticity\nTheorem 7.9 can also be stated (and proofed) for conditionally heteroskedastic error terms. In this case, \\(\\beta_n\\) is for large sample sizes \\(n\\) approximately normal distributed, \\[\n\\begin{align*}\n\\sqrt{n}(\\hat\\beta_n-\\beta)\\overset{a}{\\sim}\\mathcal{N}_K\\left(0,\\Sigma_{X'X}^{-1}E(\\varepsilon^2_iX_iX_i')\\Sigma_{X'X}^{-1}\\right)\\\\[2ex]\n\\Rightarrow\\qquad\\hat\\beta_n\\overset{a}{\\sim}\\mathcal{N}_K\\left(\\beta,n^{-1}\\Sigma_{X'X}^{-1}E(\\varepsilon^2_iX_iX_i')\\Sigma_{X'X}^{-1}\\right),\n\\end{align*}\n\\qquad(7.1)\\] where the approximation error becomes arbitrarily small as \\(n\\to\\infty.\\)\nThe asymptotic variance (under heteroskedasticity) \\[\n\\begin{align*}\n\\operatorname{AVar}(\\hat\\beta_n)\n& = \\lim_{n\\to\\infty}Var(\\sqrt{n}(\\hat\\beta_n-\\beta))\\\\[2ex]\n& =\\underbrace{\\Sigma_{X'X}^{-1}E(\\varepsilon_i^2X_iX_i')\\Sigma_{X'X}^{-1}}_{(K\\times K)}\n\\end{align*}\n\\] is, of course, usually unknown and needs to be estimated from the data by some consistent estimator such that \\[\nS_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\\to_{p} \\Sigma_{X'X}^{-1}E(\\varepsilon^2_iX_iX_i')\\Sigma_{X'X}^{-1}\n\\] as \\(n\\to\\infty.\\)\nThe estimator \\[\n\\widehat{E}(\\varepsilon^2_iX_iX_i')\n\\] is here a placeholder for one of the existing Heteroskedasticity Consistent (HC) estimators of \\(E(\\varepsilon^2X_iX_i')\\):\n\n\n\n\n\n\nHC-Type\nFormular\n\n\n\nHC0\n\\(\\widehat{E}(\\varepsilon^2_iX_iX_i')=\\frac{1}{n}\\sum_{i=1}^n\\hat\\varepsilon_i^2X_iX_i'\\)\n\n\nHC1\n\\(\\widehat{E}(\\varepsilon^2_iX_iX_i')=\\frac{1}{n}\\sum_{i=1}^n\\frac{n}{n-K}\\hat\\varepsilon_i^2X_iX_i'\\)\n\n\nHC2\n\\(\\widehat{E}(\\varepsilon^2_iX_iX_i')=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\hat{\\varepsilon}_{i}^{2}}{1-h_{i}}X_iX_i'\\)\n\n\nHC3\n\\(\\widehat{E}(\\varepsilon^2_iX_iX_i')=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\hat{\\varepsilon}_{i}^{2}}{\\left(1-h_{i}\\right)^{2}}X_iX_i'\\)\n\n\nHC4\n\\(\\widehat{E}(\\varepsilon^2_iX_iX_i')=\\frac{1}{n}\\sum_{i=1}^n\\frac{\\hat{\\varepsilon}_{i}^{2}}{\\left(1-h_{i}\\right)^{\\delta_{i}}}X_iX_i'\\)\n\n\n\nHC3 is the most often used HC-estimator.\n\n\n\n\n\n\nTip\n\n\n\nThe statistic \\(h_i:=[P_X]_{ii}\\) is called the leverage statistic of \\(X_i,\\) where\n\n\n\\(1/n\\leq h_i\\leq 1\\) and\n\n\\(\\bar{h}=n^{-1}\\sum_{i=1}^nh_i=K/n\\).\n\nObservations \\(X_i\\) with leverage statistics \\(h_i\\) that greatly exceed the average leverage value \\(K/n\\) are referred to as “high leverage” observations. High leverage observations \\(X_i\\) are observations that are far away from all other observations \\(X_j\\), \\(i\\neq j=1,\\dots,n.\\)\nHigh leverage observations \\(X_i\\) have the potential to distort the estimation results, \\(\\hat\\beta_n\\). Indeed, a high leverage observation \\(X_i\\) will have an distorting effect on the estimation results if the absolute value of the corresponding residual \\(|\\hat{\\varepsilon}_i|\\) is unusually large—such observations are called influential outliers. Such observations increase the estimation uncertainty.\nGeneral idea of the HC2-HC4 estimators is to increase the estimated variance in order to account for the effects of influential outliers. The residuals \\(\\hat\\varepsilon_i\\) belonging to \\(X_i\\) values that have a large leverage \\(h_i\\) receive a higher weight and thus increase the value of \\(\\widehat{E}(\\varepsilon^2_iX_iX_i').\\) This strategy takes into account increased estimation uncertainties due to single influential outliers. \n\n\nThe estimator HC0 was suggested in the econometrics literature by White (1980) and is justified by asymptotic (\\(n\\to\\infty\\)) arguments. The estimators HC1, HC2 and HC3 were suggested by MacKinnon and White (1985) to improve the finite sample performance of HC0. Using an extensive Monte Carlo simulation study comparing HC0-HC3, Long and Ervin (2000) concludes that HC3 provides the best overall performance in finite samples. Cribari-Neto (2004) suggested the estimator HC4 to further improve the performance in finite sample behavior, especially in the presence of influential observations (large \\(h_i\\) values).\n\nNote: Besides Heteroskedasticity Consistent (HC) estimators, there are also Heteroskedasticity and Autocorrelation Consistent (HAC) estimators, as well as estimators that consider clustered covariances. An R package that provides H(A)C estimators is sandwich. For clustered covariances, the clubSandwich package can be used.\n\n\n7.2.2 Robust Inference\n\n\n7.2.2.1 Robust Hypothesis Testing: Multiple Parameters\nLet us reconsider the following system of \\(q\\)-many null hypotheses: \\[\n\\begin{align*}\nH_0: \\underset{(q\\times K)}{R}\\underset{(K\\times 1)}{\\beta}  = \\underset{(q\\times 1)}{r^{(0)}}\\\\\nH_1: \\underset{(q\\times K)}{R}\\underset{(K\\times 1)}{\\beta}  \\neq \\underset{(q\\times 1)}{r^{(0)}}\\\\\n\\end{align*}\n\\] where the \\((q \\times K)\\) matrix \\(R\\) and the \\(q\\)-vector \\(r=(r_{1},\\dots,r_{q})'\\) are chosen by the statistician to specify her/his null hypothesis about the unknown true parameter vector \\(\\beta\\). To make sure that there are no redundant equations, it is required that \\(\\operatorname{rank}(R)=q\\).\nBy contrast to the multiple parameter tests for small samples (see Section 6.2), we can work here with a heteroskedasticity robust test statistic which is applicable for heteroskedastic error terms: \\[\n\\begin{align*}\nW&=n(R\\hat\\beta_n - r^{(0)})'[R\\,S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\\,R']^{-1}(R\\hat\\beta_n-r^{(0)})\\\\[2ex]\nW&\\overset{H_0}{\\to}_d\\chi^2(q), \\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\qquad(7.2)\\] The price to pay is that the distribution of the test statistic under the null hypothesis is only valid asymptotically; i.e. for large \\(n\\). That is, the critical values taken from the asymptotic distribution will be useful only for “largish” samples sizes.\n\nIn case of homoskedastic error terms, one can substitute \\[\nS_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\n\\] by \\[\ns_{ub}^2S_{X'X}^{-1}.\n\\]\n\n\n\n\n\n\n\nFinite-sample correction\n\n\n\nIn order to improve the finite-sample performance of this test, one usually uses the \\(F_{q,n-K}\\) distribution with \\(q\\) and \\(n-K\\) degrees of freedoms instead of the \\(\\chi^2(q)\\) distribution.\nAsymptotically (\\(n\\to\\infty\\)), \\(F_{q,n-K}\\) is equivalent to \\(\\chi^2(q)\\). However, for any finite sample size \\(n\\) (i.e., the practically relevant case) \\(F_{q,n-K}\\) leads to larger critical values which helps to account for the estimation errors in \\(S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\\) (or in \\(s_{ub}^2S_{X'X}^{-1}\\)) which are otherwise neglected by the pure asymptotic perspective.\n\n\n\n7.2.2.2 Robust Hypothesis Testing: Single Parameters\nLet us reconsider the case of hypotheses about only one parameter \\(\\beta_k,\\) with \\(k=1,\\dots,K\\) \\[\n\\begin{equation*}\n\\begin{array}{ll}\nH_0: & \\beta_k=\\beta_k^{(0)}\\\\\nH_1: & \\beta_k\\ne \\beta_k^{(0)}\\\\\n\\end{array}\n\\end{equation*}\n\\] Selecting the \\(k\\)th diagonal element of the test-statistic in Equation 7.2 and taking the square root yields \\[\n\\begin{align*}\nT&=\\frac{\\sqrt{n}\\left(\\hat{\\beta}_k-\\beta_k^{(0)}\\right)}{\\sqrt{\\left[S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\\right]_{(k,k)}}}\n\\end{align*}\n\\] where \\[\n\\begin{align*}\nT&\\overset{H_0}{\\to}_d\\mathcal{N}(0,1),\\quad\\text{as}\\quad n\\to\\infty,\n\\end{align*}\n\\] and where \\[\n\\left[S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\\right]_{(k,k)}\n\\] denotes the element in the \\(k\\)th row and \\(k\\)th column of the \\(K\\times K\\) matrix \\(S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}.\\) This \\(t\\)-test statistic allows for heteroskedastic error terms.\n\nIn case of homoskedastic error terms, one can substitute \\[\n[S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{(k,k)}\n\\] by \\[\ns_{ub}^2[S_{X'X}^{-1}]_{(k,k)}\n\\] where \\([S_{X'X}^{-1}]_{(k,k)}\\) denotes the element in the \\(k\\)th row and \\(k\\)th column of the \\(K\\times K\\) matrix \\(S_{X'X}^{-1}.\\)\n\n\n\n\n\n\n\nFinite-sample correction\n\n\n\nIn order to improve the finite-sample performance of this \\(t\\) test, one usually uses the \\(t_{(n-K)}\\) distribution with \\(n-K\\) degrees of freedoms instead of the \\(\\mathcal{N}(0,1)\\) distribution.\nAsymptotically (\\(n\\to\\infty\\)), \\(t_{(n-K)}\\) is equivalent to \\(\\mathcal{N}(0,1)\\). However, for finite sample sizes \\(n\\) (i.e., the practically relevant case) \\(t_{n-K}\\) leads to larger critical values which helps to account for the estimation errors in \\([S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{(k,k)}\\) (or in \\(s_{ub}^2[S_{X'X}^{-1}]_{(k,k)}\\)) which are otherwise neglected by the pure asymptotic perspective.\n\n\n\n7.2.2.3 Robust Confidence Intervals\nFollowing the derivations in Chapter Section 6.4, but using the expression for the robust standard errors, we get the following heteroskedasticity robust (random) \\((1-\\alpha)\\cdot 100\\%\\) confidence interval \\[\n\\operatorname{CI}_{1-\\alpha}=\n\\left[\\hat\\beta_k\\pm t_{1-\\alpha/2,n-K}\\sqrt{n^{-1}[S_{X'X}^{-1}\\widehat{E}(\\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{(k,k)}}\\right].\n\\] Here, the coverage probability is an asymptotic coverage probability with \\(P(\\beta_k\\in\\operatorname{CI}_{1-\\alpha})\\to \\gamma\\) as \\(n\\to\\infty\\), where \\(\\gamma\\geq 1-\\alpha.\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Large Sample Inference</span>"
    ]
  },
  {
    "objectID": "07-Asymptotics.html#sec-MCLS",
    "href": "07-Asymptotics.html#sec-MCLS",
    "title": "7  Large Sample Inference",
    "section": "\n7.3 Monte Carlo Simulations",
    "text": "7.3 Monte Carlo Simulations\nLet’s apply the above asymptotic inference methods using R. As in Chapter Section 6.6 we, first, program a function myDataGenerator() which allows us to generate data from the following model, i.e., from the following fully specified data generating process: \\[\n\\begin{align*}\nY_i &=\\beta_1+\\beta_2X_{i2}+\\beta_3X_{i3}+\\varepsilon_i,\\qquad i=1,\\dots,n\\\\\n\\beta &=(\\beta_1,\\beta_2,\\beta_3)'=(2,3,4)'\\\\\nX_{i2}&\\sim U[-4,4]\\\\\nX_{i3}&\\sim U[-5,5]\\\\\n\\varepsilon_i|X_i&\\sim U[-0.5 |X_{i2}|, 0.5 |X_{i2}|],\n\\end{align*}\n\\] where \\((Y_i,X_i)\\) is assumed i.i.d. across \\(i=1,\\dots,n\\) with \\(X_{i2}\\) and \\(X_{i3}\\) being independent of each other.\nBy contrast to our simulations in Chapter Section 6.6, we consider here a non-Gaussian and heteroskedastic error term \\[\nVar(\\varepsilon_i|X_i)=\\frac{1}{12}X_{i2}^2.\n\\]\n\nAs a side note: The unconditional variance follows by the law of total variance and is given by \\[\\begin{align*}\nVar(\\varepsilon_i)\n&=E(Var(\\varepsilon_i|X_i))+Var(E(\\varepsilon_i|X_i))\\\\\n&=E\\left(\\frac{1}{12}X_{i2}^2\\right)+0\\\\\n&=\\frac{1}{12}\\;E\\left(X_{i2}^2\\right)\\\\\n&=\\frac{1}{12}\\;Var\\left(X_{i2}\\right)\\quad\\text{(since $E(X_{i2}=0)$)}\\\\\n&=\\frac{1}{12}\\left(\\frac{1}{12}(4-(-4))^2\\right)=\\frac{4}{9},\n\\end{align*}\\] where the last steps follows from applying the variance formula for uniform random variables. The following R-function myDataGenerator() allows us to generate data from the above described data generating processes:\n\n\n## Function to generate artificial data\nmyDataGenerator &lt;- function(n, beta){\n  ##\n  X   &lt;- cbind(rep(1, n), \n                 runif(n, -4, 4), \n                 runif(n, -5, 5))\n  ##\n  eps  &lt;- runif(n, min = - 0.5 * abs(X[,2]), \n                   max = + 0.5 * abs(X[,2]))\n  Y    &lt;- X %*% beta + eps\n  ##\n  data &lt;- data.frame(\"Y\"   = Y, \n                     \"X_1\" = X[,1], \n                     \"X_2\" = X[,2], \n                     \"X_3\" = X[,3])\n  ##\n  return(data)\n}\n\n\n7.3.1 Check: Distribution of \\(\\hat\\beta_n\\)\n\nThe above data generating process fulfills our regulatory assumptions of this chapter. So, by theory, the estimators \\(\\hat\\beta_k\\) should be normal distributed for sufficiently large sample sizes \\(n\\). \\[\n\\sqrt{n}\\left(\\hat\\beta_{n,k}-\\beta_k\\right)\\to_d\\mathcal{N}\\left(0,\\left[\\Sigma_{X'X}^{-1}E(\\varepsilon^2_iX_iX_i')\\Sigma_{X'X}^{-1}\\right]_{(k,k)}\\right)\n\\] \\[\n\\Rightarrow\\qquad\\hat\\beta_{n,k}\\overset{a}{\\sim}\\mathcal{N}\\left(\\beta_k, \\;n^{-1}\\;\\left[\\Sigma_{X'X}^{-1}E(\\varepsilon^2_iX_iX_i')\\Sigma_{X'X}^{-1}\\right]_{(k,k)}\\right).\n\\]\nFor our above specified data generating process, we can derive all (usually unknown) population quantities:\n\nFrom the assumed distributions of \\(X_{i2}\\) and \\(X_{i3}\\) we have that: \\[\n\\begin{align*}\n\\Sigma_{X'X}\n&=E(S_{X'X})\\\\[2ex]\n&=E(X_iX_i')\\\\[2ex]\n&=\\left(\\begin{matrix}1&0&0\\\\0&E(X_{i2}^2)&0\\\\0&0&E(X_{i3}^2)\\end{matrix}\\right)\\\\[2ex]\n&=\\left(\\begin{matrix}1&0&0\\\\0&\\frac{16}{3}&0\\\\0&0&\\frac{25}{3}\\end{matrix}\\right)\n\\end{align*}\n\\] The above result follows from observing that \\(E(X^2)=Var(X)\\) if \\(X\\) has mean zero, and that the variance of uniform \\(U[a,b]\\) distributed random variables is given by \\(\\frac{1}{12}(b-a)^2\\).\nMoreover, \\[\n\\begin{align*}\nE(\\varepsilon^2_iX_iX_i')\n& =E(X_iX_i'E(\\varepsilon^2_i|X_i))\\\\[2ex]\n& =E\\left(X_iX_i'\\left(\\frac{1}{12}X_{i2}^2\\right)\\right)\n\\end{align*}\n\\] such that \\[\n\\begin{align*}\nE(\\varepsilon^2_iX_iX_i')\n&=\\left(\\begin{matrix}E\\left(\\frac{1}{12}X_{i2}^2\\right)&0&0\\\\[2ex]\n                 0&E\\left(X_{i2}^2\\cdot\\frac{1}{12}X_{i2}^2\\right)&0\\\\0&0&E\\left(X_{i3}^2\\cdot\\frac{1}{12}X_{i2}^2\\right)\n    \\end{matrix}\\right)\\\\[2ex]\n&=\\left(\\begin{matrix}\\frac{1}{12}E\\left(X_{i2}^2\\right)&0&0\\\\\n     0&\\frac{1}{12}E\\left(X_{i2}^4\\right)&0\\\\0&0&\\frac{1}{12}E\\left(X_{i2}^2\\right)\\,E\\left(X_{i3}^2\\right)\n\\end{matrix}\\right)\\\\[2ex]      \n&=\\left(\\begin{matrix}\\frac{1}{12}\\frac{16}{3}&0&0\\\\\n                    0&\\frac{1}{12}\\frac{256}{5}&0\\\\\n                    0&0&\\frac{1}{12}\\frac{16}{3}\\frac{25}{3}\\end{matrix}\\right)\\\\[2ex]\n&=\\left(\\begin{matrix}\\frac{4}{9}&0&0\\\\0&\\frac{64}{15}&0\\\\0&0&\\frac{100}{27}\\end{matrix}\\right)\n\\end{align*}\n\\] The above result follow from observing that for \\(X\\sim U[a,b]\\) one has \\(E(X^k)=\\frac{b^{k+1}-a^{k+1}}{(k+1)(b-a)}\\), \\(k=1,2,\\dots\\); see, for instance, Wikipedia.\n\nSo, for instance, for \\(\\hat{\\beta}_2\\) we have the following theoretical large sample distribution: \\[\n\\begin{align}\n\\hat\\beta_{n,2}\\overset{a}{\\sim}&\\mathcal{N}\\left(\\beta_2, \\;\\frac{1}{n}\\;\\left[\\left(\\begin{matrix}1&0&0\\\\0&\\frac{16}{3}&0\\\\0&0&\\frac{25}{3}\\end{matrix}\\right)^{-1}\\left(\\begin{matrix}\\frac{4}{9}&0&0\\\\0&\\frac{64}{15}&0\\\\0&0&\\frac{100}{27}\\end{matrix}\\right)\\left(\\begin{matrix}1&0&0\\\\0&\\frac{16}{3}&0\\\\0&0&\\frac{25}{3}\\end{matrix}\\right)^{-1}\\right]_{22}\\right)\\\\[2ex]\n\\hat\\beta_{n,2}\\overset{a}{\\sim}&\\mathcal{N}\\left(\\beta_2, \\;\\frac{1}{n}\\;\\left[\n  \\left(\n  \\begin{matrix}\n  0.444 & 0 &0\\\\\n  0 & 0.15 &0\\\\\n  0&0&0.053\n  \\end{matrix}\\right)\\right]_{22}\\right)\\\\[2ex]\n\\hat\\beta_{n,2}\\overset{a}{\\sim}&\\mathcal{N}\\left(\\beta_2, \\;\\frac{1}{n}\\;0.15\\right)\n\\end{align}\n\\] Let’s use a Monte Carlo simulation to check how well the theoretical large sample (\\(n\\to\\infty\\)) distribution of \\(\\hat\\beta_2\\) works as an approximative distribution for a practical largish sample size of \\(n=100\\).\n\nset.seed(123)\nn           &lt;- 100      # a largish sample size\nbeta_true   &lt;- c(2,3,4) # true data vector\n\n## Mean and variance of the true asymptotic \n## normal distribution of beta_hat_2:\n# true mean\nbeta_true_2     &lt;- beta_true[2] \n# true variance\nvar_true_beta_2 &lt;- 0.15 / n\n\n## Let's generate 5000 realizations from beta_hat_2, and check \n## whether their distribution is close to the true normal \n## distribution.\n## (We don't condition on X since the theoretical limit \n## distribution is unconditional on X)\nrep        &lt;- 5000 # MC replications\nbeta_hat_2 &lt;- rep(NA, times=rep)\n##\nfor(r in 1:rep){\n    MC_data &lt;- myDataGenerator(n    = n, \n                               beta = beta_true)\n    lm_obj        &lt;- lm(Y ~ X_2 + X_3, data = MC_data)\n    beta_hat_2[r] &lt;- coef(lm_obj)[2]\n}\n\n## Compare:\n## True beta_2 versus average of beta_hat_2 estimates\nc(beta_true_2, round(mean(beta_hat_2), 4))\n\n[1] 3.0000 2.9998\n\n\nGood! As expected, the average of the 5000 simulated realizations of \\(\\hat\\beta_2\\) is basically equal to the theoretical true mean \\(E(\\hat\\beta_2)=\\beta_2=3\\) which indicates a bias of zero.\n\n## True variance of beta_hat_2 versus \n## empirical variance of beta_hat_2 estimates\nc(round(var_true_beta_2, 5), round(var(beta_hat_2), 5))\n\n[1] 0.00150 0.00147\n\n\nGreat! The variance of the 5000 simulated realizations of \\(\\hat\\beta_2\\) is basically equal to the theoretical true variance \\(Var(\\hat\\beta_{n,2})=0.15/n=0.0015\\).\n\n## True normal distribution of beta_hat_2 versus \n## empirical density of beta_hat_2 estimates\nlibrary(\"scales\")\ncurve(expr = dnorm(x, \n                   mean = beta_true_2, \n                   sd   = sqrt(var_true_beta_2)), \n      xlab = \"\", \n      ylab = \"\", \n      col  = gray(.2), lwd=3, lty=1, \n      xlim = range(beta_hat_2),\n      ylim = c(0,14.1),main=paste0(\"n=\",n))\n\nlines(density(beta_hat_2, \n              bw = bw.SJ(beta_hat_2)), \n      col = alpha(\"blue\",.5), lwd = 3)\n\nlegend(\"topleft\", lty=c(1,1), lwd = c(3,3), \n     col=c(gray(.2), alpha(\"blue\",.5)), bty=\"n\", \n     legend= \n     c(expression(\n       \"Theoretical (Asymptotic) Gaussian Density of\"~hat(beta)[2]), \n      expression(\n       \"Empirical Density Estimation based on MC realizations from\"~hat(beta)[2])))\n\n\n\n\n\n\n\nGreat! The nonparametric density estimation (estimated via density()) computed from the 5000 simulated realizations of \\(\\hat\\beta_2\\) is indicating that \\(\\hat\\beta_2\\) is really normally distributed as described by our theoretical results.\nHowever, is the asymptotic distribution of \\(\\hat\\beta_2\\) also usable for (very) small samples like \\(n=5\\)? Let’s check that:\n\nset.seed(123)\nn           &lt;- 5       # a small sample size\nbeta_true   &lt;- c(2,3,4) # true data vector\n\n## Mean and variance of the true asymptotic \n## normal distribution of beta_hat_2:\n# true mean\nbeta_true_2     &lt;- beta_true[2] \n# true variance\nvar_true_beta_2 &lt;- 0.15 / n\n\n## Let's generate 5000 realizations from beta_hat_2, and check \n## whether their distribution is close to the true normal \n## distribution.\n## (We don't condition on X since the theoretical limit \n## distribution is unconditional on X)\nrep        &lt;- 5000 # MC replications\nbeta_hat_2 &lt;- rep(NA, times=rep)\n##\nfor(r in 1:rep){\n    MC_data &lt;- myDataGenerator(n    = n, \n                               beta = beta_true)\n    lm_obj        &lt;- lm(Y ~ X_2 + X_3, data = MC_data)\n    beta_hat_2[r] &lt;- coef(lm_obj)[2]\n}\n\n## Compare:\n## True beta_2 versus average of beta_hat_2 estimates\nc(beta_true_2, round(mean(beta_hat_2), 4))\n\n[1] 3.0000 2.9963\n\n\nOK, at least on average the 5000 simulated realizations of \\(\\hat\\beta_2\\) are basically equal to the true mean \\(E(\\hat\\beta_2)=\\beta_2=3\\).\n\n## True variance of beta_hat_2 versus \n## empirical variance of beta_hat_2 estimates\nc(round(var_true_beta_2, 4), round(var(beta_hat_2), 4))\n\n[1] 0.0300 0.0562\n\n\nOuch! The theoretical variance \\(Var(\\hat\\beta_2)=0.15/n=0.03\\) is about 50% smaller than the actual (small sample) variance of \\(\\hat\\beta_2\\) approximated by the empirical variance of the 5000 simulated realizations of \\(\\hat\\beta_2\\). That is, we cannot simply use a large sample result in small samples.\nReason: In small samples, the Law of Large Numbers has not kicked in yet; therefore, we cannot neglect the variability in the sample statistic \\(S_{X'X}^{-1}.\\)\nThis issue can also seen when comparing the theoretical large sample distribution of \\(\\hat\\beta_2\\) with an estimate of the actual finite-sample distribution of \\(\\hat\\beta_2.\\)\n\n## True normal distribution of beta_hat_2 versus \n## empirical density of beta_hat_2 estimates\nlibrary(\"scales\")\ncurve(expr = dnorm(x, \n                   mean = beta_true_2, \n                   sd   = sqrt(var_true_beta_2)), \n      xlab = \"\", \n      ylab = \"\", \n      col  = gray(.2), lwd = 3, lty = 1, \n      xlim = c(2,4), \n      ylim = c(0,3), main=paste0(\"n=\",n))\n\nlines(density(beta_hat_2, \n              bw = bw.SJ(beta_hat_2)), \n      col = alpha(\"blue\",.5), lwd = 3)\n\n## Legend      \nlegend(\"topleft\", lty=c(1,1), lwd=c(3,3), \n       col=c(gray(.2), alpha(\"blue\",.5)), bty=\"n\", \n       legend= \nc(expression(\n  \"Theoretical (Asymptotic) Gaussian Density of\"~hat(beta)[2]), \n  expression(\n  \"Empirical Density Estimation based on MC realizations from\"~\n  hat(beta)[2])))      \n\n\n\n\n\n\n\nNot good. The actual finite-sample distribution has substantially fatter tails. That is, if we would use the quantiles of the asymptotic distribution, we would falsely reject the null-hypothesis too often (probability of type I errors would be larger than the significance level).\nFortunately, asymptotics are usually kicking in relatively fast; here, things become much more reliable already for \\(n\\geq 15\\).\n\n7.3.2 Check: Testing Multiple Parameters\nIn the following, we do inference about multiple parameters. We test the (here correct) null hypothesis \\[\\begin{align*}\nH_0:\\;&\\beta_2=3\\quad\\text{and}\\quad\\beta_3=4\\\\\n\\text{versus}\\quad H_1:\\;&\\beta_2\\neq 3\\quad\\text{and/or}\\quad\\beta_3\\neq 4.\n\\end{align*}\\] Or equivalently \\[\\begin{align*}\nH_0:\\;&R\\beta -r^{(0)} = 0 \\\\\nH_1:\\;&R\\beta -r^{(0)} \\neq 0,\n\\end{align*}\\] where \\[\nR=\\left(\n\\begin{matrix}\n0&1&0\\\\\n0&0&1\\\\\n\\end{matrix}\\right)\\quad\\text{ and }\\quad\nr^{(0)}=\\left(\\begin{matrix}3\\\\5\\\\\\end{matrix}\\right).\n\\] The following R code can be used to test this hypothesis. Note that we use HC3 robust variance estimation sandwich::vcovHC(lm_obj, type=\"HC3\") to take into account that the error terms are heteroskedastic.\n\nsuppressMessages(library(\"car\")) # for linearHyothesis()\n# ?linearHypothesis\nlibrary(\"sandwich\") # for vcovHC(), robust variance estimations\n\nset.seed(1009)\n\n## Generate data\nMC_data &lt;- myDataGenerator(n    = 100, \n                           beta = beta_true)\n\n## Estimate the linear regression model parameters\nlm_obj &lt;- lm(Y ~ X_2 + X_3, data = MC_data)\n\nvcovHC3_mat &lt;- sandwich::vcovHC(lm_obj, type=\"HC3\")\n\n## Option 1:\n# car::linearHypothesis(model = lm_obj, \n#                       hypothesis.matrix = c(\"X_2=3\", \"X_3=4\"), \n#                       vcov = vcovHC3_mat)\n\n## Option 2:\nR &lt;- rbind(c(0,1,0),\n           c(0,0,1))\ncar::linearHypothesis(model = lm_obj, \n                      hypothesis.matrix = R, \n                      rhs  = c(3,4),\n                      vcov = vcovHC3_mat)\n\nLinear hypothesis test\n\nHypothesis:\nX_2 = 3\nX_3 = 4\n\nModel 1: restricted model\nModel 2: Y ~ X_2 + X_3\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df     F Pr(&gt;F)\n1     99                \n2     97  2 0.032 0.9685\n\n\nThe large \\(p\\)-value does not allow us to reject the null-hypothesis at any of the usual significance levels. This is good, since we test here a correct null-hypothesis and rejecting it would mean to do a false rejection (type I error, false positive).\nOn average, however, there will be some false rejections of the null-hypothesis, but this type I error rate needs to be samler or equal to \\(\\alpha\\).\n\nset.seed(1110)\n\nB        &lt;- 10000 \np_values &lt;- numeric(B)\n\nfor(r in 1:B){ \n  MC_data     &lt;- myDataGenerator(n    = 100, beta = beta_true)\n  lm_obj      &lt;- lm(Y ~ X_2 + X_3, data = MC_data)\n  vcovHC3_mat &lt;- sandwich::vcovHC(lm_obj, type=\"HC3\")\n  Ftest       &lt;- car::linearHypothesis(model = lm_obj, \n                        hypothesis.matrix = c(\"X_2=3\", \"X_3=4\"), \n                        vcov = vcovHC3_mat)\n  p_values[r] &lt;- Ftest$'Pr(&gt;F)'[2]                      \n}\n## Nominal type I error rate \nalpha &lt;- 0.05 \n## Empirical type I error rate\nlength(p_values[p_values &lt; alpha])/B\n\n[1] 0.0562\n\n\nThis is more or less ok, but you see that the upper bound is not so strict here as it was in the small sample inference case in Section 6.6. A larger sample size \\(n\\geq 100\\) would improve this simulation result.\n\n7.3.3 Check: Testing Single Parameters\nNext, we do inference about a single parameter. We test \\[\\begin{align*}\nH_0:&\\beta_3=5\\\\\n\\text{versus}\\quad H_1:&\\beta_3\\neq 5.\n\\end{align*}\\]\n\n# Load libraries\nsuppressMessages(library(\"lmtest\"))  # for coeftest()\n\n## Generate data\nn &lt;- 100\nMC_data &lt;- myDataGenerator(n    = n, \n                           beta = beta_true)\n\n## Estimate the linear regression model parameters\nlm_obj &lt;- lm(Y ~ X_2 + X_3, data = MC_data)\n\n## Robust t test\n\n## Robust standard error for \\hat{\\beta}_3:\nSE_rob &lt;- sqrt(vcovHC(lm_obj, type = \"HC3\")[3,3])\n## hypothetical (H0) value of \\beta_3:\nbeta_3_H0 &lt;- 4\n## estimate for beta_3:\nbeta_3_hat &lt;- coef(lm_obj)[3]\n## robust t-test statistic\nt_test_stat &lt;- (beta_3_hat - beta_3_H0)/SE_rob\n## p-value\nK &lt;- length(coef(lm_obj))\n##\np_value &lt;- 2 * min(   pt(q = t_test_stat, df = n - K), \n                   1- pt(q = t_test_stat, df = n - K))\np_value\n\n[1] 0.5234886\n\n\nAgian, the large \\(p\\)-value does not allow us to reject the (here true) null-hypothesis at any of the usual significance levels. Let us check the empirical type I error rate.\n\nset.seed(1009)\nn        &lt;- 100\nB        &lt;- 10000 \np_values &lt;- numeric(B)\n\nfor(r in 1:B){ \n  MC_data     &lt;- myDataGenerator(n = n, beta = beta_true)\n  lm_obj      &lt;- lm(Y ~ X_2 + X_3, data = MC_data)\n  SE_rob      &lt;- sqrt(vcovHC(lm_obj, type = \"HC3\")[3,3])\n  beta_3_hat  &lt;- coef(lm_obj)[3]\n  ## robust t-test statistic\n  t_test_stat &lt;- (beta_3_hat - beta_3_H0)/SE_rob\n  p_values[r] &lt;- 2 * min(   pt(q = t_test_stat, df = n - length(beta_true)), \n                         1- pt(q = t_test_stat, df = n - length(beta_true)))\n}\n## Nominal type I error rate \nalpha &lt;- 0.05 \n## Empirical type I error rate\nlength(p_values[p_values &lt; alpha])/B\n\n[1] 0.0496\n\n\nGood! The empirical type I error rate is bounded from above by the nominal type I error rate \\(\\alpha\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Large Sample Inference</span>"
    ]
  },
  {
    "objectID": "07-Asymptotics.html#sec-RDLSInf",
    "href": "07-Asymptotics.html#sec-RDLSInf",
    "title": "7  Large Sample Inference",
    "section": "\n7.4 Real Data Example",
    "text": "7.4 Real Data Example\nIn the following, we revisit the read data study from Section 6.7. Now, we have the tools to allow for heteroskedastic and non-Gaussian errors.\n\n## The AER package contains a lot of datasets \nsuppressPackageStartupMessages(library(AER))\n\n## Attach the DoctorVisits data to make it usable\ndata(\"DoctorVisits\")\n\nlm_obj &lt;- lm(visits ~ gender + age + income, data = DoctorVisits)\n\nThe above R codes re-estimate the following regression model \\[\nY_i = \\beta_1 + \\beta_{gender} X_{gender,i}\n              + \\beta_{age} X_{age,i}\n              + \\beta_{income} X_{income,i} + \\varepsilon_i,\n\\] where \\(i=1,\\dots,n\\) and\n\n\n\\(X_{gender,i}=1\\) if the \\(i\\)th subject is a woman and \\(X_{gender,i}=0\\) if the \\(i\\)th subject is a man\n\n\\(X_{age,i}\\) is the age of subject \\(i\\) measured in years divided by \\(100\\)\n\n\n\\(X_{income,i}\\) is the annual income of subject \\(i\\) in tens of thousands of dollars\n\nNow, to do heteroskedasticity consistent robust inference, one can use the vcocHV() frunction from the R package sandwich together with the coeftest() function from the R package lmtest.\n\n## No robust standard errors:\ncoeftest(lm_obj)\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)   0.153710   0.036069  4.2616 2.066e-05 ***\ngenderfemale  0.062446   0.023455  2.6624  0.007782 ** \nage           0.402355   0.057131  7.0427 2.132e-12 ***\nincome       -0.082306   0.031670 -2.5989  0.009380 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## HC3 robust standard errors:\ncoeftest(lm_obj, vcov = vcovHC(lm_obj, type = \"HC3\"))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)   0.153710   0.034351  4.4747 7.815e-06 ***\ngenderfemale  0.062446   0.024582  2.5403   0.01110 *  \nage           0.402355   0.060304  6.6721 2.785e-11 ***\nincome       -0.082306   0.032321 -2.5465   0.01091 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nYou can do this also using the R package stargazer which gives you (almost) puplication ready regression output tables. The following R code produced two regression outputs—one without and one with robust standard errors:\n\nsuppressPackageStartupMessages(library(\"stargazer\"))\n\n# Adjust standard errors\ncov_beta_HC3   &lt;- vcovHC(lm_obj, type = \"HC3\")\nrobust_HC3_se  &lt;- sqrt(diag(cov_beta_HC3))\n\n# Stargazer output (with and without RSE)\nstargazer(lm_obj, lm_obj, \n          se   = list(NULL, robust_HC3_se),\n          column.labels = c(\"\", \"Robust SE\"),\n          type = \"html\") # alternatively: type = \"text\" or type = \"latex\"\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nvisits\n\n\n\n\n\n\n\n\nRobust SE\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\ngenderfemale\n\n\n0.062***\n\n\n0.062**\n\n\n\n\n\n\n(0.023)\n\n\n(0.025)\n\n\n\n\n\n\n\n\n\n\n\n\nage\n\n\n0.402***\n\n\n0.402***\n\n\n\n\n\n\n(0.057)\n\n\n(0.060)\n\n\n\n\n\n\n\n\n\n\n\n\nincome\n\n\n-0.082***\n\n\n-0.082**\n\n\n\n\n\n\n(0.032)\n\n\n(0.032)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.154***\n\n\n0.154***\n\n\n\n\n\n\n(0.036)\n\n\n(0.034)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n5,190\n\n\n5,190\n\n\n\n\nR2\n\n\n0.019\n\n\n0.019\n\n\n\n\nAdjusted R2\n\n\n0.018\n\n\n0.018\n\n\n\n\nResidual Std. Error (df = 5186)\n\n\n0.791\n\n\n0.791\n\n\n\n\nF Statistic (df = 3; 5186)\n\n\n33.218***\n\n\n33.218***\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Large Sample Inference</span>"
    ]
  },
  {
    "objectID": "07-Asymptotics.html#exercises",
    "href": "07-Asymptotics.html#exercises",
    "title": "7  Large Sample Inference",
    "section": "\n7.5 Exercises",
    "text": "7.5 Exercises\n\nExercises for Chapter 7\nExercises of Chapter 7 with Solutions",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Large Sample Inference</span>"
    ]
  },
  {
    "objectID": "03-Matrix-Algebra.html",
    "href": "03-Matrix-Algebra.html",
    "title": "\n3  Matrix Algebra\n",
    "section": "",
    "text": "3.1 Basic Definitions\nLet’s start with some basic definitions and specific examples.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "03-Matrix-Algebra.html#basic-definitions",
    "href": "03-Matrix-Algebra.html#basic-definitions",
    "title": "\n3  Matrix Algebra\n",
    "section": "",
    "text": "3.1.1 Scalar, Vector and Matrix\nA scalar \\(a\\) is a single real number. We write \\(a \\in \\mathbb R\\).\nA vector \\(a\\) of length \\(k\\) is a \\(k \\times 1\\) list of real numbers \\[\na = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_k \\end{pmatrix}.\n\\] By default, when we refer to a vector, we always mean a column vector. We write \\(a \\in \\mathbb R^k\\). The value \\(a_i\\) is called \\(i\\)-th entry or \\(i\\)-th component of \\(a\\). A scalar is a vector of length 1.\nA row vector of length \\(k\\) is written as \\[\nb = (b_1, \\ldots, b_k).\n\\]\nA matrix \\(A\\) of order (or dimension) \\(k \\times m\\) is a rectangular array of real numbers \\[\nA =\\begin{pmatrix}\n             a_{11} & a_{12} & \\cdots & a_{1m}\\\\\n             a_{21} & a_{22} & \\cdots & a_{2m}\\\\\n             \\vdots & \\vdots &       & \\vdots\\\\\n             a_{k1} & a_{k2} & \\cdots & a_{km}\n\\end{pmatrix}\n\\] with \\(k\\) rows and \\(m\\) columns. We write \\(A \\in \\mathbb R^{k \\times m}\\). The value \\(a_{ij}\\) is called \\((i,j)\\)-th entry or \\((i,j)\\)-th component of \\(A\\). We also use the notation \\((A)_{i,j}\\) to denote the \\((i,j)\\)-th entry. A vector of length \\(k\\) is a \\(k \\times 1\\) matrix. A row vector of length \\(k\\) is a \\(1 \\times k\\) matrix. A scalar is a matrix of order \\(1 \\times 1\\).\nWe may describe a matrix \\(A\\) by its column or row vectors as \\[\nA = \\begin{pmatrix} a_1 & a_2 & \\ldots & a_m \\end{pmatrix}\n= \\begin{pmatrix} \\alpha_1 \\\\ \\vdots \\\\ \\alpha_k \\end{pmatrix},\n\\] where \\[\na_i = \\begin{pmatrix} a_{1i} \\\\ \\vdots \\\\ a_{ki} \\end{pmatrix}\n\\] is the \\(i\\)-th column of \\(A\\) and \\[\n\\alpha_i = (a_{i1}, \\ldots, a_{im})\n\\] is the \\(i\\)-th row.\n\n3.1.2 Some Specific Matrices\nA matrix is called square matrix if the numbers of rows and columns coincide, i.e., \\(k=m.\\) For instance, \\[\n{B} = \\begin{pmatrix}1 & 2 \\\\ 3 & 4 \\end{pmatrix}\n\\] is a \\((2\\times 2)\\) square matrix. A square matrix is called diagonal matrix if all off-diagonal elements are zero. \\[\n{C} = \\begin{pmatrix}1 & 0 \\\\ 0 & 4 \\end{pmatrix}\n\\] is a diagonal matrix. We also write \\[\nC = \\mathop{\\mathrm{diag}}(1,4).\n\\] A square matrix is called upper triangular if all elements below the main diagonal are zero, and lower triangular if all elements above the main diagonal are zero. Examples of an upper triangular matrix \\(D\\) and a lower triangular matrix \\(E\\) are \\[\n{D} = \\begin{pmatrix}1 & 2 \\\\ 0 & 4 \\end{pmatrix}\n\\quad \\text{and}\\quad\n{E} = \\begin{pmatrix}1 & 0 \\\\ 3 & 4 \\end{pmatrix}.\n\\] The \\(k \\times k\\) diagonal matrix \\[\n{I}_k=\n      \\begin{pmatrix}\n      1      & 0      & \\cdots & 0\\\\\n      0      & 1      & \\cdots & 0\\\\\n      \\vdots & \\vdots & \\ddots & \\vdots\\\\\n      0      & 0      & \\cdots & 1\n      \\end{pmatrix} = \\mathop{\\mathrm{diag}}(1, \\ldots, 1)\n\\] is called identity matrix of order \\(k\\). The \\(k \\times m\\) matrix \\[\n\\underset{(k\\times m)}{0}\\;=0=\\begin{pmatrix}\n             0      & \\cdots & 0\\\\\n            \\vdots &  \\ddots      & \\vdots\\\\\n            0      & \\cdots & 0\n       \\end{pmatrix}\n\\] is called zero matrix.\nThe zero (column) vector of length \\(k\\) is \\[\n\\underset{(k\\times 1)}{0}\\; 0\n       =\\begin{pmatrix}\n             0   \\\\\n            \\vdots \\\\\n            0\n       \\end{pmatrix}.\n\\] If the order becomes clear from the context, we omit the indices and write \\(I\\) for the identity matrix and \\(0\\) for the zero matrix or zero vector.\n\n3.1.3 Transposition\nThe transpose \\(A'\\) of the matrix \\(A\\) is obtained by flipping rows and columns on the main diagonal: \\[\nA =\\begin{pmatrix}\n             a_{11} & a_{12} & \\cdots & a_{1m}\\\\\n             a_{21} & a_{22} & \\cdots & a_{2m}\\\\\n             \\vdots & \\vdots &       & \\vdots\\\\\n             a_{k1} & a_{k2} & \\cdots & a_{km}\n\\end{pmatrix}\n\\] \\[\n{A}'=\\begin{pmatrix}\n    a_{11} & a_{21} & \\cdots & a_{k1}\\\\\n    a_{12} & a_{22} & \\cdots & a_{k2}\\\\\n    \\vdots & \\vdots &       & \\vdots\\\\\n    a_{1m} & a_{2m} & \\cdots & a_{km}\n\\end{pmatrix}.\n\\] If \\(A\\) is a matrix of order \\(k\\times m\\), then \\(A'\\) is a matrix of order \\(m \\times k\\).\nExample: \\[\n{A}=\\begin{pmatrix}\n        1 & 2\\\\\n        4 & 5\\\\\n        7 & 8\n\\end{pmatrix}\\quad\\Rightarrow\\quad\n{A}'=\\begin{pmatrix}\n        1 & 4 & 7\\\\\n        2 & 5 & 8\n\\end{pmatrix}\n\\] The definition implies that transposing twice produces the original matrix: \\[\n(A')' = A.\n\\] The transpose of a (column) vector is a row vector: \\[\na' = (a_1, \\ldots, a_k)\n\\]\nA symmetric matrix is a square matrix \\(A\\) with \\({A}'={A}\\). An example of a symmetric matrix is \\[\nA = \\begin{aligned}         \\left(\\begin{matrix}1 & 2 \\\\ 2 & 4 \\end{matrix}\\right)\\end{aligned}.\n\\]\n\n3.1.4 Matrices in R\n\n\nA = matrix(c(1,4,7,2,5,8), nrow = 3, ncol = 2)\nA\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    4    5\n[3,]    7    8\n\nt(A) #transpose of A\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n\nA[3,2] #the (3,2)-entry of A\n\n[1] 8\n\nB = matrix(c(1,2,2,4), nrow = 2, ncol = 2) # another matrix\nall(B == t(B)) #check whether B is symmetric\n\n[1] TRUE\n\ndiag(c(1,4)) #diagonal matrix\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    4\n\ndiag(1, nrow = 3) #identity matrix\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\nmatrix(0, nrow=2, ncol=5) #matrix of zeros\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    0    0    0    0    0\n\ndim(A) #number of rows and columns\n\n[1] 3 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "03-Matrix-Algebra.html#sums-and-products",
    "href": "03-Matrix-Algebra.html#sums-and-products",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n3.2 Sums and Products",
    "text": "3.2 Sums and Products\n\n3.2.1 Matrix Summation\nLet \\(A\\) and \\(B\\) both be matrices of order \\(k \\times m\\). Their sum is defined componentwise: \\[\n{A} + {B}\n=\\begin{pmatrix}\na_{11}+ b_{11} & a_{12}+ b_{12} & \\cdots & a_{1m}+ b_{1m} \\\\\na_{21}+ b_{21} & a_{22}+ b_{22} & \\cdots & a_{2m}+ b_{2m} \\\\\n\\vdots        & \\vdots        &       & \\vdots        \\\\\na_{k1}+ b_{k1} & a_{k2}+ b_{k2} & \\cdots & a_{km}+ b_{km}\n\\end{pmatrix}.\n\\] Only two matrices of the same order can be added.\nExample: \\[\n{A}=\\begin{pmatrix}2&0\\\\1&5\\\\3&2\\end{pmatrix}\\,,\\quad\n{B}=\\begin{pmatrix}-1&1\\\\7&1\\\\-5&2\\end{pmatrix}\\,,\\quad\n{A}+{B}=\\begin{pmatrix}1&1\\\\8&6\\\\-2&4\\end{pmatrix}\\,.\n\\]\nThe matrix summation satisfies the following rules: \\[\n\\begin{array}{@{}rr@{\\ }c@{\\ }l@{}r@{}}\n\\text{(i)}   & {A}+{B}       &=& {B}+{A}\\,       & \\text{(commutativity)}   \\\\\n\\text{(ii)}  & ({A}+{B})+{C} &=& {A}+({B}+{C})\\, & \\text{(associativity)}   \\\\\n\\text{(iii)} & A + 0         &=& A               & \\text{(identity element)}\\\\\n\\text{(iv)}  & (A + B)'      &=& A' + B'         & \\text{(transposition)}          \n\\end{array}\n\\]\n\n3.2.2 Scalar-Matrix Multiplication\nThe product of a \\(k \\times m\\) matrix \\({A}\\) with a scalar \\(\\lambda\\in\\mathbb{R}\\) is defined componentwise: \\[\n\\lambda {A} =\n\\begin{pmatrix}\n   \\lambda a_{11}   & \\lambda a_{12} & \\cdots  & \\lambda a_{1n} \\\\\n   \\lambda a_{21}   & \\lambda a_{22} & \\cdots  & \\lambda a_{2n} \\\\\n   \\vdots           & \\vdots &                & \\vdots         \\\\\n   \\lambda a_{m1}   & \\lambda a_{m2} & \\cdots  & \\lambda a_{mn}\n\\end{pmatrix}.\n\\]\nExample: \\[\n\\lambda=2, \\quad {A}=\\begin{pmatrix}2&0\\\\1&5\\\\3&2\\end{pmatrix},\\quad\n\\lambda{A}=\\begin{pmatrix}4&0\\\\2&10\\\\6&4\\end{pmatrix}.\\] Scalar-matrix multiplication satisfies the distributivity law: \\[\\begin{array}{@{}rr@{\\ }c@{\\ }l@{}r@{}}\n\\text{(i)}    & \\lambda({A}+{B})&=& \\lambda{A}+\\lambda{B}\\, &    \\\\\n\\text{(ii)}   & (\\lambda+\\mu){A} &=& \\lambda{A}+\\mu{A}\\,     &\n\\end{array}\n\\]\n\n3.2.3 Element-by-Element Operations in R\nBasic arithmetic operations work on an element-by-element basis in R:\n\nA = matrix(c(2,1,3,0,5,2), ncol=2)\nB = matrix(c(-1,7,-5,1,1,2), ncol=2)\nA+B #matrix summation\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    8    6\n[3,]   -2    4\n\nA-B #matrix subtraction\n\n     [,1] [,2]\n[1,]    3   -1\n[2,]   -6    4\n[3,]    8    0\n\n2*A #scalar-matrix product\n\n     [,1] [,2]\n[1,]    4    0\n[2,]    2   10\n[3,]    6    4\n\nA/2 #division of entries by 2\n\n     [,1] [,2]\n[1,]  1.0  0.0\n[2,]  0.5  2.5\n[3,]  1.5  1.0\n\nA*B #element-wise multiplication\n\n     [,1] [,2]\n[1,]   -2    0\n[2,]    7    5\n[3,]  -15    4\n\n\n\n3.2.4 Vector-Vector Multiplication\n\n3.2.4.1 Inner Product\nThe inner product (also known as dot product) of two vectors \\({a},{b}\\in\\mathbb{R}^k\\) is \\[\n{a}'{b} = a_1 b_1+a_2b_2+\\ldots+a_kb_k=\\sum_{i=1}^k a_ib_i\\in\\mathbb{R}.\n\\]\nExample: \\[\n{a}=\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix},\\quad\n{b}=\\begin{pmatrix}-2\\\\0\\\\2\\end{pmatrix},\\quad\n{a}'{b}=1\\cdot(-2)+2\\cdot0+3\\cdot2=4.\n\\]\nThe inner product is commutative: \\[\n\\begin{align*}\n  a' b = b' a.\n\\end{align*}\n\\] Two vectors \\(a\\) and \\(b\\) are called orthogonal if \\[\na' b = 0.\n\\]\nThe vectors \\(a\\) and \\(b\\) are called orthonormal if, in addition to \\(a'b=0,\\) we have \\[\na' a = 1\\quad\\text{and}\\quad b' b=1.\n\\]\n\n3.2.4.2 Outer Product\nThe outer product (also known as dyadic product) of two vectors \\({x} \\in \\mathbb R^k\\) and \\({y}\\in\\mathbb{R}^m\\) is \\[\n{x}{y}' =\n\\left(\\begin{matrix}\nx_1 y_1 & x_1 y_2  &\\ldots & x_1 y_m \\\\\nx_2 y_1 & x_2 y_2 & \\ldots & x_2 y_m \\\\\n\\vdots   & \\vdots &      & \\vdots    \\\\\nx_k y_1 & x_k y_2 & \\ldots & x_k y_m\n\\end{matrix}\\right)\\in  \\mathbb{R}^{k \\times m}.\n\\] Example: \\[\n{x}=\\begin{pmatrix}1\\\\2\\end{pmatrix}\\,,\\quad\n{y}=\\begin{pmatrix}-2\\\\0\\\\2\\end{pmatrix}\\,,\\quad\n{x}{y}'=\\left(\\begin{matrix}\n-2 & 0 & 2 \\\\\n-4 & 0 & 4\n\\end{matrix}\\right).\n\\]\n\n3.2.4.3 Vector Multiplication in R\n\nFor vector multiplication in R, we use the operator %*% (recall that * is already reserved for element-wise multiplication). Let’s implement some multiplications.\n\ny = c(2,7,4,1) #y is treated as a column vector\nt(y) %*% y #the inner product of y with itself\n\n     [,1]\n[1,]   70\n\ny %*% t(y) #the outer product of y with itself\n\n     [,1] [,2] [,3] [,4]\n[1,]    4   14    8    2\n[2,]   14   49   28    7\n[3,]    8   28   16    4\n[4,]    2    7    4    1\n\nc(1,2) %*% t(c(-2,0,2)) #the example from above\n\n     [,1] [,2] [,3]\n[1,]   -2    0    2\n[2,]   -4    0    4\n\n\n\n3.2.5 Matrix-Matrix Multiplication\nThe matrix product of a \\(k \\times m\\) matrix \\({A}\\) and a \\(m \\times n\\) matrix \\({B}\\) is the \\(k\\times n\\) matrix \\(C = {A}{B}\\) with the components \\[\nc_{ij} = a_{i1}b_{1j}+a_{i2}b_{2j}+\\ldots+a_{im}b_{mj}=\\sum_{l=1}^m a_{il}b_{lj} = a_i' b_j,\n\\] where \\(a_i = (a_{i1}, \\ldots, a_{im})'\\) is the \\(i\\)-th row of \\(A\\) written as a column vector, and \\(b_j = (b_{1j}, \\ldots, b_{mj})'\\) is the \\(j\\)-th column of \\(B\\). The full matrix product can be written as \\[\nA B = \\begin{pmatrix} a_1' \\\\ \\vdots \\\\ a_k' \\end{pmatrix}\n\\begin{pmatrix} b_1 & \\ldots & b_n \\end{pmatrix}\n= \\begin{pmatrix} a_1' b_1 & \\ldots & a_1' b_n \\\\ \\vdots & & \\vdots \\\\ a_k' b_1 & \\ldots & a_k' b_n \\end{pmatrix}.\n\\] The matrix product is only defined if the number of columns of the first matrix equals the number of rows of the second matrix. Therefore, we say that the \\(k \\times m\\) matrix \\(A\\) and the \\(m \\times n\\) matrix \\(B\\) are conformable for matrix multiplication.\nExample: Let \\[\\begin{aligned}\n    {A}=\\begin{pmatrix}\n    1 & 0\\\\\n    0 & 1\\\\\n    2 & 1\n\\end{pmatrix}, \\quad\n{B}=\\begin{pmatrix}\n    -1 & 2\\\\\n    -3 & 0\n\\end{pmatrix}.\\end{aligned}\n\\] Their matrix product is \\[\n\\begin{aligned}\n{A} {B} &= \\begin{pmatrix}\n    1 & 0\\\\\n    0 & 1\\\\\n    2 & 1\n\\end{pmatrix} \\begin{pmatrix}\n    -1 & 2\\\\\n    -3 & 0\n\\end{pmatrix} \\\\ &= \\left(\\begin{matrix}1 \\cdot (-1) + 0 \\cdot (-3) & 1 \\cdot 2 + 0 \\cdot 0 \\\\ 0 \\cdot (-1) + 1 \\cdot (-3) & 0 \\cdot 2 + 1 \\cdot 0 \\\\ 2 \\cdot (-1) + 1 \\cdot (-3) & 2 \\cdot 2 + 1 \\cdot 0 \\end{matrix}\\right)\n= \\left(\\begin{matrix}-1 & 2 \\\\ -3 & 0 \\\\ -5 & 4 \\end{matrix}\\right).\\end{aligned}\n\\]\nThe %*% operator is used in R for matrix-matrix multiplications:\n\nA = matrix(c(1,0,2,0,1,1), ncol=2)\nB = matrix(c(-1,-3,2,0), ncol=2)\nA %*% B\n\n     [,1] [,2]\n[1,]   -1    2\n[2,]   -3    0\n[3,]   -5    4\n\n\nMatrix multiplication is not commutative. In general, we have \\(A B \\neq B A\\).\nExample: \\[\n\\begin{aligned}\n    {A}{B} = \\begin{pmatrix} 1 & 2\\\\ 3 & 4\\end{pmatrix}\n    \\begin{pmatrix} 1 & 1\\\\ 1 & 2\\end{pmatrix}\n    &=\n    \\begin{pmatrix} 3 & 5\\\\ 7 & 11\\end{pmatrix}\\,,\\\\\n    {B}{A} = \\begin{pmatrix} 1 & 1\\\\ 1 & 2\\end{pmatrix}\n    \\begin{pmatrix} 1 & 2\\\\ 3 & 4\\end{pmatrix}\n    &=\n    \\begin{pmatrix} 4 & 6\\\\ 7 & 10\\end{pmatrix}\\,.\\end{aligned}\n\\]\nEven if neither of the two matrices contains zeros, the matrix product can give the zero matrix: \\[\n{A}{B} = \\begin{pmatrix} 1 & 2\\\\ 2 & 4\\end{pmatrix}\n    \\begin{pmatrix} 2 & -4\\\\ -1 & 2\\end{pmatrix}\n    =\n    \\begin{pmatrix} 0 & 0\\\\ 0 & 0\\end{pmatrix}={0}.\n\\]\nThe following rules of calculation apply (provided the matrices are conformable): \\[\n\\begin{array}{rrcl@{}r@{}}\n\\text{(i)}   & {A}({B}{C})       & = & ({A}{B}){C}\\,    &\\text{(associativity)} \\\\\n\\text{(ii)}  &   {A}({B}+{D})    & = & {A}{B}+{A}{D}\\,  & \\text{(distributivity)} \\\\\n\\text{(iii)} &   ({B}+{D}){C}    & = & {B}{C}+{D}{C}\\,  & \\text{(distributivity)} \\\\\n\\text{(iv)}  &   {A}(\\lambda {B})& = & \\lambda({A}{B})\\,& \\text{(scalar ($\\lambda\\in\\mathbb{R}$) commutativity)}\\\\\n\\text{(v)}   & {A}{I}_{n}        & = & {A}\\,            & \\text{(identity element)}\\\\\n\\text{(vi)}  & {I}_{m}{A}        & = & {A}\\,            &\n\\text{(identity element)}     \\\\\n\\text{(vii)} &   ({A}{B})'  & = & {B}'{A}'\\,         & \\text{(product transposition)} \\\\\n\\text{(viii)}&   ({A}{B} C)'  & = & C' {B}'{A}'\\,  & \\text{(product transposition)}\n\\end{array}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "03-Matrix-Algebra.html#rank-and-inverse",
    "href": "03-Matrix-Algebra.html#rank-and-inverse",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n3.3 Rank and Inverse",
    "text": "3.3 Rank and Inverse\n\n3.3.1 Linear Combination\nLet \\(x_1, \\ldots, x_n\\) be vectors of the same order, and let \\(\\lambda_1, \\ldots, \\lambda_n\\) be scalars. The vector \\[\n\\lambda_1 x_1 + \\lambda_2 x_2 + \\ldots + \\lambda_n x_n\n\\] is called linear combination of \\(x_1, \\ldots, x_n\\). A linear combination can also be written as a matrix-vector product. Let \\(X =\\begin{pmatrix} x_1 & \\ldots & x_n \\end{pmatrix}\\) be the matrix with columns \\(x_1, \\ldots, x_n\\), and let \\(\\lambda = (\\lambda_1, \\ldots, \\lambda_n)'\\). Then, \\[\n\\lambda_1 x_1 + \\lambda_2 x_2 + \\ldots + \\lambda_n x_n = X \\lambda.\n\\] The vectors \\(x_1, \\ldots, x_n\\) are called linearly dependent if at least one can be written as a linear combination of the others. That is, there exists a nonzero vector \\(\\lambda\\) with \\[\nX \\lambda = \\lambda_1 x_1 + \\ldots + \\lambda_n x_n = 0.\n\\] The vectors \\(x_1, \\ldots, x_n\\) are called linearly independent if \\[\nX \\lambda = \\lambda_1 x_1 + \\ldots + \\lambda_n x_n \\neq 0\n\\] for all nonzero vectors \\(\\lambda\\).\nTo check whether the vectors are linearly independent, we can solve the system of equations \\[\nX \\lambda = 0\n\\] by Gaussian elimination. If \\(\\lambda = 0\\) is the only solution, then the columns of \\(X\\) are linearly independent. If there is a solution \\(\\lambda\\) with \\(\\lambda \\neq 0\\), then the columns of \\(X\\) are linearly dependent.\n\n3.3.2 Column Rank\nThe rank of a \\(k \\times m\\) matrix \\(A = \\begin{pmatrix} a_1 & \\ldots & a_m \\end{pmatrix}\\), written as \\(\\mathop{\\mathrm{rank}}(A)\\), is the number of linearly independent columns \\(a_i\\). We say that \\(A\\) has full column rank if \\(\\mathop{\\mathrm{rank}}(X) = m\\).\nThe identity matrix \\(I_k\\) has full column rank (i.e., \\(\\mathop{\\mathrm{rank}}(I_n) = k\\)). As another example, consider \\[\nX = \\begin{pmatrix} 2 & 1 & 4 \\\\ 0 & 1 & 2 \\end{pmatrix},\n\\] which has linearly dependent columns since the third column is a linear combination of the first two columns: \\[\n\\begin{align*}\n&\\qquad\\qquad\\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix} = 1 \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\\\[2ex]\n&\\Leftrightarrow\\quad - 1 \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\\\[2ex]\n&\\Leftrightarrow\\quad\\underbrace{\\begin{pmatrix} 2&1&4 \\\\ 0&1&2 \\end{pmatrix}}_{=X} \\underbrace{\\begin{pmatrix} -1 \\\\ -2\\\\1 \\end{pmatrix}}_{=\\lambda} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\\\\n\\end{align*}\n\\]\nHowever, the first two columns are linearly independent since \\(\\lambda_1 = 0\\) and \\(\\lambda_2 = 0\\) are the only solutions to the equation \\[\n\\begin{align*}\n&\\qquad\\qquad\n\\lambda_1 \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\lambda_2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\\\[2ex]\n&\\Leftrightarrow\\quad \\begin{pmatrix} 2&1 \\\\ 0&1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\\\\n\\end{align*}\n\\] Therefore, we have \\(\\mathop{\\mathrm{rank}}(X) = 2\\), i.e., \\(X\\) does not have a full column rank.\nSome useful properties are\n\n\\(\\mathop{\\mathrm{rank}}(A) \\leq \\min(k,m)\\)\n\\(\\mathop{\\mathrm{rank}}(A) = \\mathop{\\mathrm{rank}}(A')\\)\n\\(\\mathop{\\mathrm{rank}}(A B) = \\min(\\mathop{\\mathrm{rank}}(A), \\mathop{\\mathrm{rank}}(B))\\)\n\n\\(\\mathop{\\mathrm{rank}}(A) = \\mathop{\\mathrm{rank}}(A' A) = \\mathop{\\mathrm{rank}}(A A')\\).\n\nWe can use the qr() function to extract the rank in R. Let’s compute the rank of the matrices \\[\nA = \\begin{pmatrix}\n1 & 2 & 3 \\\\ 3 & 9 & 1 \\\\ 0 & 11 & 5\n\\end{pmatrix},\n\\] \\[\nB = \\begin{pmatrix}\n1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1\n\\end{pmatrix},\n\\] and \\(X\\) from the example above:\n\nA = matrix(c(1,3,0,2,9,11,3,1,5), nrow=3)\nqr(A)$rank\n\n[1] 3\n\nB = matrix(c(1,1,1,1,1,1,1,1,1), nrow=3)\nqr(B)$rank\n\n[1] 1\n\nX = matrix(c(2,0,1,1,4,2), ncol=3)\nqr(X)$rank\n\n[1] 2\n\n\n\n3.3.3 Nonsingular Matrix\nA square \\(k \\times k\\) matrix \\(A\\) is called nonsingular if it has full rank, i.e., \\[\n\\mathop{\\mathrm{rank}}(A) = k.\n\\] Conversely, \\(A\\) is called singular if it does not have full rank, i.e., \\[\n\\mathop{\\mathrm{rank}}(A) &lt; k.\n\\]\n\n3.3.4 Determinant\nConsider a square \\(k \\times k\\) matrix \\(A\\).\nThe determinant \\(\\det(A)\\) is a measure of the volume of the geometric object formed by the columns of \\(A\\)\n\na parallelogram for \\(k=2\\),\na parallelepiped for \\(k=3\\),\na hyper-parallelepiped for \\(k&gt;3\\).\n\nFor \\(2 \\times 2\\) matrices, the determinant is easy to calculate: \\[\nA = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}, \\quad\n\\det(A) = ad - bc.\n\\]\nIf \\(A\\) is (upper or lower) triangular, the determinant is the product of the diagonal entries, i.e., \\[\n\\det(A) = \\prod_{i=1}^k a_{ii}.\n\\] Hence, Gaussian elimination can be used to compute the determinant by transforming the matrix to a triangular one.\nThe exact definition of the determinant is technical and of little importance to us.\nA useful relation is the following: \\[\n\\begin{align*}\n  \\det(A) = 0 \\quad &\\Leftrightarrow \\quad A \\ \\text{is singular} \\\\\n   \\det(A) \\neq 0 \\quad &\\Leftrightarrow \\quad A \\ \\text{is nonsingular}.\n\\end{align*}\n\\]\nIn R, we have the det() function to compute the determinant:\n\ndet(A)\n\n[1] 103\n\ndet(B)\n\n[1] 0\n\n\nSince \\(\\det(A) \\neq 0\\) and \\(\\det(B) = 0\\), we conclude that\n\n\n\\(A\\) is nonsingular and\n\n\\(B\\) is singular.\n\n3.3.5 Inverse Matrix\nThe inverse \\({A}^{-1}\\) of a square \\(k \\times k\\) matrix \\(A\\) is defined by the property \\[{A} {A}^{-1} = {A}^{-1} {A} ={I}_k.\\] When multiplied from the left or the right, the inverse matrix produces the identity matrix.\nThe inverse exists if and only if \\({A}\\) is nonsingular, i.e., \\(\\det(A) \\neq 0\\). Therefore, a nonsingular matrix is also called invertible matrix. Note that only square matrices can be inverted.\nFor \\(2 \\times 2\\) matrices \\[\n{A} =\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\\,,\n\\] there exists a simple formula: \\[\n{A}^{-1} = \\frac{1}{\\det({A})} \\begin{pmatrix}d&-b\\\\-c&a\\end{pmatrix}\\,,\n\\] where \\(\\det({A}) = ad - bc\\). I.e., we swap the main diagonal elements, reverse the sign of the off-diagonal elements, and divide all entries by the determinant.\nExample: \\[\n\\displaystyle{A}=\\begin{pmatrix}5&6\\\\1&2\\end{pmatrix}\n\\] We have \\(\\det({A}) = ad-bc=5\\cdot2-6\\cdot1=4\\), and \\[\n{A}^{-1}= \\frac{1}{4} \\cdot \\begin{pmatrix}2&-6\\\\-1&5\\end{pmatrix}.\n\\] Indeed, \\(A^{-1}\\) is the inverse of \\(A\\) since \\[\n\\begin{align*}\n{A}{A}^{-1}\n& =\\begin{pmatrix}5&6\\\\1&2\\end{pmatrix} \\, \\frac{1}{4} \\, \\begin{pmatrix}2&-6\\\\-1&5\\end{pmatrix}\\\\[2ex]\n&=\\frac{1}{4} \\, \\begin{pmatrix}4&0\\\\0&4\\end{pmatrix}\n= \\left(\\begin{matrix}1 & 0 \\\\ 0 & 1 \\end{matrix}\\right)\n= {I}_2\n\\end{align*}\n\\] and since \\[\n\\begin{align*}\n{A}^{-1}{A}\n&= \\frac{1}{4} \\begin{pmatrix}2&-6\\\\-1&5\\end{pmatrix} \\begin{pmatrix}5&6\\\\1&2\\end{pmatrix} \\\\[2ex]\n&=\\frac{1}{4}  \\begin{pmatrix}4&0\\\\0&4\\end{pmatrix}\n= \\left(\\begin{matrix}1 & 0 \\\\ 0 & 1 \\end{matrix}\\right)\n= {I}_2\n\\end{align*}\n\\]\nOne way to calculate the inverse of higher order square matrices is to solve equation \\[\nA A^{-1} = I\n\\] with Gaussian elimination.\nR can compute the inverse matrix quickly using the function solve():\n\nA_inf &lt;- solve(A) # inverse of A\n\n\nA_inf %*% A \n\n              [,1]         [,2] [,3]\n[1,]  1.000000e+00 0.000000e+00    0\n[2,] -2.775558e-17 1.000000e+00    0\n[3,]  5.551115e-17 1.665335e-16    1\n\nA     %*% A_inf \n\n              [,1] [,2]         [,3]\n[1,]  1.000000e+00    0 1.387779e-17\n[2,] -5.551115e-17    1 1.387779e-17\n[3,]  0.000000e+00    0 1.000000e+00\n\n\nWe have the following relationships between\n\nlinear dependency,\ninvertibility,\nrank, and\ndeterminant\n\nof a \\(k\\times k\\) square matrix \\(A\\):\n\\[\n\\begin{align*}\n  &A \\ \\text{is nonsingular, i.e., invertible} \\\\[2ex]\n  \\Leftrightarrow \\quad &\\text{all $k$ columns of} \\ A \\ \\text{are linearly independent} \\\\[2ex]\n  \\Leftrightarrow \\quad &A \\ \\text{has full column rank} \\ (\\mathop{\\mathrm{rank}}(A)=k)\\\\[2ex]\n  \\Leftrightarrow \\quad &\\text{the determinant is nonzero} \\ (\\det(A) \\neq 0).\n\\end{align*}\n\\]\nSimilarly, \\[\n\\begin{align*}\n  &A \\ \\text{is singular, i.e., not invertible} \\\\[2ex]\n  \\Leftrightarrow \\quad &A \\ \\text{has linearly dependent columns} \\\\[2ex]\n  \\Leftrightarrow \\quad &A \\ \\text{does not have full rank} \\ (\\mathop{\\mathrm{rank}}(A)&lt;k)\\\\[2ex]\n  \\Leftrightarrow \\quad &\\text{the determinant is zero} \\ (\\det(A) = 0).\n\\end{align*}\n\\]\nImportant properties for nonsingular matrices:\n\n\\(({A}^{-1})^{-1} = {A}\\)\n\\(({A}')^{-1} = ({A}^{-1})'\\)\n\n\\((\\lambda{A})^{-1} = \\frac{1}{\\lambda}{A}^{-1}\\) for any scalar \\(\\lambda \\neq 0\\)\n\n\\(\\det(A^{-1}) = \\frac{1}{\\det(A)}\\)\n\\(({A} {B})^{-1} = {B}^{-1} {A}^{-1}\\)\n\\((A B C)^{-1} = C^{-1} B^{-1} A^{-1}\\)\nIf \\({A}\\) is symmetric, then \\({A}^{-1}\\) is symmetric.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "03-Matrix-Algebra.html#advanced-concepts",
    "href": "03-Matrix-Algebra.html#advanced-concepts",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n3.6 Advanced Concepts",
    "text": "3.6 Advanced Concepts\n\n3.6.1 Eigendecomposition\n\n3.6.1.1 Eigenvalues\nAn eigenvalue \\(\\lambda\\) of a \\(k \\times k\\) square matrix is a solution to the equation \\[\n  \\det(\\lambda I_k - A) = 0.\n\\] The function \\(f(\\lambda) = \\det(\\lambda I_k - A)\\) has exactly \\(k\\) roots so that \\(\\det(\\lambda I_k - A) = 0\\) has exactly \\(k\\) solutions. The solutions \\(\\lambda_1, \\ldots, \\lambda_k\\) are the \\(k\\) eigenvalues of \\(A\\).\nMost applications of eigenvalues in econometrics concern symmetric matrices. In this case, all eigenvalues are real-valued. In the case of non-symmetric matrices, some eigenvalues may be complex-valued.\nUseful properties of the eigenvalues of a symmetric \\(k \\times k\\) matrix are:\n\n\\(\\det(A) = \\lambda_1 \\cdot \\ldots \\cdot \\lambda_k\\)\n\\(\\mathop{\\mathrm{tr}}(A) = \\lambda_1 + \\ldots + \\lambda_k\\)\n\n\\(A\\) is nonsingular if and only if all eigenvalues are nonzero\n\n\\(A B\\) and \\(B A\\) have the same eigenvalues.\n\n3.6.1.2 Eigenvectors\nIf \\(\\lambda_i\\) is an eigenvalue of \\(A\\), then \\(\\lambda_i I_k - A\\) is singular, which implies that there exists a linear combination vector \\(v_i\\) with \\((\\lambda_i I_k - A) v_i = 0\\). Equivalently, \\[\nA v_i = \\lambda_i v_i,\n\\]\nwhich can be solved by Gaussian elimination. It is convenient to normalize any solution such that \\(v_i'v_i = 1\\). The solutions \\(v_1, \\ldots, v_k\\) are called eigenvectors of \\(A\\) to corresponding eigenvalues \\(\\lambda_1, \\ldots, \\lambda_k\\).\n\n3.6.1.3 Spectral Decomposition\nIf \\(A\\) is symmetric, then \\(v_1, \\ldots, v_k\\) are pairwise orthogonal (i.e., \\(v_i' v_j = 0\\) for \\(i \\neq j\\)). Let \\(V = \\begin{pmatrix} v_1 & \\ldots & v_k \\end{pmatrix}\\) be the \\(k \\times k\\) matrix of eigenvectors and let \\(\\Lambda = \\mathop{\\mathrm{diag}}(\\lambda_1, \\ldots, \\lambda_k)\\) be the \\(k \\times k\\) diagonal matrix with the eigenvalues on the main diagonal. Then, we can write \\[\n  A = V \\Lambda V',\n\\] which is called the spectral decomposition of \\(A\\). The matrix of eigenvalues can be written as \\(\\Lambda = V' A V\\).\n\n3.6.1.4 Eigendecomposition in R\n\nThe function eigen() computes the eigenvalues and corresponding eigenvectors.\n\nB=t(A)%*%A \nB #A'A is symmetric\n\n     [,1] [,2] [,3]\n[1,]   10   29    6\n[2,]   29  206   70\n[3,]    6   70   35\n\neigen(B) #eigenvalues and eigenvector matrix\n\neigen() decomposition\n$values\n[1] 234.827160  12.582227   3.590613\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.1293953 -0.5312592  0.8372697\n[2,] -0.9346164 -0.2167553 -0.2819739\n[3,] -0.3312839  0.8190121  0.4684764\n\n\n\n3.6.2 Definiteness Property and Eigenvalues\nThe definiteness property of a symmetric matrix \\(A\\) can be determined using its eigenvalues:\n\n\n\\(A\\) is positive definite  \\(\\Leftrightarrow\\)  all eigenvalues of \\(A\\) are strictly positive\n\n\\(A\\) is negative definite  \\(\\Leftrightarrow\\)   all eigenvalues of \\(A\\) are strictly negative\n\n\\(A\\) is positive semi-definite  \\(\\Leftrightarrow\\)   all eigenvalues of \\(A\\) are non-negative\n\n\\(A\\) is negative semi-definite  \\(\\Leftrightarrow\\)   all eigenvalues of \\(A\\) are non-positive\n\n\neigen(B)$values #B is positive definite (all eigenvalues positive)\n\n[1] 234.827160  12.582227   3.590613\n\n\n\n3.6.3 Cholesky Decomposition\nAny positive definite and symmetric matrix \\(B\\) can be written as \\[\n  B = P P',\n\\] where \\(P\\) is a lower triangular matrix with strictly positive diagonal entries \\(p_{jj} &gt; 0\\). This representation is called Cholesky decomposition. The matrix \\(P\\) is unique. For a \\(2 \\times 2\\) matrix \\(B\\) we have \\[\n\\begin{align*}\n\\begin{pmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{pmatrix}\n&= \\begin{pmatrix} p_{11} & 0 \\\\ p_{21} & p_{22} \\end{pmatrix}\n\\begin{pmatrix} p_{11} & p_{21} \\\\ 0 & p_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} p_{11}^2 & p_{11} p_{21} \\\\ p_{11} p_{21} & p_{21}^2 + p_{22}^2 \\end{pmatrix},\n\\end{align*}\n\\] which implies \\(p_{11} = \\sqrt{b_{11}}\\), \\(p_{21} = b_{21}/p_{11}\\), and \\(p_{22} = \\sqrt{b_{22} - p_{21}^2}\\). For a \\(3 \\times 3\\) matrix we obtain \\[\n\\begin{align*}\n\\begin{pmatrix} b_{11} & b_{12} & b_{31} \\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{pmatrix}\n= \\begin{pmatrix} p_{11} & 0 & 0 \\\\ p_{21} & p_{22} & 0 \\\\ p_{31} & p_{32} & p_{33} \\end{pmatrix}\n\\begin{pmatrix} p_{11} & p_{21} & p_{31} \\\\ 0 & p_{22} & p_{32} \\\\ 0 & 0 & p_{33}\\end{pmatrix} \\\\\n= \\begin{pmatrix} p_{11}^2 & p_{11} p_{21} & p_{11} p_{31} \\\\ p_{11} p_{21} & p_{21}^2 + p_{22}^2 & p_{21} p_{31} + p_{22} p_{32} \\\\ p_{11}p_{31} & p_{21}p_{31} + p_{22}p_{32} & p_{31}^2 + p_{32}^2  + p_{33}^2\\end{pmatrix},\n\\end{align*}\n\\] which implies\n\\[\\begin{gather*}\np_{11}=\\sqrt{b_{11}}, \\ \\ p_{21} = \\frac{b_{21}}{p_{11}}, \\ \\ p_{31} = \\frac{b_{31}}{p_{11}}, \\ \\ p_{22} = \\sqrt{b_{22}-p_{21}^2}, \\\\\np_{32}= \\frac{b_{32}-p_{21}p_{31}}{p_{22}}, \\ \\ p_{33} = \\sqrt{b_{33} - p_{31}^2 - p_{32}^2}.\n\\end{gather*}\\]\nLet’s compute the Cholesky decomposition of \\[\nB = \\begin{pmatrix} 1 & -0.5 & 0.6 \\\\ -0.5 & 1 & 0.25 \\\\ 0.6 & 0.25 & 1 \\end{pmatrix}\n\\] using the R function chol():\n\nB = matrix(c(1, -0.5, 0.6, -0.5, 1, 0.25, 0.6, 0.25, 1), ncol=3)\nchol(B)\n\n     [,1]       [,2]      [,3]\n[1,]    1 -0.5000000 0.6000000\n[2,]    0  0.8660254 0.6350853\n[3,]    0  0.0000000 0.4864840\n\n\n\n3.6.4 Vectorization\nThe vectorization operator \\(\\mathop{\\mathrm{vec}}()\\) stacks the matrix entries column-wise into a large vector. The vectorized \\(k \\times m\\) matrix \\(A\\) is the \\(km \\times 1\\) vector \\[\n\\mathop{\\mathrm{vec}}(A) = (a_{11}, \\ldots, a_{k1}, a_{12}, \\ldots, a_{k2}, \\ldots, a_{1m}, \\ldots, a_{km})'.\n\\]\n\nc(A) #vectorize the matrix A\n\n[1]  1  3  0  2  9 11  3  1  5\n\n\n\n3.6.5 Kronecker Product\nThe Kronecker product \\(\\otimes\\) multiplies each element of the left-hand side matrix with the entire matrix on the right-hand side. For a \\(k \\times m\\) matrix \\(A\\) and a \\(r \\times s\\) matrix \\(B\\), we get the \\(kr\\times ms\\) matrix \\[\nA \\otimes B = \\begin{pmatrix} a_{11}B & \\ldots & a_{1m}B \\\\ \\vdots & & \\vdots \\\\ a_{k1}B & \\ldots & a_{km}B \\end{pmatrix},\n\\] where each entry \\(a_{ij} B\\) is a \\(r \\times s\\) matrix.\n\nA %x% B #Kronecker product in R\n\n      [,1]  [,2] [,3] [,4]  [,5]  [,6] [,7]  [,8] [,9]\n [1,]  1.0 -0.50 0.60  2.0 -1.00  1.20  3.0 -1.50 1.80\n [2,] -0.5  1.00 0.25 -1.0  2.00  0.50 -1.5  3.00 0.75\n [3,]  0.6  0.25 1.00  1.2  0.50  2.00  1.8  0.75 3.00\n [4,]  3.0 -1.50 1.80  9.0 -4.50  5.40  1.0 -0.50 0.60\n [5,] -1.5  3.00 0.75 -4.5  9.00  2.25 -0.5  1.00 0.25\n [6,]  1.8  0.75 3.00  5.4  2.25  9.00  0.6  0.25 1.00\n [7,]  0.0  0.00 0.00 11.0 -5.50  6.60  5.0 -2.50 3.00\n [8,]  0.0  0.00 0.00 -5.5 11.00  2.75 -2.5  5.00 1.25\n [9,]  0.0  0.00 0.00  6.6  2.75 11.00  3.0  1.25 5.00\n\n\n\n3.6.6 Vector and Matrix Norm\nA norm \\(\\|\\cdot\\|\\) of a vector or a matrix is a measure of distance from the origin. The most commonly used norms are the Euclidean vector norm \\[\n  \\|a\\| = \\sqrt{a' a} = \\sqrt{\\sum_{i=1}^k a_i^2}\n\\] for \\(a \\in \\mathbb R^k\\), and the Frobenius matrix norm \\[\n  \\|A \\| = \\sqrt{\\sum_{i=1}^k \\sum_{j=1}^m a_{ij}^2}\n\\] for \\(A \\in \\mathbb R^{k \\times m}\\).\nA norm satisfies the following properties:\n\n\n\\(\\|\\lambda A\\| = |\\lambda| \\|A\\|\\) for any scalar \\(\\lambda\\) (absolute homogeneity)\n\n\\(\\|A + B\\| \\leq \\|A\\| + \\|B\\|\\) (triangle inequality)\n\n\\(\\|A\\| = 0\\) implies \\(A = 0\\) (definiteness)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "03-Matrix-Algebra.html#matrix-calculus",
    "href": "03-Matrix-Algebra.html#matrix-calculus",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n3.5 Matrix Calculus",
    "text": "3.5 Matrix Calculus\nLet \\(f(\\beta_1, \\ldots, \\beta_k) = f({\\beta})\\) be a twice-differential real-valued function that depends on some vector \\(\\beta = (\\beta_1, \\ldots, \\beta_k)'\\). Examples that frequently appear in econometrics are functions of the inner product form \\(f(\\beta) = a' \\beta\\), where \\(a \\in \\mathbb R^k\\), and functions of the sandwich form \\(f(\\beta) = \\beta' A \\beta\\), where \\(A \\in \\mathbb R^{k \\times k}\\).\n\n3.5.1 Gradient\nThe first derivatives vector or gradient is \\[\n\\frac{\\partial f(\\beta)}{\\partial{\\beta}}  = \\begin{pmatrix}\\frac{\\partial f(\\beta)}{\\partial \\beta_1} \\\\ \\vdots \\\\ \\frac{\\partial f(\\beta)}{\\partial \\beta_k} \\end{pmatrix}\n\\] If the gradient is evaluated at some particular value \\(\\beta = b\\), we write \\[\n\\frac{\\partial f}{\\partial{\\beta}}(b)\n\\]\nNote that an inner product can be viewed as a linear function in \\(\\beta\\in\\mathbb{R}^k\\) \\[\nf(\\beta) = a' \\beta\n\\] and a sandwich from as a quadratic function in \\(\\beta\\in\\mathbb{R}^k\\) \\[\nf(\\beta) = \\beta' A\\beta.\n\\] Useful properties for inner products and sandwich forms are \\[\n\\begin{align*}\n(i)& \\quad &&\\frac{\\partial (a' \\beta)}{\\partial \\beta}  = a \\\\[2ex]\n(ii)& \\quad &&\\frac{\\partial ( \\beta' A \\beta)}{\\partial \\beta}   = (A + A') \\beta.\n\\end{align*}\n\\]\n\n3.5.2 Hessian\nThe second derivatives matrix or Hessian is the \\(k \\times k\\) matrix \\[\n    \\frac{\\partial^2 f(\\beta)}{\\partial{\\beta }\\partial {\\beta}'}\n    =  \\begin{pmatrix}\\frac{\\partial^2 f(\\beta)}{\\partial \\beta_1 \\partial \\beta_1} & \\ldots & \\frac{\\partial^2 f(\\beta)}{\\partial \\beta_k \\partial \\beta_1} \\\\\n    \\vdots & & \\vdots \\\\\n    \\frac{\\partial^2 f(\\beta)}{\\partial \\beta_1 \\partial \\beta_k} & \\ldots & \\frac{\\partial^2 f(\\beta)}{\\partial \\beta_k \\partial \\beta_k}\n    \\end{pmatrix}.\n\\]\nIf the Hessian is evaluated at some particular value \\(\\beta = b\\), we write \\[\n\\frac{\\partial^2 f}{\\partial{\\beta }\\partial {\\beta}'}(b)\n\\]\nThe Hessian is symmetric. Each column of the Hessian is the derivative of the components of the gradient for the corresponding variable in \\(\\beta'\\):\n\\[\\begin{align*}\n\\frac{\\partial^2 f(\\beta)}{\\partial{\\beta }\\partial {\\beta}'}\n    &= \\frac{\\partial(\\partial f(\\beta)/\\partial \\beta)}{\\partial \\beta'} \\\\\n    &= \\Bigg[ \\frac{\\partial(\\partial f(\\beta)/\\partial \\beta)}{\\partial \\beta_1} \\ \\frac{\\partial(\\partial f(\\beta)/\\partial \\beta)}{\\partial \\beta_2} \\ \\ldots \\ \\frac{\\partial(\\partial f(\\beta)/\\partial \\beta)}{\\partial \\beta_n} \\Bigg]\n\\end{align*}\\]\nThe Hessian of a sandwich form function is \\[\n  \\frac{\\partial^2 ( \\beta' A \\beta)}{\\partial \\beta \\partial \\beta'}  = A + A'.\n\\]\n\n3.5.3 Optimization\nRecall the first-order (necessary) and second-order (sufficient) conditions for optimum (maximum or minimum) in the univariate case:\n\n\nFirst-order condition: the first derivative evaluated at the optimum is zero.\n\nSecond-order condition: the second derivative at the optimum is negative for a maximum and positive for a minimum.\n\nSimilarly, we formulate first and second-order conditions for a function \\(f(\\beta)\\). The first-order condition for an optimum (maximum or minimum) at \\(b\\) is \\[\n\\frac{\\partial f}{\\partial{\\beta}}(b)  = 0.\n\\] The second-order condition is \\[\n\\begin{align*}\n  &\\frac{\\partial^2 f}{\\partial{\\beta }\\partial {\\beta}'}(b) &gt; 0 \\quad \\text{for a minimum at} \\ b, \\\\\n  &\\frac{\\partial^2 f}{\\partial{\\beta }\\partial {\\beta}'}(b) &lt; 0 \\quad \\text{for a maximum at} \\ b.\n\\end{align*}\n\\] Recall that, in the context of matrices, the notation “\\(&gt; 0\\)” means positive definite, and “\\(&lt; 0\\)” means negative definite.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "03-Matrix-Algebra.html#further-concepts",
    "href": "03-Matrix-Algebra.html#further-concepts",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n3.4 Further Concepts",
    "text": "3.4 Further Concepts\n\n3.4.1 Trace\nThe trace of a \\(k \\times k\\) square matrix \\(A\\) is the sum of the diagonal entries: \\[\\mathop{\\mathrm{tr}}(A) = \\sum_{i=1}^n a_{ii}\\]\nExample: \\[\nA = \\begin{pmatrix}\n1 & 2 & 3 \\\\ 3 & 9 & 1 \\\\ 0 & 11 & 5\n\\end{pmatrix} \\quad \\Rightarrow \\quad \\mathop{\\mathrm{tr}}(A) = 1+9+5 = 15\n\\] In Rwe have\n\nA = matrix(c(1,3,0,2,9,11,3,1,5), nrow=3)\nsum(diag(A))  #trace = sum of diagonal entries\n\n[1] 15\n\n\nThe following properties hold for square matrices \\(A\\) and \\(B\\) and scalars \\(\\lambda\\):\n\n\\(\\mathop{\\mathrm{tr}}(\\lambda A) = \\lambda \\mathop{\\mathrm{tr}}(A)\\)\n\\(\\mathop{\\mathrm{tr}}(A + B) = \\mathop{\\mathrm{tr}}(A) + \\mathop{\\mathrm{tr}}(B)\\)\n\\(\\mathop{\\mathrm{tr}}(A') = \\mathop{\\mathrm{tr}}(A)\\)\n\\(\\mathop{\\mathrm{tr}}(I_k) = k\\)\n\nFor \\(A\\in \\mathbb R^{k \\times m}\\) and \\(B \\in \\mathbb R^{m \\times k}\\) we have \\[\n\\mathop{\\mathrm{tr}}(A B) = \\mathop{\\mathrm{tr}}(B A).\n\\]\nThis is often called the trace-trick.\n\n3.4.2 Idempotent Matrix\nThe square matrix \\(A\\) is called idempotent if \\(A A = A\\). The identity matrix is idempotent: \\(I_n I_n = I_n\\). Another example is the matrix \\[\nA = \\begin{pmatrix}\n4 & -1 \\\\ 12 & -3\n\\end{pmatrix}.\n\\] We have \\[\n\\begin{align*}\nA A\n&= \\begin{pmatrix}\n4 & -1 \\\\ 12 & -3\n\\end{pmatrix}\n\\begin{pmatrix}\n4 & -1 \\\\ 12 & -3\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n16-12 & -4+3 \\\\ 48-36 & -12+9\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n4 & -1 \\\\ 12 & -3\n\\end{pmatrix}\n= A.\n\\end{align*}\n\\]\n\n3.4.3 Definite Matrix\nThe \\(k \\times k\\) square matrix \\({A}\\) is called positive definite if \\[{c}'{Ac}&gt;0\\] holds for all nonzero vectors \\({c}\\in \\mathbb{R}^k\\). If \\[{c}'{Ac}\\geq 0\\]\nfor all vectors \\({c}\\in \\mathbb{R}^k\\), the matrix is called positive semi-definite. Analogously, \\(A\\) is called negative definite if \\({c}'{Ac}&lt;0\\) and negative semi-definite if \\({c}'{Ac}\\leq 0\\) for all nonzero vectors \\(c \\in \\mathbb R^k\\). A matrix that is neither positive semi-definite nor negative semi-definite is called indefinite\nThe matrix analog of a positive or negative number (scalar) is a positive definite or negative definite matrix. Therefore, we use the notation\n\n\n\\(A &gt; 0\\)  if \\(A\\) is positive definite\n\n\\(A &lt; 0\\)  if \\(A\\) is negative definite\n\n\\(A \\geq 0\\)  if \\(A\\) is positive semi-definite\n\n\\(A \\leq 0\\)  if \\(A\\) is negative semi-definite\n\nThe notation \\(A &gt; B\\) means that the matrix \\(A - B\\) is positive definite.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "04-Monte-Carlo-Simulations.html#example-sample-mean",
    "href": "04-Monte-Carlo-Simulations.html#example-sample-mean",
    "title": "\n4  Estimation Theory and Monte Carlo Simulations\n",
    "section": "\n4.4 Example: Sample Mean",
    "text": "4.4 Example: Sample Mean\nThe following R code contains a Monte Carlo simulation with \\(B = 10000\\) replications to approximate the bias, the variance, and the mean squared error for the sample mean \\[\n(\\hat\\theta_n=)\\bar{X}_n=\\sum_{i=1}^nX_i\n\\] Setup:\n\n\n\\(X_i\\overset{iid}{\\sim}F_X\\), \\(i=1,\\dots,n\\), with \\(F_X=\\mathcal{N}(\\mu,\\sigma^2)\\)\n\nMean \\(\\mu=10\\)\n\nVariance \\(\\sigma^2=5\\)\n\nSample sizes \\(n\\in\\{5,15,50\\}\\) \n\n\n\n## Set seed for the random number generator to get reproducible results\nset.seed(3)\n## True parameter value ('theta' here 'mu')\nmu            &lt;- 10\n## Number of Monte Carlo repetitions:\nB             &lt;- 10000\n## Sequence of different sample sizes:\nn_seq         &lt;- c(5, 15, 50)\n\n## Function that generates estimator realizations \nmy_estimates_generator &lt;- function(n){\n  X_sample &lt;- rnorm(n = n, mean = mu, sd = sqrt(5))\n  ## compute the sample mean realization\n  return(mean(X_sample))\n}\n\nestimates_mat &lt;- cbind(\n  replicate(B, my_estimates_generator(n = n_seq[1])),\n  replicate(B, my_estimates_generator(n = n_seq[2])),\n  replicate(B, my_estimates_generator(n = n_seq[3]))\n)\n\n## Bias of the sample mean for different sample sizes n\nMC_Bias_n_seq &lt;- apply(estimates_mat, 2, mean) - mu\n\n## Variance of the sample mean for different sample sizes n\nMC_Var_n_seq  &lt;- apply(estimates_mat, 2, var)\n\n## Mean squared error of the sample mean for different sample sizes n\nMC_MSE_n_seq  &lt;- apply(estimates_mat, 2, function(x){mean((x - mu)^2)})\n\nTable 4.1 shows the Monte Carlo approximations for the bias, the variance, and the mean squared error of \\(\\bar{X}_n.\\)\n\n\n\nTable 4.1: Monte Carlo approximations for the true bias, true variance, and true mean squared error of sample mean.\n\n\n\n\nn\nBias (MC-Sim)\nVariance (MC-Sim)\nMSE (MC-Sim)\n\n\n\n5\n-0.001\n1.02\n1.02\n\n\n15\n0.000\n0.33\n0.33\n\n\n50\n0.001\n0.10\n0.10\n\n\n\n\n\n\n\n\nThe Monte Carlo approximations in Table 4.1 indicate that:\n\nThe bias \\(\\operatorname{Bias}(\\bar{X}_n)\\) is effectively zero for all sample sizes \\(n\\in\\{5,15,50\\}\\)\nThe mean squared error \\(\\operatorname{MSE}(\\bar{X}_n)\\) is decreasing as the sample size \\(n\\) gets larger.\nThe log-log plot in Figure 4.1 suggests a convergence rate of \\(-1\\) for \\(\\operatorname{MSE}(\\bar{X}_n)\\) i.e. \\[\n\\begin{align*}\n\\operatorname{MSE}(\\bar{X}_n) &= \\texttt{Constant} \\cdot n^{-1}\\\\[2ex]\n\\Leftrightarrow\\log\\left(\\operatorname{MSE}(\\bar{X}_n)\\right) &= \\log\\left(\\texttt{Constant}\\right) -1 \\cdot \\log\\left(n\\right),\n\\end{align*}\n\\] where \\(\\texttt{Constant}\\) depends on the data generating process and the estimator, but not on the sample size \\(n\\).\n\n\nplot(y    = MC_MSE_n_seq,  \n     x    = n_seq, \n     type = \"o\", \n     log  = \"xy\", \n     ylab = \"MSE (log scale)\",\n     xlab = \"n (log scale)\")\n\n## Slope: \n(log(0.1) - log(1)) / (log(50) - log(5))\n\n[1] -1\n\n\n\n\n\n\n\nFigure 4.1: Plotting the logarithm of the MSE-values against the logarithm of the sample size values \\(n\\) allows to approximate the convergence rate of the MSE-values as the sample size increases by the slope of the graph.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estimation Theory and Monte Carlo Simulations</span>"
    ]
  },
  {
    "objectID": "05-Multiple-Linear-Regression.html",
    "href": "05-Multiple-Linear-Regression.html",
    "title": "5  Multiple Linear Regression",
    "section": "",
    "text": "5.1 Assumptions\nThe multiple linear regression model is defined by the following assumptions which together describe the relevant theoretical aspects of the underlying data generating process:\nAssumption 1: Model and Sampling\nPart (a): Linear Model\n\\[\n\\begin{align}\n  Y_i=\\sum_{k=1}^K\\beta_k X_{ik}+\\varepsilon_i, \\quad i=1,\\dots,n.\n\\end{align}\n\\qquad(5.1)\\] Usually, a constant (intercept) is included, in this case \\(X_{i1}=1\\) for all \\(i=1,\\dots,n.\\) In the following we will always assume that \\(X_{i1}=1\\) for all \\(i\\), unless otherwise stated.\nIt is convenient to write Equation 5.1 using matrix notation \\[\n\\begin{eqnarray*}\n  Y_i&=&\\underset{(1\\times K)}{X_i'}\\underset{(K\\times 1)}{\\beta} +\\varepsilon_i, \\quad i=1,\\dots,n,\n\\end{eqnarray*}\n\\] where \\[\nX_i=\\left(\\begin{matrix}X_{i1}\\\\ \\vdots\\\\  X_{iK}\\end{matrix}\\right)\n\\quad\\text{and}\\quad\n\\beta=\\left(\\begin{matrix}\\beta_1\\\\ \\vdots\\\\ \\beta_K\\end{matrix}\\right).\n\\] Stacking all individual rows \\(i=1,\\dots,n\\) leads to \\[\n\\begin{eqnarray*}\\label{LM}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\n\\] where \\[\n\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\n\\]\nPart (b): Random Sample\nMoreover, we assume that the observed (“obs”) data points \\[\n((Y_{1,obs},X_{11,obs},\\dots,X_{1K,obs}),(Y_{2,obs},X_{21,obs},\\dots,X_{2K,obs}),\\dots,(Y_{n,obs},X_{n1,obs},\\dots,X_{nK,obs}))\n\\] are a realization of the random sample \\[\n((Y_{1},X_{11},\\dots,X_{1K}),(Y_{2},X_{21},\\dots,X_{2K}),\\dots,(Y_{n},X_{n1},\\dots,X_{nK})).\n\\]\nThat is, the \\(i\\)th observed \\(K+1\\) dimensional data point \\[(Y_{i,obs},X_{i1,obs},\\dots,X_{iK,obs})\\in\\mathbb{R}^{K+1}\n\\] is a realization of a \\(K+1\\) dimensional random variable \\[\n(Y_{i},X_{i1},\\dots,X_{iK})\\in\\mathbb{R}^{K+1},\n\\] where\nAssumption 2: Exogeneity \\[\nE(\\varepsilon_i|X_i)=0,\\quad i=1,\\dots,n\n\\qquad(5.2)\\] This assumption demands that the mean of the random error term \\(\\varepsilon_i\\) is zero irrespective of the realizations of \\(X_i\\). This exogeneity assumption is also called\nAssumption 3: Rank Condition (no perfect multicollinearity)\n\\[\n\\begin{align*}\n\\operatorname{rank}(X)&=K\\quad\\text{a.s.}\\\\\n\\Leftrightarrow P\\big(\\operatorname{rank}(X)&=K\\big)=1\n\\end{align*}\n\\] This assumption demands that, with probability one, no predictor variable \\(X_{k}\\in\\mathbb{R}^n\\) is linearly dependent of the others. (This is the literal translation of the “almost surely (a.s.)” concept.)\nNote: The assumption implies that \\(n\\geq K,\\) since \\[\n\\operatorname{rank}(X)\\leq \\min\\{n,K\\}\\quad(a.s.)\n\\]\nThis rank assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: dummy variable trap, multicollinearity, variance inflation). The violation of this assumption harms any economic interpretation since we cannot disentangle the explanatory variables’ individual effects on \\(Y\\). Therefore, this assumption is also often called an identification assumption.\nAssumption 4: Error distribution\nDepending on the context (i.e., parameter estimation vs. hypothesis testing and small \\(n\\) vs. large \\(n\\)) there are different more or less restrictive assumptions. Some of the most common ones are the following:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-Multiple-Linear-Regression.html#sec-LinModAssumptions",
    "href": "05-Multiple-Linear-Regression.html#sec-LinModAssumptions",
    "title": "5  Multiple Linear Regression",
    "section": "",
    "text": "\\(Y_i\\) is called “dependent variable” or “outcome variable” or “regressand”.\n\n\\(X_{ik}\\) is called the \\(k\\)th “independent variable” or “predictor variable” or “regressor” or “explanatory variable” or “control variable.”\n\n\\(\\varepsilon_i\\) denotes the statistical error term.\n\n\n\n\n\n\n\n\nSimple Linear Regression and Polynomial Regression Model\n\n\n\nThe special case of \\(K=2\\) \\[\nY_i = \\beta_1 + \\beta_2 X_{i2} + \\varepsilon_i\n\\] is called the simple linear regression model. With the simple linear regression model, only straight line fits are possible.\nBy contrast, with the multiple linear regression model, we can also fit polynomials. For instance, we can define \\[\nX_{i3} := X_{i2}^2\n\\] which leads to a quadratic regression model (often used for life-cycle analyses that include the predictor Age\\(_i=X_{i2}\\)) \\[\nY_i = \\beta_1 + \\beta_2 X_{i2} + \\beta_3 X_{i2}^2 + \\varepsilon_i.\n\\] Of course, further predictor variables \\(X_{i3},\\dots,X_{iK}\\) can (and should) be added to this model.\nThe same logic applies to polynomials with higher polynomial degrees \\((\\geq 2).\\) Large polynomial degrees, however, can lead to unstable estimation results.\n\n\n\n\n\n\n\n\\((Y_{i},X_{i1},\\dots,X_{iK})\\) has the identical \\(K+1\\) dimensional distribution for all \\(i=1,\\dots,n.\\)\n\n\n\\((Y_{i},X_{i1},\\dots,X_{iK})\\) is independent of \\((Y_{j},X_{j1},\\dots,X_{jK})\\) for all \\(i\\neq j=1,\\dots,n.\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nDue to Equation 5.1, this i.i.d. assumption is equivalent to assuming that the multivariate random variables \\[\n(\\varepsilon_i,X_{i1},\\dots,X_{iK})\\in\\mathbb{R}^{K+1}\n\\] are i.i.d. across \\(i=1,\\dots,n\\).\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemark: Usually, we do not use a different notation for observed realizations \\((Y_{i,obs},X_{i1,obs},\\dots,X_{iK,obs})\\in\\mathbb{R}^{K+1}\\) and for the corresponding random variable \\((Y_{i},X_{i1},\\dots,X_{iK})\\in\\mathbb{R}^{K+1}\\) since often both interpretations (random variable and its realizations) can make sense in the same statement and then it depends on the considered context whether the random variables point if view or the realization point of view applies.\n\n\n\n\n\n\n\n“orthogonality assumption” or\n“mean independence assumption.”\n\n\n\n\n\n\n\nNote\n\n\n\nTogether with the random sample assumption (Assumption 1, Part (b)) Equation 5.2 even implies strict exogeneity \\[\nE(\\varepsilon|X) = \\underset{(n\\times 1)}{0}\n\\] since we have independence across \\(i=1,\\dots,n\\). Under strict exogeneity, the mean of the random vector \\(\\varepsilon\\) is zero irrespective of the realizations of the \\((n\\times K)\\)-dimensional random predictor matrix \\(X.\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nUnder Assumption 3, we have that \\(\\operatorname{rank}(X)=K\\) (a.s.)\nThis implies that the \\((K\\times K)\\)-dimensional matrix \\(X'X\\) has full rank, i.e. that \\[\n\\operatorname{rank}(X'X)=K\\quad\\text{(a.s.)}\n\\]\nThus \\((X'X)\\) is invertible; i.e. there exists a \\((K\\times K)\\)-dimensional matrix \\((X'X)^{-1}\\) such that \\[\n(X'X)(X'X)^{-1} = (X'X)^{-1}(X'X) = I_K.\n\\]\n\n\n\n\n\n\n\nConditional distribution: \\[\n\\varepsilon_i|X_i \\sim f_{\\varepsilon|X}\n\\] for all \\(i=1,\\dots,n\\) and for any distribution \\(f_{\\varepsilon|X}\\) with two (or more) finite moments.\n\n\nExample: Conditional normal distribution \\[\n\\varepsilon_i|X_i \\sim \\mathcal{N}(0,\\sigma^2(X_i))\n\\] for all \\(i=1,\\dots,n\\).\n\n\n\nIndependence between error and predictors: \\(\\varepsilon_i\\sim f_\\varepsilon\\) for all \\(i=1,\\dots,n\\) such that \\(f_\\varepsilon=f_{\\varepsilon|X}\\) and such that \\(f_\\varepsilon\\) has two (or more) finite moments.\n\n\nExample: Independence between Gaussian error and predictors \\[\n    \\varepsilon_i\\sim \\mathcal{N}(0,\\sigma^2),\n    \\] where \\(\\sigma^2\\) is independent of \\(X.\\)\n\n\n\n\nSpherical errors (“Gauss-Markov assumptions”): The conditional distributions of \\(\\varepsilon_i|X_i\\) may generally depend on \\(X_i,\\) but only such that \\[\nE(\\varepsilon|X)=\\underset{(n\\times 1)}{0}\n\\] and \\[\n\\begin{align*}\n&\\underset{(n\\times n)}{Var\\left(\\varepsilon|X\\right)}=\\\\[2ex]\n& = \\left(\\begin{matrix}\nVar(\\varepsilon_1|X)&Cov(\\varepsilon_1,\\varepsilon_2|X)&\\dots&Cov(\\varepsilon_1,\\varepsilon_n|X)\\\\\nCov(\\varepsilon_2,\\varepsilon_1|X)&Var(\\varepsilon_2|X)&\\dots&Cov(\\varepsilon_2,\\varepsilon_n|X)\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nCov(\\varepsilon_n,\\varepsilon_1|X)&Cov(\\varepsilon_n,\\varepsilon_2|X)&\\dots&Var(\\varepsilon_n|X)\n\\end{matrix}\\right)\\\\[2ex]\n& = \\left(\\begin{matrix}\n\\sigma^2&0&\\dots&0\\\\\n0&\\sigma^2&\\dots&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n0&0&\\dots&\\sigma^2\n\\end{matrix}\\right)\n= \\sigma^2 I_n,\n\\end{align*}\n\\] where \\(I_n\\) denotes the \\((n\\times n)\\) identity matrix. Under the spherical errors assumption, one has, for all possible realizations of \\(X\\), that:\n\n\nuncorrelated: \\[\n  Cov(\\varepsilon_i,\\varepsilon_j|X)=0\n  \\] for all \\(i=1,\\dots,n\\) and all \\(j=1,\\dots,n\\) such that \\(i\\neq j\\)\n\n\nhomoskedastic: \\[\n  Var(\\varepsilon_i|X)=\\sigma^2\n  \\] for all \\(i=1,\\dots,n\\)\n\n\n\n\nHomoskedastic versus Heteroskedastic Error Terms\nThe i.i.d. assumption is not as restrictive as it may seem on first sight. It allows for dependence between \\(\\varepsilon_i\\) and \\((X_{i1},\\dots,X_{iK})\\in\\mathbb{R}^K\\). That is, the error term \\(\\varepsilon_i\\) can have a conditional distribution which depends on \\((X_{i1},\\dots,X_{iK})\\); see Section 2.2.2.5.\n\n\nThe exogeneity assumption (Assumption 2: Exogeneity) requires that the conditional mean of \\(\\varepsilon_i\\) is independent of \\(X_i\\). Besides this, dependencies between \\(\\varepsilon_i\\) and \\(X_{i1},\\dots,X_{iK}\\) are allowed. For instance, the variance of \\(\\varepsilon_i\\) can be a function of \\(X_{i1},\\dots,X_{iK}.\\) If this is the case, \\(\\varepsilon_i\\) is said to be “heteroskedastic.”\n\n\n\n\n\n\n\nDefinition 5.1 (Heteroskedastic Error Terms) The error terms \\(\\varepsilon_i\\) are called heteroskedastic if the conditional variance \\[\nVar(\\varepsilon_i|X_i=x_i)=\\sigma^2(x_i)\n\\] equals a non-constant variance function \\(\\sigma^2(x_i)&gt;0\\) for every possible realization \\(X_i=x_i.\\)\n\n\n\n\nExample: \\[\n\\varepsilon_i|X_i\\sim U[-0.5|X_{i2}|, 0.5|X_{i2}|],\n\\] with \\[\nX_{i2}\\sim U[-4,4]\n\\] for all \\(i=1,\\dots,n.\\) This error term is mean independent of \\(X_i\\) since \\(E(\\varepsilon_i|X_i)=0,\\) but it has a heteroskedastic conditional variance since \\[\nVar(\\varepsilon_i|X_i)=\\frac{1}{12}X_{i2}^2\n\\] depends on \\(X_{i2}.\\)\nSometimes, we need to be more restrictive by assuming that also the variances of the error terms \\(\\varepsilon_i\\) are independent of \\(X_i;\\) for instance, to do small sample inference (see Chapter 6).\n\n\n\n\n\n\n\nDefinition 5.2 (Homoskedastic Error Terms) The error terms \\(\\varepsilon_i\\) are called homoskedastic if the conditional variance \\[\nVar(\\varepsilon_i|X_i=x_i)=\\sigma^2\n\\] equals some constant \\(\\sigma^2&gt;0\\) for every possible realization \\(X_i=x_i.\\)\n\n\n\n\nExample: \\[\n\\varepsilon_i\\sim{\\mathcal N} (0, \\sigma^2)\n\\] for all \\(i=1,\\dots,n.\\) Here, the conditional variance of the error terms \\(\\varepsilon_i\\) given \\(X_i\\) \\[\nVar(\\varepsilon_i|X_i)=Var(\\varepsilon_i)=\\sigma^2\n\\] are equal to the constant \\(\\sigma^2&gt;0\\) for all \\(i=1,\\dots,n\\) and for every possible realization of \\(X_i.\\)\n\n5.1.1 Some Implications of the Exogeneity Assumption (Ass 2)\n\n\n\n\n\n\n\nTheorem 5.1 (Unconditional Mean) If \\(E(\\varepsilon_i|X_i)=0\\) for all \\(i=1,\\dots,n,\\) then the also the unconditional mean of the error term is zero, i.e. \\[\nE(\\varepsilon_i)=0,\\quad i=1,\\dots,n.\n\\]\n\n\n\n\n\nProof. Using the Law of Total Expectations (i.e., \\(E[E(Z|X)]=E(Z)\\)) we can rewrite \\(E(\\varepsilon_i)\\) as \\[\nE(\\varepsilon_i)=E[E(\\varepsilon_i|X_i)]\n\\] for all \\(i=1,\\dots,n.\\) But the exogeneity assumption yields \\[\nE[E(\\varepsilon_i|X_i)]=E[0]=0\n\\] for all \\(i=1,\\dots,n,\\) which completes the proof. \\(\\square\\)\n\nGenerally, two random variables \\(X\\) and \\(Y\\) are said to be orthogonal if their cross moment is zero, i.e. if \\[\nE(XY)=0.\n\\] The exogeneity assumption (Assumption 2) is sometimes also called “orthogonality” assumption, due to the following result:\n\n\n\n\n\n\n\nTheorem 5.2 (Orthogonality) If \\(E(\\varepsilon_i|X_{i})=0\\) for all \\(i=1,\\dots,n,\\) then the regressors and the error term are orthogonal to each other, i.e, \\[\nE(X_{ik}\\varepsilon_i)=0\n\\] for all \\(i=1,\\dots,n\\) and all \\(k=1,\\dots,K.\\)\n\n\n\n\n\nProof. \\[\\begin{align*}\nE(X_{ik}\\varepsilon_i)\n&=E(E(X_{ik}\\varepsilon_i|X_{ik}))\\quad{\\text{(By the Law of Total Expectations)}}\\\\\n&=E(X_{ik}E(\\varepsilon_i|X_{ik}))\\quad{\\text{(By the linearity of cond. expectations)}}\n\\end{align*}\\] Now, to show that \\(E(X_{ik}\\varepsilon_i)=0\\), we need to show that \\(E(\\varepsilon_i|X_{ik})=0,\\) which is done in the following:\nSince \\(X_{ik}\\) is an element of \\(X_i,\\) a slightly more sophisticated use of the Law of Total Expectations (i.e., \\(E(Y|X)=E(E(Y|X,Z)|X)\\)) implies that \\[\nE(\\varepsilon_i|X_{ik})=E(E(\\varepsilon_i|X_i)|X_{ik}).\n\\] So, the exogeneity assumption, \\(E(\\varepsilon_i|X_i)=0\\) yields \\[\nE(\\varepsilon_i|X_{ik})=E(\\underbrace{E(\\varepsilon_i|X_i)}_{=0}|X_{ik})=E(0|X_{ik})=0.\n\\] I.e., we have that \\(E(\\varepsilon_i|X_{ik})=0\\) which allows us to conclude that \\[\nE(X_{ik}\\varepsilon_i)=E(X_{ik}E(\\varepsilon_i|X_{ik}))=E(X_{ik}0)=0\n\\] which completes the proof. \\(\\square\\)\n\nBecause the mean of the error term is zero (\\(E(\\varepsilon_i)=0\\) for all \\(i\\) (see Theorem 5.1), it follows that the orthogonality property, \\(E(X_{ik}\\varepsilon_i)=0,\\) is equivalent to a zero correlation property.\n\n\n\n\n\n\n\nTheorem 5.3 (No Correlation) If \\(E(\\varepsilon_i|X_{i})=0\\) for all \\(i=1,\\dots,n,\\) then \\[\\begin{eqnarray*}\n  Cov(\\varepsilon_i,X_{ik})&=&0\\quad\\text{for all}\\quad i=1,\\dots,n\\quad\\text{and all}\\quad k=1,\\dots,K.\n\\end{eqnarray*}\\]\n\n\n\n\n\nProof. \\[\\begin{eqnarray*}\n  Cov(\\varepsilon_i,X_{ik})&=&E(X_{ik}\\varepsilon_i)-E(X_{ik})\\,E(\\varepsilon_i)\\quad{\\small\\text{(Def. of Cov)}}\\\\[2ex]\n  &&\\text{By the mean zero result, $E(\\varepsilon_i)=0,$ shown above:}\\\\[2ex]\n  &=&E(X_{ik}\\varepsilon_i)\\\\[2ex]\n  &&\\text{By the orthogonality result shown above:}\\\\[2ex]\n  &=&0\\quad\\square\n\\end{eqnarray*}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-Multiple-Linear-Regression.html#deriving-the-expression-of-the-ols-estimator",
    "href": "05-Multiple-Linear-Regression.html#deriving-the-expression-of-the-ols-estimator",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.2 Deriving the Expression of the OLS Estimator",
    "text": "5.2 Deriving the Expression of the OLS Estimator\n\n5.2.1 Least Squares Approach\nWe derive the expression for the OLS estimator \\[\n\\hat\\beta=\\begin{pmatrix}\\hat\\beta_1\\\\\\vdots\\\\\\hat\\beta_K\\end{pmatrix}\\in\\mathbb{R}^K\n\\] as the vector-valued minimizing argument of the sum of squared residuals, \\[\nS_n(b)=\\sum_{i=1}^n\\big(\\underbrace{Y_i-X_i'b}_{\\text{$i$th residual}}\\big)^2\n\\] with \\(b\\in\\mathbb{R}^K.\\)\nUsing matrix/vector notation we can write \\(S(b)\\) as \\[\n\\begin{align*}\nS_n(b)\n&=\\sum_{i=1}^n(Y_i-X_i'b)^2\\\\[2ex]\n&=(Y-X b)^{\\prime}(Y-X b)\\\\[2ex]\n&=Y^{\\prime}Y-2 Y^{\\prime} X b+b^{\\prime} X^{\\prime} X b.\n\\end{align*}\n\\] To find the minimizing argument \\[\n\\hat\\beta=\\arg\\min_{b\\in\\mathbb{R}^K}S_n(b)\n\\] we compute the vector containing all partial derivatives \\[\n\\begin{aligned}\n\\underset{(K\\times 1)}{\\frac{\\partial S(b)}{\\partial b}} &=-2\\left(X^{\\prime}Y -X^{\\prime} Xb\\right).\n\\end{aligned}\n\\] Setting each partial derivative to zero leads to \\(K\\) linear equations (“normal equations”) in \\(K\\) unknowns. This system of linear equations defines the OLS estimator, \\(\\hat{\\beta}.\\) \\[\n\\begin{align*}\n-2\\left(X^{\\prime}Y -X^{\\prime} X\\hat{\\beta}\\right)\n&=\\underset{(K\\times 1)}{0}\\\\[2ex]\n\\Leftrightarrow\\qquad X^{\\prime} X\\hat{\\beta}\n&=\\underset{(K\\times 1)}{X^{\\prime}Y}.\n\\end{align*}\n\\qquad(5.3)\\] From our rank assumption (Assumption 3) it follows that \\(X^{\\prime}X\\) is an invertible \\((K\\times K)\\)-dimensional matrix which allows us to solve the equation system in Equation 5.3 by \\[\n\\begin{aligned}\n\\underset{(K\\times 1)}{\\hat{\\beta}} &=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} Y.\n\\end{aligned}\n\\]\nThe following codes computes the estimate \\(\\hat{\\beta}\\) for a given dataset with \\(X_i\\in\\mathbb{R}^K\\), \\(K=3\\).\n\n# Some given data\nX_2     &lt;- c(1.9,0.8,1.1,0.1,-0.1,4.4,4.6,1.6,5.5,3.4)\nX_3     &lt;- c(66, 62, 64, 61, 63, 70, 68, 62, 68, 66)\nY       &lt;- c(0.7,-1.0,-0.2,-1.2,-0.1,3.4,0.0,0.8,3.7,2.0)\ndataset &lt;-  data.frame(\"X_2\" = X_2, \"X_3\" = X_3, \"Y\" = Y)\n## Compute the OLS estimation\nlmobj   &lt;- lm(Y ~ X_2 + X_3, data = dataset)\n## Plot sample regression surface\nlibrary(\"scatterplot3d\") # library for 3d plots\nplot3d  &lt;- scatterplot3d(x = X_2, y = X_3, z = Y,\n            angle = 33, scale.y = 0.8, pch = 16,\n            color =\"red\", \n            xlab = expression(X[2]),\n            ylab = expression(X[3]),\n            main =\"OLS Regression Surface\")\nplot3d$plane3d(lmobj, lty.box = \"solid\", col=gray(.5), draw_polygon=TRUE)\n\n\n\n\n\n\n\n\n5.2.2 Method of Moments Estimator Approach\nRemember that the exogeneity assumption (Assumption 2), \\[\nE(\\varepsilon_i|X_i)=0,\n\\] implies that \\[\nE(X_{ik}\\varepsilon_i)=0\\quad\\text{for all}\\quad k=1,\\dots,K.\n\\] Thus, the exogeneity assumption (Assumption 2) gives us a system of \\(K\\) linear equations: \\[\n\\left.\n  \\begin{array}{c}\n  E(\\varepsilon_i)=0\\\\\n  E(X_{i2}\\varepsilon_i)=0\\\\\n  \\vdots\\\\\n  E(X_{iK}\\varepsilon_i)=0\n  \\end{array}\n\\right\\}\\Leftrightarrow \\underset{(K\\times 1)}{E(X_i\\varepsilon_i)}=\\underset{(K\\times 1)}{0}\n\\]\nThis linear equation system in terms of population moments (means) \\[\n\\underset{(K\\times 1)}{E(X_i\\varepsilon_i)}=\\underset{(K\\times 1)}{0}\n\\] allows us to identify the unknown (true) parameter vector \\(\\beta\\in\\mathbb{R}^K\\) in terms of population moments: \\[\n\\begin{align*}\nE(X_i\\varepsilon_i) & = 0 \\\\[2ex]\nE(X_i(\\overbrace{Y_i - X_i'\\beta}^{=\\varepsilon_i})) &= 0\\\\[2ex]\n%\\Leftrightarrow \\hspace{1.5cm}\nE(X_iY_i) - E(X_iX_i')\\beta & = 0\\\\[2ex]\nE(X_iX_i')\\beta & =  E(X_iY_i)  \\\\[2ex]\n\\beta & = \\left(E(X_iX_i')\\right)^{-1} E(X_iY_i)\n\\end{align*}\n\\] The fundamental idea behind method of moments estimation is to define an estimator by substituting population moments by sample moment analogues (sample means): \\[\n\\begin{align*}\n\\hat\\beta_{mm}\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\frac{1}{n}\\sum_{i=1}^n X_iY_i,\n\\end{align*}\n\\qquad(5.4)\\] where \\[\n\\begin{align*}\nE(X_iX_i') & \\quad \\text{is substituted by}\\quad \\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\\\[2ex]\nE(X_iY_i') & \\quad \\text{is substituted by}\\quad \\frac{1}{n}\\sum_{i=1}^n X_iY_i'.\n\\end{align*}\n\\]\nNote that Equation 5.4 can be further simplified as following: \\[\n\\begin{align*}\n\\hat\\beta_{mm}\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\frac{1}{n}\\sum_{i=1}^n X_iY_i  \\\\\n& = \\left(\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\sum_{i=1}^n X_iY_i\\\\\n& = \\left(X'X\\right)^{-1} X'Y.\\\\\n\\end{align*}\n\\] Thus the method of moments estimator, \\(\\hat\\beta_{mm},\\) coincides with the OLS estimator \\(\\hat\\beta\\) as derived above.\n\n5.2.3 Method of Moments versus Least Squares Estimation\n\nThe method of moments estimation approach, firstly, checks whether the parameter of interest, \\(\\beta,\\) is identified; i.e., whether \\(\\beta\\) can be written in terms of population moments of observables \\[\n\\beta = \\left(E(X_iX_i')\\right)^{-1} E(X_iY_i),\n\\qquad(5.5)\\] where the identification Equation 5.5 only holds under Assumption 2 (exogeneity). The identified parameter \\(\\beta\\) is then estimated using the method of moments idea of substituting population moments by sample moments.\nNote: If Assumption 2 (exogeneity) is violated, then we generally have that \\[\n\\beta \\neq \\tilde \\beta = \\left(E(X_iX_i')\\right)^{-1} E(X_iY_i)\n\\]\nThe derivation of the OLS estimator \\[\n\\begin{align*}\n\\hat\\beta\n& = \\left(X'X\\right)^{-1} X'Y\\\\[2ex]\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\frac{1}{n}\\sum_{i=1}^n X_iY_i  \\\\\n& = \\underbrace{\\left(\\sum_{i=1}^n X_iX_i'\\right)^{-1}}_{\\approx (E(X_iX_i'))^{-1}} \\; \\underbrace{\\sum_{i=1}^n X_iY_i}_{\\approx E(X_iY_i)}\n\\end{align*}\n\\] does not require a check whether \\(\\beta\\) is actually identified. By the law of large numbers (sample means converge in probability to population means; see Chapter 7), the OLS estimator estimates \\(\\tilde\\beta\\) \\[\n\\begin{align*}\n\\hat\\beta \\to_p  \\tilde\\beta = (E(X_iX_i'))^{-1} E(X_iY_i) \\quad \\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\]\nThat is, the OLS estimator will estimate the parameter of interest \\(\\beta\\) only if Assumption 2 (exogeneity) holds since then \\[\n\\beta = \\tilde \\beta = \\left(E(X_iX_i')\\right)^{-1} E(X_iY_i).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-Multiple-Linear-Regression.html#some-quantities-of-interest",
    "href": "05-Multiple-Linear-Regression.html#some-quantities-of-interest",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.3 Some Quantities of Interest",
    "text": "5.3 Some Quantities of Interest\nPredicted values and residuals.\n\nThe (OLS) predicted values: \\[\n\\hat{Y}_i=X_i'\\hat\\beta, \\quad i=1,\\dots,n\n\\] The \\((n\\times 1)\\) vector of predicted values \\[\n\\begin{align*}\n\\hat{Y} = \\left(\\begin{matrix}\\hat{Y}_1\\\\\\hat{Y}_2\\\\ \\vdots\\\\ \\hat{Y}_n\\end{matrix}\\right)\n&=X\\hat{\\beta}\\\\[-2ex]\n&=\\underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\\\[2ex]\n&=P_X Y\n\\end{align*}\n\\]\n\nThe (OLS) residuals: \\[\n\\hat\\varepsilon_i=Y_i-\\hat{Y}_i, \\quad i=1,\\dots,n\n\\] The \\((n\\times 1)\\) vector of residuals \\[\n\\begin{align*}\n\\hat{\\varepsilon} =\n\\left(\\begin{matrix}\\hat{\\varepsilon}_1\\\\\\hat{\\varepsilon}_2\\\\ \\vdots\\\\ \\hat{\\varepsilon}_n\\end{matrix}\\right)\n&=\n\\left(\\begin{matrix}Y_1\\\\[.5ex]Y_2\\\\[.5ex] \\vdots\\\\[.5ex] Y_n\\end{matrix}\\right)-\n\\left(\\begin{matrix}\\hat{Y}_1\\\\\\hat{Y}_2\\\\ \\vdots\\\\ \\hat{Y}_n\\end{matrix}\\right)\\\\[2ex]\n&=Y - \\hat{Y}\\\\[2ex]\n%&=Y - X\\hat{\\beta}\\\\[-2ex]\n%&=Y - \\underbrace{X(X'X)^{-1}X'}_{=P_X}Y\\\\[2ex]\n&=Y - P_X Y\\\\[2ex]\n&=\\underbrace{(I_n - P_X)}_{=M_X} Y\\\\[2ex]\n&=M_XY\n\\end{align*}\n\\]\n\n\nProjection matrices.\nThe matrix \\[\nP_X=X(X'X)^{-1}X'\n\\] is the \\((n\\times n)\\) projection matrix that projects any vector from \\(\\mathbb{R}^n\\) into the column space spanned by the column vectors of \\(X\\) and \\[\nM_X=I_n-X(X'X)^{-1}X'=I_n-P_X\n\\] is the associated \\((n\\times n)\\) orthogonal projection matrix that projects any vector from \\(\\mathbb{R}^n\\) into the vector space that is orthogonal to that spanned by the column vectors of \\(X.\\)\nThe projection matrices \\(P_X\\) and \\(M_X\\) have some nice properties:\n\n\n\\(P_X\\) and \\(M_X\\) are symmetric, i.e.  \\[\nP_X=P_X'\\quad\\text{ and }\\quad M_X=M_X'\n\\]\n\n\n\\(P_X\\) and \\(M_X\\) are idempotent, i.e.  \\[\nP_XP_X=P_X\\quad\\text{ and }\\quad M_X M_X=M_X\n\\]\n\nMoreover, we have that\n\n\n\\(X'P_X=X'\\) and \\(P_X X=X\\)\n\n\n\\(X'M_X=0\\) and \\(M_XX=0\\)\n\n\n\\(P_XM_X=0\\) and \\(M_XP_X=0\\)\n\n\n\n\nThe properties (a)-(c) follow directly from the definitions of \\(P_X\\) and \\(M_X\\) (check it out).\nUsing properties (a)-(c), one can show that the residual vector \\(\\hat\\varepsilon=(\\hat\\varepsilon_1,\\dots,\\hat\\varepsilon_n)'\\) is orthogonal to each of the column vectors in \\(X\\), i.e \\[\n\\begin{eqnarray}\nX'\\hat\\varepsilon&=&X'M_XY\\quad{\\small\\text{(By Def. of $M_X$)}}\\\\[2ex]\n\\Leftrightarrow\\;\\; X'\\hat\\varepsilon&=&\\underset{(K\\times n)}{0}\\underset{(n\\times 1)}{Y}\\quad{\\small\\text{(since $X'M_X=0$)}}\\\\[2ex]\n\\Leftrightarrow\\;\\; X'\\hat\\varepsilon&=&\\underset{(K\\times 1)}{0}.\n\\end{eqnarray}\n\\qquad(5.6)\\] Note that, in the case with intercept, Equation 5.6 implies that \\[\n\\begin{align*}\n\\sum_{i=1}^n 1\\cdot \\hat\\varepsilon_i& = 0\\\\[2ex]\n\\Leftrightarrow\\;\\;\\underbrace{\\frac{1}{n}\\sum_{i=1}^n \\hat\\varepsilon_i}_{=\\bar{\\hat\\varepsilon}} & = 0\\\\[2ex]\n\\end{align*}\n\\]\nMoreover, the equation \\(X'\\hat\\varepsilon=0\\) implies also that the residual vector \\(\\hat{\\varepsilon}\\) is orthogonal to the predicted values vector, since \\[\n\\begin{align*}\nX'\\hat\\varepsilon&=0\\\\\n\\Rightarrow\\;\\hat\\beta'X'\\hat\\varepsilon&=\\hat\\beta'0\\\\\n\\Leftrightarrow\\;\\hat Y'\\hat\\varepsilon&=0.\n\\end{align*}\n\\]\nAnother insight from Equation 5.6 is that the vector \\(\\hat\\varepsilon\\) has to satisfy \\(K\\) linear restrictions which means we loose \\(K\\) degrees of freedom in the data. Consequently, the vector of residuals \\(\\hat\\varepsilon\\) has only \\(n-K\\) so-called degrees of freedom. This loss of \\(K\\) degrees of freedom also appears in the definition of the unbiased variance estimator \\[\ns_{UB}^2=\\frac{1}{n-K}\\sum_{i=1}^n\\hat\\varepsilon_i^2.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe \\(K\\) linear restrictions follow from the fact that \\[\nX'\\hat\\varepsilon=\\underset{(K\\times 1)}{0}\n\\] is a linear system of \\(K\\) equations. That is, for each \\(k=1,\\dots,K\\) the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat\\varepsilon_n\\) need to fulfill the equation\\[\n\\sum_{i=1}^nX_{ik}\\hat\\varepsilon_i=0.\n\\]\n\n\nVariance decomposition: A further useful result that can be shown using the properties of \\(P_X\\) and \\(M_X\\) is that \\(Y'Y=\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\\), i.e. \\[\\begin{eqnarray*}\nY'Y&=&(\\hat Y+\\hat\\varepsilon)'(\\hat Y+\\hat\\varepsilon)\\notag\\\\\n  &=&(P_XY+M_XY)'(P_XY+M_XY)\\notag\\\\\n  &=&(Y'P_X'+Y'M_X')(P_XY+M_XY)\\notag\\\\\n  &=&Y'P_X'P_XY+Y'M_X'M_XY+0\\notag\\\\\n  &=&\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\n\\end{eqnarray*}\\] The decomposition \\[\n\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\n\\] is the basis for the well-known variance decomposition result for OLS regressions.\n\n\n\n\n\n\n\nTheorem 5.4 For the linear regression model with intercept (Equation 5.1), the total sample variance of the dependent variable \\(Y_1,\\dots,Y_n\\) can be decomposed as following: \\[\\begin{eqnarray}\n\\underset{\\text{total sample variance}}{\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2}&=&\\underset{\\text{explained sample variance}}{\\frac{1}{n}\\sum_{i=1}^n\\left(\\hat{Y}_i-\\bar{\\hat{Y}}\\right)^2}+\\underset{\\text{unexplained sample variance}}{\\frac{1}{n}\\sum_{i=1}^n\\hat\\varepsilon_i^2,}\\label{VarDecomp}\n\\end{eqnarray}\\] where \\(\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^nY_i\\) and \\(\\bar{\\hat{Y}}=\\frac{1}{n}\\sum_{i=1}^n\\hat{Y}_i\\).\n\n\n\n\n\nProof. From equation \\(X'\\hat\\varepsilon=0\\) we have for regressions with intercept that \\(\\sum_{i=1}^n\\hat\\varepsilon_i=0\\). Hence, from \\(Y_i=\\hat{Y}_i+\\hat\\varepsilon_i\\) it follows that \\[\\begin{eqnarray*}\n  \\frac{1}{n}\\sum_{i=1}^n Y_i&=&\\frac{1}{n}\\sum_{i=1}^n \\hat{Y}_i+\\frac{1}{n}\\sum_{i=1}^n \\hat\\varepsilon_i\\\\\n  \\bar{Y}&=&\\bar{\\hat{Y}}+0\n\\end{eqnarray*}\\]\nUsing the decomposition \\(Y'Y=\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\\), we can now derive the result: \\[\\begin{eqnarray*}\n   Y'Y&=&\\hat{Y}'\\hat{Y}+\\hat\\varepsilon'\\hat\\varepsilon\\\\\n   Y'Y-n\\bar{Y}^2&=&\\hat{Y}'\\hat{Y}-n\\bar{Y}^2+\\hat\\varepsilon'\\hat\\varepsilon\\\\\n   Y'Y-n\\bar{Y}^2&=&\\hat{Y}'\\hat{Y}-n\\bar{\\hat{Y}}^2+\\hat\\varepsilon'\\hat\\varepsilon\\quad\\text{(by $\\bar{Y}=\\bar{\\hat{Y}}$)}\\\\\n   \\sum_{i=1}^nY_i^2-n\\bar{Y}^2&=&\\sum_{i=1}^n\\hat{Y}_i^2-n\\bar{\\hat{Y}}^2+\\sum_{i=1}^n\\hat\\varepsilon_i^2\\\\\n   \\sum_{i=1}^n(Y_i-\\bar{Y})^2&=&\\sum_{i=1}^n(\\hat{Y}_i-\\bar{\\hat{Y}})^2+\\sum_{i=1}^n\\hat\\varepsilon_i^2\\quad\\square\\\\\n\\end{eqnarray*}\\]\n\nCoefficients of determination: \\(R^2\\) and \\(\\overline{R}^2\\)\n\nThe larger the proportion of the explained variance, the better is the fit of the model. This motivates the definition of the so-called \\(R^2\\) coefficient of determination: \\[\\begin{eqnarray*}\nR^2=\\frac{\\sum_{i=1}^n\\left(\\hat{Y}_i-\\bar{\\hat{Y}}\\right)^2}{\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2}\\;=\\;1-\\frac{\\sum_{i=1}^n\\hat{\\varepsilon}_i^2}{\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2}\n\\end{eqnarray*}\\]\n\nObviously, we have that \\(0\\leq R^2\\leq 1\\).\nThe closer \\(R^2\\) lies to \\(1\\), the better is the fit of the model to the observed data.\n\n\n\n\n\n\n\nCaution\n\n\n\n\nA high/low \\(R^2\\) value only means that the predictors have high/low predictive power.\nA high/low \\(R^2\\) does not mean a validation/falsification of the estimated model. Any econometric model needs a plausible explanation from relevant economic theory.\nThe most often criticized disadvantage of the \\(R^2\\) is that additional regressors (relevant or not) will increase the \\(R^2\\). The below R-codes demonstrates this problem.\n\n\n\n\nset.seed(123)\nn     &lt;- 100                  # Sample size\nX     &lt;- runif(n, 0, 10)      # Relevant X variable\nX_ir  &lt;- runif(n, 5, 20)      # Irrelevant X variable\nerror &lt;- rt(n, df = 10)*10    # True error\nY     &lt;- 1 + 5 * X + error    # Y variable\nlm1   &lt;- summary(lm(Y~X))     # Correct OLS regression \nlm2   &lt;- summary(lm(Y~X+X_ir))# OLS regression with X_ir \nlm1$r.squared &lt; lm2$r.squared\n\n[1] TRUE\n\n\nSo, \\(R^2\\) increases here even though X_ir is a completely irrelevant explanatory variable.\nBecause of this, the \\(R^2\\) cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called adjusted \\(R^2\\), \\(\\overline{R}^2,\\) defined as \\[\\begin{eqnarray*}\n  \\overline{R}^2&=&1-\\frac{\\frac{1}{n-K}\\sum_{i=1}^n\\hat{\\varepsilon}^2_i}{\\frac{1}{n-1}\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2}\\leq R^2%\\\\\n  %=\\dots=\n  %&=&1-\\frac{n-1}{n-K}\\left(1-R^2\\right)\\quad{\\small\\text{(since $1-R^2=(\\sum_i\\hat\\varepsilon_i^2)/(\\sum_i(Y_i-\\bar{Y}))$)}}\\\\\n  %&=&1-\\frac{n-1}{n-K}+\\frac{n-1}{n-K}R^2\\quad+\\frac{K-1}{n-K}R^2-\\frac{K-1}{n-K}R^2\\\\\n  %&=&1-\\frac{n-1}{n-K}+R^2\\quad+\\frac{K-1}{n-K}R^2\\\\\n  %&=&-\\frac{K-1}{n-K}+R^2\\quad+\\frac{K-1}{n-K}R^2\\\\\n  %&=&R^2-\\underbrace{\\frac{K-1}{n-K}\\left(1-R^2\\right)}_{\\geq 0\\;\\text{and}\\;\\leq(K-1)/(n-K)}\\;\\leq\\;R^2\n\\end{eqnarray*}\\] The adjustment is in terms of the degrees of freedom \\(n-K\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-Multiple-Linear-Regression.html#sec-MMEstimator",
    "href": "05-Multiple-Linear-Regression.html#sec-MMEstimator",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.4 Method of Moments Estimator",
    "text": "5.4 Method of Moments Estimator\nRemember that the exogeneity assumption (Assumption 2), \\(E(\\varepsilon_i|X_i)=0,\\) implies that \\(E(X_{ik}\\varepsilon_i)=0\\) for all \\(k=1,\\dots,K\\). Thus, the exogeneity assumption gives us a system of \\(K\\) linear equations: \\[\n\\left.\n  \\begin{array}{c}\n  E(\\varepsilon_i)=0\\\\\n  E(X_{i2}\\varepsilon_i)=0\\\\\n  \\vdots\\\\\n  E(X_{iK}\\varepsilon_i)=0\n  \\end{array}\n\\right\\}\\Leftrightarrow \\underset{(K\\times 1)}{E(X_i\\varepsilon_i)}=\\underset{(K\\times 1)}{0}\n\\]\nThe linear equation system \\[\n\\underset{(K\\times 1)}{E(X_i\\varepsilon_i)}=\\underset{(K\\times 1)}{0}\n\\] allows us to identify the unknown parameter vector \\(\\beta\\in\\mathbb{R}^K\\) in terms of population moments: \\[\n\\begin{align*}\nE(X_i(\\overbrace{Y_i - X_i'\\beta}^{=\\varepsilon_i})) &= 0\\\\\n%\\Leftrightarrow \\hspace{1.5cm}\nE(X_iY_i) - E(X_iX_i')\\beta & = 0\\\\\nE(X_iX_i')\\beta & =  E(X_iY_i)  \\\\\n\\beta & = \\left(E(X_iX_i')\\right)^{-1} E(X_iY_i)  \\\\\n\\end{align*}\n\\] The fundamental idea behind method of moments estimation is to define an estimator by substituting population moments by sample moment analogues (sample means): \\[\n\\begin{align*}\n\\hat\\beta_{mm}\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\frac{1}{n}\\sum_{i=1}^n X_iY_i,\n\\end{align*}\n\\] where \\(\\hat\\beta_{mm}\\) can be simplified as following: \\[\n\\begin{align*}\n\\hat\\beta_{mm}\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\frac{1}{n}\\sum_{i=1}^n X_iY_i  \\\\\n& = \\left(\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\sum_{i=1}^n X_iY_i\\\\\n& = \\left(X'X\\right)^{-1} X'Y.\\\\\n\\end{align*}\n\\]\nThus the method of moments estimator, \\(\\hat\\beta_{mm},\\) coincides with the OLS estimator.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-Multiple-Linear-Regression.html#sec-GMTheorem",
    "href": "05-Multiple-Linear-Regression.html#sec-GMTheorem",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.4 The Gauss-Markov Theorem",
    "text": "5.4 The Gauss-Markov Theorem\n\n\n\n\n\n\nThe OLS estimator \\(\\hat\\beta\\) is\n\na linear (in \\(Y\\)) and\na (conditionally) unbiased estimator\n\nof \\(\\beta\\).\n\n\n\nShowing linearity of \\(\\hat{\\beta}\\):\nA function \\(f(Y)\\) is called linear in \\(Y\\in\\mathbb{R}^n\\) if for any two vectors \\(Y_1\\in\\mathbb{R}^n\\) and \\(Y_2\\in\\mathbb{R}^n\\) such that \\(Y=Y_1+Y_2\\) and for any scalar \\(a\\in\\mathbb{R}\\) \\[\n\\begin{align*}\nf(Y_1+Y_2)&=f(Y_1)+f(Y_2)\\\\[2ex]\nf(aY)&=af(Y).\n\\end{align*}\n\\] This property applies to the OLS estimator \\[\n\\hat\\beta=f(Y)=(X'X)^{-1}X'Y\n\\] since \\[\n\\begin{align*}\n(X'X)^{-1}X'(Y_1+Y_2)\n& = (X'X)^{-1}X'Y_1 + (X'X)^{-1}X'Y_2\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n(X'X)^{-1}X'aY & = a (X'X)^{-1}X'Y\n\\end{align*}\n\\] for any \\(Y_1\\) and \\(Y_2\\) such that \\(Y=Y_1+Y_2\\) and for any \\(a\\in\\mathbb{R}.\\)\nShowing unbiasedness of \\(\\hat{\\beta}\\):\nThe OLS estimator is unbiased if \\[\n\\operatorname{Bias}(\\hat\\beta) = E(\\hat\\beta) - \\beta = 0\n\\] This can be shown as following: Observe that \\[\n\\hat\\beta=(X'X)^{-1}X'Y\n\\] consists of two multivariate random variables \\(X\\) and \\(Y.\\) Thus one needs to show first the conditional unbiasedness of \\(\\hat\\beta\\) given \\(X\\) which effectively allows us to focus on randomness due to \\(\\varepsilon,\\)\\[\n\\begin{align*}\n\\operatorname{Bias}(\\hat\\beta|X)\n&= E(\\hat\\beta|X)                                        - \\beta \\\\[2ex]\n&= E((X'X)^{-1}X'\\underbrace{Y}_{=X\\beta+\\varepsilon}|X) - \\beta \\\\[2ex]\n&= E((X'X)^{-1}X'(X\\beta+\\varepsilon)|X)                 - \\beta \\\\[2ex]\n&= E(\\underbrace{(X'X)^{-1}X'X}_{=I_K}\\beta|X) + E((X'X)^{-1}X'\\varepsilon|X) - \\beta \\\\[2ex]\n&= \\underbrace{E(\\beta|X)}_{=\\beta} + \\underbrace{E((X'X)^{-1}X'\\varepsilon|X)}_{=(X'X)^{-1}X'E(\\varepsilon|X)} - \\beta \\\\[2ex]\n&=  (X'X)^{-1}X'\\underbrace{E(\\varepsilon|X)}_{=0} =\\underset{(K\\times 1)}{0}  \n\\end{align*}\n\\] Thus \\(\\hat\\beta\\) is unbiased conditionally on \\(X.\\) From this if follows, by the iterated law of expectations, that the OLS estimator is also unconditionally unbiased, i.e.\\[\n\\operatorname{Bias}(\\hat\\beta) = E\\left(\\operatorname{Bias}(\\hat\\beta|X)\\right) = E(0) = 0.\n\\]\nFor the Gauss-Markov Theorem, we also need the conditional variance of \\(\\hat\\beta\\) given \\(X,\\) which can be derived as following: \\[\n\\begin{align*}\nVar(\\hat\\beta|X)\n&=Var(\\;\\highlight{\\hat\\beta - \\beta}\\;|X)\\\\[2ex]\n&=Var((X'X)^{-1}X'\\varepsilon|X),\n\\end{align*}\n\\] where we used that \\[\n\\begin{align*}\n\\hat\\beta\n&=(X'X)^{-1}X'Y\\\\\n&=(X'X)^{-1}X'(X\\beta+\\varepsilon)\\\\[2ex]\n&=\\beta+(X'X)^{-1}X'\\varepsilon\\\\[2ex]\n\\Leftrightarrow \\highlight{\\hat\\beta - \\beta}\n& = \\highlight{(X'X)^{-1}X'\\varepsilon}.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nDefinition 5.3 (Conditional Variance of a Multivariate Random Variable) The conditional variance of a multivariate random variable \\(Z\\in\\mathbb{R}^K,\\) given \\(X,\\) is defined as \\[\n\\begin{align*}\nVar(Z|X)\n&=\\underbrace{E\\Big[\\overbrace{(Z-E(Z|X))}^{(K\\times 1)}\\overbrace{(Z-E(Z|X))'}^{(1\\times K)}|X\\Big]}_{K\\times K}\\\\[2ex]\n&=\\underbrace{E\\big[ZZ'|X\\big]}_{K\\times K} - \\underbrace{E\\big[Z|X\\big]E\\big[Z'|X\\big]}_{K\\times K}\n\\end{align*}\n\\]\n\n\n\n\nUsing the definition of the conditional variance (Definition 5.3) for multivariate random variables \\((Z=(X'X)^{-1}X'\\varepsilon)\\) we have that \\[\n\\begin{align*}\n&Var(\\hat\\beta|X)=\\\\[2ex]\n%&=Var(\\hat\\beta - \\beta|X)\\\\[2ex]\n&=Var((X'X)^{-1}X'\\varepsilon|X)\\\\[2ex]\n&=E\\Big[\\big((X'X)^{-1}X'\\varepsilon-\\underbrace{E((X'X)^{-1}X'\\varepsilon|X)}_{=0}\\big)\\times\\\\[2ex]\n&\\phantom{=\\Big(}\\,\\times\\big((X'X)^{-1}X'\\varepsilon-\\underbrace{E((X'X)^{-1}X'\\varepsilon|X)}_{=0}\\big)'|X\\Big]\\\\[2ex]\n&=E\\left[((X'X)^{-1}X'\\varepsilon)((X'X)^{-1}X'\\varepsilon)'|X\\right]\\\\[2ex]\n&=E\\left[(X'X)^{-1}X'\\varepsilon\\varepsilon' X(X'X)^{-1}|X\\right]\\\\[2ex]\n&=\\;\\;\\;(X'X)^{-1}X'\\underbrace{E\\left(\\varepsilon\\varepsilon'|X\\right)}_{=Var(\\varepsilon|X)}X(X'X)^{-1}\n\\end{align*}\n\\]\nUnder the assumption of spherical errors (see Assumption 4), we have that \\[\n\\begin{align*}\nVar\\left(\\varepsilon|X\\right)\n& = \\left(\\begin{matrix}\nVar(\\varepsilon_1|X)&Cov(\\varepsilon_1,\\varepsilon_2|X)&\\dots&Cov(\\varepsilon_1,\\varepsilon_n|X)\\\\\nCov(\\varepsilon_2,\\varepsilon_1|X)&Var(\\varepsilon_2|X)&\\dots&Cov(\\varepsilon_2,\\varepsilon_n|X)\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nCov(\\varepsilon_n,\\varepsilon_1|X)&Cov(\\varepsilon_n,\\varepsilon_2|X)&\\dots&Var(\\varepsilon_n|X)\n\\end{matrix}\\right)\\\\[2ex]\n& = \\left(\\begin{matrix}\n\\sigma^2&0&\\dots&0\\\\\n0&\\sigma^2&\\dots&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n0&0&\\dots&\\sigma^2\n\\end{matrix}\\right)\n= \\sigma^2 I_n,\n%& = E\\left(\\varepsilon\\varepsilon'|X\\right) - E\\left(\\varepsilon|X\\right) E\\left(\\varepsilon'|X\\right)\\\\[2ex]\n%& = E\\left(\\varepsilon\\varepsilon'|X\\right) - \\underset{(K\\times K)}{0}\\\\[2ex]\n\\end{align*}\n\\] where \\(I_n\\) is the \\((n\\times n)\\) dimensional identity matrix with ones on the diagonal and zeros everywhere else.\nThus under the assumption of spherical errors, we have that \\[\n\\begin{align*}\nVar(\\hat\\beta|X)\n&=(X'X)^{-1}X' \\left(\\sigma^2I_n\\right)X(X'X)^{-1}\\\\[2ex]\n&=\\sigma^2(X'X)^{-1}X'X(X'X)^{-1}\\\\[2ex]\n&=\\sigma^2(X'X)^{-1}.\n\\end{align*}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nSummary:\n\nThe OLS estimator belongs to the large family of linear (in \\(Y\\)) and unbiased estimators of \\(\\beta.\\)\n\nUnder the assumption of spherical errors, the conditional variance of \\(\\hat\\beta,\\) given \\(X,\\) is \\[\nVar(\\hat\\beta|X)=\\sigma^2(X'X)^{-1}.\n\\]\n\n\n\n\nThe famous Gauss-Markov Theorem states that the OLS estimator is the “best” (smallest conditional variance) estimator within the family of linear (in \\(Y\\)) and unbiased estimators.\n\n\n\n\n\n\n\nTheorem 5.5 (Gauss-Markov Theorem) If it holds true that\n\n\\(Y = X\\beta + \\varepsilon\\)\n\\(E(\\varepsilon|X) = \\underset{(n\\times 1)}{0}\\)\n\n\\(\\operatorname{rank}(X)=K\\) a.s.\n\\(Var(\\varepsilon|X) = \\sigma^2I_n\\)\n\nthen the OLS estimator \\[\n\\underset{(K\\times 1)}{\\hat\\beta}=(X'X)^{-1}X'Y\n\\] has the smallest conditional variance (given \\(X\\)) among all linear (in \\(Y\\)) and conditionally unbiased estimators \\(\\tilde{\\beta}\\in\\mathbb{R}^K\\) with \\(E(\\tilde{\\beta}|X)=\\beta.\\) That is, for any alternative linear and unbiased estimator \\(\\tilde{\\beta}\\) we have that \\[\n\\begin{align*}\n&Var(\\tilde\\beta|X)\\geq Var(\\hat\\beta|X)\\quad\\text{(in the matrix sense)}\\\\[2ex]\n\\Leftrightarrow\\quad&Var(\\tilde\\beta|X)-Var(\\hat\\beta|X)=\\underset{(K\\times K)}{D},\n\\end{align*}\n\\] where \\(D\\) is a positive semidefinite \\((K\\times K)\\) matrix which implies that \\[\nVar(\\tilde{\\beta}_k|X) \\geq Var(\\hat\\beta_k | X)\n\\] for all \\(k=1,\\dots,K\\).\n\n\n\n\n\n\nRemember\n\n\n\nA \\((K\\times K)\\) matrix \\(D\\) is called positive semidefinite if \\[\na'Da\\geq 0\n\\] for any \\(K\\)-dimensional vector \\(a\\in\\mathbb{R}^K\\).\n\n\n\n\n\n\n\nProof. Since \\(\\tilde{\\beta}\\) is assumed to be linear in \\(Y\\), we can write \\[\n\\tilde{\\beta}=CY,\n\\] where \\(C\\) is some \\((K\\times n)\\) matrix, which is a function of \\(X\\) and/or nonrandom components. Adding a \\((K\\times n)\\) zero matrix \\(0\\) yields \\[\n\\tilde{\\beta}=\\Big(C\\;\\;\\overbrace{-\\left(X'X\\right)^{-1}X'+\\left(X'X\\right)^{-1}X'\\;}^{=0}\\;\\Big)Y.\n\\] Let now \\(D=C-\\left(X'X\\right)^{-1}X'\\), then \\[\n\\begin{align*}\n\\tilde{\\beta}&=\\left(D+\\left(X'X\\right)^{-1}X'\\right)Y\\\\\n\\tilde{\\beta}&=DY + \\left(X'X\\right)^{-1}X'Y\\\\\n\\tilde{\\beta}&=D\\left(X{\\beta}+{\\varepsilon}\\right) + \\left(X'X\\right)^{-1}X'Y\n\\end{align*}\n\\] Thus \\[\n\\tilde{\\beta}=DX{\\beta}+D{\\varepsilon} + \\hat{\\beta}.\n\\qquad(5.7)\\] Moreover, \\[\nE(\\tilde{\\beta}|X)=\\underbrace{E(DX{\\beta}|X)}_{=DX\\beta}+\\underbrace{E(D\\varepsilon|X)}_{=DE(\\varepsilon|X)=0}+\\underbrace{E(\\hat{\\beta}|X)}_{=\\beta}\n\\] and thus \\[\nE(\\tilde{\\beta}|X)=DX{\\beta}+0+{\\beta}.\n\\qquad(5.8)\\]\nSince \\(\\tilde{\\beta}\\) is (by assumption) unbiased, we have that \\(E(\\tilde{\\beta}|X)={\\beta}\\). Therefore, Equation 5.8 implies that \\(DX=0_{(K\\times K)}\\) since we must have that \\[\nE(\\tilde{\\beta}|X)=DX{\\beta}+0+{\\beta}=\\beta.\n\\] Plugging \\(DX=0\\) into Equation 5.7 yields, \\[\n\\begin{align*}\n\\tilde{\\beta}&=D{\\varepsilon} + \\hat{\\beta}\\\\[2ex]\n\\tilde{\\beta}-{\\beta}&=D{\\varepsilon} + \\highlight{(\\hat{\\beta}-{\\beta})}\\\\[2ex]\n\\tilde{\\beta}-{\\beta}&=D{\\varepsilon} + \\highlight{\\left(X'X\\right)^{-1}X'{\\varepsilon}}\n\\end{align*}\n\\] such that \\[\n\\tilde{\\beta}-{\\beta}=\\left(D + \\left(X'X\\right)^{-1}X'\\right){\\varepsilon},\n\\qquad(5.9)\\] where we used that \\[\n\\begin{align*}\n\\highlight{\\hat\\beta-\\beta}&=(X'X)^{-1}X'Y-\\beta\\\\[2ex]\n&=(X'X)^{-1}X'(X\\beta+\\varepsilon)-\\beta\\\\[2ex]\n&=\\highlight{(X'X)^{-1}X'\\varepsilon}.\n\\end{align*}\n\\]\nUsing that \\(Var(\\tilde{\\beta}|X)= Var(\\tilde{\\beta}-{\\beta}|X)\\) since \\(\\beta\\) is not random and using Equation 5.9 yields \\[\n\\begin{align*}\nVar(\\tilde{\\beta}|X)\n&= Var((D + (X'X)^{-1}X'){\\varepsilon}|X)\\\\[2ex]\n&= (D + (X'X)^{-1}X')Var({\\varepsilon}|X)(D' + X(X'X)^{-1})\\\\[2ex]\n&= \\sigma^2(D + (X'X)^{-1}X')I_n(D' + X(X'X)^{-1})\\\\[2ex]\n&= \\sigma^2\\left(DD'+(X'X)^{-1}\\right)\\quad \\text{(using that $DX=0$)} \\\\[2ex]\n&\\geq\\sigma^2(X'X)^{-1} \\quad \\text{(using that $DD'\\geq 0$)}\\\\[2ex]\n&= Var(\\hat{\\beta}|X).\n\\end{align*}\n\\] Finally, we need to show that \\(DD'\\) is really positive semidefinite (i.e. \\(DD'\\geq 0\\) in matrix sense): \\[\n\\begin{align*}\na'DD'a=(D'a)'(D'a)=\\tilde{a}'\\tilde{a}=\\sum_{k=1}^K \\tilde{a}^2_k \\geq 0,\n\\end{align*}\n\\] where \\(a\\in\\mathbb{R}^K\\) is any \\(K\\)-dimensional vector.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-Multiple-Linear-Regression.html#practice",
    "href": "05-Multiple-Linear-Regression.html#practice",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.5 Practice",
    "text": "5.5 Practice\n\n5.5.1 Factor Variables and the Dummy-Variable Trap\nIn the following, we consider a simple linear regression model \\[\nY_i = \\beta_1 + \\beta_2 X_{i2} + \\varepsilon_i\n\\] that aims to predict wages \\(Y_i\\in\\mathbb{R}\\) in the year 2008 using gender \\(X_{i2}\\in\\{\\texttt{male},\\texttt{female}\\}\\) as the only predictor.\nHere \\[\nX_{i2}\\in\\{\\texttt{male},\\texttt{female}\\}\n\\] is a categorical variable also called factor variable with two categories (two factor levels).\nWe use data provided in the accompanying materials of Stock and Watson’s Introduction to Econometrics textbook (Stock and Watson 2015). You can download the data stored as an xlsx-file cps_ch3.xlsx HERE.\nLet us first prepare the dataset:\n\n## load the 'tidyverse' package\nsuppressPackageStartupMessages(library(\"tidyverse\"))\n## load the 'readxl' package\nlibrary(\"readxl\")\n\n## import the data into R\ncps &lt;- read_excel(path = \"data/cps_ch3.xlsx\")\n\n# names(cps)\n# range(cps$year)\n# range(cps$a_sex) # 1 = male, 2 = female\n\n## Data wrangling\ncps_2008 &lt;- cps %&gt;% \n  mutate(\n    wage   = ahe08,      # rename \"ahe08\" as \"wage\"    \n    gender = fct_recode( # rename factor \"a_sex\" as \"gender\"\n      as_factor(a_sex),     \n                \"male\" = \"1\",  # rename factor level \"1\" to \"male\"\n                \"female\" = \"2\" # rename factor level \"2\" to \"female\"\n             ) \n  ) %&gt;%  \n  filter(year == 2008) %&gt;%     # Only data from year 2008\n  select(wage, gender)         # Select only the variables \"wage\" and \"gender\"\n\nThe first six lines of the dataset cps_2008 look as following:\n\n\n\n\nwage\ngender\n\n\n\n38.46154\nmale\n\n\n12.50000\nmale\n\n\n17.78846\nmale\n\n\n30.38461\nmale\n\n\n23.66864\nmale\n\n\n12.01923\nfemale\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAbove in our theoretical part, we have not mentioned the possibility of categorical, i.e., factor variables.\n\nHow can one compute the estimators from non-numeric factor levels like female and male?\n\nWell, we need to transform the non-numeric factor level to numeric data. R and other statistical software-packages do this for us in the background.\n\n\nComputing the estimation results:\n\nlm_obj &lt;- lm(wage ~ gender, data = cps_2008)\n\ncoef(lm_obj)\n\ngives\n\n\n\\(\\hat\\beta_1=\\) 24.98\n\n\\(\\hat\\beta_2=\\) -4.1\n\nTo compute these estimation results, R assigns to each factor level (male and female) a numeric value. To see the numeric values used by R one can take a look at model.matrix(lm_obj):\n\n# this is the internally used numeric X-matrix\nX &lt;- model.matrix(lm_obj) \nX[1:6,]\n\n  (Intercept) genderfemale\n1           1            0\n2           1            0\n3           1            0\n4           1            0\n5           1            0\n6           1            1\n\n\nCompare this with the factor variable gender in the dataset cps_2008:\n\n# Factor variable 'gender' in the dataset cps_2008\ncps_2008$gender[1:6]\n\n[1] male   male   male   male   male   female\nLevels: male female\n\n\nThus R internally recodes \\(X_{i2}\\in\\{\\texttt{male}, \\texttt{female}\\}\\) as a dummy variable \\(X_{i2}\\in\\{0,1\\}\\) such that \\[\nX_{i2}=\n\\left\\{\\begin{array}{ll}\n0 &\\text{ if subject $i$ is male}\\\\\n1 &\\text{ if subject $i$ is female}.\n\\end{array} \\right.\n\\] Therefore, we can interpret the estimation result as \\[\n\\hat\\beta_1  + \\hat\\beta_2 X_{i2}=\n\\left\\{\\begin{array}{ll}\n\\hat\\beta_1              &\\text{ if subject $i$ is male}\\\\\n\\hat\\beta_1 + \\hat\\beta_2&\\text{ if subject $i$ is female}\n\\end{array} \\right.\n\\]\nInterpretation:\n\nThe average wage of male workers in 2008 was \\(\\hat{\\beta}_1=\\) 24.98 (USD/Hour).\nThe average wage of female workers in 2008 was \\(\\hat{\\beta}_1 + \\hat{\\beta}_2=\\) 20.87 (USD/Hour).\nThe average difference in the earnings between male and female workers in 2008 is \\(\\hat{\\beta}_2=\\) -4.1 (USD/Hour).\nDummy-Variable Trap\nAbove, we used R’s automatic recoding of categorical factor variables into numeric dummy variable which is very convenient.\nOf course, we can also construct a dummy variable for each of the levels of a factor yourself. But be careful, and do not step into the dummy variable trap!\nLet’s construct the \\(Y\\) variable and the numeric \\(X\\) variable “by hand”:\n\n## Dependent variable\nY        &lt;- cps_2008$wage\n\n## Intercept variable\nX_1      &lt;- rep(1, times = nrow(cps_2008))\n\n## 1. Dummy variable for 'female' (indicates female subjects)\nX_female &lt;- ifelse(cps_2008$gender == \"female\", 1, 0)\n\n## 2. Dummy variable for 'male' (indicates male subjects)\nX_male   &lt;- ifelse(cps_2008$gender == \"male\", 1, 0)\n\n## Construct the numeric model matrix 'X'\nX        &lt;- cbind(X_1, X_female, X_male)\n\nComputing the estimation result for \\(\\beta\\) “by hand” yields\n\n## Computing the estimator\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\n\n\nError in solve.default(X %*% t(X)) : \n  Lapack routine dgesv: system is exactly singular: U[3,3] = 0\nCalls: .main ... eval_with_user_handlers -&gt; eval -&gt; eval -&gt; solve -&gt; solve.default\nExecution halted\n\n\n\n\n\n\n\nAn error message! 🤬\n\n\n\nWe stepped into the dummy variable trap!\nThat is, we constructed a \\((n\\times K)\\)-dimensional matrix \\(X\\) for which Assumption 3 \\((\\operatorname{rank}(X)=K)\\) is violated. Therefore \\((X'X)\\) does not have full rank and is thus not invertible.\n\n\nThe estimation result is not computable since \\((X'X)\\) is not invertible due to the perfect multicollinearity between the intercept and the two dummy variables X_female and X_male, i.e. \n\nX_1 = X_female \\(+\\) X_male\n\n which violates Assumption 3 (no perfect multicollinearity).\n\n\n\n\n\n\nSolution\n\n\n\nUse one dummy-variable less than factor levels. I.e., in this example you can either drop X_female or X_male.\n\n\n\n## New model matrix after dropping X_male:\nX        &lt;- cbind(X_1, X_female)\n\n## Computing the estimator\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\nbeta_hat\n\n              [,1]\nX_1      24.978404\nX_female -4.103626\n\n\nThis give the same result as computed by R’s lm() function using the factor variable gender.\n\n5.5.2 Detecting Heteroskedasticity\nA very simple, but highly effective way to check for heteroskedastic error terms is to plot the residuals. If we have the correct model (hopefully we have), then (and only then) the residuals \\(\\hat\\varepsilon_i\\) are good approximations to the realizations of the error terms \\(\\varepsilon_i\\). Thus a plot of the residuals can be used to check for heteroskedasticity.\nYou can use R’s internal diagnostic plots that can be called using the plot() method for lm-objects:\n\n# install.packages(\"devtools\")\n# library(devtools)\n# install_git(\"https://github.com/ccolonescu/PoEdata\")\n\nlibrary(PoEdata) # for the \"food\" dataset contained in this package\n\ndata(\"food\")     # makes the dataset \"food\" usable\n\nlm_object &lt;- lm(food_exp ~ income, data = food)\n\n## Diagnostic scatter plot of residuals vs fitted values \nplot(lm_object, which = 1)\n\n\n\n\n\n\n\nInterpretation:\n\nThe plot shows the residuals \\(\\hat{\\varepsilon}_i\\) plotted against the fitted values \\(\\hat{Y}_i\\).\nThe diagnostic plot indicates that the variance increases with \\(\\hat{Y}_i=X_i'\\hat{\\beta}.\\)\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote: Plotting against the fitted values is actually a quite smart idea since this also works for multiple predictors \\(X_{i1},\\dots,X_{iK}\\) with \\(K\\geq 3\\).\n\n\n\n5.5.3 Checking (Non-)Linearity of the Regression Line\nTo check whether there are non-linear relationships between the outcome and the predictors, one should take a look at the data using a pairs-plot function that procures scatter plots for all pairs of variables in a dataset.\n\n\nBase R\nTidyverse R\n\n\n\n\ncar_data &lt;- read.csv(file = \"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv\")\n\nmy_car_df &lt;- data.frame(\n    \"MPG\"   = car_data$MPG.city,\n    \"HP\"    = car_data$Horsepower\n    )\n\npairs(my_car_df)\n\n\n\n\n\n\n\n\n\n\n## install.packages(\"tidyverse\")\n## install.packages(\"GGally\")\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nsuppressPackageStartupMessages(library(\"GGally\")) # nice pairs plot \n\ncar_data &lt;- readr::read_csv(\n  file = \"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv\",\n  show_col_types = FALSE)\n\nmy_car_df &lt;- car_data %&gt;% \n dplyr::mutate(\n  \"MPG\"   = MPG.city, \n  \"HP\"    = Horsepower) %&gt;%\n select(\"MPG\", \"HP\")\n\nggpairs(my_car_df) + theme_bw()\n\n\n\n\n\n\n\n\n\n\nThe plots indicate a positive, but non-linear relationship between the outcome variable MPG (gasoline consumption in miles per gallon) and the predictor HP (power of the mashing in horsepower).\nLet’s begin with a simple linear regression model \\[\nY_i = \\beta_1 + \\beta_2 X_{i2} +  \\varepsilon_i\n\\] with\n\n\n\\(Y_i\\) denoting MPG of car \\(i\\) and\n\n\\(X_{i2}\\) denoting HP of car \\(i.\\)\n\n\nSuch a simple model is not able to take into account the non-linear relationship between \\(Y_i\\) and \\(X_{i2}\\). Therefore, we get bad model fits which can be spotted\n\nin the scatter plot of MPG vs. HP with added linear regression line fit and\nin the diagnostic plot of the residuals \\(\\hat{\\varepsilon}_i\\) vs. the fitted values \\(\\hat{Y}_i=\\hat\\beta_1 + \\hat\\beta_2 X_{i2}.\\)\n\n\n\n\nBase R\nTidyverse R\n\n\n\n\nlm_obj_1 &lt;- lm(MPG ~ HP, data = my_car_df)\n\npar(mfrow=c(2,1))\nplot(y=my_car_df$MPG, x=my_car_df$HP, \n     main = \"Simple Linear Regression\",\n     xlab = \"Horse Power\", \n     ylab = \"Miles per Gallon\")\nabline(lm_obj_1)\n##\nplot(lm_obj_1, which=1, \n       main = \"Simple Linear Regression\")\n\n\n\n\n\n\n\n\n\n\n## install.packages(\"tidyverse\")\n## install.packages(\"ggfortify\")\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nlibrary(\"ggfortify\") # tidy diagnostic plots\n\nlm_obj_1 &lt;- lm(MPG ~ HP, data = my_car_df)\n\n## Plot: Simple Linear Regression\nmy_car_df %&gt;%\nggplot(aes(x=HP, y=MPG)) + \n    geom_point(alpha=0.8, size=3)+\n    stat_smooth(method='lm', formula = y~x) +\n    theme_classic() + \n    labs(x = \"Horse Power\", y = \"Miles per Gallon\", \n         title = \"Simple Linear Regression\")\n\n\n\n\n\n\n## Diagnostic Plot: Simple Linear Regression \nautoplot(lm_obj_1, which = 1)\n\n\n\n\n\n\n\n\n\n\nTo model the non-linear, more or less quadratic relationship between \\(Y_i\\) and \\(X_{i2}\\), we can use a a polynomial regression model with polynomial degree 2: \\[\nY_i = \\beta_1 + \\beta_2 X_{i2} + \\beta_3 X_{i2}^2 +  \\varepsilon_i\n\\] with\n\n\n\\(Y_i\\) denoting MPG of car \\(i,\\)\n\n\n\\(X_{i2}\\) denoting HP of car \\(i,\\) and\n\n\\(X_{i2}^2\\) denoting the squared value of HP of car \\(i.\\)\n\n\nThis more flexible model is indeed better in taking into account the non-linear relationship between \\(Y_i\\) and \\(X_{i2}.\\) The improved model fits can be spotted\n\nin the scatter plot of MPG vs. HP with added non-linear regression line fit and\nin the diagnostic plot of the residuals \\(\\hat{\\varepsilon}_i\\) vs. the fitted values \\(\\hat{Y}_i=\\hat\\beta_1 + \\hat\\beta_2 X_{i2} + \\hat\\beta_3 X_{i2}^2.\\)\n\n\n\n\nBase R\nTidyverse R\n\n\n\n\n## Adding new variable HP squared:\nmy_car_df$HP_sq &lt;- car_data$Horsepower^2\n\nlm_obj_2 &lt;- lm(MPG ~ HP + HP_sq, data = my_car_df)\n\npar(mfrow=c(2,1))\nplot(y=my_car_df$MPG, x=my_car_df$HP,\n     main = \"Polynomial Regression (Degree: 2)\",\n     xlab = \"Horse Power\", \n     ylab = \"Miles per Gallon\")\nX_seq &lt;- seq(from = min(my_car_df$HP), \n             to   = max(my_car_df$HP), len = 25)\nlines(y = predict(lm_obj_2, \n                  newdata=data.frame(\"HP\"    = X_seq, \n                                     \"HP_sq\" = X_seq^2)), \n      x = X_seq)\nplot(lm_obj_2, which=1, \n       main = \"Polynomial Regression (Degree: 2)\")\n\n\n\n\n\n\n\n\n\n\n## install.packages(\"tidyverse\")\n## install.packages(\"ggfortify\")\nsuppressPackageStartupMessages(library(\"tidyverse\"))\nlibrary(\"ggfortify\") # tidy diagnostic plots\n\n## Adding new variable HP squared:\nmy_car_df &lt;- my_car_df %&gt;% \n dplyr::mutate(\n  \"HP_sq\" = HP^2\n)\n\n## Polynomial Regression\n\nlm_obj_2 &lt;- lm(MPG ~ HP + HP_sq, data = my_car_df)\n\n## Plot: Polynomial Regression\nmy_car_df %&gt;%\nggplot(aes(x=HP, y=MPG)) + \n    geom_point(alpha=0.8, size=3)+\n    stat_smooth(method='lm', formula = y~poly(x, 2, raw = TRUE)) +\n    theme_classic() + \n    labs(x = \"Horse Power\", y = \"Miles per Gallon\", \n         title = \"Polynomial Regression\", \n         subtitle = \"Polynomial Degree: 2\")\n\n\n\n\n\n\n## Diagnostic Plot: Polynomial Regression\nautoplot(lm_obj_2, which = 1)     \n\n\n\n\n\n\n\n\n\n\n\n5.5.4 Behavior of the OLS Estimates for Resampled Data\nUsually, we only observe one estimate \\(\\hat{\\beta}\\) of \\(\\beta\\) computed based on one given dataset (one realization of the random sample).\nHowever, in order to understand the statistical properties of the estimators \\(\\hat{\\beta}\\) we need to view them as random variables which yield different realizations in repeated realizations of the random sample from Model (Equation 5.1).\nThis view allows us then to think about questions like:\n\nIs the estimator able to estimate the unknown parameter-value correctly on average?\n\nAre the estimation results more precise (small variance) if we have more data?\n\nA first idea about the statistical properties of the estimator \\(\\hat{\\beta}\\) can be gained using Monte Carlo simulations as following.\n\n## Sample sizes\nn_small      &lt;-  30 # smallish sample size\nn_large      &lt;- 100 # largish sample size\n\n## True parameter values\nbeta0 &lt;- 1\nbeta1 &lt;- 1\n\n## Monte-Carlo (MC) Simulation \n## 1. Generate data\n## 2. Compute and store estimates\n## Repeat steps 1. and 2. many times\nset.seed(3)\n## Number of Monte Carlo repetitions\n## How many samples to draw from the models\nB          &lt;- 1000\n\n## Containers to store the estimation results\nbeta0_estimates_n_small &lt;- numeric(B)\nbeta1_estimates_n_small &lt;- numeric(B)\nbeta0_estimates_n_large &lt;- numeric(B)\nbeta1_estimates_n_large &lt;- numeric(B)\n\nfor(b in 1:B){\n## Generate artificial samples (n_small)\nerror_n_small     &lt;- rnorm(n_small, mean = 0, sd = 5)\nX_n_small         &lt;- runif(n_small, min = 1, max = 10)\nY_n_small         &lt;- beta0 + beta1 * X_n_small + error_n_small\nlm_obj            &lt;- lm(Y_n_small ~ X_n_small) \n## Save estimation results \nbeta0_estimates_n_small[b] &lt;- lm_obj$coefficients[1]\nbeta1_estimates_n_small[b] &lt;- lm_obj$coefficients[2]\n\n## Generate artificial samples (n_large)\nerror_n_large     &lt;- rnorm(n_large, mean = 0, sd = 5)\nX_n_large         &lt;- runif(n_large, min = 1, max = 10)\nY_n_large         &lt;- beta0 + beta1 * X_n_large + error_n_large\nlm_obj            &lt;- lm(Y_n_large ~ X_n_large)\n## Save estimation results \nbeta0_estimates_n_large[b] &lt;- lm_obj$coefficients[1]\nbeta1_estimates_n_large[b] &lt;- lm_obj$coefficients[2] \n}\n\nNow, we have produced B=1000 realizations of the estimators \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) and saved these realizations in the vectors\n\nbeta0_estimates_n_small\nbeta1_estimates_n_small\nbeta0_estimates_n_large\nbeta1_estimates_n_large\n\nThese artificially generated realizations allow us to visualize the behavior of the OLS estimates for the repeatedly sampled data.\n\n\n\n\n\n\n\n\nThis are promising plots:\n\nThe realizations of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are scattered around the true (unknown) parameter values \\(\\beta_0\\) and \\(\\beta_1\\). This indicates unbiasdness of the estimators.\nIn the case of a larger sample size, the realizations of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) concentrate stronger around the true (unknown) parameter values \\(\\beta_0\\) and \\(\\beta_1.\\) This indicates consistency of the estimators.\n\n\n\n\n\n\n\nImportant\n\n\n\nHowever, this was only a simulation for one specific data generating process. Such a Monte Carlo simulation does not allow us to generalize the observed properties to other data generating processes.\nIn the following chapters, we use theoretical arguments to investigate under which assumptions we can make general statements about the distributional properties of the estimator \\(\\hat\\beta.\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-Multiple-Linear-Regression.html#exercises",
    "href": "05-Multiple-Linear-Regression.html#exercises",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.6 Exercises",
    "text": "5.6 Exercises\n\nExercises for Chapter 5\nExercises of Chapter 5 with Solutions",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-Multiple-Linear-Regression.html#references",
    "href": "05-Multiple-Linear-Regression.html#references",
    "title": "5  Multiple Linear Regression",
    "section": "References",
    "text": "References\n\n\n\n\nStock, J. H., and M. W. Watson. 2015. Introduction to Econometrics. Pearson Education.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "03-Matrix-Algebra.html#exercises",
    "href": "03-Matrix-Algebra.html#exercises",
    "title": "\n3  Matrix Algebra\n",
    "section": "\n3.7 Exercises",
    "text": "3.7 Exercises\n\nExercises for Chapter 3\nExercises for Chapter 3 with Solutions",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "05-Multiple-Linear-Regression.html#deriving-the-estimator-hatbeta",
    "href": "05-Multiple-Linear-Regression.html#deriving-the-estimator-hatbeta",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.2 Deriving the Estimator \\(\\hat\\beta\\)\n",
    "text": "5.2 Deriving the Estimator \\(\\hat\\beta\\)\n\n\n5.2.1 Least Squares Approach\nWe derive the expression for the OLS estimator \\[\n\\hat\\beta=\\begin{pmatrix}\\hat\\beta_1\\\\\\vdots\\\\\\hat\\beta_K\\end{pmatrix}\\in\\mathbb{R}^K\n\\] as the vector-valued minimizing argument of the sum of squared residuals, \\[\nS_n(b)=\\sum_{i=1}^n\\big(\\underbrace{Y_i-X_i'b}_{\\text{$i$th residual}}\\big)^2\n\\] with \\(b\\in\\mathbb{R}^K.\\)\nUsing matrix/vector notation we can write \\(S(b)\\) as \\[\n\\begin{align*}\nS_n(b)\n&=\\sum_{i=1}^n(Y_i-X_i'b)^2\\\\[2ex]\n&=(Y-X b)^{\\prime}(Y-X b)\\\\[2ex]\n&=Y^{\\prime}Y-2 Y^{\\prime} X b+b^{\\prime} X^{\\prime} X b.\n\\end{align*}\n\\] To find the minimizing argument \\[\n\\hat\\beta=\\arg\\min_{b\\in\\mathbb{R}^K}S_n(b)\n\\] we compute the vector containing all partial derivatives \\[\n\\begin{aligned}\n\\underset{(K\\times 1)}{\\frac{\\partial S(b)}{\\partial b}} &=-2\\left(X^{\\prime}Y -X^{\\prime} Xb\\right).\n\\end{aligned}\n\\] Setting each partial derivative to zero leads to \\(K\\) linear equations (“normal equations”) in \\(K\\) unknowns. This system of linear equations defines the OLS estimator, \\(\\hat{\\beta}.\\) \\[\n\\begin{align*}\n-2\\left(X^{\\prime}Y -X^{\\prime} X\\hat{\\beta}\\right)\n&=\\underset{(K\\times 1)}{0}\\\\[2ex]\n\\Leftrightarrow\\qquad X^{\\prime} X\\hat{\\beta}\n&=\\underset{(K\\times 1)}{X^{\\prime}Y}.\n\\end{align*}\n\\qquad(5.3)\\] From our rank assumption (Assumption 3) it follows that \\(X^{\\prime}X\\) is an invertible \\((K\\times K)\\)-dimensional matrix which allows us to solve the equation system in Equation 5.3 by \\[\n\\begin{aligned}\n\\underset{(K\\times 1)}{\\hat{\\beta}} &=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} Y.\n\\end{aligned}\n\\]\nThe following codes computes the estimate \\(\\hat{\\beta}\\) for a given dataset with \\(X_i\\in\\mathbb{R}^K\\), \\(K=3\\).\n\n# Some given data\nX_2     &lt;- c(1.9,0.8,1.1,0.1,-0.1,4.4,4.6,1.6,5.5,3.4)\nX_3     &lt;- c(66, 62, 64, 61, 63, 70, 68, 62, 68, 66)\nY       &lt;- c(0.7,-1.0,-0.2,-1.2,-0.1,3.4,0.0,0.8,3.7,2.0)\ndataset &lt;-  data.frame(\"X_2\" = X_2, \"X_3\" = X_3, \"Y\" = Y)\n## Compute the OLS estimation\nlmobj   &lt;- lm(Y ~ X_2 + X_3, data = dataset)\n## Plot sample regression surface\nlibrary(\"scatterplot3d\") # library for 3d plots\nplot3d  &lt;- scatterplot3d(x = X_2, y = X_3, z = Y,\n            angle = 33, scale.y = 0.8, pch = 16,\n            color =\"red\", \n            xlab = expression(X[2]),\n            ylab = expression(X[3]),\n            main =\"OLS Regression Surface\")\nplot3d$plane3d(lmobj, lty.box = \"solid\", col=gray(.5), draw_polygon=TRUE)\n\n\n\n\n\n\n\n\n5.2.2 Method of Moments Estimator Approach\nRemember that the exogeneity assumption (Assumption 2), \\[\nE(\\varepsilon_i|X_i)=0,\n\\] implies that \\[\nE(X_{ik}\\varepsilon_i)=0\\quad\\text{for all}\\quad k=1,\\dots,K.\n\\] Thus, the exogeneity assumption (Assumption 2) gives us a system of \\(K\\) linear equations: \\[\n\\left.\n  \\begin{array}{c}\n  E(\\varepsilon_i)=0\\\\\n  E(X_{i2}\\varepsilon_i)=0\\\\\n  \\vdots\\\\\n  E(X_{iK}\\varepsilon_i)=0\n  \\end{array}\n\\right\\}\\Leftrightarrow \\underset{(K\\times 1)}{E(X_i\\varepsilon_i)}=\\underset{(K\\times 1)}{0}\n\\]\nThis linear equation system in terms of population moments (means) \\[\n\\underset{(K\\times 1)}{E(X_i\\varepsilon_i)}=\\underset{(K\\times 1)}{0}\n\\] allows us to identify the unknown (true) parameter vector \\(\\beta\\in\\mathbb{R}^K\\) in terms of population moments: \\[\n\\begin{align*}\nE(X_i\\varepsilon_i) & = 0 \\\\[2ex]\nE(X_i(\\overbrace{Y_i - X_i'\\beta}^{=\\varepsilon_i})) &= 0\\\\[2ex]\n%\\Leftrightarrow \\hspace{1.5cm}\nE(X_iY_i) - E(X_iX_i')\\beta & = 0\\\\[2ex]\nE(X_iX_i')\\beta & =  E(X_iY_i)  \\\\[2ex]\n\\beta & = \\left(E(X_iX_i')\\right)^{-1} E(X_iY_i)\n\\end{align*}\n\\] The fundamental idea behind method of moments estimation is to define an estimator by substituting population moments by sample moment analogues (sample means): \\[\n\\begin{align*}\n\\hat\\beta_{mm}\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\frac{1}{n}\\sum_{i=1}^n X_iY_i,\n\\end{align*}\n\\qquad(5.4)\\] where \\[\n\\begin{align*}\nE(X_iX_i') & \\quad \\text{is substituted by}\\quad \\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\\\[2ex]\nE(X_iY_i) & \\quad \\text{is substituted by}\\quad \\frac{1}{n}\\sum_{i=1}^n X_iY_i.\n\\end{align*}\n\\]\nNote that Equation 5.4 can be further simplified as following: \\[\n\\begin{align*}\n\\hat\\beta_{mm}\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\frac{1}{n}\\sum_{i=1}^n X_iY_i  \\\\\n& = \\left(\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\sum_{i=1}^n X_iY_i\\\\\n& = \\left(X'X\\right)^{-1} X'Y.\\\\\n\\end{align*}\n\\] Thus the method of moments estimator, \\(\\hat\\beta_{mm},\\) coincides with the OLS estimator \\(\\hat\\beta\\) as derived above.\n\n5.2.3 Method of Moments versus Least Squares Estimation\n\nThe method of moments estimation approach, firstly, checks whether the parameter of interest, \\(\\beta,\\) is identified; i.e., whether \\(\\beta\\) can be written in terms of population moments of observables \\[\n\\beta = \\left(E(X_iX_i')\\right)^{-1} E(X_iY_i),\n\\qquad(5.5)\\] where the identification result of Equation 5.5 only holds under Assumption 2 (exogeneity). The identified parameter \\(\\beta\\) is then estimated using the simple method of moments idea of substituting population moments by sample moments.\nNote: If Assumption 2 (exogeneity) is violated, then we generally have that \\[\n\\beta \\neq \\tilde \\beta = \\left(E(X_iX_i')\\right)^{-1} E(X_iY_i).\n\\]\nThe derivation of the OLS estimator \\[\n\\begin{align*}\n\\hat\\beta\n& = \\left(X'X\\right)^{-1} X'Y\\\\[2ex]\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\frac{1}{n}\\sum_{i=1}^n X_iY_i  \\\\\n& = \\underbrace{\\left(\\sum_{i=1}^n X_iX_i'\\right)^{-1}}_{\\approx (E(X_iX_i'))^{-1}} \\; \\underbrace{\\sum_{i=1}^n X_iY_i}_{\\approx E(X_iY_i)}\n\\end{align*}\n\\] does not require a check whether \\(\\beta\\) is actually identified. By the law of large numbers (sample means converge in probability to population means; see Chapter 7), the OLS estimator is a consistent estimator of \\(\\tilde\\beta,\\) \\[\n\\begin{align*}\n\\hat\\beta \\to_p  \\tilde\\beta = (E(X_iX_i'))^{-1} E(X_iY_i) \\quad \\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\]\nThat is, the OLS estimator will estimate the parameter of interest \\(\\beta\\) only if\\[\n\\beta = \\tilde \\beta = \\left(E(X_iX_i')\\right)^{-1} E(X_iY_i);\n\\] i.e. only if \\(\\beta\\) is identified.\nWithout identification, the OLS estimator estimates \\(\\tilde\\beta,\\) but generally not our parameter of interest \\(\\beta.\\)\nSummary: So, at the end, both estimation approaches require that the parameter of interest, \\(\\beta,\\) is identified. The method of moments estimation approach starts with checking the identification requirement and thus frames the identification requirement more prominently as what it is: a pre-request for consistent estimation of \\(\\beta.\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "06-Small-Sample-Inference.html#normality-of-the-ols-estimator",
    "href": "06-Small-Sample-Inference.html#normality-of-the-ols-estimator",
    "title": "6  Small Sample Inference",
    "section": "",
    "text": "Theorem 6.1 (Normality of \\(\\hat\\beta\\)) Under Assumptions 1-4\\(^\\ast\\) we have that \\[\n\\hat\\beta_n|X \\sim \\mathcal{N}_K\\left(\\beta,Var(\\hat\\beta_n|X)\\right),\n\\qquad(6.1)\\] where \\[\nVar(\\hat\\beta_n|X)=\\underset{(K\\times K)}{\\sigma^2(X'X)^{-1}}.\n\\]\n\n\n\n\n\nProof. This result follows from noting that \\[\n\\begin{align*}\n\\hat\\beta_n\n&=(X'X)^{-1}X'Y\\\\[2ex]\n&=\\beta+(X'X)^{-1}X'\\varepsilon\n\\end{align*}\n\\] and because \\((X'X)^{-1}X'\\varepsilon\\) is just a linear combination of the normally distributed error terms \\(\\varepsilon\\) which, therefore, is again normally distributed, conditionally on \\(X\\). Note that the specific normal distribution depends on the observed realization of \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Small Sample Inference</span>"
    ]
  }
]