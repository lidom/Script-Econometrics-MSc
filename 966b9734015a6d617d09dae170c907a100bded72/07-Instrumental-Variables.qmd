# Instrumental Variables

This chapter builds upon Chapter 12 of @Hansen2022. 


<!-- Lecture videos for this chapter: 

* [Video 1 (Ch. 7.1 and 7.2)](https://uni-bonn.sciebo.de/s/NX9hoNeSGT9HY0V)
* [Video 2 (Ch. 7.3 and 7.4)](https://uni-bonn.sciebo.de/s/pKD0Z8nFh4We1pL)
* [Video 3 (Ch. 7.5)](https://uni-bonn.sciebo.de/s/97jkuWgDdqB8E2B)
* [Video 4 (Ch. 7.6)](https://uni-bonn.sciebo.de/s/qWpoXiZ0d4zB2GH)
* [Video 5 (Ch. 7.7)](https://uni-bonn.sciebo.de/s/3yX7SukSNus81iM) -->


## Introduction

The concepts of **endogeneity** and **instrumental variable** are fundamental to econometrics, and mark a substantial departure from other branches of statistics. 

The ideas of endogeneity arise naturally in economics from the structural model approach leading to models of simultaneous equations such as, for instance, the classic supply/demand model of price determination.

<!-- ### Overview {-} -->

We say that there is **endogeneity** in the linear model
$$
\begin{align}
  Y_i=X_i'\beta+\varepsilon_i
\end{align}
$${#eq-IVLinMod}

1. if $\beta$ is the parameter of interest and 
2. if $E(\varepsilon_i|X_i)\neq 0$ 

and thus
$$
E(X_i\varepsilon_i)\neq 0.
$${#eq-endogen}

When @eq-endogen holds, $X_i$ is **endogenous** for $\beta.$

This situation constitutes a core problem in econometrics which is not so much in the focus of the statistics literature. 


* @eq-IVLinMod is called a **structural equation** 
* $\beta$ in @eq-IVLinMod is called a **structural parameter** 

It is important to distinguish @eq-IVLinMod from the regression or **projection models** we considered so far. It may be the case that a structural model coincides with a regression/projection model, but this is not necessarily the case.


Endogeneity **cannot** happen if the model coefficient is defined by a linear projection. We can *define* the linear (population) **projection coefficient** 
$$
\beta^*=E(X_iX_i')^{-1}E(X_iY_i)
$$
and the corresponding linear (population) projection equation
$$
Y_i = X_i'\beta^* + \varepsilon_i^*,
$$ 
where then, by construction (projection properties), $\varepsilon_i^* = Y_i - X_i'\beta^*$ is a **projection error**, i.e.
$$
E(X_i\varepsilon_i^*)=0.
$$

**Caution:** Here we simply *define* $\beta^*$ as $\beta^*=E(X_iX_i')^{-1}E(X_iY)$ which results in the population version of the projection coefficient for the projection of $Y_i$ into the space spanned by $X_i$ (i.e. the population regression of $Y_i$ on $X_i$.) We did not derive the expression for $\beta^*$ using the exogeneity assumption as we did in @sec-MMEstimator; indeed, the exogeneity assumption may be violated.


The (population) projection coefficient $\beta^*$ and the structural parameter $\beta$ coincide $(\beta^*=\beta)$ under exogeneity. 


However, under endogeneity (@eq-endogen) the projection coefficient $\beta^*$ does not equal the structural parameter $(\beta^*\neq \beta):$
$$
\begin{align*}
\beta^* & =E(X_iX_i')^{-1}E(X_iY_i)\\
\beta^* & =E(X_iX_i')^{-1}E(X_i (X_i'\beta + \varepsilon_i) )\\
%\beta^* & =E(X_iX_i')^{-1}E(X_i X_i') \beta + 
%            E(X_iX_i')^{-1}E(X_i\varepsilon_i)\\
\beta^* & = \beta + E(X_iX_i')^{-1}\underbrace{E(X_i \varepsilon_i)}_{\neq 0}\neq \beta\\
\end{align*}
$$


Thus under endogeneity we cannot simply use the projection coefficient to derive an estimator since **the projection coefficient does not identify the structural parameter of interest**. 

That is, endogeneity implies that the least squares estimator is inconsistent for the structural parameter. Indeed, under i.i.d. sampling, least squares is consistent for the projection coefficient
$$
\hat\beta\to_pE(X_iX_i')^{-1}E(X_iY) = \beta^* \neq \beta,\quad n\to\infty.
$$

But since the structural parameter $\beta$ is here the parameter of interest, endogeneity requires the development of alternative estimation methods. 


## Examples for Structural Equations/Models 

The structural model approach in econometrics goes back to the seminal paper *The probability approach in econometrics* by @Haavelmo1944. 


Under the structural approach one, first, specifies a probabilistic economic model (taking into account economic theory), and then performs a quantitative analysis under the assumption that the economic model is correctly specified. This approach is reflected by Assumption 1 in  @sec-MLR. Researchers often describe this as “taking their model seriously”. A criticism of the structural approach is that it is misleading to treat an economic model as correctly specified. Rather, it is more accurate to view a model as a useful abstraction or approximation.


::: {#exm-MeasurementError}

## Measurement error in the regressor 

<br>

Suppose that:

* $(Y_i,Z_i)$ is a multivariate random variable i.i.d. across $i=1,\dots,n$ 
* $E(Y_i|Z_i)=Z_i'\beta$
* $\beta$ is the structural parameter
* $Y_i  = Z_i'\beta + \varepsilon_i$ is the structural equation for which our model assumptions of @sec-LinModAssumptions can be justified.

Unfortunately, $Z_i$ is not observed (latent), instead we observe a noisy version of $Z_i$
$$
X_i=Z_i + u_i,
$$
where $u_i$ is a $(K\times 1)$ dimensional **classic measurement error**, i.e.

* $u_i$ is independent of all other stochastic quantities in the model
* $E(u_i)=0$

such that $X_i$ is a noisy, but unbiased measure of $Z_i.$

It's easy to express $Y_i$ as a function of the observable $X_i$
$$
\begin{align*}
Y_i 
& = Z_i'\beta + \varepsilon_i \\ 
& = (X_i - u_i)'\beta + \varepsilon_i \\ 
& = X_i'\beta  + v_i,\\ 
\end{align*}
$$
where $v_i=\varepsilon_i - u_i'\beta$. 

That is, the relationship of $Y_i$ and $X_i$ is described by a linear equation
$$
Y_i = X_i'\beta  + v_i,
$$
with a stochastic error term $v_i$, but this error term is (generally) not a projection error since
$$
\begin{align}
E(X_iv_i) 
& = E((Z_i + u_i)\,(\varepsilon_i - u_i'\beta)) \\
& = \underbrace{E(Z_i \varepsilon_i)}_{=0} - \underbrace{E(Z_iu_i'\beta)}_{=0}  + \underbrace{E(u_i \varepsilon_i)}_{=0} - E(u_iu_i')\beta \\
& = - E(u_iu_i')\beta \\ 
&\neq 0 
\end{align}
$${#eq-measurementBias2}
if $\beta\neq 0$ and $E(u_iu_i')\neq 0.$ 

Generally 

* $\beta$ will not equal zero and 
* the $(K\times K)$ variance-covariance matrix $E(u_iu_i')$ will also not equal a zero matrix since this would mean that the measurement errors have variance zero.


We can calculate the (population) projection coefficient $\beta^*$ (which can be consistently estimated): 
$$
\begin{align*}
\beta^* 
& = E(X_iX_i')^{-1} E(X_i Y_i)\\
& = E(X_iX_i')^{-1} E(X_i (X_i'\beta + v_i))\\
& = \beta + E(X_iX_i')^{-1} \underbrace{E(X_i v_i)}_{\neq 0}
\end{align*}
$$
In the special case where $K=1$, we have
$$
\begin{align}
\beta^*_1 
& = \beta_1 + \frac{E(X_i v_i)}{E(X_i^2)}\\
& = \beta_1 - \frac{E(u_i^2)\beta_1}{E(X_i^2)}\\
& = \beta_1\left(1 - \frac{ E(u_i^2)}{E(X_i^2)}\right) < \beta_1,
\end{align}
$${#eq-measurementBias}
where we used that, by @eq-measurementBias2, $E(X_iv_i)=-E(u_i^2)\beta_1$, and 
where the inequality follows from observing that $E(u_i^2)/E(X_i^2)<1$ since
$$
\begin{align*}
E(X_i^2) 
&= E((Z_i + u_i)^2)\\
&= E(Z_i^2) + 2E(Z_iu_i) + E(u_i^2) \\
&= E(Z_i^2) + 0 + E(u_i^2)\\  
&> E(u_i^2)\\  
\Leftrightarrow\quad 0\leq \frac{E(u_i^2)}{E(X_i^2)}&<1.
\end{align*}
$$
@eq-measurementBias shows that projection coefficient $\beta_1^*$ shrinks the structural parameter $\beta_1$ towards zero. This is called  **measurement error bias** or **attenuation bias**.
:::


::: {#exm-SupplyAndDemand}

##  Supply and Demand

<br>


The variables $Q_i\in\mathbb{R}$ and $P_i\in\mathbb{R}$ (Quantity and Price) are determined jointly by the following simultaneous equation system consisting of the demand equation
$$
Q_i = -\beta_1 P_i + \varepsilon_{1i}
$$
and of the supply equation
$$
Q_i =  \beta_2 P_i + \varepsilon_{2i},
$$
where we assume that $Q_i$ and $P_i$ are centered such that we can drop the intercepts. 

Assume that the $(2\times 1)$ dimensional joint random error $\varepsilon_i=(\varepsilon_{1i},\varepsilon_{2i})'$ satisfies $E(\varepsilon_i)=0$ and $E(\varepsilon_i\varepsilon_i')=I_2$ (the latter for simplicity). 

> What happens if we regress $Q_i$ on $P_i$? 
$$
Q_i=\beta^* P_i+\varepsilon^*_i
$$ 

To answer this question, we need to solve $Q_i$ and $P_i$ in terms of the errors $\varepsilon_{1i}$ and $\varepsilon_{2i}$.
$$
\left(\begin{matrix}
1 & \beta_1\\
1 & -\beta_2\\
\end{matrix}
\right)
\left(\begin{matrix}
Q_i\\
P_i\\
\end{matrix}
\right) = 
\left(\begin{matrix}
\varepsilon_{1i}\\
\varepsilon_{2i}\\
\end{matrix}
\right)
$$
Rewriting yields (assuming invertibility)
$$
\begin{align}
\left(\begin{matrix}
Q_i\\
P_i\\
\end{matrix}
\right) 
&= 
\left(\begin{matrix}
1 & \beta_1\\
1 & -\beta_2\\
\end{matrix}
\right)^{-1}
\left(\begin{matrix}
\varepsilon_{1i}\\
\varepsilon_{2i}\\
\end{matrix}
\right)\\[2ex]
&= 
\frac{1}{-\beta_2 - \beta_1}
\left(\begin{matrix}
-\beta_2 &   -\beta_1\\
 - 1    &           1\\
\end{matrix}
\right)
\left(\begin{matrix}
\varepsilon_{1i}\\
\varepsilon_{2i}\\
\end{matrix}
\right)\\[2ex]
&= 
\left(\begin{matrix}
\beta_2 &   \beta_1\\
 1    &     -1\\
\end{matrix}\right)
\left(\begin{matrix}
\varepsilon_{1i}\\
\varepsilon_{2i}\\
\end{matrix}\right)
\frac{1}{\beta_1 +\beta_2}\\[2ex]
&=
\left(
\begin{matrix}
(\beta_2 \varepsilon_{1i} + \beta_1  \varepsilon_{2i})/(\beta_1 + \beta_2)\\
(\varepsilon_{1i} - \varepsilon_{2i})/(\beta_1 + \beta_2)
\end{matrix}  
\right)
\end{align}
$$
Thus, under the simplification that $E(\varepsilon_i\varepsilon_i')=I_2$
$$
\begin{align*}
E(P_iQ_i)
&=\frac{E\left((\varepsilon_{1i} - \varepsilon_{2i})(\beta_2 \varepsilon_{1i} + \beta_1  \varepsilon_{2i})\right)}{(\beta_1 + \beta_2)^2}\\ 
&=\frac{\beta_2 E\left(\varepsilon_{1i}^2\right) - \beta_1 E\left(\varepsilon_{2i}^2\right)}{(\beta_1 + \beta_2)^2}=\frac{\beta_2 - \beta_1}{(\beta_1 + \beta_2)^2}
\end{align*}
$$
$$
\begin{align*}
E(P_i^2)
&=\frac{E\left((\varepsilon_{1i} - \varepsilon_{2i})^2\right)}{(\beta_1 + \beta_2)^2}\\ 
&=\frac{E\left(\varepsilon_{1i}^2\right) + E\left(\varepsilon_{2i}^2\right)}{(\beta_1 + \beta_2)^2} = \frac{2}{(\beta_1 + \beta_2)^2}
\end{align*}
$$

The (population) projection of $Q_i$ on $P_i$ yield 
$$
Q_i=\beta^* P_i+\varepsilon^*_i
$$ 
with $E(P_i\varepsilon^*_i)=0$ and 
$$
\beta^* = \frac{E(P_iQ_i)}{E(P_i^2)} = \frac{\beta_2 - \beta_1}{2}
$$
The projection coefficient equals the average of the demand slope $\beta_1$ and the supply slope $\beta_2.$ The OLS estimator satisfies $\hat\beta\to_p\beta^*$ and thus the limit does not equal one of the structural parameters $\beta_1$ or $\beta_2.$ This is called **simultaneous equation bias**.  

Generally, when both the dependent variable and a regressor are **simultaneously determined** then the regressor should be treated as endogenous.
:::


## Endogenous Regressors

<!-- Technically, we say that a regressor $X_i$ is **exogenous** for $\beta$ if $E(X_i\varepsilon_i)=0.$ In general the distinction in an economic model is that a regressor $X_i$ is endogenous if it is jointly determined with $Y_i,$ while a regressor $X_i$ is exogenous if it is determined separately from $Y_i.$  -->

Usually, only a subset of the regressors need to be treated as **endogenous**. In the following, we partition the vector of regressors 
$$
\underset{(K\times 1)}{X_i}=
\left(
\begin{matrix}
\underset{(K_1\times 1)}{X_{i1}}\\
\underset{(K_2\times 1)}{Y_{i2}}
\end{matrix}
\right)
$$ 
with $K=K_1 + K_2$.

* $X_{i1}$: $(K_1\times 1)$ vector of **exogenous** regressors, i.e. $E(X_{i1}\varepsilon_i)=0$  
* $Y_{i2}$: $(K_2\times 1)$ vector of **endogenous** regressors, i.e. $E(Y_{i2}\varepsilon_i)\neq 0$

The structural equation is then
$$
Y_{i1}=X_{i1}'\beta_1 + Y_{i2}'\beta_2 + \varepsilon_i
$${#eq-StructuralEqEndogen1}  

* $Y_{i1}=Y_i$: **endogenous** *dependent* variable, i.e. $E(Y_{i1}\varepsilon_i)\neq 0$


This notation clarifies which variables are exogenous and which endogenous. 

## Instruments

To consistently estimate the structural parameter $\beta$ we need additional information. One type of information commonly used in econometrics are called **instruments** or **instrumental variables**.


::: {#def-InstrumentalVariable}

## Instrumental Variable

The $\ell\times 1$ random vector $Z_i$ is called an **instrumental variable** for @eq-StructuralEqEndogen1 if
$$
E(Z_i\varepsilon_i)=0
$${#eq-IV1}
$$
\operatorname{rank}\Big(E(Z_iZ_i')\Big)=\ell  
$${#eq-IV2}
$$
\operatorname{rank}\Big(\underbrace{E(Z_iX_i')}_{(\ell\times K)}\Big)=K, 
$${#eq-IV3}
where $\underset{1\times (K_1+K_2)}{X_i'}=(X_{i1}',Y_{i2}').$
:::

**Explanation of @def-InstrumentalVariable:**

* @eq-IV1 states that the instruments are uncorrelated with the error term (**exogeneity condition**)
* @eq-IV2 excludes linearly redundant instruments
* @eq-IV3 is called the **relevance condition** and is essential for the identification of the structural parameter $\beta$; see @sec-identification. 
  * A necessary condition for @eq-IV3 is that $\ell\geq K.$

Note that the exogenous regressors $X_{i1}$ fulfill @eq-IV1 (they are valid instruments for themselves) and thus should be included as (additional) instrumental variables, i.e. 
$$
\underset{(\ell\times 1)}{Z_i}=
\underset{((K_1+\ell_2)\times 1)}{\left(\begin{matrix}Z_{i1}\\Z_{i2}\end{matrix}\right)} :=
\underset{((K_1+\ell_2)\times 1)}{\left(\begin{matrix}X_{i1}\\Z_{i2}\end{matrix}\right)}
$$
with $\ell=K_1+\ell_2.$

With this notation we can also write the structural @eq-StructuralEqEndogen1 as
$$
Y_{i1} = Z_{i1}'\beta_1 + Y_{i2}'\beta_2 + \varepsilon_i
$${#eq-StructuralEqEndogen2}


* $Z_{i1}$: ($K_1\times 1$) vector of **included exogenous variables** 
<!-- $(Z_{i1}=X_{i1})$ -->
  * uncorrelated with $\varepsilon_i$ and thus potentially usable
  * **included** since they have potentially non-zero coefficients in @eq-StructuralEqEndogen2
* $Z_{i2}$: ($\ell_2\times 1$) vector of **excluded exogenous variables**
  * uncorrelated with $\varepsilon_i$  and thus potentially usable
  * **excluded** since they have **zero coefficients**[^1] in @eq-StructuralEqEndogen2

[^1]: The instrumental variables $Z_{i2}$ correlate with $Y_{i1}$ only indirectly through $X_{i}'=(X_{i1}',Y_{i2}')$ (by @eq-IV3). Therefore, since $X_{i}'=(X_{i1}',Y_{i2}')$ are included in the equation for $Y_i$, there are no variations left in $Y_{i1}$ that correlate with $Z_{i2}.$ Thus the instrumental variables $Z_{i2}$ have zero coefficients in the equation for $Y_{i1}.$


<!-- **Terminology:** -->

<!-- * $Z_{i1}$: $(K_1\times 1)$ vector of **exogenous** regressors $(Z_{i1}=X_{i1})$, i.e. $E(Z_{i1}\varepsilon_i)=0$   -->

<!-- * $Y_{i1}\in\mathbb{R}$:        *Endogenous dependent variable*
* $Z_{i1}\in\mathbb{R}^{K_1}$:  *Exogenous regressors* ($Z_{i1}=X_{i1}$)
* $Y_{i2}\in\mathbb{R}^{K_2}$:  *Endogenous regressor*
* $Z_{i2}\in\mathbb{R}^{\ell}$: *Instrumental variables* -->


We say that the model is 

  * **just-identified** if $\ell = K_1 + \ell_2 = K$ 
  * **over-identified** if $\ell = K_1 + \ell_2 > K$


> What variables can be used as Instrumental Variables (IVs)? 

1. Instrumental variables must be uncorrelated with the error term (@eq-IV1).
2. Instrumental variables must be correlated with the endogenous variables $Y_{i2}$ also after controlling for the other already included exogenous variables $Z_{i1}$ (@eq-IV3).[^2] I.e. the IVs must contain *additional information*, and thus there must not be a perfect multicollinearity between the IVs, $Z_{i2},$ and the already included exogenous regressors $Z_{i1}.$

[^2]: Thus if you regress each of the instruments in $Z_{i2}$ on the exogenous variables $Z_{i1}$, then the residuals of these regressions should still correlate with the endogenous variables in $Y_{i2}.$

<!-- From the rank condition  it follows that the  (Otherwise, the instrumental variables would not add any new information since they could be expressed as linear combinations of the already included exogenous variables $Z_{i1}.$) -->


These two requirements mean that the IVs, $Z_{i2},$ ...

* ... are determined outside the systems for the endogenous variables $Y_{i1}$ and $Y_{i2}.$
* ... causally determine the endogenous variables $Y_{i2}.$
* ... not causally determine the dependent variable $Y_{i1},$ except indirectly through $Y_{i2}.$


<!-- **Measurement error in the regressor (@exm-MeasurementError continued)** 

When $X_{i}=Z_i+u_i$ is a mis-measured version of the latent $Z_i,$ a common choice for an instrument $Z_{i2}$ is an alternative (additional) measurement 
$$
Z_{i2} =  Z_i + \tilde{u}_{i}
$$
of $Z_i.$ This additional measurement $Z_{i2}$ is an IV if

* $\tilde{u}_{i}$ is independent of $X_i.$ Since then $E(Z_{i2})$
* 

satisfy the property of an instrumental variable the measurement error in $Z_{i2}$ must be independent of that in $X_i.$ -->



## Reduced Form

The **reduced form** models are **projection models** giving us the relationships 

1. between the $(K_2\times 1)$ endogenous regressors $Y_{i2}$ and the $(\ell\times 1)$  instruments $Z_{i}.$
2. between the $(1\times 1)$ endogenous regressors $Y_{i1}$ and the $(\ell\times 1)$  instruments $Z_{i}.$


The linear reduced form model for $Y_{i2}$ is given by the population projection model that regresses $Y_{i2}$ on the exogenous instruments $Z_{i}:$
$$
\begin{align}
Y_{i2} 
&= \Gamma'Z_i + u_{i2}\\ 
&= \Gamma_{12}'Z_{i1} + \Gamma_{22}'Z_{i2} + u_{i2},
\end{align}
$${#eq-reducedFormModelY2}
which implies that $E(Z_iu_{i2}')=0.$ 

The $(\ell\times K_2)$ dimensional coefficient matrix $\Gamma$ is defined by the projection coefficient
$$
\underset{(\ell\times K_2)}{\Gamma}=\left(\begin{matrix}\underset{(K_1\times K_2)}{\Gamma_{12}}\\ \underset{(\ell_2\times K_2)}{\Gamma_{22}}\end{matrix}\right) = E(Z_iZ_i')^{-1}E(Z_iY_{i2}')
$${#eq-Gamma}


> Note: @eq-Gamma defines a *matrix-valued* regression/projection parameter and can be thought as lining up single ordinary vector-valued regression parameters $[\Gamma]_{\cdot,l} = E(Z_iZ_i')^{-1}E(Z_iY_{i2,l}')$, $l=1,\dots,\ell$, where $[\Gamma]_{\cdot,l}$ is the $l$th column of $\Gamma$ and $Y_{i2,l}$ the $l$th element of $Y_{i2}$. More details can be found, for instance, in Chapter 11 of @Hansen2022.

The projection coefficient $\Gamma$ (@eq-Gamma) is well defined and unique if @eq-IV2 holds since then $E(Z_iZ_i')$ is invertible.  


The linear reduced form projection model for $Y_{i1}$ is derived from substituting the reduced form projection model for  $Y_{i2}$ (@eq-reducedFormModelY2) into the structural equation for $Y_{i1}$ (@eq-StructuralEqEndogen2):
$$
\begin{align}
Y_{i1} 
&= Z_{i1}'\beta_1 + Y_{i2}'\beta_2 + \varepsilon_i\\ 
&= Z_{i1}'\beta_1 + (\Gamma_{12}'Z_{i1} + \Gamma_{22}'Z_{i2} + u_{i2})'\beta_2 + \varepsilon_i\\ 
&= Z_{i1}'\beta_1 + Z_{i1}'\Gamma_{12}\beta_2 + Z_{i2}'\Gamma_{22}\beta_2  + u_{i2}'\beta_2 + \varepsilon_i\\ 
&= Z_{i1}' (\beta_1 + \Gamma_{12}\beta_2) + Z_{i2}' (\Gamma_{22}\beta_2)  + (u_{i2}'\beta_2 + \varepsilon_i)\\ 
&= Z_{i1}' \lambda_1 + Z_{i2}' \lambda_2  + u_{i1}\\ 
&= Z_{i}' \lambda + u_{i1}, 
\end{align}
$${#eq-reducedFormModelY1}
where $u_{i1} = u_{i2}'\beta_2 + \varepsilon_i.$ Since @eq-reducedFormModelY1 is a projection model, we have that $E(Z_iu_{i1})=0$ such that the $(\ell\times 1)$ dimensional coefficient $\lambda$ is defined by the projection coefficient
$$
\underset{(\ell\times 1)}{\lambda}=\left(\begin{matrix}\underset{(K_1\times 1)}{\lambda_{1}}\\ \underset{(\ell_2\times 1)}{\lambda_{2}}\end{matrix}\right) = E(Z_iZ_i')^{-1}E(Z_iY_{i})
$${#eq-lambda}

Note that the thus estimable projection parameter $\lambda$ of the reduced form projection model for $Y_{i1}$ (@eq-reducedFormModelY1) can written as a linear function $\bar{\Gamma}\beta$ of the (not directly estimable) structural parameter $\beta$: 
$$
\begin{align*}
\underset{(\ell\times 1)}{
\lambda} = 
\left(
\begin{matrix}
\lambda_1\\ 
\lambda_2
\end{matrix}
\right) 
&= 
\left(
\begin{matrix}
\beta_1 + \Gamma_{12}\beta_2 \\ 
\Gamma_{22}\beta_2
\end{matrix}
\right) \\
&= 
\underset{(\ell\times (K_1+K_2))}{
\left[
\begin{matrix}
I_{K_1} &  \Gamma_{12}\\ 
0 &  \Gamma_{22}
\end{matrix}
\right]} 
\underset{((K_1+K_2)\times 1)}{
\left(
\begin{matrix}
\beta_1\\ 
\beta_2
\end{matrix}
\right)} = \bar{\Gamma} \beta. 
\end{align*}
$${#eq-lambdaGammabeta}
Luckily, the unknown components $\Gamma_{12}$ and $\Gamma_{12}$ of $\bar{\Gamma}$ are together the estimable projection parameter $\Gamma$ (@eq-Gamma). Moreover, we can generalize @eq-Gamma for $\Gamma$ to an expression for $\bar{\Gamma}$:
<!-- $$
\underset{(\ell\times K_2)}{\Gamma}=\left(\begin{matrix}\underset{(K_1\times K_2)}{\Gamma_{12}}\\ \underset{(\ell_2\times K_2)}{\Gamma_{22}}\end{matrix}\right) & = E(Z_iZ_i')^{-1}E(Z_iY_{i2}')
$$ -->
$$
\underset{(\ell\times K )}{\bar{\Gamma}}=\left[\begin{matrix}I_{K_1}&\underset{(K_1\times K_2)}{\Gamma_{12}}\\ \underset{(\ell_2\times K_1)}{0}& \underset{(\ell_2\times K_2)}{\Gamma_{22}}\end{matrix}\right] = E(Z_iZ_i')^{-1}E\big(Z_{i}\;\overbrace{\underset{(1\times (K_1+K_2))}{(Z_{i1}',Y_{i2}')}}^{=X_i'}\;\big)
$${#eq-GammaBar}

<!-- Zero coefficients: The $\ell_2$ instruments shall not be causally related with $Y_{i1}$ which is determined by the $K_1$ included exogenous variables $Z_{i1}.$ -->


The least squares estimators of the regression/projection parameters

* $\Gamma$ (@eq-Gamma)
* $\bar{\Gamma}$ (@eq-GammaBar)
* $\lambda$ (@eq-lambda) 

are given by the following moment estimators:
$$
\begin{align*}
\hat{\Gamma} 
&= \left(\sum_{i=1}^nZ_iZ_i'\right)^{-1}\left(\sum_{i=1}^nZ_iY_{i2}'\right)\\ 
&= \left(Z'Z\right)^{-1}(Z'Y_{2})
\end{align*}
$${#eq-GammaHat}
$$
\begin{align*}
\widehat{\bar{\Gamma}} 
&= \left(\sum_{i=1}^nZ_iZ_i'\right)^{-1}\left(\sum_{i=1}^nZ_i X_i'\right)\\ 
&= \left(Z'Z\right)^{-1}(Z'X)
\end{align*}
$${#eq-GammaBarHat}
$$
\begin{align*}
\hat{\lambda} 
&= \left(\sum_{i=1}^nZ_iZ_i'\right)^{-1}\left(\sum_{i=1}^nZ_iY_{i1}\right)\\ 
&= \left(Z'Z\right)^{-1}(Z'Y_1)
\end{align*}
$${#eq-lambdaHat}
where 

* $Z$ is a $(n\times \ell)$ matrix with $Z_i'=(Z_{i1}',Z_{i2}')$ in the $i$th row
* $X$ is a $(n\times K)$ matrix with $X_i'=(X_{i1}',Y_{i2}')$ in the $i$th row 
* $Y_1$ is a $(n\times 1)$ vector with $Y_{i1}$ in the $i$th element
* $Y_2$ is a $(n\times K_2)$ matrix with $Y_{i2}'$ in the $i$th row

with $i=1,\dots,n$


## Identification {#sec-identification}

A parameter is **identified** if it is a unique function of the probability distribution of the observables. 


One way to show that a parameter is identified is to write it as an explicit function of population moments. For example, the reduced form coefficient matrices $\Gamma$ and $\lambda$ are identified because they can be written as explicit functions of the moments of the variables $(Y_{i1},Y_{i2},X_i,Z_i).$ That is,
$$
\underset{(\ell\times K_2)}{\Gamma}=\left(\begin{matrix}\underset{(K_1\times K_2)}{\Gamma_{12}}\\ \underset{(\ell_2\times K_2)}{\Gamma_{22}}\end{matrix}\right) = E(Z_iZ_i')^{-1}E(Z_iY_{i2}')
$$
and
$$
\underset{(\ell\times 1)}{\lambda}=\left(\begin{matrix}\underset{(K_1\times 1)}{\lambda_{1}}\\ \underset{(\ell_2\times 1)}{\lambda_{2}}\end{matrix}\right) = E(Z_iZ_i')^{-1}E(Z_iY_{i})
$$
are uniquely determined by the probability distribution of $(Y_{i1},Y_{i2},Z_i)$ if @def-InstrumentalVariable holds because this definition includes the requirement that $E(Z_iZ_i')$ has full rank (@eq-IV2). 


We are interested in the structural parameter $\beta,$ which relates to $\Gamma$ and $\lambda$ through equation @eq-lambdaGammabeta; i.e. through
$$
\lambda = \underset{(\ell\times K)}{\bar{\Gamma}}\beta.
$$  

The structural parameter $\beta$ is **identified** if it is uniquely determined by this relation, which is a set of $\ell$ equations with $K$ unknown, where $\ell\geq K.$ Form linear algebra we know that there is a unique solution if and only if $\bar{\Gamma}$ has full column rank $K$
$$
\operatorname{rank}\left(\bar{\Gamma}\right)=K.
$${#eq-BarGammaRank}
<!-- If @eq-BarGammaRank fails, then @eq-lambdaGammabeta has fewer (linear independent) equations than coefficients so there is no unique solution.  -->

From @eq-GammaBar we know that we can write $\bar{\Gamma}$ as 
$$
\bar{\Gamma}=\left[\begin{matrix}I_{K_1}&\underset{(K_1\times K_2)}{\Gamma_{12}}\\ 0& \underset{(\ell_2\times K_2)}{\Gamma_{22}}\end{matrix}\right] = E(Z_iZ_i')^{-1}E(Z_iX_i').
$$
Combining this with @eq-lambdaGammabeta, we obtain
$$
\begin{align*}
\overbrace{E(Z_iZ_i')^{-1}E(Z_iY_{i})}^{\lambda} & = \overbrace{E(Z_iZ_i')^{-1}E(Z_iX_i')}^{\bar{\Gamma}} \beta \\
\Leftrightarrow \underset{(\ell\times 1)}{E(Z_iY_{i})} & =  \underset{(\ell\times K)}{E(Z_iX_i')} \underset{(K\times 1)}{\beta} 
\end{align*}
$$
which is a set of $\ell$ equations in $K$ unknowns. This system of linear equations has a unique solution if and only if 
$$
\operatorname{rank}\left(E(Z_iX_i')\right)=K
$$
which is the required **identification/relevance condition** (@eq-IV3) in the definition of instrumental variables (@def-InstrumentalVariable).


It is useful to have explicit expressions for the solution $\beta.$ The easiest case is when $\ell = K,$ because then $\beta=E(Z_iX_i')^{-1}E(Z_iY_{i})$ and we can get an estimator for $\beta$ by substituting the population moments with sample moments (methods of moments estimator). 

The two-state least squares estimator, discussed in the next section, allows us to consider the information of more instruments $\ell\geq K.$

## Two-Stage Least Squares

From the reduced form projection @eq-reducedFormModelY1 and @eq-lambdaGammabeta (i.e. $\lambda = \bar{\Gamma}\beta$), we have
$$
\begin{align*}
Y_i 
& = Z_i' \lambda + u_{i1}  \\
& = Z_i' \bar{\Gamma}\beta + u_{i1}  \\
E(Z_iu_{i1}) & = 0.   
\end{align*}
$$
The parameter $\bar{\Gamma}$ is unknown, but consistently estimable (see @eq-GammaBarHat). 

Let us, for a moment, assume we would know $\bar{\Gamma}$ and let  
$$
W_{i}=\bar{\Gamma}'Z_i.
$$ 
$W_{i}$ is thus a specific linear combination of the $\ell$ instruments $Z_i$ which allows us to identify the structural parameter $\beta$ in a regression/projection model
$$
\begin{align*}
Y_i & = W_i'\beta + u_{i1}  \\
E(W_iu_{i1}) & = 0.
\end{align*}
$$

Thus, if we would know $\bar{\Gamma}$, we would estimate $\beta$ by 
$$
\begin{align*}
\hat{\beta} 
& = \left(\sum_{i=1}^nW_iW_i'\right)^{-1}\left(\sum_{i=1}^nW_iY_{i1}\right)\\ 
& = \left(W'W\right)^{-1}\left(WY_{1}\right)\\
& = \left(\bar{\Gamma}'Z'Z\bar{\Gamma}\right)^{-1}\left(\bar{\Gamma}'Z'Y_{1}\right)\\
\end{align*}
$$
To make $\hat{\beta}$ a feasible estimator, we simply replace the unknown $\bar{\Gamma}$ by its estimator (@eq-GammaBarHat) 
$$
\begin{align*}
\widehat{\bar{\Gamma}} 
& = \left(\sum_{i=1}^nZ_iZ_i'\right)^{-1}\left(\sum_{i=1}^nZ_iX_{i}'\right)\\ 
& = \left(Z'Z\right)^{-1}\left(Z'X\right),
\end{align*}
$$
where the $(n\times \ell)$ dimensional matrix $Z$ contains in the $i$th row the $(1\times \ell)$ vector $Z_i',$ and where the $(n\times K)$ dimensional matrix $X$ contains in the $i$th row the $(1\times K)$ vector $X_i'.$ 

This then yields the **Two Stage Lease Squares (2SLS) estimator**:
$$
\begin{align*}
\hat{\beta}_{2SLS} 
& = \left(\widehat{\bar{\Gamma}}'Z'Z\widehat{\bar{\Gamma}}\right)^{-1}\left(\widehat{\bar{\Gamma}}'Z'Y_{1}\right)\\
& = \left(X'Z\left(Z'Z\right)^{-1}Z'Z\left(Z'Z\right)^{-1}Z'X\right)^{-1}\left(X'Z\left(Z'Z\right)^{-1}Z'Y_{1}\right)\\
& = \left(X'Z\left(Z'Z\right)^{-1}Z'X\right)^{-1}\left(X'Z\left(Z'Z\right)^{-1}Z'Y_{1}\right)\\
& = \left(X'P_ZX\right)^{-1}\left(X'P_ZY_{1}\right),\\
\end{align*}
$$
where $P_Z=Z\left(Z'Z\right)^{-1}Z'$ is the projection matrix that projects into the vector spaced spanned by the columns of $Z.$ 


The projection matrix $P_Z$ motivates the name **"Two Stage Least Squares"**, since computing $\hat{\beta}_{2SLS}$ is equivalent to conduct the following two stage procedure: 

1. Compute the fitted values when regressing $X$ on $Z$: $\hat{X}=P_ZX$
2. Regress $Y_1$ on $\hat{X}:\;$  $\hat{\beta}_{2SLS}=\left(\hat{X}'\hat{X}\right)^{-1}\hat{X}'Y_1$

This two stage approach is indeed equivalent to the above direct approach:
$$
\begin{align*}
\left(\hat{X}'\hat{X}\right)^{-1}\hat{X}'Y_1
& = \left((P_ZX)'P_ZX\right)^{-1}\left((P_ZX)'Y_{1}\right)\\
& = \left(X'P_Z'P_ZX\right)^{-1}\left(X'P_Z'Y_{1}\right)\\
& = \left(X'P_ZP_ZX\right)^{-1}\left(X'P_ZY_{1}\right)\\
& = \left(X'P_ZX\right)^{-1}\left(X'P_ZY_{1}\right) = \hat{\beta}_{2SLS}
\end{align*}
$$


## Monte-Carlo Simulation 

Let's consider the following data generating process with endogenous regressor $X_{i3}$:
$$
\begin{align*}
\overbrace{Y_i}^{=Y_{i1}} 
& = \beta_1 + \beta_2 X_{i2}  + \beta_3 \overbrace{X_{i3}}^{=Y_{i2}} + \varepsilon_i\\ 
& = X_i'\beta + \varepsilon_i,
\end{align*}
$$
where $X_i'=(1,X_{i2},X_{i3})$ with 
$$
\begin{align*}
\beta         & = (\beta_1,\beta_2,\beta_3)'=(2,2,2)'\\ 
\varepsilon_i & \sim \mathcal{N}(0,1)\\ 
X_{i2}        & \sim \mathcal{N}(3,4)\\ 
V_{i}         & \sim \mathcal{N}(1,9)\\
X_{i3}        & = V_{i}  + 2 \varepsilon_i
\end{align*}
$$

The following `R` code draws `B=10000` many i.i.d. samples from this data generating process and checks the (un-)biasedness of the OLS estimators $\hat\beta_2$ and $\hat\beta_2$:
```{r}
n         <- 100      # Sample size 
beta_true <- c(2,2,2) # Structural parameter

B         <- 10000    # Number of MC replications
beta_hat  <- matrix(data = NA, # Container for 
                    nrow = 3,  # MC simulations
                    ncol = B)

## Monte Carlo (MC) simulation for generating 
## estimates when X3 is endogenous for beta3:
for(b in 1:B){
  eps          <- rnorm(n)                # error term 
  X_2          <- rnorm(n, m = 3, sd = 2) # exogenous
  V            <- rnorm(n, m = 1, sd = 3) # exogenous
  X_3          <- V + 2 * eps             # endogenous
  ## Dependent variable
  Y            <- cbind(1, X_2, X_3) %*% beta_true + eps
  ## Estimation
  beta_hat[,b] <- coef(lm(Y ~ X_2 + X_3))
}

## MC means of \hat{\beta}_2 and \hat{\beta}_3 
mean_beta_hat_2 <- round(mean(beta_hat[2,]), 2)
mean_beta_hat_3 <- round(mean(beta_hat[3,]), 2)

## Unbiased estimator \hat{\beta}_2:
mean_beta_hat_2  - beta_true[2]

## Biased estimator \hat{\beta}_3:
mean_beta_hat_3  - beta_true[3]
```
<!-- Caution: Since intercept is determined by the slope estimates and the regressor means, it must be biased if one of the slope estimates is biased.  -->

The estimator $\hat\beta_3$ is clearly biased due to the endogeneity of $X_{i3}.$


### Instrumental Variable {-}

Let's say there is a real-valued instrumental variable 
$$
Z_{i2}=V_{i} + u_i,\quad \text{where}\quad u_i\sim\mathcal{U}(0,1).
$$ 
That is, $Z_{i2}$ correlates with the endogenous $X_{i3}$ through $V_i,$ but is independent of $\varepsilon_i.$ 

The $\ell=3$ dimensional instrumental variables vector $Z_i$ collects all exogenous information, i.e.
$$
Z_i = 
\left(\begin{matrix}
1\\ 
X_{i2}\\ 
Z_{i2}
\end{matrix}\right).
$$

The $Z_i$ vector is a valid instrumental variables vector, if it fulfills the conditions of @eq-IV1 -- @eq-IV3 of @def-InstrumentalVariable. 

* Checking @eq-IV1, i.e., $E(Z_i\varepsilon_i)=0$: 
 <!-- i.e. $E(Z_i\varepsilon_i)=0$ is fulfilled if each of the elements in $Z_i$ and $\varepsilon_i$ are **uncorrelated** since
$$
\begin{align*}
\operatorname{Cov}(Z_{i2},\varepsilon_i) &= E(Z_{i2}\varepsilon_i) - E(Z_{i2})\;\overbrace{E(\varepsilon_i)}^{=0}\\ 
\Rightarrow \operatorname{Cov}(Z_{i2},\varepsilon_i) &= E(Z_{i2}\varepsilon_i) 
\end{align*} 
$$
Checking the single elements in $Z_i = (1,X_{i2},Z_{i2})'$: 
-->
$$
\begin{align*}
E(Z_i\varepsilon_i)
&=
\left(\begin{matrix}
E(1\varepsilon_i)\\
E(X_{i2}\varepsilon_i)\\
E(Z_{i2}\varepsilon_i)
\end{matrix}\right)
=
\left(\begin{matrix}
\operatorname{Cov}(1,\varepsilon_i)\\
\operatorname{Cov}(X_{i2},\varepsilon_i)\\
\operatorname{Cov}(Z_{i2},\varepsilon_i)
\end{matrix}\right)\\
&=
\left(\begin{matrix}
0\\
0\\
\underbrace{\operatorname{Cov}(V_{i},\varepsilon_i)}_{=0} + \underbrace{\operatorname{Cov}(u_i,\varepsilon_i)}_{=0}
\end{matrix}\right)
=\underset{(\ell\times 1)}{0}
\end{align*}
$$
Thus, the instruments do not correlate with the error term $\varepsilon_i$.
  <!-- * $E(1\varepsilon_i)=E(\varepsilon_i)=0$ 
  * $E(X_{i2}\varepsilon_i)=\operatorname{Cov}(X_{i2},\varepsilon_i)=0$ 
  * $E(Z_{i2}\varepsilon_i)=\operatorname{Cov}(Z_{i2},\varepsilon_i)=\underbrace{\operatorname{Cov}(V_{i},\varepsilon_i)}_{=0} + \underbrace{\operatorname{Cov}(u_i,\varepsilon_i)}_{=0}$  -->
* Checking @eq-IV2, i.e., $\operatorname{rank}(E(Z_iZ_i'))=\ell=3$: 
<!-- is fulfilled if no instrument is a linear combination of the others. For $Z_i = (1,X_{i2},Z_{i2})'$ the condition that $\operatorname{rank}(E(Z_iZ_i'))=\ell$, i.e., that  -->
$$
\begin{align*}
&\operatorname{rank}(E(Z_iZ_i'))\\ 
=&\operatorname{rank}\left(
  \begin{matrix}
  1 \cdot\;1                   & E\left(1\;\cdot X_{i2}\right)  & E\left(1\;\cdot Z_{i2}\right) \\ 
  E\left(X_{i2}\cdot\;1\right) & E\left(X_{i2} X_{i2}\right)    & E\left(X_{i2} Z_{i2}\right) \\ 
  E\left(Z_{i2}\cdot\;1\right) & E\left(X_{i2} Z_{i2}\right)    & E\left(Z_{i2} Z_{i2}\right) \\ 
  \end{matrix}\right)
\end{align*}
$$ 
A linear dependency would occur only ...
  * if $Z_{i2}= 1$ (between 1st and 3rd column) 
  * if $Z_{i2}= X_{i2}$ (between 2nd and 3rd column) 
  * if $Z_{i2}= 0$ (between zero column)

Thus the "no redundancies" condition, $\operatorname{rank}(E(Z_iZ_i'))=3$, is fulfilled since 
$$
\begin{align*}
Z_{i2}=V_i + u_i 
& \neq 1\\ 
&\neq X_{i2}\\
&\neq 0.
\end{align*}
$$    

* Checking @eq-IV3, i.e., $\operatorname{rank}(E(Z_iX_i'))=K=3$:  
$$
\begin{align*}
&\operatorname{rank}(E(Z_iX_i'))\\ 
=&\operatorname{rank}\left(
  \begin{matrix}
  1  \cdot\;1                  & E\left(1\;\cdot X_{i2}\right) & E\left(1\cdot X_{i3}\right) \\ 
  E\left(X_{i2}\cdot\;1\right) & E\left(X_{i2} X_{i2}\right)    & E\left(X_{i2} X_{i3}\right) \\ 
  E\left(Z_{i2}\cdot\;1\right) & E\left(Z_{i2} X_{i2}\right)    & E\left(Z_{i2} X_{i3}\right) \\ 
  \end{matrix}\right)
\end{align*}
$$   
Since $X_{i2}$ and $X_{i3}$ are independent, $E(X_{i2} X_{i3})=E(X_{i2})E(X_{i3})$, which yields
$$
\begin{align*}
&\operatorname{rank}(E(Z_iX_i'))\\ 
=&\operatorname{rank}\left(
  \begin{matrix}
  1\cdot\;1                    & E\left(1\;\cdot X_{i2}\right)& \;\;1\;\cdot\;\; E\left(X_{i3}\right) \\ 
  E\left(X_{i2}\right)\cdot\;1 & E\left(X_{i2} X_{i2}\right)  & E\left(X_{i2}\right)\cdot E\left(X_{i3}\right) \\ 
  E\left(Z_{i2}\right)\cdot\;1 & E\left(Z_{i2} X_{i2}\right)  & E\left(Z_{i2}\;\;\cdot\;\;X_{i3}\right) \\ 
  \end{matrix}\right)  
\end{align*}
$$ 
Thus, to guarantee that $\operatorname{rank}(E(Z_iX_i'))=3$ we need that 
$$
E\left(Z_{i2} X_{i3}\right)\neq E\left(Z_{i2}\right)E\left(X_{i3}\right)
$$ 
otherwise the first and the third column are linearly dependent. <br>
Note that
$$
\begin{align*}
E(Z_{i2} X_{i3}) \neq  E(Z_{i2}) E(X_{i3}) \Leftrightarrow \operatorname{Cov}(Z_{i2},X_{i3}) \neq 0 
\end{align*}
$$
since 
$$
\begin{align*}
\operatorname{Cov}(Z_{i2},X_{i3})&=E(Z_{i2} X_{i3}) - E(Z_{i2}) E(X_{i3})\\ 
%\Leftrightarrow
%E(Z_{i2} X_{i3}) &= E(Z_{i2}) E(X_{i3}) + \underbrace{\operatorname{Cov}(Z_{i2},X_{i3})}_{\neq 0\;?}. 
\end{align*}
$$
That is, we need that the instrument $Z_{i3}$ is correlated with the endogenous variable $X_{i3}$, i.e. that $\operatorname{Cov}(Z_{i2},X_{i3}) \neq 0.$ This is fulfilled here since
$$
\begin{align*}
\operatorname{Cov}(Z_{i2},X_{i3})
&=\operatorname{Cov}(V_{i}+u_i,X_{i3})\\ 
&=\operatorname{Cov}(V_{i},X_{i3})+\operatorname{Cov}(u_i,X_{i3})\\ 
&=\operatorname{Cov}(V_{i},X_{i3})+0\\  
&=\operatorname{Cov}(V_{i},V_i+2\varepsilon_{i})\\  
&=\operatorname{Cov}(V_{i},V_i) + 2\operatorname{Cov}(V_{i},\varepsilon_{i})\\  
&=\operatorname{Var}(V_{i}) + 0 \\ 
&=9
\end{align*}
$$

That is, the instrumental variables vector $Z_i=(1,X_{i2},Z_{i2})'$ is a valid instrument. 


### R function `ivreg()` {-}

The function `ivreg()` from the `R` package `AER` carries out 2SLS estimation. It is used similarly as the `lm()` function. Instruments can be added to the usual specification of the regression formula using a vertical bar separating the model equation from the instruments. Thus, for the regression at hand the correct formula is: 
<center>
`ivreg(Y ~ X_2 + X_3 | X_2 + Z_2)`
</center>

```{r}
library("AER")             # contains ivreg()

n              <- 100      # Sample size 
beta_true      <- c(2,2,2) # Structural parameter

B              <- 10000    # Number of MC replications
beta_hat_2SLS  <- matrix(data = NA, # Container for 
                         nrow = 3,  # MC simulations
                         ncol = B)

## Monte Carlo (MC) simulation for generating 
## estimates when X3 is endogenous for beta3:
for(b in 1:B){
  eps          <- rnorm(n)                # error term 
  X_2          <- rnorm(n, m = 3, sd = 2) # exogenous
  V            <- rnorm(n, m = 1, sd = 3) # exogenous
  X_3          <- V + 2 * eps             # endogenous
  ##
  Z_2          <- V + runif(n, 0, 1)      # Instrument
  ## Dependent variable
  Y            <- cbind(1, X_2, X_3) %*% beta_true + eps
  ## 2SLS Estimation
  beta_hat_2SLS[,b] <- coef(ivreg(Y ~ X_2 + X_3 | X_2 + Z_2))
}

## MC means of \hat{\beta}_{2SLS,2} and \hat{\beta}_{2SLS,3} 
mean_beta_hat_2SLS_2 <- round(mean(beta_hat_2SLS[2,]), 2)
mean_beta_hat_2SLS_3 <- round(mean(beta_hat_2SLS[3,]), 2)

## Unbiased estimator \hat{\beta}_{2SLS,2}:
mean_beta_hat_2SLS_2  - beta_true[2]

## Unbiased estimator \hat{\beta}_{2SLS,3}:
mean_beta_hat_2SLS_3  - beta_true[3]
```


## References


