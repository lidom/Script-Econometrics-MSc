# Small Sample Inference {#sec-ssinf}

The content of this chapter is very much inspired by Chapter 1 of @Hayashi2000. 


It's is very hard to say when a sample size $n$ is small. Often people say something like: 

* $n<30$ means small samples and $n\geq 30$ large samples, or
* $n/K<10$ means small samples and $n/K\geq 10$ large samples,

but these are only a very rough rules of thumb and may not apply in practice.  

The core issue with small sample sizes is that we cannot do inference using the law of large numbers and the central limit theorem. Thus we need rather strict assumptions on the distribution of the error term, in order to do inference in finite samples. If these assumption are fulfilled, however, then we do **exact** inference. 

**Exact inference:**  By "exact inference" we mean correct inference for each sample size $n$. That is, no asymptotic $(n\to\infty)$ arguments will be used.  

**Assumptions:** Recall that we, in general, did not impose a complete distributional assumption on $\varepsilon$ in Assumption 4 (@sec-MLR). For instance, the i.i.d. normal case in Assumption 4 was only one possible *option*.  However, to do exact inference, the normality Assumption on the error terms is not a mere option, but a *necessity*. 

For this chapter we assume that Assumptions 1-3 from @sec-MLR hold and that additionally the following assumption holds: 

**Assumption 4$^\boldsymbol{\ast}$: Conditional Gaussian error distribution:** The error terms are 
Gaussian and homoskedastik, i.e., 
$$
\varepsilon_i|X_i\sim\mathcal{N}(0,\sigma^2)
$$ 
for all $i=1,\dots,n.$ 


Assumption 4$^\boldsymbol{\ast}$ together with the random sample assumption of Assumption 1, part (b), leads to Gaussian spherical errors conditionally on $X$,
$$
\varepsilon|X\sim\mathcal{N}\left(0,\sigma^2I_n\right),
$$
where $\varepsilon=(\varepsilon_1,\dots,\varepsilon_n)'$.

\bigskip

::: {.callout-note icon=false}
##
::: {#thm-normalbeta}
# Normality of $\hat\beta$ 

Under Assumptions 1-4$^\ast$ we have that 
$$
\hat\beta_n|X \sim \mathcal{N}\left(\beta,Var(\hat\beta_n|X)\right),
$${#eq-ssnorm}
where $Var(\hat\beta_n|X)=\sigma^2(X'X)^{-1}$.
:::
::: 

::: {.proof}
This result follows from noting that 
\begin{align*}
\hat\beta_n
&=(X'X)^{-1}X'Y\\[2ex]
&=\beta+(X'X)^{-1}X'\varepsilon
\end{align*} 
and because $(X'X)^{-1}X'\varepsilon$ is just a linear combination of the normally distributed error terms $\varepsilon$ which, therefore, is again normally distributed, conditionally on $X$. Note that the specific normal distribution depends on the observed realization of $X$. 
:::

**Remark:** The subscript $n$ in $\hat\beta_n$ is here only to emphasize that the distribution of $\hat\beta_n$ depends on $n$; we will, however, often simply write $\hat\beta$.


## Hypothesis Tests about Multiple Parameters (F-Tests) {#sec-testmultp}

Let us consider the following system of $q$-many null hypotheses:
\begin{align*}
H_0: \underset{(q\times K)}{R}\underset{(K\times 1)}{\beta} - \underset{(q\times 1)}{r} = \underset{(q\times 1)}{0},
\end{align*}
where the $(q \times K)$ matrix $R$ and the $q$-vector $r=(r_{1},\dots,r_{q})'$ are chosen by the statistician to specify her/his null hypothesis about the unknown true parameter vector $\beta$. To make sure that there are no redundant equations, it is required that $\operatorname{rank}(R)=q$.

We must also specify the alternative against which we are testing the null hypothesis, for instance
\begin{equation*}
H_A: R\beta -r \neq 0
\end{equation*}

The above multiple parameter hypotheses cover also the special case of single parameter hypothesis; for instance, by setting $R=(0,1,0\dots,0)$ and $r=0$ one get's
\begin{equation*}
\begin{array}{ll}
H_0:  & \beta_{k}=0 \\
H_A:  & \beta_{k} \ne 0 \\
\end{array}
\end{equation*}

Under our assumptions (Assumptions 1 to 4$^\ast$), we have that 
$$
\begin{align*}
(R\hat\beta_n-r)|X&\sim\mathcal{N}\left(R\beta -r,RVar(\hat\beta_n|X)R'\right)\\
(R\hat\beta_n-r)|X&\overset{H_0}{\sim}\mathcal{N}\left(0,RVar(\hat\beta_n|X)R'\right)
\end{align*}
$$ 

That is, 

* the realizations of $(R\hat\beta_n -r)|X$ will scatter around the *unknown* $(R\beta -r)$ in a Gaussian fashion. 
* if the null hypothesis is correct (i.e., $(R\beta-r)=0$), the realizations of $(R\hat\beta_n-r)|X$ scatter around the $(q\times 1)$ vector $0$.  

We use a test statistic to detect a systematic location shift away from the zero vector. 
<!-- * if the alternative hypothesis is correct (i.e., $(R\beta-r)=a\neq 0$), there will be a systematic location-shift in $(R\hat\beta_n-r)|X$ which we try to detect using statistical hypothesis testing.  -->

<!-- the realizations of $R\hat\beta_n-r|X$ scatter around the $q$-vector $a\neq 0$.  So, under the alternative hypothesis,  -->


### The Test Statistic and its Null Distribution

The fact that $(R\hat\beta_n-r)\in\mathbb{R}^q$ is a $q$-dimensional random variable makes it a little bothersome to use as a test-statistic.  Fortunately, we can turn $(R\hat\beta_n-r)$ into a scalar-valued test statistic using the following quadratic form:
$$
W=\underbrace{(R\hat\beta_n -r)'}_{(1\times q)}\underbrace{[RVar(\hat\beta_n|X)R']^{-1}}_{(q\times q)}\underbrace{(R\hat\beta_n -r)}_{(q\times 1)}
$$
Note that the test statistic $W$ is simply measuring the distance (it's a weighted L2-distance) between the $(q\times 1)$ vectors $R\hat\beta_n$ and $r$. 

Under the null hypothesis (i.e., if $H_0$ is true), $W|X$ is a sum of $q$-many independent squared standard normal random variables. Therefore, under the null hypothesis, $W|X$ is chi-square distributed with $q$ degrees of freedom (see @sec-chisqdist), 
$$
\begin{align*}
W|X&\overset{H_0}{\sim} \chi^2_{(q)}\\ 
\Rightarrow \quad\quad W&\overset{H_0}{\sim} \chi^2_{(q)}
\end{align*}
$$
Note that the distribution of $W|X$ does not depend on $X,$ (i.e. it's a $\chi^2_{(q)}$-distribution no matter the realization of $X$) and thus our test decisions do not depend on the values of $X.$ (Good news!)


Usually, we do not know $Var(\hat\beta_n|X),$ and thus we need to estimate this quantity from the data. Unfortunately, in the small sample case, we can only deal with homoskedastic error terms. For truly exact finite sample inference, we need a variance estimator for which we can derive the exact small sample distribution. Therefore, we require Assumption 4$^*$ of spherical errors (i.e., $Var(\varepsilon|X)=I_n\sigma^2$) which yields that $Var(\hat\beta_n|X)=\sigma^2(X'X)^{-1}$, and where $\sigma^2$ can be estimated by the unbiased ($UB$) variance estimator  
$$
s_{UB}^2=(n-K)^{-1}\sum_{i=1}^n\hat\varepsilon_i^2.
$$  
From the normality assumption in Assumption 4$^*$, it follows then that 
$$
\frac{(n-K)}{\sigma^{2}}s_{UB}^2\sim\chi^2_{(n-K)}.
$${#eq-distsquared} 

The $F$ test statistic uses then $s_{UB}^2$ as an estimator of $\sigma^2$
$$
F=(R\hat\beta_n -r)'[R(s_{UB}^2(X'X)^{-1})R']^{-1}(R\hat\beta_n -r)/q
$$
and takes into account the additional randomness (estimation errors) due to $s_{UB}^2$, which leads to the following exact null distribution of the $F$ test
$$
F\overset{H_0}{\sim} F_{(q,n-K)},
$${#eq-Ftest}
where $F_{(q,n-K)}$ denotes the $F$-distribution with $q$ numerator and $n-K$ denominator degrees of freedom. 


As in the case of $W$, the distribution of $F$ conditional on $X$ does not depend on $X$; i.e. $F|X\overset{H_0}{\sim}F_{(q,n-K)},$ but $F_{(q,n-K)}$ does not depend on $X,$ thus we can write $F\overset{H_0}{\sim}F_{(q,n-K)}.$

The distributional statements in @eq-distsquared and @eq-Ftest are a little cumbersome to derive and we do not go into details here, but in case you're interested you can find some more details, for instance, in Chapter 1 of @Hayashi2000. 

By contrast to $W,$ $F$ is now a practically useful test statistic, and we can use the observed value $F_{\text{obs}}$ to measure the distance of our estimate $R\hat\beta_n$ from its hypothetical value $r.$  

Observed values, $F_{\text{obs}}$, that are "unusually large" under the null hypothesis, lead to a rejection of the null hypothesis. The null distribution $F_{(q,n-K)}$ of $F$ is used to judge what's "unusually large" under the null hypothesis. 


**The F distribution.** The F distribution is a ratio of two $\chi^2$ distributions. It has two parameters: the numerator degrees of freedom, and the denominator degrees of freedom. Each combination of the parameters yields a different F distribution. See @sec-Fdist for more information on the $F$ distribution. 
```{r, fig.align="center", echo=FALSE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
# When fixing rate (lambda) and changing shape (r) for Gamma Distribution,
# When the shape (r) increases, based on the formula, 
# the mean increases (shift to the right),
# the variance increases
# the skewness decreases
# the excess kurtosis decreases

### F Distribution
# Plot 1: Fix df2 and changing df1
par(mfrow=c(1,2))
curve(expr = df(x = x, df1 = 3, df2 = 5),
      xlab = "", ylab = "", main = "",
      lwd = 2, col = 1, xlim = c(0, 4),
      ylim = c(0, 1))

for (i in 1:2) {
  curve(expr = df(x = x, df1  = 3, 
                  df2=c(15,500)[i]),
        lwd = 2, col = (2:3)[i], add = TRUE)
}  
legend(x = "topright", legend = c("F(3,5)", "F(3,15)", "F(3,500)"),
       lwd = 2, col = 1:3)
curve(expr = df(x = x, df1 = 1, df2 = 30),
      xlab = "", ylab = "", main = "",
      lwd = 2, col = 1, xlim = c(0, 4),
      ylim = c(0, 1))

for (i in 1:2) {
  curve(expr = df(x = x, df1  = c(3,15)[i], 
                  df2=30),
        lwd = 2, col = (4:6)[i], add = TRUE)
}  
legend(x = "topright", legend = c("F(1,30)", "F(3,30)", "F(15,30)"),
       lwd = 2, col = 4:6)
```


## Tests about One Parameter (t-Tests) {#ch:testingsinglep}

For testing a hypothesis about only one parameter $\beta_k$, with $k=1,\dots,K$
\begin{equation*}
\begin{array}{ll}
H_0: & \beta_k=r\\
H_A: & \beta_k\ne r\\
\end{array}
\end{equation*}
the $(q\times K)$-matrix $R$ becomes a $(1\times K)$ row-vector of zeros but with a one as the $k$th element. For instance, for testing a null hypothesis about $\beta_k$ with, e.g., $k=2$, we have $R=(0,1,0,\dots,0),$ and thus the $F$ test statistic simplifies to
$$
F=\frac{\left(\hat{\beta}_k-r\right)^2}{\widehat{Var}(\hat{\beta}_k|X)}\overset{H_0}{\sim}F_{(1,n-K)},
$$
where $\widehat{Var}(\hat{\beta}_k|X)=s^2_{UB}[(X'X)^{-1}]_{kk}.$ 

Taking square roots yields the $t$ test statistic 
$$
t=\frac{\hat{\beta}_k-r}{\widehat{\operatorname{SE}}(\hat{\beta}_k|X)}\overset{H_0}{\sim}t_{(n-K)},
$$
where $\widehat{\operatorname{SE}}(\hat{\beta}_k|X)=s_{UB}[(X'X)^{-1/2}]_{kk},$ and where $t_{(n-K)}$ denotes the $t$-distribution with $n-K$ degrees of freedom. 

Thus the $t$-distribution with $n-K$ degrees of freedom is the appropriate distribution to judge whether or not an observed value $t_{obs}$ of the test statistic is "unusually large" under the null hypothesis. 

**Note:** All commonly used statistical software packages report $t$-tests testing the null hypothesis $H_0:\beta_k=0$, i.e., with $r=0$.  This means to test the null hypothesis that $X_k$ has "no (linear) effect" on $Y$. 

**The $t$ distribution.** The following plot illustrates that as the degrees of freedom increase, the shape of the $t$ distribution comes closer to that of a standard normal bell curve. Already for $25$ degrees of freedom we find little difference to the standard normal density. In case of small degrees of freedom values, we find the distribution to have heavier tails than a standard normal. See @sec-tdist for more information about the $t$-distribution.

```{r, fig.align="center", echo=FALSE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"} 
# plot the standard normal density
curve(dnorm(x), 
      xlim = c(-4, 4), 
      xlab = "", 
      lty = 2, 
      ylab = "", 
      main = "")

# plot the t density for M=2
curve(dt(x, df = 2), 
      xlim = c(-4, 4), 
      col = 2, 
      add = T)

# plot the t density for M=4
curve(dt(x, df = 4), 
      xlim = c(-4, 4), 
      col = 3, 
      add = T)

# plot the t density for M=25
curve(dt(x, df = 25), 
      xlim = c(-4, 4), 
      col = 4, 
      add = T)

# add a legend
legend("topright", bty="n", 
       c("N(0, 1)", expression(t[2]), expression(t[4]), expression(t[25])), 
       col = 1:4, 
       lty = c(2, 1, 1, 1))
```

## Testtheory

### Significance Level $\alpha$

To actually test the null hypothesis (e.g., $H_0$: $R\beta-r=0$ or $H_0$: $\beta_k=0$), we need to have a decision rule on when we will reject or not reject the null hypothesis. This amounts to deciding on a probability with which we are comfortable with committing a Type I error ($\alpha$-error): rejecting the null hypothesis when it is in fact true. The probability of such a Type I error shall be bounded from above by a (small) significance level $\alpha$, that is 
$$
P(\text{reject } H_0| H_0\text{ is true})=P(\text{Type I Error})=\alpha
$$
For a given significance level (e.g., $\alpha=0.05$) and a given alternative hypothesis, we can divide the range of all possible values of the test statistic (i.e., $\mathbb{R}$ since both $t\in\mathbb{R}$ and $F\in\mathbb{R}$) into a **rejection region** and a **non-rejection region** by using **critical values** derived from the distribution of the test statistic under the null hypothesis.  We can do this because the test statistics $t$ and $F$ have known distributions under the null hypothesis ($t\overset{H_0}{\sim}t_{n-K}$ and $F\overset{H_0}{\sim}F_{(q,n-K)}$). 

Indeed, under Assumption 4$^\ast$, we know the *exact* null distributions for every sample size $n$. Having decided on the rejection and non-rejection regions, we only need to check whether the observed (obs) sample values $t_{obs}$ or $F_{obs}$ of the test statistics $t$ or $F$ are either in the rejection or in the non-rejection region to either rejection the null hypothesis or not to rejection the null hypothesis. 


**Non-conservative versus conservative tests:** Since the test statistics $F$ and $t$ are continuous random variables of which we know the *exact* distributions (under Assumptions 1-4$^\ast$), we can find critical values such that 
$$
P(\text{Type I Error})=\alpha
$$
We call such tests "non-conservative" since the probability of a type I error equals the significance level $\alpha$.  Test statistics with 
$$
P(\text{Type I Error})<\alpha
$$
are called *conservative* test statistics; they lead to valid inferences, but will detect a correct violation of the null hypothesis less often than a non-conservative test. A test statistic with $P(\text{Type I Error})>\alpha$ leads to *invalid* inferences! 


### Critical Values 

#### The $F$-Test {-}

The critical value $c_{1-\alpha}>0$ defines the 

- rejection region, $]c_{1-\alpha},\infty[$, and 
- non-rejection region, $]0,c_{1-\alpha}]$ 

which together divide the range of test-statistic values (here $\mathbb{R}^+$ since $F\in\mathbb{R}^+$) for a given significance level $\alpha\in(0,1)$, such that
$$
P(\text{Type I Error})=P_{H_0}\Big(F\in]c_{1-\alpha},\infty[\Big)=\alpha,
$$
where $c_{1-\alpha}$ is here the $(1-\alpha)$ quantile of the $F$-distribution with $(q,n-K)$ degrees of freedom, and where $P_{H_0}$ means that we compute the probability under the assumption that $H_0$ is true.  
```{r, fig.align="center", echo=FALSE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
library("scales")
curve(expr = df(x = x, df1 = 9, df2 = 120),
      xlab = "", ylab = "", main = "",
      lwd = 2, col = 1, xlim = c(0, 4),
      ylim = c(0, 1), xaxs="i",yaxs="i")

alpha <- 0.05
q     <- qf(p = 1-alpha, df1 = 9, df2 = 120)
xx    <- seq(0,q,len=25)
yy    <- df(x = xx, df1 = 9, df2 = 120)

polygon(x = c(xx,rev(xx)), y=c(yy, rep(0,length(yy))), border = NA, col = alpha("green", 0.25))
q     <- qf(p = 1-alpha, df1 = 9, df2 = 120)
xx    <- seq(q,4,len=25)
yy    <- df(x = xx, df1 = 9, df2 = 120)

polygon(x = c(xx,rev(xx)), y=c(yy, rep(0,length(yy))), border = NA, col = alpha("red", 0.25))

lines(x=c(0,q-0.02),y=c(0,0), col="darkgreen", lwd=10)
lines(x=c(q+0.02,4),y=c(0,0), col="red", lwd=10)

legend("topright", pch=c(22,NA, 22, NA), lty=c(NA,1,NA,1), lwd=c(NA,4,NA,4), cex=1, 
       col = c(alpha("green", 0.25),"darkgreen",alpha("red", 0.25),"red"), 
       legend=c(expression(1-alpha==~"95% of"~F['9,120']),"Non-Rejection Region",
                expression(alpha==~"5% of"~F['9,120']),"Rejection Region"), 
       bty="n", pt.bg = c(alpha("green", 0.25),alpha("green", 0.25),alpha("red", 0.25),alpha("red", 0.25)))
curve(expr = df(x = x, df1 = 9, df2 = 120), lwd = 2, col = 1, add=TRUE, from = 0, to = 4)
lines(x=c(q,q), y=c(0,.6),lwd=2,lty=2)
text(x = q, y = .65, labels = expression(c[1-alpha]==1.9588))
```

**The rejection region:** The rejection region describes a range of values of the test statistic $F$ which we rarely see if the null hypothesis is true (only in at most $\alpha \cdot 100\%$ cases). If the observed value of the test statistic, $F_{\text{obs}}$, falls in this region, we will reject the null  hypothesis and accept Type I error rate of $\alpha$. 

**The non-rejection region:** The non-rejection region describes a range of values of the test statistic $F$ which we expect to see (in $(1-\alpha) \cdot 100\%$ cases) if the null hypothesis is true. If the observed value of the test statistic, $F_{\text{obs}}$ falls in this region, we will not reject the null hypothesis. 


**Caution:** Not rejecting the null hypothesis does not mean that we can conclude that the null hypothesis is true. We only had no sufficiently strong evidence against the null hypothesis. A violation of the null hypothesis, for instance $R\beta -r=a\neq 0$, may simply be too small (too small $a$ value) to stand out from the estimation errors (measured by the standard error) in $\hat\beta_k.$


Fortunately, you do not need to read old-school distribution tables to find the critical value $c_{1-\alpha}$, but can simply use `R`
```{r}
df1   <- 9    # numerator df
df2   <- 120  # denominator df
alpha <- 0.05 # significance level
## Critical value:
crit_value <- qf(p = 1-alpha, df1 = df1, df2 = df2)
crit_value
```
\noindent Changing the significance level from $\alpha=0.05$ to $\alpha=0.01$ makes the critical value $c_{1-\alpha}$ larger and, therefore, the  rejection region smaller (fewer Type I errors)
```{r}
alpha <- 0.01
## Critical value:
crit_value <- qf(p = 1-alpha, df1 = df1, df2 = df2)
crit_value
```


#### The $t$-Test {-}

In case of the $t$-test, we need to differentiate between two-sided and one-sided testing.

##### Two-Sided $t$-Test {-}

Two-sided hypothesis:
\begin{equation*}
\begin{array}{ll}
H_0: & \beta_k=r \\
H_A: & \beta_k\ne r
\end{array}
\end{equation*}
In case of a two-sided $t$-test, we reject the null hypothesis if the observed realization of the $t$-test, $t_{obs}$, is "far away" from zero either by being sufficiently smaller or greater than $r$. The corresponding two-sided critical values are denoted by $-c_{1-\alpha/2}=c_{\alpha/2}<0$ and $c_{1-\alpha/2}>0$, where $c_{1-\alpha/2}>0$ is the $(1-\alpha/2)$ quantile of the $t$-distribution with $(n-K)$ degrees of freedom, and where $-c_{1-\alpha/2}=c_{\alpha/2}$ due to the symmetry of the $t$-distribution. These critical values defines the following rejection and the non-rejection regions 
\begin{align*}
\text{rejection region:}&\hspace{1cm}]-\infty,c_{\alpha/2}[\;\;\cup\;\;]c_{1-\alpha/2}, \infty[\\
\text{non-rejection region:}&\hspace{1cm}[c_{\alpha/2},c_{1-\alpha/2}].
\end{align*}
For this rejection region it holds true that
$$
P(\text{Type I Error})=P_{H_0}\Big(t\in\;]-\infty,c_{\alpha/2}[\;\;\cup\;\;]c_{1-\alpha/2}, \infty[\Big)=\alpha.
$$
```{r, fig.align="center", echo=FALSE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
library("scales")
curve(expr = dt(x = x, df = 12),
      xlab = "", ylab = "", main = "",
      lwd = 2, col = 1, xlim = c(-5, 5),
      ylim = c(0, .6), xaxs="i",yaxs="i")

alpha <- 0.05/2
q      <- qt(p = 1-alpha, df=12)
xx1    <- seq(-5,-q,len=25)
yy1    <- dt(x = xx1, df = 12)
xx2    <- seq(q,5,len=25)
yy2    <- dt(x = xx2, df = 12)
xx3    <- seq(-q,q,len=25)
yy3    <- dt(x = xx3, df = 12)

polygon(x = c(xx1,rev(xx1)), y=c(yy1, rep(0,length(yy1))), border = NA, col = alpha("red", 0.25))

polygon(x = c(xx2,rev(xx2)), y=c(yy2, rep(0,length(yy2))), border = NA, col = alpha("red", 0.25))

polygon(x = c(xx3,rev(xx3)), y=c(yy3, rep(0,length(yy3))), border = NA, col = alpha("green", 0.25))

legend("topright", pch=c(22,22), lty=c(NA,NA), lwd=c(NA,NA), cex=1, 
       col = c(alpha("green", 0.25),alpha("red", 0.25)), 
       legend=c(expression("95% of"~t['12']),
                expression("5% of"~t['12'])), 
       bty="n", pt.bg = c(alpha("green", 0.25),alpha("red", 0.25)))
curve(expr = dt(x = x, df= 12), lwd = 2, col = 1, add=TRUE)
lines(x=c(q,q), y=c(0,.35),lwd=2,lty=2)
lines(x=c(-q,-q), y=c(0,.35),lwd=2,lty=2)
text(x =  q, y = .45, labels = expression(c[1-alpha/2]==2.18))
text(x = -q, y = .45, labels = expression(c[alpha/2]==-2.18))
```

##### One-Sided $t$-Test {-}

One-sided hypothesis:
\begin{equation*}
\begin{array}{lll}
&H_0: & \beta_k \leq r\\
(\text{or}&H_0: & \beta_k \geq r)\\
&H_A: & \beta_k >r\\
(\text{or}&H_A: & \beta_k< r)
\end{array}
\end{equation*}
In case of a one-sided $t$-test, we will reject the null if $t_{obs}$ is sufficiently "far away" from zero in the relevant direction of $H_A$. The corresponding critical value is either 

* $-c_{1-\alpha}$ (when $H_A:\beta_k< r$) or 
* $\phantom{-}c_{1-\alpha}$ (when $H_A:\beta_k> r$), 

where $c_{1-\alpha}$ is the $(1-\alpha)$ quantile of the $t$-distribution with $(n-K)$ degrees of freedom, and where 
$$
-c_{1-\alpha}=c_{\alpha}
$$ 
due to the symmetry of the $t$-distribution. 


The critical value $c_{1-\alpha}$ defines the following rejection and the non-rejection regions:


For $H_0: \beta_k=0\quad$ versus $\quad H_A: \beta_k < 0$:
\begin{align*}
\text{rejection region:}   &\hspace{2cm}]-\infty,c_{\alpha}[ \\
\text{non-rejection region:}&\hspace{2cm}[c_{\alpha},\infty[
\end{align*}
such that 
$$
P(\text{Type I Error})=P_{H_0}\Big(t\in\;]-\infty,c_{\alpha}[\Big)=\alpha.
$$



For $H_0: \beta_k=0\quad$ versus $\quad H_A: \beta_k > 0$:
\begin{align*}
\text{rejection region:}&\hspace{1cm}]c_{1-\alpha}, \infty[\\
\text{non-rejection region:}&\hspace{1cm}]-\infty,c_{1-\alpha}]
\end{align*}
such that 
$$
P(\text{Type I Error})=P_{H_0}\Big(t\in\;]c_{1-\alpha}, \infty[\Big)=\alpha.
$$
```{r, fig.align="center", echo=FALSE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
library("scales")
alpha <- 0.05
q      <- qt(p = 1-alpha, df=12)

xx1    <- seq(-5,-q,len=25)
yy1    <- dt(x = xx1, df = 12)
xx2    <- seq(-q,5,len=25)
yy2    <- dt(x = xx2, df = 12)
##
xx3    <- seq(q,5,len=25)
yy3    <- dt(x = xx3, df = 12)
xx4    <- seq(-5,q,len=25)
yy4    <- dt(x = xx4, df = 12)

par(mfrow=c(1,2))
curve(expr = dt(x = x, df = 12),
      xlab = "", ylab = "", main = "",
      lwd = 2, col = 1, xlim = c(-5, 5),
      ylim = c(0, .6), xaxs="i",yaxs="i")
polygon(x = c(xx1,rev(xx1)), y=c(yy1, rep(0,length(yy1))), border = NA, col = alpha("red", 0.25))
polygon(x = c(xx2,rev(xx2)), y=c(yy2, rep(0,length(yy2))), border = NA, col = alpha("green", 0.25))
lines(x=c(-q,-q), y=c(0,.35),lwd=2,lty=2)
text(x = -q, y = .45, labels = expression(-c[1-alpha]==-1.78))
legend("topright", pch=c(22,22), lty=c(NA,NA), lwd=c(NA,NA), cex=1, 
       col = c(alpha("green", 0.25),alpha("red", 0.25)), 
       legend=c(expression("95% of"~t['12']),
                expression("5% of"~t['12'])), 
       bty="n", pt.bg = c(alpha("green", 0.25),alpha("red", 0.25)))

###########

curve(expr = dt(x = x, df = 12),
      xlab = "", ylab = "", main = "",
      lwd = 2, col = 1, xlim = c(-5, 5),
      ylim = c(0, .6), xaxs="i",yaxs="i")
polygon(x = c(xx3,rev(xx3)), y=c(yy3, rep(0,length(yy3))), border = NA, col = alpha("red", 0.25))
polygon(x = c(xx4,rev(xx4)), y=c(yy4, rep(0,length(yy4))), border = NA, col = alpha("green", 0.25))
lines(x=c(q,q), y=c(0,.35),lwd=2,lty=2)
text(x =  q, y = .45, labels = expression(c[1-alpha]==1.78))
legend("topright", pch=c(22,22), lty=c(NA,NA), lwd=c(NA,NA), cex=1, 
       col = c(alpha("green", 0.25),alpha("red", 0.25)), 
       legend=c(expression("95% of"~t['12']),
                expression("5% of"~t['12'])), 
       bty="n", pt.bg = c(alpha("green", 0.25),alpha("red", 0.25)))
par(mfrow=c(1,1))
```


Fortunately, you do not need to read old-school distribution tables to find the critical values, but you can simply use `R`
```{r}
df    <- 16   # degrees of freedom 
alpha <- 0.05 # significance level
## One-sided critical value (= (1-alpha) quantile):
c_oneSided <- qt(p = 1-alpha, df = df)
c_oneSided
## Two-sided critical value (= (1-alpha/2) quantile):
c_twoSided <- qt(p = 1-alpha/2, df = df)
## lower critical value
-c_twoSided
## upper critical value
c_twoSided
```


### Type II Error and Power

A Type II error is the mistake of not rejecting the null hypothesis when in fact it should have been rejected. The probability of making a Type II error equals one minus the probability of correctly rejecting the null hypothesis ("Power"). For instance, in the case of using the $t$-test to test the null hypothesis $H_0: \beta_k=0$ versus the one-sided alternative hypothesis $H_A:\beta_k>0$) we have that 
\begin{align*}
P(\text{Type II Error})
&=P_{H_A}\Big(t\;\in\;\overbrace{]-\infty,c_{1-\alpha}]}^{\text{non-rejection region}}\Big)\\
&=1-\underbrace{P_{H_A}\Big(t\;\in\;\overbrace{]c_{1-\alpha},\infty[}^{\text{rejection region}}\Big)}_{\text{"Power"}},
\end{align*}
where $P_{H_A}$ means that we compute the probability under the assumption that $H_A$ is true. 

There is a trade off between the probability of making a Type I error and the probability of making a Type II error: a lower significance level $\alpha$, decreases $P(\text{Type I Error})$, but necessarily increases $P(\text{Type II Error})$ and vice versa.  Ideally, we would have some sense of the costs of making each of these errors, and would choose our significance level to minimize these total costs. However, the costs are often difficult to know. Moreover, the probability of making a Type II error is usually impossible to compute, since we usually do not know the true distribution of $\hat\beta_k$ under the alternative. 


For illustration purposes, consider the case of a $t$ test for a one-sided hypothesis
\begin{equation*}
\begin{array}{ll}
H_0:  & \beta_k=0 \\
H_A:  & \beta_k>0,
\end{array}
\end{equation*}
where the true (usually unknown) parameter value is $\beta_k=3$ and where the true (usually also unknown) standard error is $\operatorname{SE}(\hat\beta_k|X)=\sqrt{\sigma^2[(X'X)^{-1}]_{kk}}=1.5$. Only with the knowledge about these usually unknown quantities, we can derive the distribution of the $t$-test statistic under the alternative hypothesis. The distribution of the $t$-test statistic becomes here a standard normal distribution, since we assume $\operatorname{SE}(\hat\beta_k|X)=\sqrt{\sigma^2[(X'X)^{-1}]_{kk}}=1.5$ to be a **known** quantity for some given sample size $n$. (This completely unrealistic assumption is only used for illustrative purposes to explain the probability of making a Type II error and the power (i.e. $1-P(\text{Type II Error})$).)  

Under this setup, the distribution under the null hypothesis (i.e., if $\beta_k=0$ were true) is:
$$
t=\frac{\hat\beta_k-0}{\sqrt{\sigma^2[(X'X)^{-1}]_{kk}}}\overset{H_0}{\sim}\mathcal{N}(0,1)
$$
Likewise, the distribution under the alternative hypothesis (i.e., for the actual $\beta_k=3$) is:
\begin{align*}
t&=\frac{\hat\beta_k-0}{\sqrt{\sigma^2[(X'X)^{-1}]_{kk}}}
=\frac{\hat\beta_k+3-3-0}{\sqrt{\sigma^2[(X'X)^{-1}]_{kk}}}\\[2ex]
&=\underbrace{\frac{\hat\beta_k-3}{\sqrt{\sigma^2[(X'X)^{-1}]_{kk}}}}_{\sim \mathcal{N}(0,1)}+\underbrace{\frac{3-0}{\sqrt{\sigma^2[(X'X)^{-1}]_{kk}}}}_{=\Delta\text{ (mean-shift)}}\overset{H_A}{\sim}\mathcal{N}(\Delta,1)
\end{align*}

So, the mean-shift $\Delta$ depends on:

* The value of $\sqrt{\sigma^2[(X'X)^{-1}]_{kk}}$ (here $1.5$).  
* The difference between the actual parameter value $(\beta_k=3)$ and the hypothetical parameter $(r=0)$ value under the null-hypothesis. 

The following Graphic illustrates the probability of a type II error and the power for the case where $\sqrt{\sigma^2[(X'X)^{-1}]_{kk}}=1.5$ such that $\Delta=3/1.5=2$. 
```{r, fig.align="center", echo=FALSE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
library("scales") # transperent color
mean.alt <- 2

x  <- seq(-4, 4, length=1000)
hx <- dnorm(x)
alpha <- 0.05

plot(x, hx, type="n", xlim=c(-4, 7), ylim=c(0, 0.65), ylab = "", xlab = "", axes=T)
#axis(1)

xfit2 <- x + mean.alt
yfit2 <- dnorm(x)

## Print null hypothesis area
polygon(c(min(x), x,  max(x)), 
        c(0,      hx, 0), 
        col   =alpha("grey", 0.5), 
        border=alpha("grey", 0.9))

ub <- max(x)
lb <- round(qnorm(1-alpha),2)

## The green area: Power
i <- xfit2 >= lb
polygon(c(min(xfit2[i]), xfit2[i], max(xfit2[i])), 
        c(0,  yfit2[i], 0), 
        col=alpha("green", 0.25),
        border=alpha("green", 0.25))

## The blue area: P(Type II error)
lb <- min(xfit2)
ub <- round(qnorm(1-alpha),2)

i <- xfit2 >= lb & xfit2 <= ub
polygon(c(lb,xfit2[i],ub), c(0,yfit2[i],0), col=alpha("darkblue", 0.25), border=alpha("darkblue", 0.25))

lines(x=c(ub,ub), y=c(0,.47),lwd=2,lty=2)
text(x = ub, y = .57, labels = expression(c[1-alpha]==1.64))

text(x=0+.25,y=.425, "N(0,1)", pos=2)
text(x=2+.5,y=.425, "N(2,1)", pos=4)
legend(x=-4.5,y=.65, title=NULL, bty="n", 
   c(expression("Null Distribution"~"N(0,1)"),"P(Type II Error)","P(Type I Error)", expression(paste("Power")))[-3], 
    fill=c(alpha("grey", 0.5), alpha("darkblue", 0.25), alpha("red", 0.25), alpha("green", 0.5))[-3], horiz=FALSE)
```


### $p$-Value 

The $p$-value of a test statistic is the significance level we would obtain if we took the sample value of the observed test statistic, $F_{\text{obs}}$ or $t_{\text{obs}},$ as the border between the rejection and non-rejection regions. 

* **$F$-test:** $p=P_{H_0}(F\geq F_{\text{obs}})$
* **$t$-test:** 
   * Two sided, i.e.,  $H_0:\beta_k = r$ vs. $H_A:\beta_k\neq r$: <br> 
    $p=2\cdot\min\{P_{H_0}(t\leq t_{\text{obs}}),P_{H_0}(t\geq t_{\text{obs}})\}=P_{H_0}(|t|\geq|t_{obs}|)$ <br>
    The latter equality holds since the $t$ distrbution is symmetric. 
   * One sided, i.e., $H_0:\beta_k \leq r$ vs. $H_A:\beta_k> r$: <br> 
     $p=P_{H_0}(t\geq t_{\text{obs}})$
   * One sided, i.e., $H_0:\beta_k \geq r$ vs. $H_A:\beta_k< r$: <br> 
     $p=P_{H_0}(t\leq t_{\text{obs}})$

Put another way, the $p$-value is the greatest significance level for which we just fail to reject the null. Therefore, the $p$-value is sometimes also called "marginal significance value". 

If the $p$-value is strictly smaller than the chosen significance level $\alpha$, we reject the null hypothesis. 

## Confidence Intervals {#sec-CIsmallsample}

We define a two-sided $(1-\alpha)\cdot 100\%$ percent confidence interval for the *deterministic* (unknown) true $\beta_k$ as the **random interval** $\operatorname{CI}_{k,1-\alpha}$ for which 
$$
P\Big(\beta_k\in\operatorname{CI}_{k,1-\alpha}\Big)\geq 1-\alpha.
$$
Derivation of the random interval $\operatorname{CI}_{k,1-\alpha}$: 

Observe that
$$
\frac{\hat\beta_k-\beta_k}{\widehat{\operatorname{SE}}(\hat\beta_k|X)}\sim t_{(n-K)}
$$
Therefore,
\begin{align*}
P\left(-t_{1-\alpha/2,n-K}\leq\frac{\hat\beta_k-\beta_k}{\widehat{\operatorname{SE}}(\hat\beta_k|X)}\leq t_{1-\alpha/2,n-K}\right)=1-\alpha,
\end{align*}
where $t_{1-\alpha/2,n-K}$ denotes the $(1-\alpha)$ quantile of the $t$-distribution with $n-K$ degrees of freedom. Next, we can do the following equivalent transformations
\begin{align*}
P\left(-t_{1-\alpha/2,n-K}\leq\frac{\hat\beta_k-\beta_k}{\widehat{\operatorname{SE}}(\hat\beta_k|X)}\leq t_{1-\alpha/2,n-K}\right)&=1-\alpha\\
\Leftrightarrow P\left(\hat\beta_k-t_{1-\alpha/2,n-K}\widehat{\operatorname{SE}}(\hat\beta_k|X)\leq \beta_k\leq\hat\beta_k +t_{1-\alpha/2,n-K}\widehat{\operatorname{SE}}(\hat\beta_k|X)\right)&=1-\alpha\\
\Leftrightarrow P\left(\beta_k\in\underbrace{\left[\hat\beta_k-t_{1-\alpha/2,n-K}\widehat{\operatorname{SE}}(\hat\beta_k|X),\;\hat\beta_k +t_{1-\alpha/2,n-K}\widehat{\operatorname{SE}}(\hat\beta_k|X)\right]}_{=:\operatorname{CI}_{k,1-\alpha}}\right)&=1-\alpha
\end{align*}
That is, the random interval 
$$
\operatorname{CI}_{k,1-\alpha}=\left[\hat\beta_k-t_{1-\alpha/2,n-K}\widehat{\operatorname{SE}}(\hat\beta_k|X),\;\hat\beta_k +t_{1-\alpha/2,n-K}\widehat{\operatorname{SE}}(\hat\beta_k|X)\right]
$$
is our $(1-\alpha)\cdot 100\%$ percent confidence interval for $\beta_k$.

**Interpretation:** The random interval  $\operatorname{CI}_{k,1-\alpha}$ for $\beta_k$ contains the true parameter value $\beta_k$ in $(1-\alpha)\cdot 100\%$ cases of its realizations when considering infinitely many realizations. It's best to take a look at dynamic viszualizations like this one: 

<center>
[https://rpsychologist.com/d3/ci/](https://rpsychologist.com/d3/ci/)
</center>


Unfortunately, this "frequentist" interpretation is not a statement about a single given $\operatorname{CI}_{k,1-\alpha}$ realization computed for a given data set. A given, realized $\operatorname{CI}_{k,1-\alpha}$ will either contain the true parameter $\beta_k$ or not, and usually we do not know the answer. So, confidence intervals are quite hard to interpret. However, they are very well suited as a tool to visualize estimation uncertainties in different parameter estimators, for instance, across $\hat\beta_k$, $k=1,\dots,K$.   

<center>
![](images/Meme_CI_2.jpg)
</center>


## Monte Carlo Simulations {#sec-PSSI}

Let's check the above exact inference results using `R` and Monte Carlo simulations. First, we program a function `myDataGenerator()` which allows us to generate data from the following model, i.e., from the following fully specified data generating process:
\begin{align*}
Y_i   &=\beta_1+\beta_2X_{i2}+\beta_3X_{i3}+\varepsilon_i,\qquad i=1,\dots,n\\
\beta &=(\beta_1,\beta_2,\beta_3)'=(2,3,4)'\\
X_{i2}&\sim U[2,10]\\
X_{i3}&\sim U[12,22]\\
\varepsilon_i|X&\sim\mathcal{N}(0,3^2),
\end{align*}
where $(Y_i,X_i)$ is i.i.d. across $i=1,\dots,n$. Let us consider a small sample size of $n=7$. 


The below function `myDataGenerator()` allows to sample new realizations of $Y_1,\dots,Y_n$ conditionally on a given data matrix $X$.  Moreover, you can provide your own values for the sample size $n$ and for the parameter vector $\beta=(\beta_1,\beta_2,\beta_3)'$. 
```{r}
## Function to generate artificial data
## If the user provides 'X_cond' data, 
## the sampling of new Y variables is 
## conditionally on the given X_cond variables.
## If X_cond = NULL, sampling is done unconditionally. 

myDataGenerator <- function(n, beta, X_cond = NULL){
  if(is.null(X_cond)){
      X   <- cbind(rep(1, n), 
                 runif(n, 2, 10), 
                 runif(n,12, 22))
  }else{
      X   <- X_cond
  }
  eps  <- rnorm(n, sd = 3)
  Y    <- X %*% beta + eps
  data <- data.frame("Y"   = Y, 
                     "X_1" = X[,1], 
                     "X_2" = X[,2], 
                     "X_3" = X[,3])
  ##
  return(data)
}


## Small sample size
n             <- 7        

## Define a true beta vector
beta_true     <- c(2,3,4)

## Generate Y and X data 
test_data     <- myDataGenerator(n = n, beta=beta_true)

## Store the X data as 'X_cond'  
X_cond        <- as.matrix(test_data[,-1]) # as matrix allows matrix multiplications

## Generate new Y data conditionally on X            
test_data_X_cond <- myDataGenerator(n    = n, 
                                    beta = beta_true, 
                                    X    = X_cond)
## compare
round(head(test_data,     3), 2)    # New Y, new X
round(head(test_data_X_cond, 3), 2) # New Y, old X (conditionally on X)
```


### Check: Distribution of $\hat\beta|X$ vs Distribution of $\hat\beta$

The above data generating process fulfills Assumptions 1-4$^*$. So, by theory, the estimators $\hat\beta|X$ should be normal distributed conditionally on $X$,
$$
\hat\beta_k|X\sim\mathcal{N}(\beta_k,\sigma^2[(X'X)^{-1}]_{kk}),\quad k=1,\dots,K=3,
$$ 
where $[(X'X)^{-1}]_{kk}$ denotes the element in the $k$th row and $k$th column of the $K\times K$ matrix $(X'X)^{-1}$. 


In order to check the effect of "conditioning on $X$", let us focus on $\hat\beta_2$ and use two different Monte Carlo simulations:

1. Generate `B`$=10000$ realizations of $\hat\beta_2$ conditionally on $X$.
2. Generate `B`$=10000$ realizations of $\hat\beta_2$ **un**conditionally on $X$.

Then estimate the distributons from both Monte Carlo samples and compare them with the theoretical distribution 
$$
\hat\beta_2|X\sim\mathcal{N}(\beta_k,\sigma^2[(X'X)^{-1}]_{22})
$$
```{r}
set.seed(11100) # seed of the random number generator

## A function to generate realizations of the estimator 
## \hat{beta}_2 conditionally or unconditionally on X:
hatbeta2_sim_fun <- function(conditional, X_cond = NULL){
      if(conditional == TRUE){
            data     <- myDataGenerator(n = n, beta = beta_true, X_cond = X_cond)
            lm_obj   <- lm(Y ~ X_2 + X_3, data = data)
            hatbeta2 <- coef(lm_obj)[2]
      }
      if(conditional == FALSE){
            data     <- myDataGenerator(n = n, beta = beta_true)
            lm_obj   <- lm(Y ~ X_2 + X_3, data = data)
            hatbeta2 <- coef(lm_obj)[2]
      }
  return(hatbeta2)
}

## Number of Monte Carlo replications
B <- 10000

## Draw realizations of \hat{beta}_2
## 1. Generate \hat{beta}_2 realizations conditionally on X
hatbeta2_sim_cond   <- replicate(B, hatbeta2_sim_fun(conditional = TRUE,  X_cond = X_cond))
## 2. Generate \hat{beta}_2 realizations unconditionally on X
hatbeta2_sim_uncond <- replicate(B, hatbeta2_sim_fun(conditional = FALSE))

library("scales") # transparent colors with the alpha() function 

## Plot
## Theoretical normal distribution of beta_hat_2 versus the
## estimated densities based on the two Monte Carlo samples
## true beta_2
beta_true_2     <- beta_true[2] 
## true standard deviation of the error term
sigma           <- 3
## true variance of \hat\beta_2 conditionally on X_cond
var_true_beta_2 <- sigma^2 * solve(t(X_cond) %*% X_cond)[2,2]

## Theoretical Gaussian density of \hat\beta_2 conditionally on X_cond
curve(expr = dnorm(x, mean = beta_true_2, sd=sqrt(var_true_beta_2)), 
xlab="", ylab="", col=gray(.2), lwd=3, lty=1, xlim=c(1,5), ylim=c(0,1.5))
## Estimated density based on the MC-sample of \hat\beta_2 conditionally on X_cond
lines(density(hatbeta2_sim_cond, bw = bw.SJ(hatbeta2_sim_cond)), col=alpha("blue",.5), lwd=3)
## Estimated density based on the MC-sample of \hat\beta_2 *un*conditionally on X_cond
lines(density(hatbeta2_sim_uncond, bw=bw.SJ(hatbeta2_sim_uncond)), col=alpha("red",.5), lwd=3)
legend("topleft", lty=c(1,1,1), lwd=c(3,3,3),
     col=c(gray(.2), alpha("blue",.45), alpha("red",.5)), bty="n", legend= 
c(expression("Theoretical Gaussian Density of"~hat(beta)[2]~'|'~X), 
  expression("Empirical Density based on the 10000 MC realizations of"~hat(beta)[2]~'|'~X),
  expression("Empirical Density based on the 10000 MC realizations of"~hat(beta)[2])))
```

Observations:

* The empirical conditional distribution of $\hat{\beta}_2|X$ matches with the theoretical conditional distribution of $\hat{\beta}_2|X$.
* The empirical **un**conditional distribution of $\hat{\beta}_2$ does **not** match with the theoretical conditional distribution of $\hat{\beta}_2|X$.

Thus, in small sample, the distribution of $\hat{\beta}_2$ is strongly influenced by the $X$ we condition on. This affects also important features of the distribution of $\hat{\beta}_2$ like the variance. 


#### Variance of $\hat\beta|X$ vs. Variance of $\hat\beta$ {-}

The theoretical variance of $\hat{\beta}_2|X$ is:<br> 

<center>
$Var(\hat{\beta}_2|X)=\sigma^2[(X'X)^{-1}]_{22}=$ `var_true_beta_2`.
</center>

```{r}
var_true_beta_2 <- sigma^2 * solve(t(X_cond) %*% X_cond)[2,2]
round(var_true_beta_2, 3)
```


The Monte Carlo realizations of $\hat{\beta}_2|X$, i.e. conditionally on $X$, have an empirical variance of 
```{r}
round(var(hatbeta2_sim_cond), 3)
```
which basically matches the theoretical counterpart.


The Monte Carlo realizations of $\hat{\beta}_2$, i.e. **un**conditionally on $X$, have an empirical variance of 
```{r}
round(var(hatbeta2_sim_uncond), 3)
```
which is clearly different from the theoretical counterpart.


<!-- While the unconditional distribution has a larger variance.  -->


<!-- ```{r} -->
<!-- ## True mean and variance of the true normal distribution  -->
<!-- ## of beta_hat_2|X=X_cond: -->
<!-- # true mean -->

<!-- # true variance -->
<!-- var_true_beta_2 <- sigma^2 * diag(solve(t(X_cond) %*% X_cond))[2] -->

<!-- ## Let's generate 5000 realizations from beta_hat_2  -->
<!-- ## conditionally on X=X_cond and check whether the empirical   -->
<!-- ## distribution of these 5000 realizations is close  -->
<!-- ## to the true normal distribution of beta_hat_2: -->
<!-- B          <- 5000 # MC replications -->
<!-- beta_hat_2 <- rep(NA, times=rep) -->
<!-- ## -->
<!-- for(r in 1:B){ -->
<!--     MC_data <- myDataGenerator(n    = n,  -->
<!--                                beta = beta_true,  -->
<!--                                X    = X_cond) -->
<!--     lm_obj        <- lm(Y ~ X_2 + X_3, data = MC_data) -->
<!--     beta_hat_2[r] <- coef(lm_obj)[2] -->
<!-- } -->

<!-- ## Compare -->
<!-- ## True beta_2 versus average of beta_hat_2 estimates -->
<!-- c(beta_true_2, round(mean(beta_hat_2), 4)) -->
<!-- ``` -->


<!-- The values are very close to each other, thus, on average the estimation results are basically equal to the true parameter value.  -->

<!-- ```{r} -->
<!-- ## Compare -->
<!-- ## True variance of beta_hat_2 versus  -->
<!-- ## empirical variance of beta_hat_2 estimates -->
<!-- c(round(var_true_beta_2, 4), round(var(beta_hat_2), 4)) -->
<!-- ``` -->
<!-- The values are very close to each other, thus the theoretical variance describes very well the actual variance of $\hat\beta_2|X$.  -->

<!-- ```{r, fig.align="center", echo=TRUE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"} -->
<!-- ## True normal distribution of beta_hat_2 versus  -->
<!-- ## empirical density of beta_hat_2 estimates -->
<!-- library("scales") -->
<!-- curve(expr = dnorm(x, mean = beta_true_2,  -->
<!--                    sd=sqrt(var_true_beta_2)),  -->
<!--       xlab="",ylab="", col=gray(.2), lwd=3, lty=1,  -->
<!--       xlim=range(beta_hat_2), ylim=c(0,1.1)) -->
<!-- hist(beta_hat_2, freq=FALSE, col=alpha("blue",.35), add=TRUE) -->
<!-- lines(density(beta_hat_2, bw = bw.SJ(beta_hat_2)),  -->
<!--       col=alpha("blue",.5), lwd=3) -->
<!-- legend("topleft", lty=c(1,NA,1), lwd=c(3,NA,3), pch=c(NA,15,NA), pt.cex=c(NA,2,NA), -->
<!--      col=c(gray(.2), alpha("blue",.45), alpha("blue",.5)), bty="n", legend=  -->
<!-- c(expression( -->
<!--   "Theoretical Gaussian Density of"~hat(beta)[2]~'|'~X),  -->
<!--   expression( -->
<!--   "Histogram based on the 5000 MC realizations of"~ -->
<!--   hat(beta)[2]~'|'~X),  -->
<!--   expression( -->
<!--   "Nonparametric Density Estimation based on the 5000 MC realizations of"~ -->
<!--   hat(beta)[2]~'|'~X))) -->
<!-- ``` -->
<!-- Great! The nonparametric density estimation (estimated via `density()`) and the histogram computed based on the 5000 simulated realizations of $\hat\beta_2|X$ are indicating that $\hat\beta_2|X$ is really normally distributed as described by our theoretical result in Equation @eq-ssnorm.  -->

<!-- However, what would happen if we would sample *unconditionally* on $X$? How does the distribution of $\hat\beta_2$ would then look like?  -->
<!-- ```{r} -->
<!-- set.seed(1110) -->
<!-- ## Let's generate 5000 realizations from beta_hat_2  -->
<!-- ## WITHOUT conditioning on X -->
<!-- beta_hat_2_uncond <- rep(NA, times=rep) -->
<!-- ## -->
<!-- for(r in 1:rep){ -->
<!--     MC_data <- myDataGenerator(n    = n,  -->
<!--                                beta = beta_true) -->
<!--     lm_obj               <- lm(Y ~ X_2 + X_3, data = MC_data) -->
<!--     beta_hat_2_uncond[r] <- coef(lm_obj)[2] -->
<!-- } -->

<!-- ## Compare: -->
<!-- ## True beta_2 versus average of beta_hat_2 estimates -->
<!-- c(beta_true_2, round(mean(beta_hat_2_uncond), 4)) -->
<!-- ``` -->
<!-- OK, at least on average the point point estimates are basically equal to the true parameter value.  -->

<!-- ```{r} -->
<!-- ## Compare:  -->
<!-- ## True variance of beta_hat_2 versus  -->
<!-- ## empirical variance of beta_hat_2 estimates -->
<!-- c(round(var_true_beta_2, 4), round(var(beta_hat_2_uncond), 4)) -->
<!-- ``` -->
<!-- Ouch! The theoretical variance of $\hat\beta_2|X$ is quite different from the actual variance of $\hat\beta_2$ (i.e. simulated without conditioning in $X$). That is, we cannot simply neglect that the variance of $\hat\beta_2$ depends on the observed realization of $X$ in small samples.  -->


<!-- ```{r, fig.align="center", echo=TRUE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"} -->
<!-- ## Plotting the theoretical distribution of beta_hat_2|X  -->
<!-- ## versus the simulated distribution -->
<!-- curve(expr = dnorm(x, mean = beta_true_2, sd=sqrt(var_true_beta_2)),  -->
<!--       xlab="",ylab="", col=gray(.2), lwd=3, lty=1,  -->
<!--       xlim=range(beta_hat_2_uncond), ylim=c(0,1.5)) -->
<!-- hist(beta_hat_2_uncond, freq=FALSE, col=alpha("blue",.35), add=TRUE) -->
<!-- lines(density(beta_hat_2_uncond, bw=bw.SJ(beta_hat_2_uncond)),  -->
<!--       col=alpha("blue",.5), lwd=3) -->
<!-- legend("topleft", lty=c(1,NA,1), lwd=c(3,NA,3), pch=c(NA,15,NA), pt.cex=c(NA,2,NA), -->
<!--      col=c(gray(.2), alpha("blue",.45), alpha("blue",.5)), bty="n", legend=  -->
<!-- c(expression( -->
<!--   "Theoretical Gaussian Density of"~hat(beta)[2]~'|'~X),  -->
<!-- expression( -->
<!--   "Histogram based on the 5000 MC realizations of"~ -->
<!--   hat(beta)[2]),  -->
<!-- expression("Nonparam. Density Estimation based on the 5000 MC realizations of"~ -->
<!--   hat(beta)[2]))) -->
<!-- ``` -->
<!-- Not so good. The theoretical distribution of $\hat\beta_2|X$ has much fatter tails than the simulated distribution of $\hat\beta_2$  (i.e. simulated without conditioning in $X$). That is, we cannot simply neglect that the distribution of $\hat\beta_2$ depends on the observed realization of $X$ in small samples. In large sample, however, the effect of a given realization $X$ is (fortunately) negligible, as we will see in @sec-lsinf.  -->


### Check: Testing Multiple Parameters 

In the following, we do inference about multiple parameters. We test 
\begin{align*}
H_0:\;&\beta_2=3\quad\text{and}\quad\beta_3=4\\
\text{versus}\quad H_A:\;&\beta_2\neq 3\quad\text{and/or}\quad\beta_3\neq 4.
\end{align*}
Or equivalently
\begin{align*}
H_0:\;&R\beta -r = 0 \\
H_A:\;&R\beta -r \neq 0,
\end{align*}
where 
$$
R=\left(
\begin{matrix}
0&1&0\\
0&0&1\\
\end{matrix}\right)\quad\text{ and }\quad 
r=\left(\begin{matrix}3\\4\\\end{matrix}\right).
$$
The following `R` code can be used to test this hypothesis:
```{r}
## Library containing the function 'linearHyothesis()' 
## for testing multiple parameters 
suppressMessages(library("car")) 
## See ?linearHypothesis

## Generate one Monte Carlo sample (under H0)
data   <- myDataGenerator(n = n, beta = beta_true)

## Estimate the linear regression model parameters
lm_obj <- lm(Y ~ X_2 + X_3, data = data)

## Option 1:
test_result <- car::linearHypothesis(
      model = lm_obj, 
      hypothesis.matrix = c("X_2=3", "X_3=4"))   
test_result        
```
The $p$-value $p=$ `r round(test_result[[6]][2],4)` is larger than the chosen significance level $\alpha=0.05$ thus we cannot reject the null hypothesis "$H_0:\;\beta_2=3$ and $\beta_3=4.$" 

The following codes gives an alternative, equivalent way to compute the test result:
```{r, echo=TRUE, eval=FALSE}
## Option 2:
R <- rbind(c(0,1,0),
           c(0,0,1))
car::linearHypothesis(model = lm_obj, 
                      hypothesis.matrix = R, 
                      rhs = c(3,4))
```


Here, we simulated data "under the null hypothesis" and thus it is not surpising that we cannot reject the null hypothesis at a significance level of, for instance, $\alpha=0.05$. However, in repeated samples we should nevertheless observe $\alpha\cdot 100\%$ type I errors (false rejections of $H_0$) under the null hypothesis. Let's check the type I error rate using the following Monte Carlo simulation:
```{r}
## Let's generate 5000 F-test decisions and check 
## whether the empirical rate of type I errors is 
## close to the theoretical significance level. 
B               <- 5000 # MC replications
F_test_pvalues  <- rep(NA, times=B)
##
for(r in 1:B){
  ## generate new data (under H0)
  MC_data <- myDataGenerator(n = n, beta = beta_true)
  ## estimate 
  lm_obj  <- lm(Y ~ X_2 + X_3, data = MC_data)
  ## compute test and p-value
  p       <- linearHypothesis(lm_obj, c("X_2=3", "X_3=4"))$`Pr(>F)`[2]
  ## save the p-value
  F_test_pvalues[r] <- p
}

## Is the significance level ("nominal type I error rate") 
## smaller or equal to the actual type I error rate?

alpha        <-  0.05          # signif level
rejections   <- F_test_pvalues[F_test_pvalues < alpha]
round(length(rejections)/B, 4) # actual type I error rate 
## 
alpha        <-  0.01  # signif level
rejections   <- F_test_pvalues[F_test_pvalues < alpha]
round(length(rejections)/B, 4) # actual type I error rate 
```

Observations:

1. We correctly control for the type I error rate since the empirical type I error rate is not larger than the chosen significance level $\alpha$ (i.e. the nominal type I error rate). 
2. Second, the $F$ test is not conservative since the empirical type I error rates essentially matche the chosen significance levels $\alpha$  (i.e. the nominal Type I error rate). 
   2.1 In fact, if we would increase the number of Monte Carlo repetitions, the empirical type I error rate would converge to the nominal type I error rate $\alpha$ due to the law of large numbers.
3. Last but not least: All this works **unconditionally** on $X$ since the distribution of the $F$ statistic does not depend on $X$. 
<!-- (Thus is also works conditionally on $X$ and you may check this at home.) -->

Next, we check how well the $F$ test detects certain **violations of the null hypothesis**. We do this by using the same data generating process, but by testing the following <span style="color:#FF0000">incorrect</span> null hypothesis:
\begin{align*}
H_0:\;&{\color{red}\beta_2=4}\quad\text{and}\quad\beta_3=4\\
H_A:\;&\beta_2\neq 4\quad\text{and/or}\quad\beta_3\neq 4
\end{align*}
```{r}
B               <- 5000 # MC replications
F_test_pvalues  <- rep(NA, times=B)
##
for(r in 1:B){
  ## generate new data 
  MC_data <- myDataGenerator(n    = n, beta = beta_true)
  ## estimate 
  lm_obj  <- lm(Y ~ X_2 + X_3, data = MC_data)
  ## compute test and p-value (for a false H0)
  p       <- linearHypothesis(lm_obj, c("X_2=4", "X_3=4"))$`Pr(>F)`[2]
  ## save the p-value
  F_test_pvalues[r] <- p
}

## Checking the power of the F test 

alpha       <-  0.05  # signif_level
rejections  <- F_test_pvalues[F_test_pvalues < alpha]
length(rejections)/B  # power 
```

We can now correctly reject the false null hypothesis in approximately `r length(rejections)/B *100` % of all Monte Carlo replications. 

**Caution:** This means that we are not able to detect the violation of the null hypothesis in `r 100 - length(rejections)/B *100` % of cases. Therefore, we can never use an insignificant test result ($p$-value $\geq\alpha$) as a confirmation of the null hypothesis. Obviously, there are type II error events (not rejecting a false H$_0$), but since we typically do not know the distribution of the test statistic under the alternative hypothesis, we cannot control the type II error rate. We can only control the type I error rate by using a small significance level $\alpha$. 

Moreover, note that the $F$ test is not informative about which part of the null hypothesis ($\beta_2=4$ and/or $\beta_3=4$) is violated. We only get the information that at least one of the multiple parameter hypotheses is violated.


### Check: Dualty of Confidence Intervals and Hypothesis Tests

Confidence intervals can be computed using `R` as following:
```{r}
## Significance level
alpha        <- 0.05
## Confidence level
conf_level   <- 1 - alpha

## 95% CI for beta_2
confint(lm_obj, parm = "X_2", level = conf_level)
## 95% CI for beta_3 
confint(lm_obj, parm = "X_3", level = conf_level)
```
We can use these two-sided confidence intervals to conduct hypotheses tests. This property of confidence intervals is called the **duality of confidence intervals and hypothesis tests**. 

For instance, when testing the null hypothesis
\begin{align*}
H_0:&\beta_2=3\\
\text{versus}\quad H_A: &\beta_2\neq 3
\end{align*}
we can either use a $t$-test or equivalently check whether the confidence interval $\operatorname{CI}_{2,1-\alpha}$ for $\beta_2$ contains the hypothetical value $4$ or not. 

* In case of $3    \in\operatorname{CI}_{2,1-\alpha}$, we cannot reject the null hypothesis H$_0$: $\beta_2=3.$ 
* In case of $3\not\in\operatorname{CI}_{2,1-\alpha}$, we can reject the null hypothesis H$_0$: $\beta_2=3.$

If the Assumptions 1-4$^\ast$ hold true, then $\operatorname{CI}_{2,1-\alpha}$ is an exact confidence interval. That is, under the null hypothesis, it falsely rejects the null hypothesis in only $\alpha\cdot 100\%$ of resamplings. Let's check this in the following Monte Carlo simulation:
```{r, fig.align="center", echo=TRUE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
## Significance level
alpha      <- 0.05
## Container to save all CI realizations
confint_m  <- matrix(NA, nrow=2, ncol=B)
##
for(r in 1:B){
  ## generate new data 
  MC_data <- myDataGenerator(n = n, beta = beta_true)
  ## estimate
  lm_obj  <- lm(Y ~ X_2 + X_3, data = MC_data)
  ## compute confidence interval 
  CI <- confint(lm_obj, parm="X_2", level = 1 - alpha)
  ## save confidence interval
  confint_m[,r] <- CI
}
## check whether true parameter is inside the CI
inside_CI  <- confint_m[1,] <= beta_true_2 & 
                beta_true_2 <= confint_m[2,]

## CI-lower, CI-upper, beta_true_2 inside?
head(cbind(t(confint_m), inside_CI))
```

The following code computes the relative frequency of confidence intervals **not containing** the true parameter value $(\beta_2=3)$:
```{r, fig.align="center", echo=TRUE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
round(length(inside_CI[inside_CI == FALSE])/B, 4)
```
That's good! The relative frequency is basically equal to the chosen $\alpha=0.05$ value. 


Next, we visualize a subsample of `100` confidence intervals from the total sample of `5000` generated confidence interval realizations: 
```{r, fig.align="center", echo=TRUE, fig.width = 8, fig.height = 5, out.width = "1\\textwidth"}
nCIs <- 100
plot(x=0, y=0,type="n", xlim=c(0,nCIs), ylim=range(confint_m[,1:nCIs]),
     ylab="", xlab="Resamplings", main="Confidence Intervals")
for(r in 1:nCIs){
  if(inside_CI[r]==TRUE){
      lines(x=c(r,r), y=c(confint_m[1,r], confint_m[2,r]), lwd=2, col=gray(.5,.5))
  }else{
      lines(x=c(r,r), y=c(confint_m[1,r], confint_m[2,r]), lwd=2, col="darkred")
    }
}
axis(4, at=beta_true_2, labels = expression(beta[2]))
abline(h=beta_true_2)
```

As expected, only about $\alpha\cdot 100\%=5\%$ of all confidence intervals do not contain the true parameter value $\beta_2=3$, but about $(1-\alpha)\cdot 100\%=95\%$ of all confidence intervals contain the true parameter value $\beta_2=3$. 


## Real Data Example {#sec-RDSSInf}

```{r}
## The AER package contains a lot of datasets 
suppressPackageStartupMessages(library(AER))

## Attach the DoctorVisits data to make it usable
data("DoctorVisits")

lm_obj <- lm(visits ~ gender + age + income, data = DoctorVisits)
```


The above `R` codes estimate the following regression model
$$
Y_i = \beta_1 + \beta_{gender} X_{gender,i} 
              + \beta_{age} X_{age,i}
              + \beta_{income} X_{income,i} + \varepsilon_i,
$$
where $i=1,\dots,n$ and

* $X_{gender,i}=1$ if the $i$th subject is a woman and $X_{gender,i}=0$ if the $i$th subject is a man
* $X_{age,i}$ is the age of subject $i$ measured in years divided by $100$
* $X_{income,i}$ is the annual income of subject $i$ in tens of thousands of dollars


The following `R` codes produces the classic regression output table (simular tables are produced by all statistical/econometric software packages):
```{r, echo=TRUE, eval=TRUE}
lm_obj_summary <- summary(lm_obj)
lm_obj_summary
```

The above regression output table contains the following information:

* **Estimate:** The column "Estimate" containes the estimates 
$$
\hat\beta_{j},\quad j\in\{1,gender, age, income\}
$$
You can extract them using `coef(lm_obj)`.


* **Std. Error:** The column "Std. Error" containes the estimates 
$$
\widehat{\operatorname{SE}}(\hat\beta_{j}|X),\quad j\in\{1,gender, age, income\}
$$ 
  * You can extract the total $(K\times K)=(4\times 4)$ variance-covariance matrix estimate $\widehat{Var}(\hat\beta|X)$ using `vcov(lm_obj)`. 
  * The diagonal `diag(vcov(lm_obj))` contains the variance estimates $\widehat{Var}(\hat\beta_j|X)$, $j\in\{1,gender, age, income\}$. 
  * The square root of the diagonal `sqrt(diag(vcov(lm_obj)))` allows you to compute the estimated standard errors shown in the regression table.

* **t value:** The column "t value" contains the observed $t$ test statistics 
$$
t_{obs,j}=\frac{\hat\beta_{j}}{\widehat{\operatorname{SE}}(\hat\beta_{j}|X)},\quad j\in\{1,gender, age, income\}
$$
You can extract the values using `lm_obj_summary$coefficients[,3]`.


* **Pr(>|t|):** The column "Pr(>|t|)" contains the $p$ values
$$
P_{H_0}(|t|>t_{obs,j}),\quad j\in\{1,gender, age, income\}
$$
You can extract the values using `lm_obj_summary$coefficients[,4]`.

* **Residual standard error**  $\sqrt{\frac{1}{n-K}\sum_{i=1}^n\hat\varepsilon^2_i}=$ `sqrt(sum(resid(lm_obj)^2)/(n-4))` $=$ `r round(sqrt(sum(resid(lm_obj)^2)/(nrow(DoctorVisits)-4)), 4)`

* **Multiple R-squared:** $R^2=$ `lm_obj_summary$r.squared` $=$ `r round(lm_obj_summary$r.squared, 5)`

* **Adjusted R-squared:** $\bar{R}^2=$ `lm_obj_summary$adj.r.squared` $=$ `r round(lm_obj_summary$adj.r.squared, 5)`

* **F-statistic:** This is a standard $F$ test that tests the null hypothesis that all parameters except the intercept are zero; i.e.<br> 
$H_0$: $\beta_{gender}=\beta_{age}=\beta_{income}=0$<br>
versus<br> 
$H_A$: At least one parameter is not zero. `R`'s `summary()` functions reports an observed $F$ statistic value of $33.22$ which needs to be evaluated for an $F$ distribution with $3$ and $5186$ degrees of freedom leading to a $p$-value $p< 0.00001.$
<br><br>
You can replicate this $F$-test result using the following `R` code:
```{r, eval=TRUE, echo=TRUE}
car::linearHypothesis(
      model = lm_obj, 
      hypothesis.matrix = c("genderfemale=0", "age=0", "income=0"))  
```

<!-- #### Interpretation of $\hat\beta_{gender}$ {-} -->

<!-- Since $X_{gender,i}$ is a dummy variable,  -->

#### `R` Package Stargazer {-}

Beautiful and "publication ready" regression outputs can be produced using the `R` package `stargazer` and its function `stargazer()`:

```{r, echo=FALSE, eval=TRUE}
suppressPackageStartupMessages(library(stargazer))
```

<center>
```{r, echo=TRUE, eval=TRUE, results='asis'}
## Hint: use type = "latex" 
## to produce a latex table
stargazer(lm_obj, type="html")
```
</center>


#### Critical Discussion of the Regression above Results {-}

The above real data analysis does **not** fit into the small sample inference framework we introduced in this chapter. 

1. The dependent variable $Y_i$ `visits` is a *categorial* variable taking finitely many discrete values, indeed 
<center>
`unique(DoctorVisits$visits)` = `r unique(DoctorVisits$visits)`. 
</center>
Consequently, the error term $\varepsilon_i$ **cannot be normal distributed**. 
2. The diagnostic plot ("Residuals versus Fitted") indicates a possible issue **violation of the homoskedasticity assumption**. In case of homokedastic variances, the data points $(\hat\varepsilon_i,\hat{Y}_i)$, $i=1,\dots,n$ should roughly show a homogenous scattering across the fitted values $\hat{Y}_i=X\hat\beta$. This seems not to be the case here.
```{r}
## Diagonstic Plot 
## Residuals versus fitted values
plot(lm_obj, which = 1)
```


Lukily, the data set `DoctorVisits` actually has a **large sample size** of $n=$ `r nrow(DoctorVisits)` and thus there is a way out of this problem: The large sample inference framework introduced in the next chapter. 


<!-- ## Exercises -->

<!-- * [Exercises for Chapter 5](Exercises/Ch5_Exercises.pdf) -->

<!-- * [Exercises of Chapter 5 with Solutions](Exercises/Ch5_Exercises_with_Solutions.pdf) -->

## References {-}

