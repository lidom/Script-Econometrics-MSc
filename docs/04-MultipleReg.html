<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Econometrics M.Sc. - 3&nbsp; Multiple Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05-Small-Sample-Inference.html" rel="next">
<link href="./02-Review-Prob_n_Stats.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Econometrics M.Sc.</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-Introduction-to-R.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to R</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-Review-Prob_n_Stats.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Review: Probability and Statistics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-MultipleReg.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-Small-Sample-Inference.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Small Sample Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-Asymptotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Large Sample Inference</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#assumptions" id="toc-assumptions" class="nav-link active" data-scroll-target="#assumptions"><span class="toc-section-number">3.1</span>  Assumptions</a>
  <ul class="collapse">
  <li><a href="#some-implications-of-the-exogeneity-assumption" id="toc-some-implications-of-the-exogeneity-assumption" class="nav-link" data-scroll-target="#some-implications-of-the-exogeneity-assumption"><span class="toc-section-number">3.1.1</span>  Some Implications of the Exogeneity Assumption</a></li>
  </ul></li>
  <li><a href="#deriving-the-expression-of-the-ols-estimator" id="toc-deriving-the-expression-of-the-ols-estimator" class="nav-link" data-scroll-target="#deriving-the-expression-of-the-ols-estimator"><span class="toc-section-number">3.2</span>  Deriving the Expression of the OLS Estimator</a></li>
  <li><a href="#some-quantities-of-interest" id="toc-some-quantities-of-interest" class="nav-link" data-scroll-target="#some-quantities-of-interest"><span class="toc-section-number">3.3</span>  Some Quantities of Interest</a></li>
  <li><a href="#method-of-moments-estimator" id="toc-method-of-moments-estimator" class="nav-link" data-scroll-target="#method-of-moments-estimator"><span class="toc-section-number">3.4</span>  Method of Moments Estimator</a></li>
  <li><a href="#unbiasedness-of-hatbetax-and-hatbeta" id="toc-unbiasedness-of-hatbetax-and-hatbeta" class="nav-link" data-scroll-target="#unbiasedness-of-hatbetax-and-hatbeta"><span class="toc-section-number">3.5</span>  Unbiasedness of <span class="math inline">\(\hat\beta|X\)</span> and <span class="math inline">\(\hat\beta\)</span></a></li>
  <li><a href="#ch:VarEstBeta" id="toc-ch:VarEstBeta" class="nav-link" data-scroll-target="#ch\:VarEstBeta"><span class="toc-section-number">3.6</span>  Variance and Standard Error of <span class="math inline">\(\hat\beta|X\)</span></a></li>
  <li><a href="#the-gauss-markov-theorem" id="toc-the-gauss-markov-theorem" class="nav-link" data-scroll-target="#the-gauss-markov-theorem"><span class="toc-section-number">3.7</span>  The Gauss-Markov Theorem</a></li>
  <li><a href="#practice-real-data" id="toc-practice-real-data" class="nav-link" data-scroll-target="#practice-real-data"><span class="toc-section-number">3.8</span>  Practice: Real Data</a>
  <ul class="collapse">
  <li><a href="#the-r-function-i" id="toc-the-r-function-i" class="nav-link" data-scroll-target="#the-r-function-i">The <code>R</code> Function <code>I()</code></a></li>
  <li><a href="#dummy-variables-contrast-codings-and-interactions" id="toc-dummy-variables-contrast-codings-and-interactions" class="nav-link" data-scroll-target="#dummy-variables-contrast-codings-and-interactions">Dummy Variables, Contrast Codings, and Interactions</a></li>
  </ul></li>
  <li><a href="#practice-simulation" id="toc-practice-simulation" class="nav-link" data-scroll-target="#practice-simulation"><span class="toc-section-number">3.9</span>  Practice: Simulation</a>
  <ul class="collapse">
  <li><a href="#behavior-of-the-ols-estimates-for-resampled-data-conditionally-on-x_i" id="toc-behavior-of-the-ols-estimates-for-resampled-data-conditionally-on-x_i" class="nav-link" data-scroll-target="#behavior-of-the-ols-estimates-for-resampled-data-conditionally-on-x_i"><span class="toc-section-number">3.9.1</span>  Behavior of the OLS Estimates for Resampled Data (conditionally on <span class="math inline">\(X_i\)</span>)</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-MLR" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>In the following we focus on case of random designs <span class="math inline">\(X\)</span> (i.e.&nbsp;<span class="math inline">\(X\)</span> being a random variable), since, first, this is the more relevant case in econometrics and, second, it includes the case of fixed designs (i.e.&nbsp;<span class="math inline">\(X\)</span> being deterministic) as a special case (“degenerated random variable”). Caution: A random <span class="math inline">\(X\)</span> requires us to consider conditional means and variances “given <span class="math inline">\(X\)</span>.” That is, if we would be able to resample from the model, we do so by fixing (conditioning on) the in-principle random explanatory variable <span class="math inline">\(X\)</span>.</p>
<section id="assumptions" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="assumptions"><span class="header-section-number">3.1</span> Assumptions</h2>
<p>The multiple linear regression model is defined by the following assumptions:</p>
<p><strong>Assumption 1: The Linear Model Assumption (Data Generating Process)</strong></p>
<p><strong>Part (a): Model</strong></p>
<p><span id="eq-LinMod"><span class="math display">\[
\begin{align}
  Y_i=\sum_{k=1}^K\beta_k X_{ik}+\varepsilon_i, \quad i=1,\dots,n.
\end{align}
\tag{3.1}\]</span></span> Usually, a constant (intercept) is included, in this case <span class="math inline">\(X_{i1}=1\)</span> for all <span class="math inline">\(i\)</span>. In the following we will always assume that <span class="math inline">\(X_{i1}=1\)</span> for all <span class="math inline">\(i\)</span>, unless otherwise stated.<br>
It is convenient to write <a href="#eq-LinMod">Equation&nbsp;<span>3.1</span></a> using matrix notation <span class="math display">\[\begin{eqnarray*}
  Y_i&amp;=&amp;\underset{(1\times K)}{X_i'}\underset{(K\times 1)}{\beta} +\varepsilon_i, \quad i=1,\dots,n,
\end{eqnarray*}\]</span> where <span class="math inline">\(X_i=(X_{i1},\dots,X_{iK})'\)</span> and <span class="math inline">\(\beta=(\beta_1,\dots,\beta_K)'\)</span>. Stacking all individual rows <span class="math inline">\(i\)</span> leads to <span class="math display">\[\begin{eqnarray*}\label{LM}
  \underset{(n\times 1)}{Y}&amp;=&amp;\underset{(n\times K)}{X}\underset{(K\times 1)}{\beta} + \underset{(n\times 1)}{\varepsilon},
\end{eqnarray*}\]</span> where <span class="math display">\[\begin{equation*}
Y=\left(\begin{matrix}Y_1\\ \vdots\\Y_n\end{matrix}\right),\quad X=\left(\begin{matrix}X_{11}&amp;\dots&amp;X_{1K}\\\vdots&amp;\ddots&amp;\vdots\\ X_{n1}&amp;\dots&amp;X_{nK}\\\end{matrix}\right),\quad\text{and}\quad \varepsilon=\left(\begin{matrix}\varepsilon_1\\ \vdots\\ \varepsilon_n\end{matrix}\right).
\end{equation*}\]</span></p>
<p><strong>Part (b): Random Sampling</strong></p>
<p>Moreover, we assume that the observed (“obs”) data points <span class="math display">\[
((Y_{1,obs},X_{11,obs},\dots,X_{1K,obs}),(Y_{2,obs},X_{21,obs},\dots,X_{2K,obs}),\dots,(Y_{n,obs},X_{n1,obs},\dots,X_{nK,obs}))
\]</span> are a realizations of an <strong>independent and identically distributed (i.i.d.)</strong> random sample <span class="math display">\[
((Y_{1},X_{11},\dots,X_{1K}),(Y_{2},X_{21},\dots,X_{2K}),\dots,(Y_{n},X_{n1},\dots,X_{nK}))
\]</span></p>
<p>That is, the <span class="math inline">\(i\)</span>th observed <span class="math inline">\(K+1\)</span> dimensional data point <span class="math inline">\((Y_{i,obs},X_{i1,obs},\dots,X_{iK,obs})\in\mathbb{R}^{K+1}\)</span> is a realization of a <span class="math inline">\(K+1\)</span> dimensional random variable <span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1},\)</span> where <span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\)</span> has the identical joint distribution as <span class="math inline">\((Y_{j},X_{j1},\dots,X_{jK})\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and all <span class="math inline">\(j=1,\dots,n\)</span>, and where <span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\)</span> is independent of <span class="math inline">\((Y_{j},X_{j1},\dots,X_{jK})\)</span> for all <span class="math inline">\(i\neq j=1,\dots,n.\)</span></p>
<p>Note: Due to <a href="#eq-LinMod">Equation&nbsp;<span>3.1</span></a>, this i.i.d. assumption is equivalent to assuming that the multivariate random variables <span class="math inline">\((\varepsilon_i,X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1}\)</span> are i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span>.</p>
<p><strong>Remark:</strong> Usually, we do not use a different notation for observed realizations <span class="math inline">\((Y_{i,obs},X_{i1,obs},\dots,X_{iK,obs})\in\mathbb{R}^{K+1}\)</span> and for the corresponding random variable <span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1}\)</span> since often both interpretations (random variable and its realizations) can make sense in the same statement and then it depends on the considered context whether the random variables point if view or the realization point of view applies.</p>
<!--  That is, the multivariate distribution of $(\varepsilon_i,X_{i1},\dots,X_{iK})$ is assumed equal 
across $i=1,\dots,n$, but the multivariate random variables $(\varepsilon_i,X_{i1},\dots,X_{iK})$ 
and $(\varepsilon_j,X_{j1},\dots,X_{jK})$ are independent for all $i\neq j$. -->
<!-- **Note.:** Of course, in principle, Assumption 1 can be violated; however, in classic econometrics one usually does not bother with a possibly violated model assumption (Assumption 1). In modern econometrics, this is a big deal.  -->
<!-- The following assumptions are the so-called *classic assumptions* and we will be often concerned about violations of these assumptions.  -->
<p><strong>Assumption 2: Exogeneity</strong> <span class="math display">\[E(\varepsilon_i|X_i)=0,\quad i=1,\dots,n\]</span> This assumption demands that the mean of the random error term <span class="math inline">\(\varepsilon_i\)</span> is zero irrespective of the realizations of <span class="math inline">\(X_i\)</span>. Note that together with the random sampling assumption (in Assumption 1) this assumption implies even <strong>strict</strong> exogeneity <span class="math inline">\(E(\varepsilon_i|X) = 0\)</span> since we have independence across <span class="math inline">\(i=1,\dots,n\)</span>. Under strict exogeneity, the mean of <span class="math inline">\(\varepsilon_i\)</span> is zero irrespective of the realizations of <span class="math inline">\(X_1,\dots,X_n\)</span>. The exogeneity assumption is also called “orthogonality assumption” or “mean independence assumption”.</p>
<!-- For one example, it cannot be fulfilled when the regressors include lagged dependent variables. -->
<!-- Notice that in the presence of a constant explanatory variable, setting the expectation to zero is a normalization.  -->
<p><strong>Assumption 3: Rank Condition (no perfect multicollinearity)</strong></p>
<p><span class="math display">\[\operatorname{rank}(X)=K\quad\text{a.s.}\]</span> This assumption demands that the event of one explanatory variable being linearly dependent on the others occurs with a probability equal to zero. (This is the literal translation of the “almost surely (a.s.)” concept.) The assumption implies that <span class="math inline">\(n\geq K\)</span>.</p>
<p>This assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: multicollinearity, dummy variable trap, variance inflation). The violation of this assumption harms any economic interpretation as we cannot disentangle the explanatory variables’ individual effects on <span class="math inline">\(Y\)</span>. Therefore, this assumption is also often called an <strong>identification assumption</strong>.</p>
<p><strong>Assumption 4: Error distribution</strong></p>
<p>Depending on the context (i.e., parameter estimation vs.&nbsp;hypothesis testing and small <span class="math inline">\(n\)</span> vs.&nbsp;large <span class="math inline">\(n\)</span>) there are different more or less restrictive assumptions. Some of the most common ones are the following:</p>
<ul>
<li><strong>Conditional Distribution:</strong> <span class="math inline">\(\varepsilon_i|X_i \sim f_{\varepsilon|X}\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and for any distribution <span class="math inline">\(f_{\varepsilon|X}\)</span> such that <span class="math inline">\(\varepsilon_i|X_i\)</span> has two (or more) finite moments.</li>
<li><strong>Conditional Normal Distribution:</strong> <span class="math inline">\(\varepsilon_i|X_i \sim \mathcal{N}(0,\sigma^2(X_i))\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>.</li>
<li><strong>i.i.d.:</strong> <span class="math inline">\(\varepsilon_i\overset{\operatorname{i.i.d.}}{\sim}f_\varepsilon\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and for any distribution <span class="math inline">\(f_\varepsilon\)</span> such that <span class="math inline">\(\varepsilon_i\)</span> has two (or more) finite moments. Assuming that the error terms <span class="math inline">\(\varepsilon_i\)</span> are themselves i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span> implies that they do not depend on <span class="math inline">\(X_i\)</span>.</li>
<li><strong>i.i.d. Normal:</strong> As above, but with <span class="math inline">\(f=\mathcal{N}(0,1)\)</span>, i.e., <span class="math inline">\(\varepsilon_i\overset{\operatorname{i.i.d.}}{\sim}\mathcal{N}(0,\sigma^2)\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>.</li>
<li><strong>Spherical errors (“Gauss-Markov assumptions”):</strong> The conditional distributions of <span class="math inline">\(\varepsilon_i|X_i\)</span> may generally depend on <span class="math inline">\(X_i\)</span>, but without affecting the second moments such that <span class="math display">\[\begin{align*}
E(\varepsilon_i^2|X_i)         &amp;=\sigma^2&gt;0\quad\text{for all }i=1,\dots,n\\
E(\varepsilon_i\varepsilon_j|X)&amp;=0\quad\text{for all }i\neq j\quad\text{with}\quad i=1,\dots,n\quad\text{and}\quad j=1,\dots,n.
\end{align*}\]</span> Thus, here one assumes that, for a given realization of <span class="math inline">\(X_i\)</span>, the error process is uncorrelated (i.e.&nbsp;<span class="math inline">\(Cov(\varepsilon_i,\varepsilon_j|X)=E(\varepsilon_i\varepsilon_j|X)=0\)</span> for all <span class="math inline">\(i\neq j\)</span>) and homoscedastic (i.e.&nbsp;<span class="math inline">\(Var(\varepsilon_i|X)=\sigma^2\)</span> for all <span class="math inline">\(i\)</span>).</li>
</ul>
<p><strong>Technical Note:</strong> When we write that <span class="math inline">\(Var(\varepsilon_i|X)=\sigma^2\)</span> or <span class="math inline">\(Var(\varepsilon_i|X_i)=\sigma^2_i,\)</span> we implicitly assume that these second moments exists and that they are finite.</p>
<section id="homoscedastic-versus-heteroscedastic-error-terms" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="homoscedastic-versus-heteroscedastic-error-terms">Homoscedastic versus Heteroscedastic Error Terms</h4>
<p>The i.i.d. assumption is not as restrictive as it may seem on first sight. It allows for dependence between <span class="math inline">\(\varepsilon_i\)</span> and <span class="math inline">\((X_{i1},\dots,X_{iK})\in\mathbb{R}^K\)</span>. That is, the error term <span class="math inline">\(\varepsilon_i\)</span> can have a conditional distribution which depends on <span class="math inline">\((X_{i1},\dots,X_{iK})\)</span>; see <a href="02-Review-Prob_n_Stats.html#sec-condDistr"><span>Section&nbsp;2.2.2.5</span></a>.</p>
<!-- However, we need to rule out one certain 
dependency between $\varepsilon_i$ and $X_i$; namely the conditional mean of $\varepsilon_i$ is not 
allowed to depend on $X_i$ (see Assumption 2: Exogeneity). -->
<!-- Example: $\varepsilon|X_i\sim U[-0.5|X_{i2}+X_{i3}|, 0.5|X_{i2}+X_{i3}|]$, 
with $X_{i2}\sim U[-4,4]$. This error term is mean independent of $X_i$ since 
$E(\varepsilon_i|X_i)=0$, but it has heteroscedastic conditional variance 
$Var(\varepsilon_i|X_i)=\frac{1}{12}X_i^2$. -->
<p>The exogeneity assumption (Assumption 2: Exogeneity) requires that the conditional mean of <span class="math inline">\(\varepsilon_i\)</span> is independent of <span class="math inline">\(X_i\)</span>. Besides this, dependencies between <span class="math inline">\(\varepsilon_i\)</span> and <span class="math inline">\(X_{i1},\dots,X_{iK}\)</span> are allowed. For instance, the variance of <span class="math inline">\(\varepsilon_i\)</span> can be a function of <span class="math inline">\(X_{i1},\dots,X_{iK}\)</span>. If this is the case, <span class="math inline">\(\varepsilon_i\)</span> is said to be <strong>heteroscedastic</strong>.</p>
<ul>
<li><strong>Heteroscedastic error terms:</strong> The conditional variances <span class="math inline">\(Var(\varepsilon_i|X_i=x_i)=\sigma^2(x_i)\)</span> are equal to a non-constant variance-function <span class="math inline">\(\sigma^2(x_i)&gt;0\)</span> which is a function of the realization <span class="math inline">\(x_i\in\mathbb{R}^K\)</span> of <span class="math inline">\(X_i\in\mathbb{R}^K\)</span>.</li>
</ul>
<p><strong>Example:</strong> <span class="math inline">\(\varepsilon_i|X_i\sim U[-0.5|X_{i2}|, 0.5|X_{i2}|],\)</span> with <span class="math inline">\(X_{i2}\sim U[-4,4]\)</span>. This error term is mean independent of <span class="math inline">\(X_i\)</span> since <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span>, but it has a heteroscedastic conditional variance since <span class="math inline">\(Var(\varepsilon_i|X_i)=\frac{1}{12}X_i^2\)</span> depends on <span class="math inline">\(X_i\)</span>.</p>
<p>Sometimes, we need to be more restrictive by assuming that also the variances of the error terms <span class="math inline">\(\varepsilon_i\)</span> are independent from <span class="math inline">\(X_i\)</span>. (Higher moments may still depend on <span class="math inline">\(X_i\)</span>.) This assumption leads to <strong>homoscedastic</strong> error terms.</p>
<ul>
<li><strong>Homoscedastic error terms:</strong> The conditional variances <span class="math inline">\(Var(\varepsilon_i|X_i=x_i)=\sigma^2\)</span> are equal to some constant <span class="math inline">\(\sigma^2&gt;0\)</span> for every possible realization <span class="math inline">\(x_i\in\mathbb{R}^K\)</span> of <span class="math inline">\(X_i\in\mathbb{R}^K\)</span>.</li>
</ul>
<!-- Sometimes, we need to be even more restrictive by assuming that the error terms $\varepsilon_i$  -->
<!-- are themselves **i.i.d.** across $i=1,\dots,n$. This is more restrictive than the assumption that the  -->
<!-- multivariate random variables $(\varepsilon_i,X_{i1},\dots,X_{iK})$ are i.i.d. across $i=1,\dots,n$  -->
<!-- since it implies that the whole distribution (not only the first two moments) of $\varepsilon_i$  -->
<!-- does not depend on $X_i$. This assumption also implies  **homoscedastic** error terms since when  -->
<!-- the whole distribution of $\varepsilon_i$ does not depend on $X_i\in\mathbb{R}^K$ also its variance  -->
<!-- doesn't depend on $X_i$.  -->
<p><strong>Example:</strong> For doing small sample inference (see <a href="05-Small-Sample-Inference.html"><span>Chapter&nbsp;4</span></a>), we need to assume that the error terms <span class="math inline">\(\varepsilon_i\)</span> are i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span> plus the normality assumption, i.e., <span class="math inline">\(\varepsilon_i\stackrel{\textrm{i.i.d.}}{\sim}{\mathcal N} (0, \sigma^2)\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> which leads to homoscedastic variances <span class="math inline">\(Var(\varepsilon_i|X_i)=\sigma^2\)</span> for every possible realization of <span class="math inline">\(X_i\)</span>.</p>
</section>
<section id="some-implications-of-the-exogeneity-assumption" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="some-implications-of-the-exogeneity-assumption"><span class="header-section-number">3.1.1</span> Some Implications of the Exogeneity Assumption</h3>
<div id="thm-a" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1 </strong></span>If <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>, then the unconditional mean of the error term is zero, i.e. <span class="math display">\[\begin{eqnarray*}
  E(\varepsilon_i)&amp;=&amp;0,\quad i=1,\dots,n
\end{eqnarray*}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<!-- Proof: Hayashi p.9 -->
<p>Using the <em>Law of Total Expectations</em> (i.e., <span class="math inline">\(E[E(Z|X)]=E(Z)\)</span>) we can rewrite <span class="math inline">\(E(\varepsilon_i)\)</span> as <span class="math display">\[
E(\varepsilon_i)=E[E(\varepsilon_i|X_i)],\;\text{for all}\;i=1,\dots,n.
\]</span> But the exogeneity assumption yields <span class="math display">\[
E[E(\varepsilon_i|X_i)]=E[0]=0\;\text{for all}\;i=1,\dots,n,
\]</span> which completes the proof. <span class="math inline">\(\square\)</span></p>
</div>
<p>Generally, two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be <strong>orthogonal</strong> if their cross moment is zero, i.e.&nbsp;if <span class="math inline">\(E(XY)=0\)</span>. Exogeneity is sometimes also called “orthogonality,” due to the following result.</p>
<div id="thm-b" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.2 </strong></span>Under exogeneity, i.e.&nbsp;if <span class="math inline">\(E(\varepsilon_i|X_{i})=0\)</span>, the regressors and the error term are orthogonal to each other, i.e, <span class="math display">\[\begin{eqnarray*}
E(X_{ik}\varepsilon_i)&amp;=&amp;0\quad\text{for all}\quad i=1,\dots,n\quad\text{and}\quad k=1,\dots,K.
\end{eqnarray*}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<!-- Proof: % Hayashi p.9 -->
<p><span class="math display">\[\begin{eqnarray*}
  E(X_{ik}\varepsilon_i)&amp;=&amp;E(E(X_{ik}\varepsilon_i|X_{ik}))\quad{\small\text{(By the Law of Total Expectations)}}\\
  &amp;=&amp;E(X_{ik}E(\varepsilon_i|X_{ik}))\quad{\small\text{(By the linearity of cond.~expectations)}}
\end{eqnarray*}\]</span> Now, to show that <span class="math inline">\(E(X_{ik}\varepsilon_i)=0\)</span>, we need to show that <span class="math inline">\(E(\varepsilon_i|X_{ik})=0\)</span>, which is done in the following:</p>
<p>Since <span class="math inline">\(X_{ik}\)</span> is an element of <span class="math inline">\(X_i\)</span>, a slightly more sophisticated use of the <em>Law of Total Expectations</em> (i.e., <span class="math inline">\(E(Y|X)=E(E(Y|X,Z)|X)\)</span>) implies that <span class="math display">\[E(\varepsilon_i|X_{ik})=E(E(\varepsilon_i|X_i)|X_{ik}).\]</span> So, the exogeneity assumption, <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> yields <span class="math display">\[E(\varepsilon_i|X_{ik})=E(\underbrace{E(\varepsilon_i|X_i)}_{=0}|X_{ik})=E(0|X_{ik})=0.\]</span> I.e., we have that <span class="math inline">\(E(\varepsilon_i|X_{ik})=0\)</span> which allows us to conclude that <span class="math display">\[E(X_{ik}\varepsilon_i)=E(X_{ik}E(\varepsilon_i|X_{ik}))=E(X_{ik}0)=0.\quad\square\]</span></p>
</div>
<p>Because the mean of the error term is zero (<span class="math inline">\(E(\varepsilon_i)=0\)</span> for all <span class="math inline">\(i\)</span> (see <a href="#thm-a">Theorem&nbsp;<span>3.1</span></a>), it follows that the orthogonality property, <span class="math inline">\(E(X_{ik}\varepsilon_i)=0,\)</span> is equivalent to a zero correlation property.</p>
<div id="thm-c" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.3 </strong></span>If <span class="math inline">\(E(\varepsilon_i|X_{i})=0\)</span>, then <span class="math display">\[\begin{eqnarray*}
  Cov(\varepsilon_i,X_{ik})&amp;=&amp;0\quad\text{for all}\quad i=1,\dots,n\quad\text{and}\quad k=1,\dots,K.
\end{eqnarray*}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<!-- Proof: % Hayashi p.9 -->
<p><span class="math display">\[\begin{eqnarray*}
  Cov(\varepsilon_i,X_{ik})&amp;=&amp;E(X_{ik}\varepsilon_i)-E(X_{ik})\,E(\varepsilon_i)\quad{\small\text{(Def. of Cov)}}\\
  &amp;=&amp;E(X_{ik}\varepsilon_i)\quad{\small\text{(By point (a): $E(\varepsilon_i)=0$)}}\\
  &amp;=&amp;0\quad{\small\text{(By orthogonality result in point (b))}}\quad\square
\end{eqnarray*}\]</span></p>
</div>
<!-- ## The Algebra of Least Squares -->
<!-- In this section, we take the point of view where $(Y^{obs},X^{obs})$ is an observed data-set, i.e., a given realization of the random sample $((Y_1,X_1),\dots,(Y_n,X_n))$. That is, we are not yet interested in the statistical properties of estimators, but are only in doing some linear algebra.  -->
</section>
</section>
<section id="deriving-the-expression-of-the-ols-estimator" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="deriving-the-expression-of-the-ols-estimator"><span class="header-section-number">3.2</span> Deriving the Expression of the OLS Estimator</h2>
<p>We derive the expression for the OLS estimator <span class="math inline">\(\hat\beta=(\hat\beta_1,\dots,\hat\beta_K)'\in\mathbb{R}^K\)</span> as the vector-valued minimizing argument of the sum of squared residuals, <span class="math inline">\(S_n(b)\)</span> with <span class="math inline">\(b\in\mathbb{R}^K\)</span>, for a given sample <span class="math inline">\(((Y_1,X_1),\dots,(Y_n,X_n))\)</span>. In matrix terms this is <span class="math display">\[\begin{align*}
S_n(b)&amp;=(Y-X b)^{\prime}(Y-X b)=Y^{\prime}Y-2 Y^{\prime} X b+b^{\prime} X^{\prime} X b.
\end{align*}\]</span> To find the minimizing argument <span class="math display">\[\hat\beta=\arg\min_{b\in\mathbb{R}^K}S_n(b)\]</span> we compute all partial derivatives <span class="math display">\[
\begin{aligned}
\underset{(K\times 1)}{\frac{\partial S(b)}{\partial b}} &amp;=-2\left(X^{\prime}Y -X^{\prime} Xb\right).
\end{aligned}
\]</span> and set them equal to zero which leads to <span class="math inline">\(K\)</span> linear equations (the “normal equations”) in <span class="math inline">\(K\)</span> unknowns. This system of equations defines the OLS estimates, <span class="math inline">\(\hat{\beta}\)</span>, for a given data-set: <span class="math display">\[
\begin{aligned}
-2\left(X^{\prime}Y -X^{\prime} X\hat{\beta}\right)=\underset{(K\times 1)}{0}.
\end{aligned}
\]</span> From our rank assumption (Assumption 3) it follows that <span class="math inline">\(X^{\prime}X\)</span> is an invertible matrix which allows us to solve the equation system by <span class="math display">\[
\begin{aligned}
\underset{(K\times 1)}{\hat{\beta}} &amp;=\left(X^{\prime} X\right)^{-1} X^{\prime} Y
\end{aligned}
\]</span></p>
<p>The following codes computes the estimate <span class="math inline">\(\hat{\beta}\)</span> for a given realization <span class="math inline">\((Y,X)\)</span> of the random sample <span class="math inline">\((Y,X)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Some given data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>X_1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1.9</span>,<span class="fl">0.8</span>,<span class="fl">1.1</span>,<span class="fl">0.1</span>,<span class="sc">-</span><span class="fl">0.1</span>,<span class="fl">4.4</span>,<span class="fl">4.6</span>,<span class="fl">1.6</span>,<span class="fl">5.5</span>,<span class="fl">3.4</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>X_2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">66</span>, <span class="dv">62</span>, <span class="dv">64</span>, <span class="dv">61</span>, <span class="dv">63</span>, <span class="dv">70</span>, <span class="dv">68</span>, <span class="dv">62</span>, <span class="dv">68</span>, <span class="dv">66</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>Y   <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.7</span>,<span class="sc">-</span><span class="fl">1.0</span>,<span class="sc">-</span><span class="fl">0.2</span>,<span class="sc">-</span><span class="fl">1.2</span>,<span class="sc">-</span><span class="fl">0.1</span>,<span class="fl">3.4</span>,<span class="fl">0.0</span>,<span class="fl">0.8</span>,<span class="fl">3.7</span>,<span class="fl">2.0</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>dataset <span class="ot">&lt;-</span>  <span class="fu">cbind.data.frame</span>(X_1,X_2,Y)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Compute the OLS estimation</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>my.lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X_1 <span class="sc">+</span> X_2, <span class="at">data =</span> dataset)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot sample regression surface</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"scatterplot3d"</span>) <span class="co"># library for 3d plots</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plot3d <span class="ot">&lt;-</span> <span class="fu">scatterplot3d</span>(<span class="at">x =</span> X_1, <span class="at">y =</span> X_2, <span class="at">z =</span> Y,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>          <span class="at">angle=</span><span class="dv">33</span>, <span class="at">scale.y=</span><span class="fl">0.8</span>, <span class="at">pch=</span><span class="dv">16</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>          <span class="at">color =</span><span class="st">"red"</span>, <span class="at">main =</span><span class="st">"OLS Regression Surface"</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plot3d<span class="sc">$</span><span class="fu">plane3d</span>(my.lm, <span class="at">lty.box =</span> <span class="st">"solid"</span>, <span class="at">col=</span><span class="fu">gray</span>(.<span class="dv">5</span>), </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>          <span class="at">draw_polygon=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04-MultipleReg_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="some-quantities-of-interest" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="some-quantities-of-interest"><span class="header-section-number">3.3</span> Some Quantities of Interest</h2>
<p><strong>Predicted values and residuals.</strong></p>
<ul>
<li>The (OLS) <strong>predicted values</strong>: <span class="math inline">\(\hat{Y}_i=X_i'\hat\beta\)</span>.<br>
In matrix notation: <span class="math inline">\(\hat Y=X\underbrace{(X'X)^{-1}X'Y}_{\hat\beta}=P_X Y\)</span></li>
<li>The (OLS) <strong>residuals</strong>: <span class="math inline">\(\hat\varepsilon_i=Y_i-\hat{Y}_i\)</span>. In matrix notation: <span class="math inline">\(\hat\varepsilon=Y-\hat{Y}=\left(I_n-X(X'X)^{-1}X'\right)Y=M_X Y\)</span></li>
</ul>
<p><strong>Projection matrices.</strong></p>
<p>The matrix <span class="math display">\[
P_X=X(X'X)^{-1}X'
\]</span> is the <span class="math inline">\((n\times n)\)</span> <strong>projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the column space spanned by the column vectors of <span class="math inline">\(X\)</span> and <span class="math display">\[
M_X=I_n-X(X'X)^{-1}X'=I_n-P_X
\]</span> is the associated <span class="math inline">\((n\times n)\)</span> <strong>orthogonal projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the vector space that is orthogonal to that spanned by <span class="math inline">\(X\)</span>.</p>
<p>The projection matrices <span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> have some nice properties:</p>
<ol type="a">
<li><span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> are <strong>symmetric</strong>, i.e.&nbsp;<span class="math inline">\(P_X=P_X'\)</span> and <span class="math inline">\(M_X=M_X'\)</span>.</li>
<li><span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> are <strong>idempotent</strong>, i.e.&nbsp;<span class="math inline">\(P_XP_X=P_X\)</span> and <span class="math inline">\(M_X M_X=M_X\)</span>.</li>
<li>Moreover, we have that <span class="math inline">\(X'P_X=X'\)</span>, <span class="math inline">\(P_XX=X\)</span>, <span class="math inline">\(X'M_X=0\)</span>, <span class="math inline">\(M_XX=0\)</span>, and <span class="math inline">\(P_XM_X=0\)</span>.</li>
</ol>
<p>All of these properties follow directly from the definitions of <span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> (check it out). Using these properties one can show that the residual vector <span class="math inline">\(\hat\varepsilon=(\hat\varepsilon_1,\dots,\hat\varepsilon_n)'\)</span> is orthogonal to each of the column vectors in <span class="math inline">\(X\)</span>, i.e <span class="math display">\[\begin{eqnarray}
X'\hat\varepsilon&amp;=&amp;X'M_XY\quad\text{\small(By Def.~of $M_X$)}\\
\Leftrightarrow X'\hat\varepsilon&amp;=&amp;\underset{(K\times n)}{0}\underset{(n\times 1)}{Y}\quad\text{\small(since $X'M_X=0$)}\\
\Leftrightarrow X'\hat\varepsilon&amp;=&amp;\underset{(K\times 1)}{0}
\end{eqnarray}\]</span> <!-- \begin{align} --> <!-- X'\hat\varepsilon=\underset{(K\times 1)}{0}.\label{eq:orthXe} --> <!-- \end{align} --> Note that, in the case with intercept, the result <span class="math inline">\(X'\hat\varepsilon=0\)</span> implies that <span class="math inline">\(\sum_{i=1}^n\hat\varepsilon_i=0\)</span>. Moreover, the equation <span class="math inline">\(X'\hat\varepsilon=0\)</span> implies also that the residual vector <span class="math inline">\(\hat{\varepsilon}\)</span> is orthogonal to the predicted values vector, since <span class="math display">\[\begin{align*}
X'\hat\varepsilon&amp;=0\\
\Rightarrow\;\hat\beta'X'\hat\varepsilon&amp;=\hat\beta'0\\
\Leftrightarrow\;\hat Y'\hat\varepsilon&amp;=0.
\end{align*}\]</span></p>
<p>Another insight from equation <span class="math inline">\(X'\hat\varepsilon=0\)</span> is that the vector <span class="math inline">\(\hat\varepsilon\)</span> has to satisfy <span class="math inline">\(K\)</span> linear restrictions which means it looses <span class="math inline">\(K\)</span> degrees of freedom.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Consequently, the vector of residuals <span class="math inline">\(\hat\varepsilon\)</span> has only <span class="math inline">\(n-K\)</span> so-called <em>degrees of freedom</em>. This loss of <span class="math inline">\(K\)</span> degrees of freedom also appears in the definition of the <em>unbiased</em> variance estimator <span class="math display">\[\begin{align}
  s_{UB}^2&amp;=\frac{1}{n-K}\sum_{i=1}^n\hat\varepsilon_i^2\label{EqVarEstim}.
\end{align}\]</span></p>
<p><strong>Variance decomposition:</strong> A further useful result that can be shown using the properties of <span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> is that <span class="math inline">\(Y'Y=\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon\)</span>, i.e. <span class="math display">\[\begin{eqnarray*}
Y'Y&amp;=&amp;(\hat Y+\hat\varepsilon)'(\hat Y+\hat\varepsilon)\notag\\
  &amp;=&amp;(P_XY+M_XY)'(P_XY+M_XY)\notag\\
  &amp;=&amp;(Y'P_X'+Y'M_X')(P_XY+M_XY)\notag\\
  &amp;=&amp;Y'P_X'P_XY+Y'M_X'M_XY+0\notag\\
  &amp;=&amp;\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon
\end{eqnarray*}\]</span> The decomposition <span class="math display">\[
\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon
\]</span> is the basis for the well-known variance decomposition result for OLS regressions.</p>
<div id="thm-vardecomp" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.4 </strong></span>For the linear OLS regression model <a href="#eq-LinMod">Equation&nbsp;<span>3.1</span></a> with intercept, the total sample variance of the dependent variable <span class="math inline">\(Y_1,\dots,Y_n\)</span> can be decomposed as following: <span class="math display">\[\begin{eqnarray}
\underset{\text{total sample variance}}{\frac{1}{n}\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}&amp;=&amp;\underset{\text{explained sample variance}}{\frac{1}{n}\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}+\underset{\text{unexplained sample variance}}{\frac{1}{n}\sum_{i=1}^n\hat\varepsilon_i^2,}\label{VarDecomp}
\end{eqnarray}\]</span> where <span class="math inline">\(\bar{Y}=\frac{1}{n}\sum_{i=1}^nY_i\)</span> and <span class="math inline">\(\bar{\hat{Y}}=\frac{1}{n}\sum_{i=1}^n\hat{Y}_i\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From equation <span class="math inline">\(X'\hat\varepsilon=0\)</span> we have for regressions with intercept that <span class="math inline">\(\sum_{i=1}^n\hat\varepsilon_i=0\)</span>. Hence, from <span class="math inline">\(Y_i=\hat{Y}_i+\hat\varepsilon_i\)</span> it follows that <span class="math display">\[\begin{eqnarray*}
  \frac{1}{n}\sum_{i=1}^n Y_i&amp;=&amp;\frac{1}{n}\sum_{i=1}^n \hat{Y}_i+\frac{1}{n}\sum_{i=1}^n \hat\varepsilon_i\\
  \bar{Y}&amp;=&amp;\bar{\hat{Y}}+0
\end{eqnarray*}\]</span></p>
<p>Using the decomposition <span class="math inline">\(Y'Y=\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon\)</span>, we can now derive the result: <span class="math display">\[\begin{eqnarray*}
   Y'Y&amp;=&amp;\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon\\
   Y'Y-n\bar{Y}^2&amp;=&amp;\hat{Y}'\hat{Y}-n\bar{Y}^2+\hat\varepsilon'\hat\varepsilon\\
   Y'Y-n\bar{Y}^2&amp;=&amp;\hat{Y}'\hat{Y}-n\bar{\hat{Y}}^2+\hat\varepsilon'\hat\varepsilon\quad\text{(by $\bar{Y}=\bar{\hat{Y}}$)}\\
   \sum_{i=1}^nY_i^2-n\bar{Y}^2&amp;=&amp;\sum_{i=1}^n\hat{Y}_i^2-n\bar{\hat{Y}}^2+\sum_{i=1}^n\hat\varepsilon_i^2\\
   \sum_{i=1}^n(Y_i-\bar{Y})^2&amp;=&amp;\sum_{i=1}^n(\hat{Y}_i-\bar{\hat{Y}})^2+\sum_{i=1}^n\hat\varepsilon_i^2\quad\square\\
\end{eqnarray*}\]</span></p>
</div>
<section id="coefficients-of-determination-r2-and-overliner2" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="coefficients-of-determination-r2-and-overliner2">Coefficients of determination: <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\overline{R}^2\)</span></h4>
<p>The larger the proportion of the explained variance, the better is the fit of the model. This motivates the definition of the so-called <span class="math inline">\(R^2\)</span> coefficient of determination: <span class="math display">\[\begin{eqnarray*}
R^2=\frac{\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\;=\;1-\frac{\sum_{i=1}^n\hat{\varepsilon}_i^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}
\end{eqnarray*}\]</span> Obviously, we have that <span class="math inline">\(0\leq R^2\leq 1\)</span>. The closer <span class="math inline">\(R^2\)</span> lies to <span class="math inline">\(1\)</span>, the better is the fit of the model to the observed data. However, a high/low <span class="math inline">\(R^2\)</span> does not mean a validation/falsification of the estimated model. Any relation (i.e., model assumption) needs a plausible explanation from relevant economic theory. The most often criticized disadvantage of the <span class="math inline">\(R^2\)</span> is that additional regressors (relevant or not) will always increase the <span class="math inline">\(R^2\)</span>. Here is an example of the problem.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">100</span>               <span class="co"># Sample size</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)   <span class="co"># Relevant X variable</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>X_ir  <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">5</span>, <span class="dv">20</span>)   <span class="co"># Irrelevant X variable</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>error <span class="ot">&lt;-</span> <span class="fu">rt</span>(n, <span class="at">df =</span> <span class="dv">10</span>)<span class="sc">*</span><span class="dv">10</span>  <span class="co"># True error</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>Y     <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">5</span> <span class="sc">*</span> X <span class="sc">+</span> error    <span class="co"># Y variable</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>lm1   <span class="ot">&lt;-</span> <span class="fu">summary</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))     <span class="co"># Correct OLS regression </span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>lm2   <span class="ot">&lt;-</span> <span class="fu">summary</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X<span class="sc">+</span>X_ir))<span class="co"># OLS regression with X_ir </span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>lm1<span class="sc">$</span>r.squared <span class="sc">&lt;</span> lm2<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
<p>So, <span class="math inline">\(R^2\)</span> increases here even though <code>X_ir</code> is a completely irrelevant explanatory variable. Because of this, the <span class="math inline">\(R^2\)</span> cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called <strong>adjusted</strong> <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\overline{R}^2,\)</span> defined as <span class="math display">\[\begin{eqnarray*}
  \overline{R}^2&amp;=&amp;1-\frac{\frac{1}{n-K}\sum_{i=1}^n\hat{\varepsilon}^2_i}{\frac{1}{n-1}\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\leq R^2%\\
  %=\dots=
  %&amp;=&amp;1-\frac{n-1}{n-K}\left(1-R^2\right)\quad{\small\text{(since $1-R^2=(\sum_i\hat\varepsilon_i^2)/(\sum_i(Y_i-\bar{Y}))$)}}\\
  %&amp;=&amp;1-\frac{n-1}{n-K}+\frac{n-1}{n-K}R^2\quad+\frac{K-1}{n-K}R^2-\frac{K-1}{n-K}R^2\\
  %&amp;=&amp;1-\frac{n-1}{n-K}+R^2\quad+\frac{K-1}{n-K}R^2\\
  %&amp;=&amp;-\frac{K-1}{n-K}+R^2\quad+\frac{K-1}{n-K}R^2\\
  %&amp;=&amp;R^2-\underbrace{\frac{K-1}{n-K}\left(1-R^2\right)}_{\geq 0\;\text{and}\;\leq(K-1)/(n-K)}\;\leq\;R^2
\end{eqnarray*}\]</span> The adjustment is in terms of the degrees of freedom <span class="math inline">\(n-K\)</span>.</p>
</section>
</section>
<section id="method-of-moments-estimator" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="method-of-moments-estimator"><span class="header-section-number">3.4</span> Method of Moments Estimator</h2>
<p>The methods of moments estimator exploits the exogeneity assumption that <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> (Assumption 2). Remember that <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> implies that <span class="math inline">\(E(X_{ik}\varepsilon_i)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and all <span class="math inline">\(k=1,\dots,K\)</span>. The fundamental idea behind “method of moments estimation” is to use the sample analogues of the population moment restrictions <span class="math inline">\(E(X_{ik}\varepsilon_i)=0\)</span>, <span class="math inline">\(k=1,\dots,K,\)</span> for deriving the estimator: <span class="math display">\[
\begin{array}{c||c}
\text{$K$ population moment restrictions\quad}&amp;\text{$K$ sample moment restrictions}\\[2ex]
\left.\begin{array}{c}
E(\varepsilon_i)=0\\
E(X_{i2}\varepsilon_i)=0\\
\vdots\\
E(X_{iK}\varepsilon_i)=0
\end{array}
\right\}\Leftrightarrow E(X_i\varepsilon_i)=\underset{(K\times 1)}{0} &amp;
\left.\begin{array}{c}
\displaystyle
\frac{1}{n}\sum_{i=1}^n\hat\varepsilon_i=0\\
\displaystyle
\frac{1}{n}\sum_{i=1}^nX_{i2}\hat\varepsilon_i=0\\
\vdots\\
\displaystyle
\frac{1}{n}\sum_{i=1}^nX_{iK}\hat\varepsilon_i=0\\
\end{array}
\right\}\Leftrightarrow \displaystyle\frac{1}{n}\sum_{i=1}^nX_i\hat\varepsilon_i=\underset{(K\times 1)}{0}
\end{array}
\]</span></p>
<p>Under our set of assumptions (Assumptions 1-4), the sample means <span class="math inline">\(n^{-1}\sum_{i=1}^nX_i\hat\varepsilon_i\)</span> are consistent estimators of the population means <span class="math inline">\(E(X_i\varepsilon_i)\)</span>. The idea is now to find <span class="math inline">\(\hat\beta_0,\dots,\hat\beta_K\)</span> values which lead to residuals <span class="math inline">\(\hat\varepsilon_i=Y_i-\sum_{k=1}^K\hat\beta_kX_{ik}\)</span> that fulfill the above sample moment restrictions. This should in principle be possible since we have a linear system of <span class="math inline">\(K\)</span> equations <span class="math inline">\(\frac{1}{n}\sum_{i=1}^nX_i\hat\varepsilon=0\)</span> and <span class="math inline">\(K\)</span> unknowns <span class="math inline">\(\hat\beta=(\hat\beta_0,\dots,\hat\beta_K)'\)</span>. Solving the equation system yields</p>
<p><span class="math display">\[\begin{align*}
\frac{1}{n}\sum_{i=1}^n X_i\hat\varepsilon_i&amp;=\underset{(K\times 1)}{0}\\
\frac{1}{n}\sum_{i=1}^n X_i\left(Y_i-X_i'\hat\beta\right)&amp;=\underset{(K\times 1)}{0}\\
\frac{1}{n}\sum_{i=1}^n X_i Y_i-\frac{1}{n}\sum_{i=1}^n X_iX_i'\hat\beta &amp;=\underset{(K\times 1)}{0}\\
\frac{1}{n}\sum_{i=1}^n X_iX_i'\hat\beta &amp;=\frac{1}{n}\sum_{i=1}^n X_i Y_i\\
\hat\beta &amp;= \left(\frac{1}{n}\sum_{i=1}^n X_iX_i'\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i Y_i\\
\hat\beta &amp;= \left(X'X\right)^{-1} X' Y,\\
\end{align*}\]</span> which equals the OLS estimator of <span class="math inline">\(\beta\)</span>; although, we used now a different approach to derive the estimator.</p>
<p>Once again we see the importance of the exogeneity assumption <span class="math inline">\(E(\varepsilon_i|X_i)\)</span> which we used here as the starting point for the derivation of the methods of moments estimator. However, unlike with deriving the OLS estimator as the estimator that minimizes the sum of squared residuals, here we derived the estimator from the exogeineity assumptions. The method of moments is a very general method, which usually has good properties. We will return to the method of moments several times throughout the semester.</p>
</section>
<section id="unbiasedness-of-hatbetax-and-hatbeta" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="unbiasedness-of-hatbetax-and-hatbeta"><span class="header-section-number">3.5</span> Unbiasedness of <span class="math inline">\(\hat\beta|X\)</span> and <span class="math inline">\(\hat\beta\)</span></h2>
<p>Once again, but now using matrix algebra, we can show that the OLS (or likewise the Methods-of-Moments estimator) <span class="math inline">\(\hat\beta = \left(X'X\right)^{-1}X'Y\)</span> is unbiased conditionally on <span class="math inline">\(X\)</span>: <span class="math display">\[\begin{align*}
E[\hat{\beta}|X]
&amp;=E\left[\left(X^{\prime} X\right)^{-1} X^{\prime} Y|X\right]\\
&amp;=E\left[\left(X^{\prime} X\right)^{-1} X^{\prime}(X \beta+\varepsilon)|X\right] \\
&amp;=E\left[\left(X^{\prime} X\right)^{-1} X^{\prime} X \beta+\left(X^{\prime} X\right)^{-1} X^{\prime} \varepsilon|X\right] \\
&amp;=\beta+E\left[\left(X^{\prime} X\right)^{-1} X^{\prime} \varepsilon|X\right] \\
&amp;=\beta+\left(X^{\prime} X\right)^{-1} X^{\prime}\underbrace{E[\varepsilon|X]}_{=0}=\beta\\
\Leftrightarrow\;\underbrace{E[\hat{\beta}|X] - \beta}_{=\operatorname{Bias}[\hat{\beta}|X]}&amp;=0\\
%\Leftrightarrow\;\operatorname{Bias}[\hat{\beta}|X]&amp;=0\\
\end{align*}\]</span></p>
<p>We can apply the total (or iterated) law of expectations to show that <span class="math inline">\(\hat\beta = \left(X'X\right)^{-1}X'Y\)</span> is also unbiased unconditionally on <span class="math inline">\(X\)</span>: <span class="math display">\[\begin{align*}
E[E[\hat{\beta}|X]] - E[\beta]&amp;=E(0)\\
\Leftrightarrow\;\underbrace{E[\hat{\beta}] - \beta}_{=\operatorname{Bias}[\hat{\beta}]}&amp;=0\\
%\Leftrightarrow\;\underbrace{E\big(\operatorname{Bias}[\hat{\beta}|X]\big)}_{=\operatorname{Bias}[\hat{\beta}]}&amp;=E(0)\\
%\Leftrightarrow\;E\big(\operatorname{Bias}[\hat{\beta}]\big)&amp;=0\\
\end{align*}\]</span></p>
<p><strong>Note:</strong> This result only requires the strict exogeneity assumption <span class="math inline">\(E(\varepsilon|X)=0\)</span> which follows from our Assumption 2 (i.e.&nbsp;<span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>) together with Assumption 1 (b) (i.e.&nbsp;<span class="math inline">\((Y_i,X_i)\)</span> is i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span>). In particular, we did not need to assume homoscedasticity (and it also holds for auto-correlated error terms).</p>
</section>
<section id="ch:VarEstBeta" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="ch:VarEstBeta"><span class="header-section-number">3.6</span> Variance and Standard Error of <span class="math inline">\(\hat\beta|X\)</span></h2>
<p>The conditional variance of <span class="math inline">\(\hat\beta\)</span> given <span class="math inline">\(X\)</span> is given by <span class="math display">\[\begin{align*}
Var(\hat{\beta}|X)=&amp; E\left[(\hat{\beta}-\beta)(\hat{\beta}-\beta)^{\prime}|X\right] \\
=&amp; E\left[\left(\left(X^{\prime} X\right)^{-1} X^{\prime} \varepsilon\right)\left(\left(X^{\prime} X\right)^{-1} X^{\prime} \varepsilon\right)^{\prime}|X\right] \\
=&amp; E\left[\left(X^{\prime} X\right)^{-1} X^{\prime} \varepsilon\varepsilon^{\prime} X\left(X^{\prime} X\right)^{-1}|X\right] \\
=&amp; \left(X^{\prime} X\right)^{-1} X^{\prime} E\left[\varepsilon\varepsilon^{\prime}|X\right] X\left(X^{\prime} X\right)^{-1}\\
=&amp; \underbrace{\left(X^{\prime} X\right)^{-1} X^{\prime} \overbrace{Var(\varepsilon|X)}^{(n\times n)} X\left(X^{\prime} X\right)^{-1}}_{(K\times K)\text{-dimesnional}},\\
\end{align*}\]</span> In the above derivations we used, first, that <span class="math display">\[\begin{align*}
\hat\beta-\beta
&amp;=\left(X^{\prime} X\right)^{-1} X^{\prime} Y-\beta\\
&amp;=\left(X^{\prime} X\right)^{-1} X^{\prime}(X\beta+\varepsilon)-\beta\\
&amp;=\beta+\left(X^{\prime} X\right)^{-1} X^{\prime}\varepsilon)-\beta\\
&amp;=\left(X^{\prime} X\right)^{-1} X^{\prime}\varepsilon
\end{align*}\]</span> and, second, that <span class="math inline">\(E\left[\varepsilon\varepsilon^{\prime}|X\right]=Var(\varepsilon|X)\)</span> since <span class="math inline">\(E(\varepsilon|X)=0\)</span> under our assumptions.</p>
<p>The above variance expression is the general version of <span class="math inline">\(Var(\hat{\beta}|X)\)</span> which can be further simplified using specific assumptions on the distribution of the error term <span class="math inline">\(\varepsilon\)</span>:</p>
<ul>
<li>In case of spherical errors (“Gauss-Markov assumptions”), i.e.&nbsp;no heteroscedasticity and non auto-correlations, we have that <span class="math inline">\(Var(\varepsilon|X)=\sigma^2 I_n\)</span> such that <span class="math display">\[
Var(\hat{\beta}|X)=\underset{(K\times K)}{\sigma^{2} \left(X^{\prime} X\right)^{-1}}
\]</span></li>
<li>In case of heteroscedastic error terms, we have that <span class="math display">\[Var(\varepsilon|X)=\left(\begin{matrix}\sigma_1^2&amp;0&amp;\dots&amp;0\\0&amp;\sigma_2^2&amp;\dots&amp;0\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0&amp;0&amp;\dots&amp;\sigma_n^2\end{matrix}\right)=\operatorname{diag}(\sigma_1^2,\dots,\sigma_n^2),\]</span> where the variances <span class="math inline">\(\sigma_i^2\)</span> may be functions of <span class="math inline">\(X_i\)</span>, i.e.&nbsp;<span class="math inline">\(\sigma^2_i\equiv\sigma_i^2(X_i)\)</span>. Under conditional heteroscedasticitiy we have the following sandwich form <span class="math display">\[
Var(\hat{\beta}|X)=\left(X^{\prime} X\right)^{-1} X^{\prime} \operatorname{diag}(\sigma_1^2,\dots,\sigma_n^2) X\left(X^{\prime} X\right)^{-1}.
\]</span></li>
</ul>
<p><strong>Note.</strong> The above variance expressions are rather useless in practice since usually we do not know the values of <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\sigma_i^2\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>. The problem of estimating <span class="math inline">\(Var(\varepsilon|X)\)</span> under the assumption of spherical errors is considered in <a href="05-Small-Sample-Inference.html"><span>Chapter&nbsp;4</span></a>. The problem of estimating <span class="math inline">\(Var(\varepsilon|X)\)</span> under the assumption of heteroscedastic errors is considered in <a href="06-Asymptotics.html"><span>Chapter&nbsp;5</span></a>.</p>
<!-- **Practical estimation of the standard errors.:** The diagonal elements in the $(K\times K)$ matrix $Var(\hat{\beta}|X)$ are the variance expressions for the single estimators $\hat\beta_k$ with $k=1,\dots,K$, -->
<!-- \begin{align*} -->
<!-- Var(\hat{\beta}_k|X)=&\left[\left(X^{\prime} X\right)^{-1} X^{\prime} Var(\varepsilon|X) X\left(X^{\prime} X\right)^{-1}\right]_{kk}\quad\small{\text{(generally)}}\\ -->
<!-- =&\sigma^2\left[\left(X^{\prime} X\right)^{-1}\right]_{kk},\hspace*{4cm}\small{\text{(spherical errors)}} -->
<!-- \end{align*} -->
<!-- where $[A]_{kl}$ denotes the $kl$th element of $A$ that is in the $k$th row and $l$th column of $A$. Taking square roots leads to the **standard errors** of the estimators $\hat\beta_k|X$ -->
<!-- \begin{align*} -->
<!-- \operatorname{SE}(\hat{\beta}_k|X)=&\left(\left[\left(X^{\prime} X\right)^{-1} X^{\prime} Var(\varepsilon|X) X\left(X^{\prime} X\right)^{-1}\right]_{kk}\right)^{1/2}\quad\small{\text{(generally)}}\\ -->
<!-- =&\left(\sigma^2\left[\left(X^{\prime} X\right)^{-1}\right]_{kk}\right)^{1/2},\hspace*{4cm}\small{\text{(spherical errors)}} -->
<!-- \end{align*} -->
<!-- Of course, the above expressions for $Var(\hat{\beta}_k|X)$ and $\operatorname{SE}(\hat{\beta}_k|X)$ are generally useless in practice since we typically do not know the $(n\times n)$ variance matrix $Var(\varepsilon|X)$ or $\sigma^2$, but need to estimate them from the data. So, we typically need to work with -->
<!-- \begin{align*} -->
<!-- \widehat{Var}(\hat{\beta}_k|X)=&\left[\left(X^{\prime} X\right)^{-1} X^{\prime} \widehat{Var}(\varepsilon|X) X\left(X^{\prime} X\right)^{-1}\right]_{kk}\hspace*{1.5cm}\small{\text{(generally)}}\\ -->
<!-- =&\hat\sigma^2\left[\left(X^{\prime} X\right)^{-1}\right]_{kk},\hspace*{5cm}\small{\text{(spherical errors)}}\\ -->
<!-- \text{and}\hspace*{3cm}&\\ -->
<!-- \widehat{\operatorname{SE}}(\hat{\beta}_k|X)=&\left(\left[\left(X^{\prime} X\right)^{-1} X^{\prime} \widehat{Var}(\varepsilon|X) X\left(X^{\prime} X\right)^{-1}\right]_{kk}\right)^{1/2}\quad\small{\text{(generally)}}\\ -->
<!-- =&\left(\hat\sigma^2\left[\left(X^{\prime}X\right)^{-1}\right]_{kk}\right)^{1/2},\hspace*{4cm}\small{\text{(spherical errors)}} -->
<!-- \end{align*} -->
<!-- For the case of spherical errors, we already know a possible estimator, namely, $\hat\sigma^2=s_{UB}^2=(n-K)^{-1}\sum_{i=1}^n\hat\varepsilon_i^2$. However, finding a reasonable estimator $\widehatVar(\varepsilon|X)=\operatorname{diag}(\hat v_1,\dots,\hat v_n)$ for the general heteroscedastic case is a little more tricky. The econometric literature knows the following heteroscedasticity-consistent (HC) robust approaches: -->
<!-- \begin{align*} -->
<!-- \hspace*{-2cm}\text{HC0:}\quad & \hat v_{i}=\hat{\varepsilon}_{i}^{2} \\ -->
<!-- \text{HC1:}\quad & \hat v_{i}=\frac{n}{n-K} \hat{\varepsilon}_{i}^{2} \\ -->
<!-- \text{HC2:}\quad & \hat v_{i}=\frac{\hat{\varepsilon}_{i}^{2}}{1-h_{i}} \\ -->
<!-- \text{HC3:}\quad & \hat v_{i}=\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{2}}\text{\quad($\leftarrow$ Most often used)} \\ -->
<!-- \text{HC4:}\quad & \hat v_{i}=\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{\delta_{i}}} -->
<!-- \end{align*} -->
<!-- where $h_i=X_i'(X'X)^{-1}X_i=[P_X]_{ii}$ is the $i$th diagonal element of the projection matrix $P_X$, $\bar{h}=n^{-1}\sum_{i=1}^nh_i$, and $\delta_i=\min\{4,h_i/\bar{h}\}$.  -->
<!-- **Side Note.:** The statistic $1/n\leq h_i\leq 1$ is called the "leverage" of $X_i$, where (by construction) average leverage is $n^{-1}\sum_{i=1}^nh_i=K/n$. Observations $X_i$ with leverage statistics $h_i$ that greatly exceed the average leverage value $K/n$ are referred to as "high-leverage" observations.  High-leverage observations have potentially a large influence on the estimation result. Typically, high-leverage observations $X_i$ distort the estimation results, $\hat\beta$, if the absolute value of the corresponding residual $|\hat{\varepsilon}_i|$ is unusually large ("outlier").  -->
<!-- \bigskip -->
<!-- The estimator HC0 was suggested in the econometrics literature by \cite{White1980} and is justified by asymptotic arguments. The estimators HC1, HC2 and HC3 were suggested by \cite{MacKinnon_White_1985} to improve -->
<!-- the performance in small samples. A more extensive study of small sample behavior was carried out by \cite{Long_Ervin_2000} which arrive at the conclusion that HC3 provides the best performance in small samples as it inflates the $\hat\varepsilon_i$ values which is thought to adjust for the "over-influence" of observations with large leverage values $h_i$. \cite{Cribari_2004} suggested the estimator HC4 to further improve small sample performance, especially in the presence of influential observations (large $h_i$ values). -->
<!-- The following `R` code shows how to compute HC robust variance/standard error estimators: -->
<!-- ```{r, fig.align="center"} -->
<!-- set.seed(2) -->
<!-- n      <- 100 -->
<!-- K      <- 3 -->
<!-- X      <- matrix(runif(n*(K-1), 2, 10), n, K-1) -->
<!-- X      <- cbind(1,X) -->
<!-- beta   <- c(1,5,5) -->
<!-- # heteroscedastic errors: -->
<!-- sigma  <- abs(X[,2] + X[,3])^1.5 -->
<!-- error  <- rnorm(n, mean = 0, sd=sigma) -->
<!-- Y      <- beta[1]*X[,1] + beta[2]*X[,2] + beta[3]*X[,3] + error -->
<!-- ## -->
<!-- lm_fit <- lm(Y~X -1 ) -->
<!-- ## Caution! By default R computes the standard errors  -->
<!-- ## assuming homoscedastic errors. This can lead to  -->
<!-- ## false inferences under heteroscedastic errors. -->
<!-- summary(lm_fit)$coefficients  -->
<!-- library("sandwich") # HC robust variance estimation  -->
<!-- library("lmtest") -->
<!-- ## Robust estimation of the variance of \hat{\beta}: -->
<!-- Var_beta_hat_robust <- sandwich::vcovHC(lm_fit, type="HC3") -->
<!-- Var_beta_hat_robust  -->
<!-- ## Corresponding regression-output: -->
<!-- lmtest::coeftest(lm_fit, vcov = Var_beta_hat_robust) -->
<!-- ``` -->
<!-- Observe that the HC robust variance estimation leads to larger variances than the classic variance estimation for homoscedastic errors. This is typically, but not always, the case.  -->
</section>
<section id="the-gauss-markov-theorem" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="the-gauss-markov-theorem"><span class="header-section-number">3.7</span> The Gauss-Markov Theorem</h2>
<div id="thm-GMT" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.5 (The Gauss-Markov Theorem) </strong></span>Let’s assume Assumptions 1-4 hold with spherical errors, i.e., with <span class="math inline">\(E(\varepsilon\varepsilon'|X)=\sigma^{2} I_{n}\)</span>. Then the OLS estimator <span class="math inline">\(\hat\beta=(X'X)^{-1}X'Y\)</span> has the smallest variance (in a matrix sense) among all linear and unbiased estimators of <span class="math inline">\(\beta\)</span>. That is, for any alternative linear and unbiased estimator <span class="math inline">\(\tilde{\beta}\)</span> we have that <span class="math display">\[\begin{align*}
&amp;Var(\tilde\beta|X)\geq Var(\hat\beta|X)\quad{\small\text{("in the matrix sense")}}\\
\Leftrightarrow&amp;Var(\tilde\beta|X)-Var(\hat\beta|X)=\underset{(K\times K)}{D},
\end{align*}\]</span> where <span class="math inline">\(D\)</span> is a <em>positive semidefinite</em> <span class="math inline">\((K\times K)\)</span> matrix, i.e., <span class="math inline">\(a'Da\geq 0\)</span> for any <span class="math inline">\(K\)</span>-dimensional vector <span class="math inline">\(a\in\mathbb{R}^K\)</span>.</p>
</div>
<p>Observe that <a href="#thm-GMT">Theorem&nbsp;<span>3.5</span></a> implies that <span class="math inline">\(Var(\tilde{\beta}_k|X) \geq Var(\hat\beta_k | X)\)</span> for any <span class="math inline">\(k=1,\dots,K\)</span>.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Since <span class="math inline">\(\tilde{\beta}\)</span> is assumed to be linear in <span class="math inline">\(Y\)</span>, we can write <span class="math display">\[
\tilde{\beta}=CY,
\]</span> where <span class="math inline">\(C\)</span> is some <span class="math inline">\((K\times n)\)</span> matrix, which is a function of <span class="math inline">\(X\)</span> and/or nonrandom components. Adding a <span class="math inline">\((K\times n)\)</span> zero matrix <span class="math inline">\(0\)</span> yields <span class="math display">\[
\tilde{\beta}=\Big(C\overbrace{-\left(X'X\right)^{-1}X'+\left(X'X\right)^{-1}X'}^{=0}\Big)Y.
\]</span> Let now <span class="math inline">\(D=C-\left(X'X\right)^{-1}X'\)</span>, then <span class="math display">\[\begin{align*}
\tilde{\beta}&amp;=\left(D+\left(X'X\right)^{-1}X'\right)Y\nonumber\\
\tilde{\beta}&amp;=DY + \left(X'X\right)^{-1}X'Y\nonumber\\
\tilde{\beta}&amp;=D\left(X{\beta}+{\varepsilon}\right) + \underbrace{\left(X'X\right)^{-1}X'Y}_{=\hat{\beta}}.
\end{align*}\]</span> Such that <span id="eq-c3e16"><span class="math display">\[
\tilde{\beta}=DX{\beta}+D{\varepsilon} + \hat{\beta}.
\tag{3.2}\]</span></span></p>
<p>Taking means yields <span class="math display">\[\begin{align*}
E(\tilde{\beta}|X)&amp;=\underbrace{E(DX{\beta}|X)}_{=DX{\beta}}+\underbrace{E(D{\varepsilon}|X)}_{=0}+\underbrace{E(\hat{\beta}|X)}_{=\beta}.
\end{align*}\]</span> Such that <span id="eq-c3e17"><span class="math display">\[
E(\tilde{\beta}|X)-\beta=DX{\beta}.
\tag{3.3}\]</span></span></p>
<p>Since <span class="math inline">\(\tilde{\beta}\)</span> is (by assumption) unbiased, we have that <span class="math inline">\(E(\tilde{\beta}|X)={\beta}\)</span>. Therefore, <a href="#eq-c3e17">Equation&nbsp;<span>3.3</span></a> implies that <span class="math inline">\(DX=0_{(K\times K)}\)</span>. Plugging <span class="math inline">\(DX=0\)</span> into <a href="#eq-c3e16">Equation&nbsp;<span>3.2</span></a> yields, <span class="math display">\[\begin{align*}
\tilde{\beta}&amp;=D{\varepsilon} + \hat{\beta}\\
\tilde{\beta}-{\beta}&amp;=D{\varepsilon} + ({\color{red}\hat{\beta}-{\beta}})\\
\tilde{\beta}-{\beta}&amp;=D{\varepsilon} + \left(X'X\right)^{-1}X'{\varepsilon}\\
\end{align*}\]</span> such that <span id="eq-c3e18"><span class="math display">\[
\tilde{\beta}-{\beta}=\left(D + \left(X'X\right)^{-1}X'\right){\varepsilon},
\tag{3.4}\]</span></span> where we used that <span class="math display">\[\begin{align*}
{\color{red}\hat\beta-\beta}&amp;=(X'X)^{-1}X'Y-\beta\\
&amp;=(X'X)^{-1}X'(X\beta+\varepsilon)-\beta\\
&amp;=(X'X)^{-1}X'\varepsilon.
\end{align*}\]</span></p>
<p>Now, we can conclude the proof using the following arguments using <a href="#eq-c3e18">Equation&nbsp;<span>3.4</span></a>: <span class="math display">\[\begin{align*}
  Var(\tilde{\beta}|X)
  &amp;= Var(\tilde{\beta}-{\beta}|X)\quad \text{(since $\beta$ is not random)}\\
  &amp;= Var((D + (X'X)^{-1}X'){\varepsilon}|X)\\
  &amp;= (D + (X'X)^{-1}X')Var({\varepsilon}|X)(D' + X(X'X)^{-1})\\
  &amp;= \sigma^2(D + (X'X)^{-1}X')I_n(D' + X(X'X)^{-1})\\
  &amp;= \sigma^2(DD' + DX(X'X)^{-1} +  (X'X)^{-1}X'D' + (X'X)^{-1}X'X(X'X)^{-1})\\
  &amp;= \sigma^2\left(DD'+(X'X)^{-1}\right)\quad \text{(using that $DX=0$)} \\
  &amp;\geq\sigma^2(X'X)^{-1} \quad \text{(Since $DD'$ is pos.~semidef.)}\\
  &amp;= Var(\hat{\beta}|X),
\end{align*}\]</span> where the inequality follows from the fact that <span class="math inline">\(DD'\)</span> is positive semidefinite: <span class="math display">\[\begin{align*}
a'DD'a=(D'a)'(D'a)=\tilde{a}'\tilde{a}\geq 0,
\end{align*}\]</span> where <span class="math inline">\(\tilde{a}\)</span> is a <span class="math inline">\(K\)</span> dimensional column-vector. </p>
</div>
<!-- Remember: -->
<!-- \begin{itemize} -->
<!-- \item $(A+\hat{\beta})'=A'+\hat{\beta}'$ -->
<!-- \item $(AB)'=\hat{\beta}'A'$ -->
<!-- \item $A' = A$ $\Leftrightarrow$ $A$ is a symmetric matrix -->
<!-- \end{itemize} -->
<!-- **Note.:**  To reiterate: The unbiasedness of $\hat{\beta}$ did not depend on any assumptions about the distribution of $\varepsilon$,  except that $E(\varepsilon|X)=0$ which follows from our Assumption 2 together with the i.i.d. assumption in Assumption 1.  Once we imposed additionally the assumption of spherical errors $E(\varepsilon\varepsilon|X)=\sigma^{2}I_n$ we can show that $\hat{\beta}$ has the smallest variance of all linear unbiased estimators.  -->
</section>
<section id="practice-real-data" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="practice-real-data"><span class="header-section-number">3.8</span> Practice: Real Data</h2>
<p>The following practice part is taken from .</p>
<p>The <code>R</code> package <code>AER</code> contains many useful functions and data sets for applied regression analysis. In the following we consider <code>CPS1988</code> data frame collected in the March 1988. Current Population Survey (CPS) by the US Census Bureau. These are cross-section data on males aged between 18 and 70 with annual income greater than 50 US-Dollar in the year 1991.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages("AER")</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(<span class="st">"AER"</span>)) <span class="co"># load the R package</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(CPS1988)                                  <span class="co"># attach the data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The simplest option to get a summary of the data is <code>summary(CPS1988)</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(CPS1988)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      wage            education       experience   ethnicity     smsa      
 Min.   :   50.05   Min.   : 0.00   Min.   :-4.0   cauc:25923   no : 7223  
 1st Qu.:  308.64   1st Qu.:12.00   1st Qu.: 8.0   afam: 2232   yes:20932  
 Median :  522.32   Median :12.00   Median :16.0                           
 Mean   :  603.73   Mean   :13.07   Mean   :18.2                           
 3rd Qu.:  783.48   3rd Qu.:15.00   3rd Qu.:27.0                           
 Max.   :18777.20   Max.   :18.00   Max.   :63.0                           
       region     parttime   
 northeast:6441   no :25631  
 midwest  :6863   yes: 2524  
 south    :8760              
 west     :6091              
                             
                             </code></pre>
</div>
</div>
<p>Here, <code>wage</code> is the wage in dollars per week, <code>education</code> and <code>experience</code> are measured in years, and <code>ethnicity</code> is a factor with levels Caucasian (<code>"cauc"</code>) and African-American (<code>"afam"</code>). There are three further factors, <code>smsa</code>, <code>region</code>, and <code>parttime</code>, indicating residence in a standard metropolitan statistical area (SMSA), the region within the United States of America, and whether the individual works part-time. Experience is not actual experience but a proxi for potential experience computed as <code>age</code> - <code>eduction</code> - <code>6</code>; thus this quantity may be negative which is actually the case for 438 observations in the <code>CPS1988</code> data frame.</p>
<p>Our model of interest is <span class="math display">\[\begin{align*}
\log(\texttt{wage})=&amp;\beta_1 + \beta_2 \texttt{experiences} + \beta_3 \texttt{experiences}^2 \\
&amp;+\beta_4 \texttt{education} +\beta_5 \texttt{ethnicity} + \varepsilon  
\end{align*}\]</span> You can fit this model in <code>R</code> as following:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>cps_lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(wage) <span class="sc">~</span> experience <span class="sc">+</span> <span class="fu">I</span>(experience<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>               education <span class="sc">+</span> ethnicity, <span class="at">data =</span> CPS1988)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The formula in the <code>lm()</code> call takes into account the semilogarithmic form and also specifies the squared regressor <code>experiences^2</code>. It has to be insulated by <code>I()</code> so that the operator <code>^</code> has its original arithmetic meaning (and not its meaning as a formula operator for specifying interactions; see below).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Regression output without (!) robust standard errors (SEs)</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Generally, do not do this, but use robust SEs (see Chapter 5).</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(cps_lm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log(wage) ~ experience + I(experience^2) + education + 
    ethnicity, data = CPS1988)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.9428 -0.3162  0.0580  0.3756  4.3830 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      4.321e+00  1.917e-02  225.38   &lt;2e-16 ***
experience       7.747e-02  8.800e-04   88.03   &lt;2e-16 ***
I(experience^2) -1.316e-03  1.899e-05  -69.31   &lt;2e-16 ***
education        8.567e-02  1.272e-03   67.34   &lt;2e-16 ***
ethnicityafam   -2.434e-01  1.292e-02  -18.84   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5839 on 28150 degrees of freedom
Multiple R-squared:  0.3347,    Adjusted R-squared:  0.3346 
F-statistic:  3541 on 4 and 28150 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<!-- ## But do this:  -->
<!-- ## Heteroscedasticity robust variance estimation:  -->
<!-- library("sandwich")  -->
<!-- library("lmtest") -->
<!-- ## Robust estimation of the variance of \hat{\beta}: -->
<!-- Var_beta_hat_robust <- sandwich::vcovHC(cps_lm, type="HC3") -->

<!-- ## Regression output table with robust SEs: -->
<!-- lmtest::coeftest(cps_lm, vcov = Var_beta_hat_robust) -->
<p>The summary reveals that all coefficients have the expected sign, and the corresponding variables are highly significant (not surprising in such a large sample with <span class="math inline">\(n=28155\)</span> observations). Specifically, according to this specification, the return on education is <span class="math inline">\(8.57\%\)</span> per year.</p>
<p><strong>Interpretation of the Results:</strong> The linear model structure facilitates a very simple interpration. For the unknown parameters <span class="math inline">\(\beta_1, \dots,\beta_K\)</span> we have that <span class="math display">\[\begin{align*}
\dfrac{\partial E[Y_i| X_i]}{\partial X_{ik}} = \beta_k\qquad\text{with}\qquad
E[Y_i | X_i] &amp; = \beta_1 + \beta_2 X_{i2} + \dots + \beta_2 X_{iK}.
\end{align*}\]</span> That is, <span class="math inline">\(\beta_k\)</span> is the true (unknown) <strong>marginal effect</strong> of a one unit change in <span class="math inline">\(X_{ik}\)</span> on <span class="math inline">\(Y_i\)</span>. Therefore, <span class="math inline">\(\hat\beta_k\)</span> is the <strong>estimated marginal effect</strong> of a one unit change in <span class="math inline">\(X_{ik}\)</span> on <span class="math inline">\(Y_i\)</span>: <span class="math display">\[\begin{align*}
\widehat{\dfrac{\partial E[Y_i| X_i]}{\partial X_{ik}}} = \hat\beta_k\qquad\text{with}\qquad
\widehat{E[Y | X]} &amp;= \hat\beta_1 + \hat\beta_2 X_{i2} + \dots + \hat\beta_2 X_{iK}.
\end{align*}\]</span></p>
<p><strong>Caution:</strong> Econometricians usually would argue that you cannot interpret the above empirical result <em>causally</em> since <code>wage</code> is also influenced by many further factors that cannot be measured easily such as, for instance, <code>personal drive to get stuff done</code> and these missing factors potentially correlate with <code>education</code>.</p>
<section id="the-r-function-i" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="the-r-function-i">The <code>R</code> Function <code>I()</code></h3>
<p>Some further details on the specification of regression model formulas in <code>R</code> are in order. We have already seen that the arithmetic operator <code>+</code> has a different meaning in formulas: it is employed to add regressors (main effects). Additionally, the operators <code>:</code>, <code>*</code>, <code>/</code>, <code>^</code> have special meanings, all related to the specification of so-called interaction effects.</p>
<p>To be able to use the arithmetic operators in their original meaning in a model formula in <code>R</code>, they can be protected from the formula interpretation by insulating them inside a function, as in <code>log(x1 * x2)</code>. If the problem at hand does not require a transformation, <code>R</code>’s <code>I()</code> function can be used, which returns its argument “as is”. This was used for computing experience squared in the regression above.</p>
</section>
<section id="dummy-variables-contrast-codings-and-interactions" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="dummy-variables-contrast-codings-and-interactions">Dummy Variables, Contrast Codings, and Interactions</h3>
<p>Note that the level <code>"cauc"</code> of ethnicity does not occur in the output, as it is taken as the reference category. Hence, there is only one ethnicity effect, which gives the difference in intercepts between the <code>"afam"</code> and the <code>"cauc"</code> groups. In statistical terminology, this is called a “treatment contrast” (where the “treatment” <code>"afam"</code> is compared with the reference group <code>"cauc"</code>) and corresponds to what is called a “dummy variable” (or “indicator variable”) for the level <code>"afam"</code> in econometric jargon.</p>
<p>In <code>R</code>, (unordered) factors are automatically handled like this when they are included in a regression model. Internally, <code>R</code> produces a dummy variable for each level of a factor and resolves the resulting overspecification of the model (if an intercept or another factor is included in the model) by applying “contrasts”; i.e., a constraint on the underlying parameter vector. Contrasts are attributed to each factor and can be queried and changed by <code>contrasts()</code>. The default for unordered factors is to use all dummy variables except the one for the reference category (<code>"cauc"</code> in the example above). This is typically what is required for fitting econometric regression models, and hence changing the contrasts is usually not necessary.</p>
<p>The above result shows that there is an associative effect of <code>ethnicity</code> on <code>wage</code>, since <code>ethnicityafam</code>, i.e., the <code>"afam"</code>-level of the <code>ethnicity</code>-factor variables has a significant mean-shift effect. In such cases, one has often also further heterogenouse effects that can be considered using <strong>interactions</strong>. The following code checks whether <code>education</code> has a different slope value for <code>cauc</code>-people and <code>afam</code>-people, by computing the <strong>interaction-effect</strong> between <code>education</code> and <code>ethnicity</code>. The formula-notation in <code>R</code> for this is <code>education*ethnicity</code> which automatically adds the single regressors <code>education</code> and <code>ethnicity</code> and the interacted regressor <code>education</code><span class="math inline">\(\times\)</span><code>ethnicity</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>cps_lm_2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(wage) <span class="sc">~</span> experience <span class="sc">+</span> <span class="fu">I</span>(experience<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>                 education<span class="sc">*</span>ethnicity, <span class="at">data =</span> CPS1988)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Regression output table:</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(cps_lm_2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log(wage) ~ experience + I(experience^2) + education * 
    ethnicity, data = CPS1988)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.9451 -0.3162  0.0578  0.3761  4.3929 

Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              4.313e+00  1.959e-02 220.170   &lt;2e-16 ***
experience               7.752e-02  8.803e-04  88.063   &lt;2e-16 ***
I(experience^2)         -1.318e-03  1.901e-05 -69.339   &lt;2e-16 ***
education                8.631e-02  1.309e-03  65.944   &lt;2e-16 ***
ethnicityafam           -1.239e-01  5.903e-02  -2.099   0.0358 *  
education:ethnicityafam -9.648e-03  4.651e-03  -2.074   0.0380 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5839 on 28149 degrees of freedom
Multiple R-squared:  0.3348,    Adjusted R-squared:  0.3347 
F-statistic:  2834 on 5 and 28149 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<!-- ## Robust estimation of the variance of \hat{\beta}: -->
<!-- Var_beta_hat_robust <- sandwich::vcovHC(cps_lm_2, type="HC3") -->

<!-- ## Regression output table with robust SEs: -->
<!-- lmtest::coeftest(cps_lm_2, vcov = Var_beta_hat_robust) -->
<p>This result suggests that one year more <code>education</code> increases the average <code>wage</code> by <span class="math inline">\(0.0863\)</span> for Caucasian-people (<code>cauc</code>), but only by <span class="math inline">\(0.0863 - 0.0096=0.0767\)</span> for African-American-people (<code>afam</code>). The direct effect of <code>ethnicity</code> and the interaction effect <code>education</code><span class="math inline">\(\times\)</span><code>ethnicity</code> are here not significant at the <span class="math inline">\(&lt;0.001\)</span> level, but “only” at the <span class="math inline">\(0.05\)</span> level. However, generally we should not trust these inference results too much since we do not allow for heteroscedastic errors here.</p>
</section>
</section>
<section id="practice-simulation" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="practice-simulation"><span class="header-section-number">3.9</span> Practice: Simulation</h2>
<section id="behavior-of-the-ols-estimates-for-resampled-data-conditionally-on-x_i" class="level3" data-number="3.9.1">
<h3 data-number="3.9.1" class="anchored" data-anchor-id="behavior-of-the-ols-estimates-for-resampled-data-conditionally-on-x_i"><span class="header-section-number">3.9.1</span> Behavior of the OLS Estimates for Resampled Data (conditionally on <span class="math inline">\(X_i\)</span>)</h3>
<p>Usually, we only observe the <strong>estimate</strong> <span class="math inline">\(\hat{\beta}\)</span> computed for a given data set. However, in order to understand the statistical properties (unbiasedness and variance) of the <strong>estimator</strong> <span class="math inline">\(\hat{\beta}\)</span> we need to view them as random variables which yield different realizations in repeated samples generated from <a href="#eq-LinMod">Equation&nbsp;<span>3.1</span></a> conditionally on <span class="math inline">\(X_1,\dots,X_n\)</span>. This allows us then to think about questions like:</p>
<ul>
<li>“Is the estimator able to estimate the unknown parameter-value correctly on average (conditionally on a given set of <span class="math inline">\(X_1,\dots,X_n\)</span>)?”<br>
</li>
<li>“Are the estimation results more precise if we have more data?”</li>
</ul>
<p>Before we approach these questions theoretically in <a href="05-Small-Sample-Inference.html"><span>Chapter&nbsp;4</span></a> and <a href="06-Asymptotics.html"><span>Chapter&nbsp;5</span></a>, we can try to get a first idea about the statistical properties (unbiasedness and variance) of the estimator <span class="math inline">\(\hat{\beta}\)</span> using Monte Carlo simulations as following.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Sample sizes</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>n_small      <span class="ot">&lt;-</span>  <span class="dv">10</span> <span class="co"># small sample size</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>n_large      <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="co"># large sample size</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="do">## True parameter values</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate explanatory variables (random design)</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>X_n_small  <span class="ot">&lt;-</span> <span class="fu">runif</span>(n_small, <span class="at">min =</span> <span class="dv">1</span>, <span class="at">max =</span> <span class="dv">10</span>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>X_n_large  <span class="ot">&lt;-</span> <span class="fu">runif</span>(n_large, <span class="at">min =</span> <span class="dv">1</span>, <span class="at">max =</span> <span class="dv">10</span>)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Monte-Carlo (MC) Simulation </span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 1. Generate data</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="do">## 2. Compute and store estimates</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Repeat steps 1. and 2. many times</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">3</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of Monte Carlo repetitions</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="do">## How many samples to draw from the models</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>rep          <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Containers to store the lm-results</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>n_small_list <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">"list"</span>, <span class="at">length =</span> rep)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>n_large_list <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">"list"</span>, <span class="at">length =</span> rep)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>rep){</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="do">## Sampling from the model conditionally on X_n_small</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>error_n_small     <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_small, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">5</span>)</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>Y_n_small         <span class="ot">&lt;-</span> beta0 <span class="sc">+</span> beta1 <span class="sc">*</span> X_n_small <span class="sc">+</span> error_n_small</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>n_small_list[[r]] <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y_n_small <span class="sc">~</span> X_n_small)  </span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="do">## Sampling from the model conditionally on X_n_large</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>error_n_large     <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_large, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">5</span>)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>Y_n_large         <span class="ot">&lt;-</span> beta0 <span class="sc">+</span> beta1 <span class="sc">*</span> X_n_large <span class="sc">+</span> error_n_large</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>n_large_list[[r]] <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y_n_large <span class="sc">~</span> X_n_large)  </span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a><span class="do">## Reading out the parameter estimates</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>beta0_estimates_n_small <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, rep)</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>beta1_estimates_n_small <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, rep)</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>beta0_estimates_n_large <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, rep)</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>beta1_estimates_n_large <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, rep)</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>rep){</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>beta0_estimates_n_small[r] <span class="ot">&lt;-</span> n_small_list[[r]]<span class="sc">$</span>coefficients[<span class="dv">1</span>]</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>beta1_estimates_n_small[r] <span class="ot">&lt;-</span> n_small_list[[r]]<span class="sc">$</span>coefficients[<span class="dv">2</span>]</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>beta0_estimates_n_large[r] <span class="ot">&lt;-</span> n_large_list[[r]]<span class="sc">$</span>coefficients[<span class="dv">1</span>]</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>beta1_estimates_n_large[r] <span class="ot">&lt;-</span> n_large_list[[r]]<span class="sc">$</span>coefficients[<span class="dv">2</span>]</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we have produced realizations of the estimators <span class="math inline">\(\hat\beta_0|X\)</span> and <span class="math inline">\(\hat\beta_1|X\)</span> conditionally on <span class="math display">\[X=\begin{pmatrix}1&amp;X_1\\\vdots&amp;\vdots\\1&amp;X_n\end{pmatrix}\]</span> and we have saved these realizations in <code>beta0_estimates_n_small</code>, <code>beta1_estimates_n_small</code>, <code>beta0_estimates_n_large</code>, and <code>beta1_estimates_n_large</code>. This allows us to visualize the behavior of the OLS estimates for the repeatedly sampled data (conditionally on <span class="math inline">\(X_i\)</span>).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Plotting the results</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"scales"</span>) <span class="co"># alpha() produces transparent colors</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Define a common y-axis range</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>y_range <span class="ot">&lt;-</span> <span class="fu">range</span>(beta0_estimates_n_small,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>                 beta1_estimates_n_small)<span class="sc">*</span><span class="fl">1.1</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate the plot</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">family =</span> <span class="st">"serif"</span>) <span class="co"># Serif fonts</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Layout of plotting area</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="fu">layout</span>(<span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>), <span class="dv">2</span>, <span class="dv">3</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>), <span class="at">widths =</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot 1</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="dv">0</span>, <span class="at">y=</span><span class="dv">0</span>, <span class="at">axes=</span><span class="cn">FALSE</span>, <span class="at">xlab=</span><span class="st">"X"</span>, <span class="at">ylab=</span><span class="st">"Y"</span>, <span class="at">type=</span><span class="st">"n"</span>,</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">10</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">35</span>), <span class="at">main=</span><span class="st">"Small Sample (n=10)"</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">tick =</span> <span class="cn">FALSE</span>); <span class="fu">axis</span>(<span class="dv">2</span>, <span class="at">tick =</span> <span class="cn">FALSE</span>, <span class="at">las =</span> <span class="dv">2</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>rep){</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(n_small_list[[r]], <span class="at">lty=</span><span class="dv">2</span>, <span class="at">lwd =</span> <span class="fl">1.3</span>, <span class="at">col=</span><span class="st">"darkorange"</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> beta0, <span class="at">b =</span> beta1, <span class="at">lwd=</span><span class="fl">1.3</span>, <span class="at">col=</span><span class="st">"darkblue"</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="at">col=</span><span class="fu">c</span>(<span class="st">"darkorange"</span>, <span class="st">"darkblue"</span>), <span class="at">legend=</span><span class="fu">c</span>(</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="st">"Sample regression lines from</span><span class="sc">\n</span><span class="st">repeated samples (cond. on X)"</span>, </span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>                  <span class="st">"Population regression line"</span>), </span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd=</span><span class="fl">1.3</span>, <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>), <span class="at">bty=</span><span class="st">"n"</span>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot 2</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="fu">rep</span>(<span class="dv">0</span>,rep), <span class="at">y=</span>beta0_estimates_n_small, <span class="at">axes=</span><span class="cn">FALSE</span>, </span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">""</span>, <span class="at">ylab=</span><span class="st">""</span>, <span class="at">pch=</span><span class="dv">19</span>, <span class="at">cex=</span><span class="fl">1.2</span>, <span class="at">ylim=</span>y_range,</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">main=</span><span class="fu">expression</span>(<span class="fu">hat</span>(beta)[<span class="dv">0</span>]<span class="sc">~</span><span class="st">'|'</span><span class="sc">~</span>X), <span class="at">col=</span><span class="fu">alpha</span>(<span class="st">"red"</span>,<span class="fl">0.2</span>))</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y=</span>beta0, <span class="at">pch=</span><span class="st">"-"</span>, <span class="at">cex =</span> <span class="fl">1.2</span>, <span class="at">col=</span><span class="st">"black"</span>)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span><span class="dv">0</span>, <span class="at">y=</span>beta0, <span class="at">labels =</span> <span class="fu">expression</span>(beta[<span class="dv">0</span>]), <span class="at">pos =</span> <span class="dv">4</span>)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot 3</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="fu">rep</span>(<span class="dv">0</span>,rep), <span class="at">y=</span>beta1_estimates_n_small, <span class="at">axes=</span><span class="cn">FALSE</span>, </span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">""</span>, <span class="at">ylab=</span><span class="st">""</span>, <span class="at">pch=</span><span class="dv">19</span>, <span class="at">cex=</span><span class="fl">1.2</span>, <span class="at">ylim=</span>y_range,</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>  <span class="at">main=</span><span class="fu">expression</span>(<span class="fu">hat</span>(beta)[<span class="dv">1</span>]<span class="sc">~</span><span class="st">'|'</span><span class="sc">~</span>X), <span class="at">col=</span><span class="fu">alpha</span>(<span class="st">"red"</span>,<span class="fl">0.2</span>))</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y=</span>beta1, <span class="at">pch=</span><span class="st">"-"</span>, <span class="at">cex =</span> <span class="fl">1.2</span>, <span class="at">col=</span><span class="st">"black"</span>)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span><span class="dv">0</span>, <span class="at">y=</span>beta1, <span class="at">labels =</span> <span class="fu">expression</span>(beta[<span class="dv">1</span>]), <span class="at">pos =</span> <span class="dv">4</span>)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot 4</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="dv">0</span>, <span class="at">y=</span><span class="dv">0</span>, <span class="at">axes=</span><span class="cn">FALSE</span>, <span class="at">xlab=</span><span class="st">"X"</span>, <span class="at">ylab=</span><span class="st">"Y"</span>, <span class="at">type=</span><span class="st">"n"</span>,</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">10</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">35</span>), <span class="at">main=</span><span class="st">"Large Sample (n=100)"</span>)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">tick =</span> <span class="cn">FALSE</span>); <span class="fu">axis</span>(<span class="dv">2</span>, <span class="at">tick =</span> <span class="cn">FALSE</span>, <span class="at">las =</span> <span class="dv">2</span>)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>rep){</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(n_large_list[[r]], <span class="at">lty=</span><span class="dv">2</span>, <span class="at">lwd =</span> <span class="fl">1.3</span>, <span class="at">col=</span><span class="st">"darkorange"</span>)</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> beta0, <span class="at">b =</span> beta1, <span class="at">lwd=</span><span class="fl">1.3</span>, <span class="at">col=</span><span class="st">"darkblue"</span>)</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot 5</span></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="fu">rep</span>(<span class="dv">0</span>,rep), <span class="at">y=</span>beta0_estimates_n_large, <span class="at">axes=</span><span class="cn">FALSE</span>, </span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">""</span>, <span class="at">ylab=</span><span class="st">""</span>, <span class="at">pch=</span><span class="dv">19</span>, <span class="at">cex=</span><span class="fl">1.2</span>, <span class="at">ylim=</span>y_range,</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>  <span class="at">main=</span><span class="fu">expression</span>(<span class="fu">hat</span>(beta)[<span class="dv">0</span>]<span class="sc">~</span><span class="st">'|'</span><span class="sc">~</span>X), <span class="at">col=</span><span class="fu">alpha</span>(<span class="st">"red"</span>,<span class="fl">0.2</span>))</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y=</span>beta0, <span class="at">pch=</span><span class="st">"-"</span>, <span class="at">cex =</span> <span class="fl">1.2</span>, <span class="at">col=</span><span class="st">"black"</span>)</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span><span class="dv">0</span>, <span class="at">y=</span>beta0, <span class="at">labels =</span> <span class="fu">expression</span>(beta[<span class="dv">0</span>]), <span class="at">pos =</span> <span class="dv">4</span>)</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot 6</span></span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="fu">rep</span>(<span class="dv">0</span>,rep), <span class="at">y=</span>beta1_estimates_n_large, <span class="at">axes=</span><span class="cn">FALSE</span>, </span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">""</span>, <span class="at">ylab=</span><span class="st">""</span>, <span class="at">pch=</span><span class="dv">19</span>, <span class="at">cex=</span><span class="fl">1.2</span>, <span class="at">ylim=</span>y_range,</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>  <span class="at">main=</span><span class="fu">expression</span>(<span class="fu">hat</span>(beta)[<span class="dv">1</span>]<span class="sc">~</span><span class="st">'|'</span><span class="sc">~</span>X), <span class="at">col=</span><span class="fu">alpha</span>(<span class="st">"red"</span>,<span class="fl">0.2</span>))</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x=</span><span class="dv">0</span>, <span class="at">y=</span>beta1, <span class="at">pch=</span><span class="st">"-"</span>, <span class="at">cex =</span> <span class="fl">1.2</span>, <span class="at">col=</span><span class="st">"black"</span>)</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span><span class="dv">0</span>, <span class="at">y=</span>beta1, <span class="at">labels =</span> <span class="fu">expression</span>(beta[<span class="dv">1</span>]), <span class="at">pos =</span> <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04-MultipleReg_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This are promising plots:</p>
<ul>
<li>The realizations of <span class="math inline">\(\hat\beta_0|X\)</span> and <span class="math inline">\(\hat\beta_1|X\)</span> are scattered around the true (unknown) parameter values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> for both small and large samples.</li>
<li>The realizations of <span class="math inline">\(\hat\beta_0|X\)</span> and <span class="math inline">\(\hat\beta_1|X\)</span> concentrate more and more around the true (unknown) parameter values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> as the sample size increases.</li>
</ul>
<p>However, this was only a simulation for one specific data generating process. Such a Monte Carlo simulation does not allow us to generalize these properties. In <a href="05-Small-Sample-Inference.html"><span>Chapter&nbsp;4</span></a> and <a href="06-Asymptotics.html"><span>Chapter&nbsp;5</span></a>, we use theoretical arguments to show that these properties also hold in general.</p>
</section>
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>The <span class="math inline">\(K\)</span> linear restrictions follow from the fact that <span class="math inline">\(X'\hat\varepsilon=0\)</span> are <span class="math inline">\(K\)</span> equations <span class="math inline">\(\sum_{i=1}^nX_{ik}\hat\varepsilon_i=0\)</span> for <span class="math inline">\(k=1,\dots,K\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02-Review-Prob_n_Stats.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Review: Probability and Statistics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05-Small-Sample-Inference.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Small Sample Inference</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>