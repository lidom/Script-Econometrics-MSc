<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.8 Asymptotic Theory of Maximum-Likelihood Estimators | Econometrics (M.Sc.)</title>
  <meta name="description" content="7.8 Asymptotic Theory of Maximum-Likelihood Estimators | Econometrics (M.Sc.)" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="7.8 Asymptotic Theory of Maximum-Likelihood Estimators | Econometrics (M.Sc.)" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/mylogo.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.8 Asymptotic Theory of Maximum-Likelihood Estimators | Econometrics (M.Sc.)" />
  
  
  <meta name="twitter:image" content="/images/mylogo.png" />

<meta name="author" content="Prof. Dr. Dominik Liebl" />


<meta name="date" content="2021-11-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="consistency-of-hatbeta_ml-and-s_ml2.html"/>
<link rel="next" href="ivr.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#organization-of-the-course"><i class="fa fa-check"></i>Organization of the Course</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#literature"><i class="fa fa-check"></i>Literature</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>1</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="short-glossary.html"><a href="short-glossary.html"><i class="fa fa-check"></i><b>1.1</b> Short Glossary</a></li>
<li class="chapter" data-level="1.2" data-path="first-steps.html"><a href="first-steps.html"><i class="fa fa-check"></i><b>1.2</b> First Steps</a></li>
<li class="chapter" data-level="1.3" data-path="further-data-objects.html"><a href="further-data-objects.html"><i class="fa fa-check"></i><b>1.3</b> Further Data Objects</a></li>
<li class="chapter" data-level="1.4" data-path="simple-regression-analysis-using-r.html"><a href="simple-regression-analysis-using-r.html"><i class="fa fa-check"></i><b>1.4</b> Simple Regression Analysis using R</a></li>
<li class="chapter" data-level="1.5" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>1.5</b> Programming in R</a></li>
<li class="chapter" data-level="1.6" data-path="r-packages.html"><a href="r-packages.html"><i class="fa fa-check"></i><b>1.6</b> R-packages</a></li>
<li class="chapter" data-level="1.7" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>1.7</b> Tidyverse</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="tidyverse.html"><a href="tidyverse.html#tidyverse-plotting-basics"><i class="fa fa-check"></i><b>1.7.1</b> Tidyverse: Plotting Basics</a></li>
<li class="chapter" data-level="1.7.2" data-path="tidyverse.html"><a href="tidyverse.html#tidyverse-data-wrangling-basics"><i class="fa fa-check"></i><b>1.7.2</b> Tidyverse: Data Wrangling Basics</a></li>
<li class="chapter" data-level="1.7.3" data-path="tidyverse.html"><a href="tidyverse.html#the-pipe-operator"><i class="fa fa-check"></i><b>1.7.3</b> The pipe operator <code>%&gt;%</code></a></li>
<li class="chapter" data-level="1.7.4" data-path="tidyverse.html"><a href="tidyverse.html#the-group_by-function"><i class="fa fa-check"></i><b>1.7.4</b> The <code>group_by()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="further-links.html"><a href="further-links.html"><i class="fa fa-check"></i><b>1.8</b> Further Links</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="further-links.html"><a href="further-links.html#further-r-intros"><i class="fa fa-check"></i><b>1.8.1</b> Further R-Intros</a></li>
<li class="chapter" data-level="1.8.2" data-path="further-links.html"><a href="further-links.html#version-control-gitgithub"><i class="fa fa-check"></i><b>1.8.2</b> Version Control (Git/GitHub)</a></li>
<li class="chapter" data-level="1.8.3" data-path="further-links.html"><a href="further-links.html#r-ladies"><i class="fa fa-check"></i><b>1.8.3</b> R-Ladies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="review-probability-and-statistics.html"><a href="review-probability-and-statistics.html"><i class="fa fa-check"></i><b>2</b> Review: Probability and Statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2.1</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="probability-theory.html"><a href="probability-theory.html#sample-spaces-and-elementary-events"><i class="fa fa-check"></i><b>2.1.1</b> Sample Spaces and (Elementary) Events</a></li>
<li class="chapter" data-level="2.1.2" data-path="probability-theory.html"><a href="probability-theory.html#probability"><i class="fa fa-check"></i><b>2.1.2</b> Probability</a></li>
<li class="chapter" data-level="2.1.3" data-path="probability-theory.html"><a href="probability-theory.html#independent-events"><i class="fa fa-check"></i><b>2.1.3</b> Independent Events</a></li>
<li class="chapter" data-level="2.1.4" data-path="probability-theory.html"><a href="probability-theory.html#conditional-probability"><i class="fa fa-check"></i><b>2.1.4</b> Conditional Probability</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>2.2</b> Random Variables</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="random-variables.html"><a href="random-variables.html#univariate-distribution-and-probability-functions"><i class="fa fa-check"></i><b>2.2.1</b> Univariate Distribution and Probability Functions</a></li>
<li class="chapter" data-level="2.2.2" data-path="random-variables.html"><a href="random-variables.html#multivariate-distribution-and-probability-functions"><i class="fa fa-check"></i><b>2.2.2</b> Multivariate Distribution and Probability Functions</a></li>
<li class="chapter" data-level="2.2.3" data-path="random-variables.html"><a href="random-variables.html#means-and-moments"><i class="fa fa-check"></i><b>2.2.3</b> Means and Moments</a></li>
<li class="chapter" data-level="2.2.4" data-path="random-variables.html"><a href="random-variables.html#unconditional-means"><i class="fa fa-check"></i><b>2.2.4</b> Unconditional Means</a></li>
<li class="chapter" data-level="2.2.5" data-path="random-variables.html"><a href="random-variables.html#conditional-means"><i class="fa fa-check"></i><b>2.2.5</b> Conditional Means</a></li>
<li class="chapter" data-level="2.2.6" data-path="random-variables.html"><a href="random-variables.html#means-of-transformed-random-variables-and-moments"><i class="fa fa-check"></i><b>2.2.6</b> Means of Transformed Random Variables and Moments</a></li>
<li class="chapter" data-level="2.2.7" data-path="random-variables.html"><a href="random-variables.html#independent-random-variables"><i class="fa fa-check"></i><b>2.2.7</b> Independent Random Variables</a></li>
<li class="chapter" data-level="2.2.8" data-path="random-variables.html"><a href="random-variables.html#i.i.d.-samples"><i class="fa fa-check"></i><b>2.2.8</b> I.I.D. Samples</a></li>
<li class="chapter" data-level="2.2.9" data-path="random-variables.html"><a href="random-variables.html#some-important-discrete-random-variables"><i class="fa fa-check"></i><b>2.2.9</b> Some Important Discrete Random Variables</a></li>
<li class="chapter" data-level="2.2.10" data-path="random-variables.html"><a href="random-variables.html#some-important-continuous-random-variables"><i class="fa fa-check"></i><b>2.2.10</b> Some Important Continuous Random Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch:SLR.html"><a href="ch:SLR.html"><i class="fa fa-check"></i><b>3</b> Review: Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-simple-linear-regression-model.html"><a href="the-simple-linear-regression-model.html"><i class="fa fa-check"></i><b>3.1</b> The Simple Linear Regression Model</a>
<ul>
<li class="chapter" data-level="" data-path="the-simple-linear-regression-model.html"><a href="the-simple-linear-regression-model.html#the-data-generating-process"><i class="fa fa-check"></i>The Data-Generating Process</a></li>
<li class="chapter" data-level="3.1.1" data-path="the-simple-linear-regression-model.html"><a href="the-simple-linear-regression-model.html#assumptions-about-the-error-term"><i class="fa fa-check"></i><b>3.1.1</b> Assumptions About the Error Term</a></li>
<li class="chapter" data-level="3.1.2" data-path="the-simple-linear-regression-model.html"><a href="the-simple-linear-regression-model.html#the-population-regression-line"><i class="fa fa-check"></i><b>3.1.2</b> The Population Regression Line</a></li>
<li class="chapter" data-level="3.1.3" data-path="the-simple-linear-regression-model.html"><a href="the-simple-linear-regression-model.html#terminology-estimates-versus-estimators"><i class="fa fa-check"></i><b>3.1.3</b> Terminology: Estimates versus Estimators</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ordinary-least-squares-estimation.html"><a href="ordinary-least-squares-estimation.html"><i class="fa fa-check"></i><b>3.2</b> Ordinary Least Squares Estimation</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ordinary-least-squares-estimation.html"><a href="ordinary-least-squares-estimation.html#terminology-sample-regression-line-prediction-and-residuals"><i class="fa fa-check"></i><b>3.2.1</b> Terminology: Sample Regression Line, Prediction and Residuals</a></li>
<li class="chapter" data-level="3.2.2" data-path="ordinary-least-squares-estimation.html"><a href="ordinary-least-squares-estimation.html#sec:SLROLS"><i class="fa fa-check"></i><b>3.2.2</b> Deriving the Expression of the OLS Estimator</a></li>
<li class="chapter" data-level="3.2.3" data-path="ordinary-least-squares-estimation.html"><a href="ordinary-least-squares-estimation.html#behavior-of-the-ols-estimates-for-resampled-data-conditionally-on-x_i"><i class="fa fa-check"></i><b>3.2.3</b> Behavior of the OLS Estimates for Resampled Data (conditionally on <span class="math inline">\(X_i\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="properties-of-the-ols-estimator.html"><a href="properties-of-the-ols-estimator.html"><i class="fa fa-check"></i><b>3.3</b> Properties of the OLS Estimator</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="properties-of-the-ols-estimator.html"><a href="properties-of-the-ols-estimator.html#mean-and-bias-of-the-ols-estimator"><i class="fa fa-check"></i><b>3.3.1</b> Mean and Bias of the OLS Estimator</a></li>
<li class="chapter" data-level="3.3.2" data-path="properties-of-the-ols-estimator.html"><a href="properties-of-the-ols-estimator.html#variance-of-the-ols-estimator"><i class="fa fa-check"></i><b>3.3.2</b> Variance of the OLS Estimator</a></li>
<li class="chapter" data-level="3.3.3" data-path="properties-of-the-ols-estimator.html"><a href="properties-of-the-ols-estimator.html#consistency-of-the-ols-estimator"><i class="fa fa-check"></i><b>3.3.3</b> Consistency of the OLS Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch:MLR.html"><a href="ch:MLR.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="assumptions.html"><a href="assumptions.html"><i class="fa fa-check"></i><b>4.1</b> Assumptions</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="assumptions.html"><a href="assumptions.html#some-implications-of-the-exogeneity-assumption"><i class="fa fa-check"></i><b>4.1.1</b> Some Implications of the Exogeneity Assumption</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="deriving-the-expression-of-the-ols-estimator.html"><a href="deriving-the-expression-of-the-ols-estimator.html"><i class="fa fa-check"></i><b>4.2</b> Deriving the Expression of the OLS Estimator</a></li>
<li class="chapter" data-level="4.3" data-path="some-quantities-of-interest.html"><a href="some-quantities-of-interest.html"><i class="fa fa-check"></i><b>4.3</b> Some Quantities of Interest</a></li>
<li class="chapter" data-level="4.4" data-path="method-of-moments-estimator.html"><a href="method-of-moments-estimator.html"><i class="fa fa-check"></i><b>4.4</b> Method of Moments Estimator</a></li>
<li class="chapter" data-level="4.5" data-path="unbiasedness-of-hatbetax-and-hatbeta.html"><a href="unbiasedness-of-hatbetax-and-hatbeta.html"><i class="fa fa-check"></i><b>4.5</b> Unbiasedness of <span class="math inline">\(\hat\beta|X\)</span> and <span class="math inline">\(\hat\beta\)</span></a></li>
<li class="chapter" data-level="4.6" data-path="ch:VarEstBeta.html"><a href="ch:VarEstBeta.html"><i class="fa fa-check"></i><b>4.6</b> Variance of <span class="math inline">\(\hat\beta|X\)</span></a></li>
<li class="chapter" data-level="4.7" data-path="the-gauss-markov-theorem.html"><a href="the-gauss-markov-theorem.html"><i class="fa fa-check"></i><b>4.7</b> The Gauss-Markov Theorem</a></li>
<li class="chapter" data-level="4.8" data-path="practice.html"><a href="practice.html"><i class="fa fa-check"></i><b>4.8</b> Practice</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="practice.html"><a href="practice.html#dummy-variables-and-contrast-codings"><i class="fa fa-check"></i><b>4.8.1</b> Dummy variables and contrast codings</a></li>
<li class="chapter" data-level="4.8.2" data-path="practice.html"><a href="practice.html#the-function"><i class="fa fa-check"></i><b>4.8.2</b> The function </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch:SSINF.html"><a href="ch:SSINF.html"><i class="fa fa-check"></i><b>5</b> Small Sample Inference</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch:testmultp.html"><a href="ch:testmultp.html"><i class="fa fa-check"></i><b>5.1</b> Hypothesis Tests about Multiple Parameters</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch:testmultp.html"><a href="ch:testmultp.html#the-test-statistic-and-its-null-distribution"><i class="fa fa-check"></i><b>5.1.1</b> The Test Statistic and its Null Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch:testingsinglep.html"><a href="ch:testingsinglep.html"><i class="fa fa-check"></i><b>5.2</b> Tests about One Parameter</a></li>
<li class="chapter" data-level="5.3" data-path="testtheory.html"><a href="testtheory.html"><i class="fa fa-check"></i><b>5.3</b> Testtheory</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="testtheory.html"><a href="testtheory.html#significance-level"><i class="fa fa-check"></i><b>5.3.1</b> Significance Level</a></li>
<li class="chapter" data-level="5.3.2" data-path="testtheory.html"><a href="testtheory.html#critical-value-for-the-f-test"><i class="fa fa-check"></i><b>5.3.2</b> Critical Value for the <span class="math inline">\(F\)</span>-Test</a></li>
<li class="chapter" data-level="5.3.3" data-path="testtheory.html"><a href="testtheory.html#critical-values-for-the-t-test"><i class="fa fa-check"></i><b>5.3.3</b> Critical Value(s) for the <span class="math inline">\(t\)</span>-Test</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="type-ii-error-and-power.html"><a href="type-ii-error-and-power.html"><i class="fa fa-check"></i><b>5.4</b> Type II Error and Power</a></li>
<li class="chapter" data-level="5.5" data-path="p-value.html"><a href="p-value.html"><i class="fa fa-check"></i><b>5.5</b> <span class="math inline">\(p\)</span>-Value</a></li>
<li class="chapter" data-level="5.6" data-path="CIsmallsample.html"><a href="CIsmallsample.html"><i class="fa fa-check"></i><b>5.6</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.7" data-path="PSSI.html"><a href="PSSI.html"><i class="fa fa-check"></i><b>5.7</b> Practice: Small Sample Inference</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="PSSI.html"><a href="PSSI.html#normally-distributed-hatbetax"><i class="fa fa-check"></i><b>5.7.1</b> Normally Distributed <span class="math inline">\(\hat\beta|X\)</span></a></li>
<li class="chapter" data-level="5.7.2" data-path="PSSI.html"><a href="PSSI.html#testing-multiple-parameters"><i class="fa fa-check"></i><b>5.7.2</b> Testing Multiple Parameters</a></li>
<li class="chapter" data-level="5.7.3" data-path="PSSI.html"><a href="PSSI.html#dualty-of-confidence-intervals-and-hypothesis-tests"><i class="fa fa-check"></i><b>5.7.3</b> Dualty of Confidence Intervals and Hypothesis Tests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch:LSINF.html"><a href="ch:LSINF.html"><i class="fa fa-check"></i><b>6</b> Large Sample Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="tools-for-asymptotic-statistics.html"><a href="tools-for-asymptotic-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Tools for Asymptotic Statistics</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="tools-for-asymptotic-statistics.html"><a href="tools-for-asymptotic-statistics.html#modes-of-convergence"><i class="fa fa-check"></i><b>6.1.1</b> Modes of Convergence</a></li>
<li class="chapter" data-level="" data-path="tools-for-asymptotic-statistics.html"><a href="tools-for-asymptotic-statistics.html#four-important-modes-of-convergence"><i class="fa fa-check"></i>Four Important Modes of Convergence</a></li>
<li class="chapter" data-level="" data-path="tools-for-asymptotic-statistics.html"><a href="tools-for-asymptotic-statistics.html#relations-among-modes-of-convergence"><i class="fa fa-check"></i>Relations among Modes of Convergence</a></li>
<li class="chapter" data-level="6.1.2" data-path="tools-for-asymptotic-statistics.html"><a href="tools-for-asymptotic-statistics.html#continuous-mapping-theorem-cmt"><i class="fa fa-check"></i><b>6.1.2</b> Continuous Mapping Theorem (CMT)</a></li>
<li class="chapter" data-level="6.1.3" data-path="tools-for-asymptotic-statistics.html"><a href="tools-for-asymptotic-statistics.html#slutsky-theorem"><i class="fa fa-check"></i><b>6.1.3</b> Slutsky Theorem</a></li>
<li class="chapter" data-level="6.1.4" data-path="tools-for-asymptotic-statistics.html"><a href="tools-for-asymptotic-statistics.html#law-of-large-numbers-lln-and-central-limit-theorem-clt"><i class="fa fa-check"></i><b>6.1.4</b> Law of Large Numbers (LLN) and Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="6.1.5" data-path="tools-for-asymptotic-statistics.html"><a href="tools-for-asymptotic-statistics.html#estimators-as-a-sequences-of-random-variables"><i class="fa fa-check"></i><b>6.1.5</b> Estimators as a Sequences of Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="asymptotics-under-the-classic-regression-model.html"><a href="asymptotics-under-the-classic-regression-model.html"><i class="fa fa-check"></i><b>6.2</b> Asymptotics under the Classic Regression Model</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="asymptotics-under-the-classic-regression-model.html"><a href="asymptotics-under-the-classic-regression-model.html#the-case-of-heteroscedasticity"><i class="fa fa-check"></i><b>6.2.1</b> The Case of Heteroscedasticity</a></li>
<li class="chapter" data-level="6.2.2" data-path="asymptotics-under-the-classic-regression-model.html"><a href="asymptotics-under-the-classic-regression-model.html#hypothesis-testing-and-confidence-intervals"><i class="fa fa-check"></i><b>6.2.2</b> Hypothesis Testing and Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="robust-confidence-intervals.html"><a href="robust-confidence-intervals.html"><i class="fa fa-check"></i><b>6.3</b> Robust Confidence Intervals</a></li>
<li class="chapter" data-level="6.4" data-path="practice-large-sample-inference.html"><a href="practice-large-sample-inference.html"><i class="fa fa-check"></i><b>6.4</b> Practice: Large Sample Inference</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="practice-large-sample-inference.html"><a href="practice-large-sample-inference.html#normally-distributed-hatbeta-for-ntoinfty"><i class="fa fa-check"></i><b>6.4.1</b> Normally Distributed <span class="math inline">\(\hat\beta\)</span> for <span class="math inline">\(n\to\infty\)</span></a></li>
<li class="chapter" data-level="6.4.2" data-path="practice-large-sample-inference.html"><a href="practice-large-sample-inference.html#testing-multiple-and-single-parameters"><i class="fa fa-check"></i><b>6.4.2</b> Testing Multiple and Single Parameters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html"><i class="fa fa-check"></i><b>7</b> Maximum Likelihood</a>
<ul>
<li class="chapter" data-level="7.1" data-path="likelihood-principle.html"><a href="likelihood-principle.html"><i class="fa fa-check"></i><b>7.1</b> Likelihood Principle</a></li>
<li class="chapter" data-level="7.2" data-path="properties-of-maximum-likelihood-estimators.html"><a href="properties-of-maximum-likelihood-estimators.html"><i class="fa fa-check"></i><b>7.2</b> Properties of Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="7.3" data-path="the-log-likelihood-function.html"><a href="the-log-likelihood-function.html"><i class="fa fa-check"></i><b>7.3</b> The (Log-)Likelihood Function</a></li>
<li class="chapter" data-level="7.4" data-path="optimization-non-analytical-solutions.html"><a href="optimization-non-analytical-solutions.html"><i class="fa fa-check"></i><b>7.4</b> Optimization: Non-Analytical Solutions</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="optimization-non-analytical-solutions.html"><a href="optimization-non-analytical-solutions.html#newton-raphson-optimization"><i class="fa fa-check"></i><b>7.4.1</b> Newton-Raphson Optimization</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ols-estimation-as-ml-estimation.html"><a href="ols-estimation-as-ml-estimation.html"><i class="fa fa-check"></i><b>7.5</b> OLS-Estimation as ML-Estimation</a></li>
<li class="chapter" data-level="7.6" data-path="variance-of-ml-estimators-hatbeta_ml-and-s2_ml.html"><a href="variance-of-ml-estimators-hatbeta_ml-and-s2_ml.html"><i class="fa fa-check"></i><b>7.6</b> Variance of ML-Estimators <span class="math inline">\(\hat\beta_{ML}\)</span> and <span class="math inline">\(s^2_{ML}\)</span></a></li>
<li class="chapter" data-level="7.7" data-path="consistency-of-hatbeta_ml-and-s_ml2.html"><a href="consistency-of-hatbeta_ml-and-s_ml2.html"><i class="fa fa-check"></i><b>7.7</b> Consistency of <span class="math inline">\(\hat\beta_{ML}\)</span> and <span class="math inline">\(s_{ML}^2\)</span></a></li>
<li class="chapter" data-level="7.8" data-path="asymptotic-theory-of-maximum-likelihood-estimators.html"><a href="asymptotic-theory-of-maximum-likelihood-estimators.html"><i class="fa fa-check"></i><b>7.8</b> Asymptotic Theory of Maximum-Likelihood Estimators</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ivr.html"><a href="ivr.html"><i class="fa fa-check"></i><b>8</b> Instrumental Variables Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="TIVEWASRAASI.html"><a href="TIVEWASRAASI.html"><i class="fa fa-check"></i><b>8.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="TIVEWASRAASI.html"><a href="TIVEWASRAASI.html#the-two-stage-least-squares-estimator"><i class="fa fa-check"></i><b>8.1.1</b> The Two-Stage Least Squares Estimator</a></li>
<li class="chapter" data-level="8.1.2" data-path="TIVEWASRAASI.html"><a href="TIVEWASRAASI.html#application-demand-for-cigarettes-12"><i class="fa fa-check"></i><b>8.1.2</b> Application: Demand For Cigarettes (1/2)</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="TGIVRM.html"><a href="TGIVRM.html"><i class="fa fa-check"></i><b>8.2</b> The General IV Regression Model</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="TGIVRM.html"><a href="TGIVRM.html#application-demand-for-cigarettes-22"><i class="fa fa-check"></i><b>8.2.1</b> Application: Demand for Cigarettes (2/2)</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="civ.html"><a href="civ.html"><i class="fa fa-check"></i><b>8.3</b> Checking Instrument Validity</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="civ.html"><a href="civ.html#instrument-relevance"><i class="fa fa-check"></i><b>8.3.1</b> Instrument Relevance</a></li>
<li class="chapter" data-level="8.3.2" data-path="civ.html"><a href="civ.html#instrument-validity"><i class="fa fa-check"></i><b>8.3.2</b> Instrument Validity</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="attdfc.html"><a href="attdfc.html"><i class="fa fa-check"></i><b>8.4</b> Application to the Demand for Cigarettes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics (M.Sc.)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="asymptotic-theory-of-maximum-likelihood-estimators" class="section level2" number="7.8">
<h2><span class="header-section-number">7.8</span> Asymptotic Theory of Maximum-Likelihood Estimators</h2>
<p>So far, we only considered consistency of the ML-estimators. In the following, we consider the asymptotic distribution of ML-estimators. We only consider the simplest situation: Assume an i.i.d. sample <span class="math inline">\(X_1,\dots,X_n\)</span>, and suppose that
the distribution of <span class="math inline">\(X_i\)</span> possesses a density <span class="math inline">\(f(x|\theta)\)</span>. The true parameter <span class="math inline">\(\theta\in \mathbb{R}\)</span> is unknown. (Example: density of an exponential distribution <span class="math inline">\(f(x|\theta)=\theta\exp(- \theta x)\)</span>)</p>
<p>It can generally be shown that <span class="math inline">\(\hat{\theta}_n\)</span> is a consistent estimator of
<span class="math inline">\(\theta\)</span>. Derivation of the asymptotic distribution relies on a Taylor expansion (around <span class="math inline">\(\theta\)</span>) of the derivative
<span class="math inline">\(\ell_n&#39;(\cdot)\)</span> of the log-likelihood function. By the so called , we then know that for some <span class="math inline">\(\psi_n\)</span> between <span class="math inline">\(\hat{\theta}_n\)</span> and <span class="math inline">\(\theta\)</span> we have
<span class="math display">\[
\ell_n&#39;(\hat{\theta}_n)=\ell_n&#39;(\theta)+\ell_n&#39;&#39;(\psi_n)(\hat{\theta}_n-\theta)\quad\quad\text{(Mean Value Theorem)}
\]</span>
Since <span class="math inline">\(\hat{\theta}_n\)</span> maximizes the log-Likelihood function it follows that <span class="math inline">\(\ell_n&#39;(\hat{\theta}_n)=0\)</span>.
This implies (since <span class="math inline">\(\ell_n&#39;(\hat{\theta}_n)=\ell_n&#39;(\theta)-\ell_n&#39;&#39;(\psi_n)(\hat{\theta}_n-\theta)\)</span>) that
<span class="math display">\[\begin{equation}
\ell_n&#39;(\theta)=-\ell_n&#39;&#39;(\psi_n)(\hat{\theta}_n-\theta).\label{eq:ml2}
\end{equation}\]</span>
Note that necessarily
<span class="math inline">\(\int_{-\infty}^{\infty} f(x|\theta)dx=1\)</span> for all possible values of the true parameter
<span class="math inline">\(\theta\)</span>. Therefore,
<span class="math inline">\(\int_{-\infty}^{\infty} \frac{\partial}{\partial \theta}f(x|\theta)dx=0\)</span> and
<span class="math inline">\(\int_{-\infty}^{\infty} \frac{\partial^2}{\partial \theta^2}f(x|\theta)dx=0\)</span>.</p>
<p>Using this, we now show that the average
<span class="math display">\[
\frac{1}{n}\sum_{i=1}^n\frac{\partial}{\partial \theta} \ln f(X_i|\theta)
\]</span>
is asymptotically normal. For the mean of <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n\frac{\partial}{\partial \theta} \ln f(X_i|\theta)\)</span> one gets:
<span class="math display">\[
E\left(\frac{1}{n}\ell_n&#39;(\theta)\right)=\frac{1}{n}E\left(\sum_{i=1}^n\frac{\partial}{\partial \theta} \ln f(X_i|\theta)\right)
=\frac{n}{n}\int_{-\infty}^{\infty} \frac{\frac{\partial}{\partial \theta}  f(x|\theta)}
{f(x|\theta)}f(x|\theta)dx=0.
\]</span>
For the variance of <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n\frac{\partial}{\partial \theta} \ln f(X_i|\theta)\)</span> one gets:
<span class="math display">\[
\V\left(\frac{1}{n}\ell_n&#39;(\theta)\right)=\frac{n}{n^2}\V\left(\frac{\partial}{\partial \theta} \ln f(X_i|\theta)\right)
=\frac{1}{n}\underbrace{E\left(\left(\frac{\frac{\partial}{\partial \theta}  f(X_i|\theta)}
{f(X_i|\theta)}\right)^2\right)}_{=:\mathcal{J}(\theta)}=\frac{1}{n}\mathcal{J}(\theta)
\]</span>
<!-- Define $\mathcal{J}(\theta):=E\left(\left(\frac{\frac{\partial}{\partial \theta}f(X_i|\theta)} -->
<!-- {f(X_i|\theta)}\right)^2\right)$.  -->
Moreover, the average <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n\frac{\partial}{\partial \theta} \ln f(X_i|\theta)\)</span> is taken over i.i.d.~random variables <span class="math inline">\(\frac{\partial}{\partial \theta} \ln f(X_i|\theta)\)</span>. So, we can apply the Lindeberg-L'evy central limit theorem from which it follows that
<span class="math display">\[
\frac{\frac{1}{n}\ell_n&#39;(\hat{\theta}_n)}{\sqrt{\frac{1}{n}\mathcal{J}(\theta)} }=\frac{\ell_n&#39;(\hat{\theta}_n)}{\sqrt{n\mathcal{J}(\theta)} } \to_d N(0,1)
\]</span>
Thus (using ) we also have
<span class="math display">\[\begin{equation}
\frac{-\ell_n&#39;&#39;(\psi_n)}{\sqrt{n \cdot \mathcal{J}(\theta)}}(\hat{\theta}_n-\theta) \to_d N(0,1)\label{eq:MLNorm}
\end{equation}\]</span>
Further analysis requires to study the term <span class="math inline">\(\ell_n&#39;&#39;(\psi_n)\)</span>. We begin this with studying the mean of
<span class="math display">\[\begin{align*}
\frac{1}{n}\ell_n&#39;&#39;(\theta)
&amp;=\frac{1}{n}\sum_{i=1}^n\frac{\partial^2}{\partial \theta\partial \theta}\ln f(X_i|\theta)
=\frac{1}{n}\sum_{i=1}^n\frac{\partial}{\partial \theta}\left(\frac{\partial}{\partial\theta}\ln f(X_i|\theta)\right)\\
&amp;=\frac{1}{n}\sum_{i=1}^n\frac{\partial}{\partial \theta}\left(\frac{\frac{\partial}{\partial \theta}f(X_i|\theta)}{f(X_i|\theta)}\right)\\
&amp;=\frac{1}{n}\sum_{i=1}^n
\left(
\frac{\left(\frac{\partial^2}{\partial \theta\partial \theta}f(X_i|\theta)\right) f(X_i|\theta)-\frac{\partial}{\partial\theta}f(X_i|\theta)\frac{\partial}{\partial\theta} f(X_i|\theta)}{\left(f(X_i|\theta)\right)^2}\right)
\end{align*}\]</span>
Taking the mean of <span class="math inline">\(\frac{1}{n}\ell_n&#39;&#39;(\theta)\)</span> yields:
<span class="math display">\[\begin{align*}
\frac{1}{n}E(\ell_n&#39;&#39;(\theta))
&amp;=\frac{n}{n}E\left( \frac{\frac{\partial^2}{\partial \theta^2}  f(X_i|\theta)}
{f(X_i|\theta)}-\left( \frac{\frac{\partial}{\partial \theta}  f(X_i|\theta)}
{f(X_i|\theta)}\right)^2\right)\\
&amp;=0 - E\left(\left( \frac{\frac{\partial}{\partial \theta}  f(X_i|\theta)}
{f(X_i|\theta)}\right)^2\right)=-\mathcal{J}(\theta)
\end{align*}\]</span>
Taking the variance of <span class="math inline">\(\frac{1}{n}\ell_n&#39;&#39;(\theta)\)</span> yields:
<span class="math display">\[
\V\left(\frac{1}{n}\ell_n&#39;&#39;(\theta)\right)=\frac{1}{n^2}n
\underbrace{\V\left(\frac{\partial^2}{\partial \theta \partial \theta}  \ln f(X_i|\theta)\right)}_{=\text{some fixed, deterministic number}}\to 0\quad\text{as}\quad n\to\infty
\]</span>
So, <span class="math inline">\(\frac{1}{n}\ell_n&#39;&#39;(\theta)\)</span> is an unbiased estimator for <span class="math inline">\(-\mathcal{J}(\theta)\)</span> and thus
<span class="math display">\[
E\left(\left(\frac{1}{n}\ell_n&#39;&#39;(\theta) -\left(-\mathcal{J}(\theta)\right)\right)^2\right)=\V\left(\frac{1}{n}\ell_n&#39;&#39;(\theta)\right)\to 0\quad\text{as}\quad n\to\infty
\]</span>
That is <span class="math inline">\(\frac{1}{n}\ell_n&#39;&#39;(\theta)\)</span> is mean square consistent
<span class="math display">\[
\frac{1}{n}\ell_n&#39;&#39;(\theta)\to_{m.s.} -\mathcal{J}(\theta)\quad \hbox{as}\quad n\to\infty
\]</span>
which implies that <span class="math inline">\(\frac{1}{n}\ell_n&#39;&#39;(\theta)\)</span> is also (weakly) consistent
<span class="math display">\[
\frac{1}{n}\ell_n&#39;&#39;(\theta)\to_p -\mathcal{J}(\theta)\quad \hbox{as}\quad n\to\infty
\]</span>
since mean square convergence implies convergence in probability.</p>
<p>Remember: We wanted to study <span class="math inline">\(\ell_n&#39;&#39;(\psi_n)\)</span> in  not <span class="math inline">\(\frac{1}{n}\ell_n&#39;&#39;(\theta)\)</span>, but we are actually close now. We know that the ML estimator <span class="math inline">\(\hat\theta_n\)</span> is (weakly) consistent, i.e., <span class="math inline">\(\hat\theta_n\to_p\theta\)</span>. From this it follows that also <span class="math inline">\(\psi_n\to_p \theta\)</span> since <span class="math inline">\(\psi_p\)</span> is a value between <span class="math inline">\(\hat\theta_n\)</span> and <span class="math inline">\(\theta\)</span> (Mean Value Theorem). Therefore, we have that also
<span class="math display">\[
\frac{1}{n}\ell_n&#39;&#39;(\psi_n)\to_p -\mathcal{J}(\theta)\quad \hbox{ as }\quad n\to\infty.
\]</span>
Multiplying by <span class="math inline">\(-1/\sqrt{ \mathcal{J}(\theta)}\)</span> yields
<span class="math display">\[
\frac{-\frac{1}{n}\ell_n&#39;&#39;(\psi_n)}{\sqrt{ \mathcal{J}(\theta)}}=
n^{-1/2}\frac{-\ell_n&#39;&#39;(\psi_n)}{\sqrt{n \cdot \mathcal{J}(\theta)}}\to_p \sqrt{ \mathcal{J}(\theta)}.
\]</span>
Rewriting the quotient in  a little bit yields
<span class="math display">\[
\frac{-\ell_n&#39;&#39;(\psi_n)}{\sqrt{n \cdot \mathcal{J}(\theta)}}(\hat{\theta}_n-\theta)=
\underbrace{n^{-1/2}\frac{-\ell_n&#39;&#39;(\psi_n)}{\sqrt{n \cdot \mathcal{J}(\theta)}}}_{\to_p \sqrt{ \mathcal{J}(\theta)}}\cdot n^{1/2}(\hat{\theta}_n-\theta).
\]</span>
Thus we can conclude that (using ):
<span class="math display">\[
\sqrt{ \mathcal{J}(\theta)}n^{1/2}(\hat{\theta}_n-\theta)\to_d N(0,1),
\]</span>
or equivalently
<span class="math display">\[(\hat{\theta}_n-\theta)\to_d N\left(0,\frac{1}{n \mathcal{J}(\theta)}\right)\]</span>
with <span class="math inline">\(n \mathcal{J}(\theta)=-E(\ell_n&#39;&#39;(\theta))=\mathcal{I}(\theta)\)</span> which is the asymptotic normality result we aimed, where <span class="math inline">\(\mathcal{I}(\theta)\)</span> is called the .</p>
<p>The above arguments can easily be generalized to multidimensional parameter vectors <span class="math inline">\(\theta\in\mathbb{R}^K\)</span>. In this case, <span class="math inline">\(\mathcal{J}(\theta)\)</span> becomes a <span class="math inline">\(K\times K\)</span> matrix, and
<span class="math display">\[\hat{\theta}_n-\theta\to_dN_K\left(0,\frac{1}{n} \mathcal{J}(\theta)^{-1}\right),\]</span>
where <span class="math inline">\(n\mathcal{J}(\theta)=-E(\ell_n&#39;&#39;(\theta))=\mathcal{I}(\theta)\)</span> is called .</p>
<!-- \paragraph*{Example:} Assume an i.i.d. sample $X_1,\dots,X_n$ from an exponential distribution, i.e. the underlying density of $X_i$ is given by $f(x|\theta)=\theta\exp(-\theta x)$. We then have $\mu:=E(X_i)=\frac{1}{\theta}$ as well as $\sigma^2_X:=\textrm{var}(X_i)=\frac{1}{\theta^2}$. The -->
<!-- log-likelihood functions is given by  -->
<!-- $$l(\theta)=\sum_{i=1}^n \ln (\theta\exp(-\theta X_i)))=n \ln \theta -\sum_{i=1}^n \theta X_i$$ -->
<!-- $$\Rightarrow \quad \ell_n'(\theta)=n\frac{1}{\theta} + \sum_{i=1}^n X_i.$$ -->
<!-- As already mentioned above, the maximum-likelihood estimator of $\theta$ then is $\hat\theta_n=\frac{1}{\bar X}$. -->
<!-- Inference may then be based on likelihood-theory. We have -->
<!-- $$\mathcal{J}(\theta)=-\frac{1}{n}E(\ell''(\theta))=\frac{1}{\theta^2},$$ -->
<!-- and by the above theorem -->
<!-- $$\frac{1}{\bar X}-\theta\sim AN(0,\frac{1}{n \mathcal{J}(\theta)})\overset{a}{\sim}AN(0,\frac{\theta^2}{n}).$$ -->
<!-- This obviously coincides with the result obtained by the delta-method. -->
<!-- ## Discussion of Assumptions and Results {-} -->
<!-- \begin{itemize} -->
<!-- \item \textbf{Strict exogeneity}:  Needed to assume $\E[\eps | X]=0$ to show consistency of $\hat\beta_{ML}$.  -->
<!-- \item \textbf{Homoskedasticity and non-autocorrelation}:  We used the assumption that $\E[\eps eps']\sim(0, \sigma^2 I)$ to derive estimator of $\sigma^2$.   -->
<!-- \item \textbf{Normality}:  The normality assumption is used \textbf{only} to derive small-sample properties of the estimators. By using asymptotic arguments one can show that both $\hat\beta_{ML}$ and $s_{ML}^2$ will be distributed -->
<!-- asymptotically normally also without the normality assumption. -->
<!-- \end{itemize} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Best Linear Unbiased Estimator} -->
<!-- Given our assumptions, then by the Gauss-Markov theorem, it is possible to show that  -->
<!-- \begin{itemize}  -->
<!-- \item<1->$\hat\beta$ is the Best Linear Unbiased (BLUE) estimator of $\beta$ -->
<!-- \item<2-> The best linear unbiased estimator of any linear combination of the $\beta$'s is the same linear combination -->
<!-- of the $\hat\beta$'s. -->
<!-- \item<3-> The Best Linear Unbiased Predictor (BLUP) of $Y$ based on the vector $X_s$ is $\hat y_s=X'_s\hat\beta$ -->
<!-- \end{itemize} -->
<!-- \end{frame} -->
<!-- ## Hypothesis Testing -->
<!-- ### Testing Hypotheses about One Parameter -->
<!-- \noindent\textbf{Definition of the Score} -->
<!-- Define the \textbf{score of the log likelihood} (also known as the \textbf{gradient vector} -->
<!-- for observation $i$ -->
<!-- \begin{equation*} -->
<!-- s_i(\beta)\equiv \left(\dfrac{\partial L_i}{\partial \beta_0}(\beta), \dfrac{\partial L_i}{\partial \beta_1}(\beta), \dots, \dfrac{\partial L_i}{\partial \beta_k}(\beta)\right)' -->
<!-- \end{equation*} -->
<!-- %In the logit and probit cases, this can be shown to be -->
<!-- %\begin{equation*} -->
<!-- %s_i(\beta)\equiv\dfrac{g(x_i\beta)[y_i-G(x_i\beta)]} -->
<!-- %{G(x_i\beta)[1-G(x_i\beta)]}x_i' -->
<!-- %\end{equation*} -->
<!-- %Since $x_i$ is $1 \times (k+1)$, the score is a $(k+1) \times 1$ vector.  Recalling that in the probit %case -->
<!-- %\begin{center} -->
<!-- %$g(z)=\phi(z)$ and $G(z)=\Phi(z)$ -->
<!-- %\end{center} -->
<!-- %while with logit -->
<!-- %\begin{center} -->
<!-- %$g(z)=\exp(z)/[1+\exp(z)]^2$ and $G(z)=\exp(z)/[1+\exp(z)]$. -->
<!-- %\end{center} -->
<!-- #### Variance-Covariance Matrix {-} -->
<!-- Using the standard maximum likelihood theory it can be -->
<!-- show that the asymptotic-variance covariance matrix of the MLE $\hat\beta_{ML}$ is given by -->
<!-- \begin{equation*} -->
<!-- \text{Asy.~Var}(\hat\beta_{ML})=\left[\sum_{i=1}^N s_i(\hat\beta)s_i(\hat\beta)'\right]^{-1} -->
<!-- \end{equation*} -->
<!-- %and therefore in our case we have -->
<!-- %\begin{equation*} -->
<!-- %\text{Asy. Var-Cov}(\hat\beta)=\left[\sum_{i=1}^N\dfrac{[g(x_i\hat\beta)]^2 x_i' x_i}{G(x_i\hat\beta) -->
<!-- %[1-G(x_i\hat\beta)]}\right]^{-1} -->
<!-- %\end{equation*} -->
<!-- %with $g(\cdot)$ and $G(\cdot)$ defined as above. -->
<!-- %\vskip .1in -->
<!-- The square roots of the diagonals of this matrix will give us the -->
<!-- \textbf{standard errors} of the estimates. -->
<!-- \frametitle{Cramer-Rao Lower Bound} -->
<!-- Fisher, Cramer, and Rao showed that for any unbiased estimator $\hat\theta$, its variance-covariance -->
<!-- matrix cannot be smaller than $I^{-1}(\theta)$ where $I(\theta)$ is the \textbf{information matrix} -->
<!-- of the estimator, given by  -->
<!-- $$I(\theta) \equiv E[s(y,\theta)s(y,\theta)']$$ -->
<!-- where $s(\cdot)$ is the gradient or score.  Thus, the MLE attains the Cramer-Rao lower bound and will therefore be asymptotically efficient. -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Asymptotic Distribution} -->
<!-- Now, by the usual asymptotic theory, we have -->
<!-- \begin{equation*} -->
<!-- \dfrac{\hat\beta_j - \beta_j^0}{\text{std. err.}(\hat\beta_j)}\stackrel{a}{\sim} \mathcal{N}(0,1) -->
<!-- \end{equation*} -->
<!-- where $\beta_j^0$ is the value of the parameter under the null hypothesis. -->
<!-- So, we can do our usual "$t$-tests" although because we rely on asymptotics, -->
<!-- they should probably be more properly called $z$-tests. -->
<!-- \end{frame} -->
<!-- \subsection{Testing Hypotheses about Multiple Parameters} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Testing Joint Hypotheses} -->
<!-- We may also want to test hypotheses about multiple parameters.  Here it will -->
<!-- be useful to think about the regressions implied by imposing the restrictions. -->
<!-- So, for example,  -->
<!-- \begin{equation*} -->
<!-- \begin{array}{ll} -->
<!-- H_0: & R\beta - r = 0\\ -->
<!-- H_A: & H_0 \text{ is not true} \\ -->
<!-- \end{array} -->
<!-- \end{equation*} -->
<!-- where $R$ is a $q \times (k+1)$ matrix that defines the $q$ restrictions placed on the parameters -->
<!-- under the null hypothesis and $r$ is a $q \times 1$ vector of constants. -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Restricted and Unrestricted Regressions} -->
<!-- We will define the \textbf{restricted regression} as one in which we force -->
<!-- the $R\hat\beta$ to be equal to  -->
<!-- $r$ (i.e. under the null hypothesis), and the -->
<!-- \textbf{unrestricted regression} to be one in which we allow the data to tell -->
<!-- us what the values of $\beta$ should be. -->
<!-- \vskip .2in -->
<!-- Define $L_r$ as the log-likelihood corresponding to the restricted regeression -->
<!-- and $L_u$ as the log-likelihood corresponding to the unrestricted regression. -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Three Asymptotically Equivalent Tests} -->
<!-- We will discuss three asymptotically equivalent tests: -->
<!-- \begin{itemize} -->
<!-- \item \textbf{Wald test}: based on the unrestricted regression -->
<!-- \item \textbf{Likelihood ratio test}: based on both the restricted and unrestrcited regressions -->
<!-- \item \textbf{Lagrange multiplier test}: based on the restricted regression. -->
<!-- \end{itemize} -->
<!-- All three tests will give us the same answer asymptotically, but will differ -->
<!-- in their values in finite samples. -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Wald Test (1)} -->
<!-- From maximum likelihood theory, we know that  -->
<!-- \begin{equation*} -->
<!-- \hat\beta \adist \mathcal{N}(\beta,V) -->
<!-- \end{equation*} -->
<!-- and therefore that $R\hat\beta$ also has an asymptotically normal distribution -->
<!-- (since it is just a linear combination of asymptotically normal variables): -->
<!-- \begin{equation*} -->
<!-- (R\hat\beta - R\beta) \adist \mathcal{N}(0, RVR') -->
<!-- \end{equation*} -->
<!-- This suggests a quadratic form which we can use to test hypotheses -->
<!-- \begin{equation*} -->
<!-- W\equiv(R\hat\beta - r)'[R \hat V_u R']^{-1}(R\hat\beta - r) \adist \chi_q^2 -->
<!-- \end{equation*} -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Wald Test (2)} -->
<!-- Thus, with the \textbf{Wald test}, we need only estimate the \textit{unrestricted} regression. -->
<!-- \vskip .25in -->
<!-- It measures how far apart the estimated parameters are from the values of  -->
<!-- the parameters under the null hypothesis. -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Likelihood Ratio Test} -->
<!-- More conceptually simple, perhaps, is the \textbf{Likelihood Ratio Test}. -->
<!-- \vskip .15in -->
<!-- If the null hypothesis holds, imposing restrictions on the data should lead -->
<!-- to values of $L_r$ and $L_u$ that are ``close''.  The question then, is what -->
<!-- metric to use to judget how ``close '' they are. -->
<!-- \vskip .15in -->
<!-- It can be shown that -->
<!-- \begin{equation*} -->
<!-- LR\equiv -2 [L_r - L_u] \adist \chi^2_q -->
<!-- \end{equation*} -->
<!-- Therefore the $\chi^2_q$ distribution is the proper metric for judging how close -->
<!-- the likelihoods are. -->
<!-- \vskip .15in -->
<!-- We must fit both models to calculate the differences between the restricted -->
<!-- and restricted likelihoods. -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Lagrange Multiplier Test: Motivation} -->
<!-- The \textbf{Lagrange Multiplier Test} (also called the \textbf{Score Test}) is based -->
<!-- on the score, or gradient, vector (as defined earlier).  The idea is to measure -->
<!-- how far away from the peak of the \textit{unrestricted} likelihood imposing the -->
<!-- restrctions forces us, which is some akin to the notion of the likelihood ratio -->
<!-- test.  -->
<!-- \vskip.15in -->
<!-- At the peak of the unrestricted log likelihood, the score would be a vector of -->
<!-- zeros.  Intuitively, then, the Lagrante Multiplier Test will measure how ``close'' -->
<!-- the score vector when we estimate the \textit{restricted} regression is to  -->
<!-- the vector of zeroes. -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Lagrange Multiplier Test: Derivation (1)} -->
<!-- We can think about finding the maximum of the log likelihood subject to -->
<!-- the constraints imposed by the null hypothesis.  To simplify things, suppose we have only two -->
<!-- parameters, $\beta_1$ and $\beta_2$ with $H_0: \beta_2=c$. -->
<!-- Then: -->
<!-- \begin{equation*} -->
<!-- H(\beta, \lambda)=\sum_{i=1}^N  L_i(\beta) - \lambda'(\beta_2-c) -->
<!-- \end{equation*} -->
<!-- where $\lambda$ is the Lagrange multiplier.  Then the first order conditions -->
<!-- are -->
<!-- \begin{align*} -->
<!-- \sum_{i=1}^N  \dfrac{\partial L_i (\beta)}{\partial \beta_1} \big\vert_{\tilde\beta} -->
<!-- &=\sum_{i=1}^N s_{i1}(\tilde\beta)=0\\ -->
<!-- \tilde\lambda=\sum_{i=1}^N \dfrac{\partial L_i (\beta)}{\partial \beta_1} \big\vert_{\tilde\beta} &=\sum_{i=1}^N s_{i2}(\tilde\beta)\\ -->
<!-- \end{align*} -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Lagrange Multiplier Test: Derivation (2)} -->
<!-- Define $s_{i1}$ and $s_{i2}$ are the subvectors of $s_i(\beta)$ corresponding to  -->
<!-- $\beta_1$ and $\beta_2$, respectively. -->
<!-- \vskip .15in -->
<!-- So we are in some sense testing whether $\tilde\lambda$ is ``close'' to zero or -->
<!-- not, evaluated at the restricted values of the parameters. -->
<!-- \vskip .15in -->
<!-- It's possible to show, then, that -->
<!-- \begin{equation*} -->
<!-- LM\equiv  s'(\tilde\beta) \tilde V_r^{-1} s(\tilde\beta) \adist \chi^2_q -->
<!-- \end{equation*} -->
<!-- where $s(\tilde\beta)$ is the score evaluated at the \textit{restricted} estimates of -->
<!-- the parameters, and $\tilde V_r$ is the estimated variance-covariance matrix from the \textit{restricted} regression. -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Relationshiop between W, LR, and LM tests} -->
<!-- \includegraphics[angle=90, scale=.60]{wald-lm-lr.ps} -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Relationship between W, LR, and LM} -->
<!-- While all three tests are asymptotically equivalent, it can be shown that in finite -->
<!-- samples -->
<!-- \begin{center} -->
<!-- $LM < LR < W$ -->
<!-- \end{center} -->
<!-- meaning that LM tests will favor not rejecting the null and W tests will favor rejecting -->
<!-- the null. -->
<!-- \end{frame} -->
<!-- \end{document} -->
<!-- \section{Goodness of Fit Measures} -->
<!-- \subsection{Goodness of Fit Measures} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Goodness of Fit in Probit and Logit} -->
<!-- As in the linear regression model, we would like to have some measure -->
<!-- of how well our model fits the data.  Unlike linear models, however, where -->
<!-- $R^2$ serves as the primary goodness-of-fit measure, there is no -->
<!-- standard metric that is used. -->
<!-- \vskip .15in -->
<!-- Now, define $L_0$ as the log likelihood of a model in which we constrain -->
<!-- all of the coefficients (except the constant) to be equal to zero. -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- %------------------------------------------------- -->
<!-- %\begin{frame} -->
<!-- %\frametitle{A Note on $L_0$} -->
<!-- %Note that we do not actually need to run a  regression to estimate $L_0$. -->
<!-- %\vskip .15in -->
<!-- %With just a constant term in the model, the likelihood function is given by -->
<!-- %\begin{align*} -->
<!-- %L_0&=\sum y_i \ln(N_1/N) + \sum (1-y_i) \ln(1-N_1/N)\\ -->
<!--  %    &=N_1 \ln(N_1/N) + N_0\ln(N_0/N)\\ -->
<!-- %\end{align*} -->
<!-- %where $N_1$ indicates the number of success and $N_0$ is the number of failures. -->
<!-- %\end{frame} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{Pseudo-$R^2$} -->
<!-- The first goodness-of-fit measure is meant as an analog to the $R^2$ from -->
<!-- linear regression, called the pseudo-$R^2$.  It is defined as -->
<!-- \begin{equation*} -->
<!-- \text{pseudo}-R^2=1-\dfrac{1}{1+2(L_u - L_0)/N} -->
<!-- \end{equation*} -->
<!-- Intuitively, the greater the distance between the restricted and -->
<!-- unrestricted log likelihoods, the more the model explains the variation -->
<!-- in $y$, and the greater the pseudo-$R^2$ will be. -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- \begin{frame} -->
<!-- \frametitle{McFadden's $R^2$} -->
<!-- McFadden suggested an alternative goodness of fit-measures: -->
<!-- \begin{equation*} -->
<!-- \text{McFadden}-R^2= 1- L_u/L_0 -->
<!-- \end{equation*} -->
<!-- since the log likelihood is just the sum of log probabilities, it must be that -->
<!-- $L_0 < L_u < 0$. -->
<!-- \end{frame} -->
<!-- %------------------------------------------------- -->
<!-- %\begin{frame} -->
<!-- %\frametitle{Proportion of Correct Predictions} -->
<!-- %An additional measure of the fit of the model is the number of observations for -->
<!-- %which the model correctly predicts the outcome. -->
<!-- %\end{frame} -->

</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="consistency-of-hatbeta_ml-and-s_ml2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ivr.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Script_Econometrics_MSc.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed",
"download": "https://uni-bonn.sciebo.de/s/IZ2yeLeA4dRTFUY",
"search": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
