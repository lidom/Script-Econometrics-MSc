<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.1 Probability Theory | Econometrics (M.Sc.)</title>
  <meta name="description" content="2.1 Probability Theory | Econometrics (M.Sc.)" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="2.1 Probability Theory | Econometrics (M.Sc.)" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/mylogo.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.1 Probability Theory | Econometrics (M.Sc.)" />
  
  
  <meta name="twitter:image" content="/images/mylogo.png" />

<meta name="author" content="Prof. Dr. Dominik Liebl" />


<meta name="date" content="2021-10-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2-review-probability-and-statistics.html"/>
<link rel="next" href="2.2-random-variables.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="organization-of-the-course.html"><a href="organization-of-the-course.html"><i class="fa fa-check"></i>Organization of the Course</a></li>
<li class="chapter" data-level="" data-path="literature.html"><a href="literature.html"><i class="fa fa-check"></i>Literature</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-introduction-to-r.html"><a href="1-introduction-to-r.html"><i class="fa fa-check"></i><b>1</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1.1-short-glossary.html"><a href="1.1-short-glossary.html"><i class="fa fa-check"></i><b>1.1</b> Short Glossary</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-first-steps.html"><a href="1.2-first-steps.html"><i class="fa fa-check"></i><b>1.2</b> First Steps</a></li>
<li class="chapter" data-level="1.3" data-path="1.3-further-data-objects.html"><a href="1.3-further-data-objects.html"><i class="fa fa-check"></i><b>1.3</b> Further Data Objects</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-simple-regression-analysis-using-r.html"><a href="1.4-simple-regression-analysis-using-r.html"><i class="fa fa-check"></i><b>1.4</b> Simple Regression Analysis using R</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-programming-in-r.html"><a href="1.5-programming-in-r.html"><i class="fa fa-check"></i><b>1.5</b> Programming in R</a></li>
<li class="chapter" data-level="1.6" data-path="1.6-r-packages.html"><a href="1.6-r-packages.html"><i class="fa fa-check"></i><b>1.6</b> R-packages</a></li>
<li class="chapter" data-level="1.7" data-path="1.7-tidyverse.html"><a href="1.7-tidyverse.html"><i class="fa fa-check"></i><b>1.7</b> Tidyverse</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="1.7-tidyverse.html"><a href="1.7-tidyverse.html#tidyverse-plotting-basics"><i class="fa fa-check"></i><b>1.7.1</b> Tidyverse: Plotting Basics</a></li>
<li class="chapter" data-level="1.7.2" data-path="1.7-tidyverse.html"><a href="1.7-tidyverse.html#tidyverse-data-wrangling-basics"><i class="fa fa-check"></i><b>1.7.2</b> Tidyverse: Data Wrangling Basics</a></li>
<li class="chapter" data-level="1.7.3" data-path="1.7-tidyverse.html"><a href="1.7-tidyverse.html#the-pipe-operator"><i class="fa fa-check"></i><b>1.7.3</b> The pipe operator <code>%&gt;%</code></a></li>
<li class="chapter" data-level="1.7.4" data-path="1.7-tidyverse.html"><a href="1.7-tidyverse.html#the-group_by-function"><i class="fa fa-check"></i><b>1.7.4</b> The <code>group_by()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="1.8-further-links.html"><a href="1.8-further-links.html"><i class="fa fa-check"></i><b>1.8</b> Further Links</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="1.8-further-links.html"><a href="1.8-further-links.html#further-r-intros"><i class="fa fa-check"></i><b>1.8.1</b> Further R-Intros</a></li>
<li class="chapter" data-level="1.8.2" data-path="1.8-further-links.html"><a href="1.8-further-links.html#version-control-gitgithub"><i class="fa fa-check"></i><b>1.8.2</b> Version Control (Git/GitHub)</a></li>
<li class="chapter" data-level="1.8.3" data-path="1.8-further-links.html"><a href="1.8-further-links.html#r-ladies"><i class="fa fa-check"></i><b>1.8.3</b> R-Ladies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-review-probability-and-statistics.html"><a href="2-review-probability-and-statistics.html"><i class="fa fa-check"></i><b>2</b> Review: Probability and Statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2.1-probability-theory.html"><a href="2.1-probability-theory.html"><i class="fa fa-check"></i><b>2.1</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="2.1-probability-theory.html"><a href="2.1-probability-theory.html#sample-spaces-and-events"><i class="fa fa-check"></i><b>2.1.1</b> Sample Spaces and Events</a></li>
<li class="chapter" data-level="2.1.2" data-path="2.1-probability-theory.html"><a href="2.1-probability-theory.html#probability"><i class="fa fa-check"></i><b>2.1.2</b> Probability</a></li>
<li class="chapter" data-level="2.1.3" data-path="2.1-probability-theory.html"><a href="2.1-probability-theory.html#independent-events"><i class="fa fa-check"></i><b>2.1.3</b> Independent Events</a></li>
<li class="chapter" data-level="2.1.4" data-path="2.1-probability-theory.html"><a href="2.1-probability-theory.html#conditional-probability"><i class="fa fa-check"></i><b>2.1.4</b> Conditional Probability</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2.2-random-variables.html"><a href="2.2-random-variables.html"><i class="fa fa-check"></i><b>2.2</b> Random Variables</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="2.2-random-variables.html"><a href="2.2-random-variables.html#univariate-distribution-and-probability-functions"><i class="fa fa-check"></i><b>2.2.1</b> Univariate Distribution and Probability Functions</a></li>
<li class="chapter" data-level="2.2.2" data-path="2.2-random-variables.html"><a href="2.2-random-variables.html#multivariate-distribution-and-probability-functions"><i class="fa fa-check"></i><b>2.2.2</b> Multivariate Distribution and Probability Functions</a></li>
<li class="chapter" data-level="2.2.3" data-path="2.2-random-variables.html"><a href="2.2-random-variables.html#means-and-moments"><i class="fa fa-check"></i><b>2.2.3</b> Means and Moments</a></li>
<li class="chapter" data-level="2.2.4" data-path="2.2-random-variables.html"><a href="2.2-random-variables.html#unconditional-means"><i class="fa fa-check"></i><b>2.2.4</b> Unconditional Means</a></li>
<li class="chapter" data-level="2.2.5" data-path="2.2-random-variables.html"><a href="2.2-random-variables.html#conditional-means"><i class="fa fa-check"></i><b>2.2.5</b> Conditional Means</a></li>
<li class="chapter" data-level="2.2.6" data-path="2.2-random-variables.html"><a href="2.2-random-variables.html#means-of-transformed-random-variables-and-moments"><i class="fa fa-check"></i><b>2.2.6</b> Means of Transformed Random Variables and Moments</a></li>
<li class="chapter" data-level="2.2.7" data-path="2.2-random-variables.html"><a href="2.2-random-variables.html#independent-random-variables"><i class="fa fa-check"></i><b>2.2.7</b> Independent Random Variables</a></li>
<li class="chapter" data-level="2.2.8" data-path="2.2-random-variables.html"><a href="2.2-random-variables.html#i.i.d.-samples"><i class="fa fa-check"></i><b>2.2.8</b> I.I.D. Samples</a></li>
<li class="chapter" data-level="2.2.9" data-path="2.2-random-variables.html"><a href="2.2-random-variables.html#some-important-discrete-random-variables"><i class="fa fa-check"></i><b>2.2.9</b> Some Important Discrete Random Variables</a></li>
<li class="chapter" data-level="2.2.10" data-path="2.2-random-variables.html"><a href="2.2-random-variables.html#some-important-continuous-random-variables"><i class="fa fa-check"></i><b>2.2.10</b> Some Important Continuous Random Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch:SLR.html"><a href="3-ch:SLR.html"><i class="fa fa-check"></i><b>3</b> Review: Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3.1-the-simple-linear-regression-model.html"><a href="3.1-the-simple-linear-regression-model.html"><i class="fa fa-check"></i><b>3.1</b> The Simple Linear Regression Model</a>
<ul>
<li class="chapter" data-level="" data-path="3.1-the-simple-linear-regression-model.html"><a href="3.1-the-simple-linear-regression-model.html#the-data-generating-process"><i class="fa fa-check"></i>The Data-Generating Process</a></li>
<li class="chapter" data-level="3.1.1" data-path="3.1-the-simple-linear-regression-model.html"><a href="3.1-the-simple-linear-regression-model.html#assumptions-about-the-error-term"><i class="fa fa-check"></i><b>3.1.1</b> Assumptions About the Error Term</a></li>
<li class="chapter" data-level="3.1.2" data-path="3.1-the-simple-linear-regression-model.html"><a href="3.1-the-simple-linear-regression-model.html#the-population-regression-line"><i class="fa fa-check"></i><b>3.1.2</b> The Population Regression Line</a></li>
<li class="chapter" data-level="3.1.3" data-path="3.1-the-simple-linear-regression-model.html"><a href="3.1-the-simple-linear-regression-model.html#terminology-estimates-versus-estimators"><i class="fa fa-check"></i><b>3.1.3</b> Terminology: Estimates versus Estimators</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3.2-ordinary-least-squares-estimation.html"><a href="3.2-ordinary-least-squares-estimation.html"><i class="fa fa-check"></i><b>3.2</b> Ordinary Least Squares Estimation</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-ordinary-least-squares-estimation.html"><a href="3.2-ordinary-least-squares-estimation.html#terminology-sample-regression-line-prediction-and-residuals"><i class="fa fa-check"></i><b>3.2.1</b> Terminology: Sample Regression Line, Prediction and Residuals</a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-ordinary-least-squares-estimation.html"><a href="3.2-ordinary-least-squares-estimation.html#sec:SLROLS"><i class="fa fa-check"></i><b>3.2.2</b> Deriving the Expression of the OLS Estimator</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-ordinary-least-squares-estimation.html"><a href="3.2-ordinary-least-squares-estimation.html#behavior-of-the-ols-estimates-for-resampled-data-conditionally-on-x_i"><i class="fa fa-check"></i><b>3.2.3</b> Behavior of the OLS Estimates for Resampled Data (conditionally on <span class="math inline">\(X_i\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-properties-of-the-ols-estimator.html"><a href="3.3-properties-of-the-ols-estimator.html"><i class="fa fa-check"></i><b>3.3</b> Properties of the OLS Estimator</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-properties-of-the-ols-estimator.html"><a href="3.3-properties-of-the-ols-estimator.html#mean-and-bias-of-the-ols-estimator"><i class="fa fa-check"></i><b>3.3.1</b> Mean and Bias of the OLS Estimator</a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-properties-of-the-ols-estimator.html"><a href="3.3-properties-of-the-ols-estimator.html#variance-of-the-ols-estimator"><i class="fa fa-check"></i><b>3.3.2</b> Variance of the OLS Estimator</a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-properties-of-the-ols-estimator.html"><a href="3.3-properties-of-the-ols-estimator.html#consistency-of-the-ols-estimator"><i class="fa fa-check"></i><b>3.3.3</b> Consistency of the OLS Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch:MLR.html"><a href="4-ch:MLR.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4.1-assumptions.html"><a href="4.1-assumptions.html"><i class="fa fa-check"></i><b>4.1</b> Assumptions</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-assumptions.html"><a href="4.1-assumptions.html#some-implications-of-the-exogeneity-assumption"><i class="fa fa-check"></i><b>4.1.1</b> Some Implications of the Exogeneity Assumption</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-deriving-the-expression-of-the-ols-estimator.html"><a href="4.2-deriving-the-expression-of-the-ols-estimator.html"><i class="fa fa-check"></i><b>4.2</b> Deriving the Expression of the OLS Estimator</a></li>
<li class="chapter" data-level="4.3" data-path="4.3-some-quantities-of-interest.html"><a href="4.3-some-quantities-of-interest.html"><i class="fa fa-check"></i><b>4.3</b> Some Quantities of Interest</a></li>
<li class="chapter" data-level="4.4" data-path="4.4-method-of-moments-estimator.html"><a href="4.4-method-of-moments-estimator.html"><i class="fa fa-check"></i><b>4.4</b> Method of Moments Estimator</a></li>
<li class="chapter" data-level="4.5" data-path="4.5-unbiasedness-of-hatbetax.html"><a href="4.5-unbiasedness-of-hatbetax.html"><i class="fa fa-check"></i><b>4.5</b> Unbiasedness of <span class="math inline">\(\hat\beta|X\)</span></a></li>
<li class="chapter" data-level="4.6" data-path="4.6-ch:VarEstBeta.html"><a href="4.6-ch:VarEstBeta.html"><i class="fa fa-check"></i><b>4.6</b> Variance of <span class="math inline">\(\hat\beta|X\)</span></a></li>
<li class="chapter" data-level="4.7" data-path="4.7-the-gauss-markov-theorem.html"><a href="4.7-the-gauss-markov-theorem.html"><i class="fa fa-check"></i><b>4.7</b> The Gauss-Markov Theorem</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-ch:SSINF.html"><a href="5-ch:SSINF.html"><i class="fa fa-check"></i><b>5</b> Small Sample Inference</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5.1-ch:testmultp.html"><a href="5.1-ch:testmultp.html"><i class="fa fa-check"></i><b>5.1</b> Hypothesis Tests about Multiple Parameters</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="5.1-ch:testmultp.html"><a href="5.1-ch:testmultp.html#the-test-statistic-and-its-null-distribution"><i class="fa fa-check"></i><b>5.1.1</b> The Test Statistic and its Null Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5.2-ch:testingsinglep.html"><a href="5.2-ch:testingsinglep.html"><i class="fa fa-check"></i><b>5.2</b> Tests about One Parameter</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-testtheory.html"><a href="5.3-testtheory.html"><i class="fa fa-check"></i><b>5.3</b> Testtheory</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="5.3-testtheory.html"><a href="5.3-testtheory.html#significance-level"><i class="fa fa-check"></i><b>5.3.1</b> Significance Level</a></li>
<li class="chapter" data-level="5.3.2" data-path="5.3-testtheory.html"><a href="5.3-testtheory.html#critical-value-for-the-f-test"><i class="fa fa-check"></i><b>5.3.2</b> Critical Value for the <span class="math inline">\(F\)</span>-Test</a></li>
<li class="chapter" data-level="5.3.3" data-path="5.3-testtheory.html"><a href="5.3-testtheory.html#critical-values-for-the-t-test"><i class="fa fa-check"></i><b>5.3.3</b> Critical Value(s) for the <span class="math inline">\(t\)</span>-Test</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5.4-type-ii-error-and-power.html"><a href="5.4-type-ii-error-and-power.html"><i class="fa fa-check"></i><b>5.4</b> Type II Error and Power</a></li>
<li class="chapter" data-level="5.5" data-path="5.5-p-value.html"><a href="5.5-p-value.html"><i class="fa fa-check"></i><b>5.5</b> <span class="math inline">\(p\)</span>-Value</a></li>
<li class="chapter" data-level="5.6" data-path="5.6-CIsmallsample.html"><a href="5.6-CIsmallsample.html"><i class="fa fa-check"></i><b>5.6</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.7" data-path="5.7-PSSI.html"><a href="5.7-PSSI.html"><i class="fa fa-check"></i><b>5.7</b> Practice: Small Sample Inference</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="5.7-PSSI.html"><a href="5.7-PSSI.html#normally-distributed-hatbetax"><i class="fa fa-check"></i><b>5.7.1</b> Normally Distributed <span class="math inline">\(\hat\beta|X\)</span></a></li>
<li class="chapter" data-level="5.7.2" data-path="5.7-PSSI.html"><a href="5.7-PSSI.html#testing-multiple-parameters"><i class="fa fa-check"></i><b>5.7.2</b> Testing Multiple Parameters</a></li>
<li class="chapter" data-level="5.7.3" data-path="5.7-PSSI.html"><a href="5.7-PSSI.html#dualty-of-confidence-intervals-and-hypothesis-tests"><i class="fa fa-check"></i><b>5.7.3</b> Dualty of Confidence Intervals and Hypothesis Tests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-ch:LSINF.html"><a href="6-ch:LSINF.html"><i class="fa fa-check"></i><b>6</b> Large Sample Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="6.1-tools-for-asymptotic-statistics.html"><a href="6.1-tools-for-asymptotic-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Tools for Asymptotic Statistics</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-tools-for-asymptotic-statistics.html"><a href="6.1-tools-for-asymptotic-statistics.html#modes-of-convergence"><i class="fa fa-check"></i><b>6.1.1</b> Modes of Convergence</a></li>
<li class="chapter" data-level="" data-path="6.1-tools-for-asymptotic-statistics.html"><a href="6.1-tools-for-asymptotic-statistics.html#four-important-modes-of-convergence"><i class="fa fa-check"></i>Four Important Modes of Convergence</a></li>
<li class="chapter" data-level="" data-path="6.1-tools-for-asymptotic-statistics.html"><a href="6.1-tools-for-asymptotic-statistics.html#relations-among-modes-of-convergence"><i class="fa fa-check"></i>Relations among Modes of Convergence</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-tools-for-asymptotic-statistics.html"><a href="6.1-tools-for-asymptotic-statistics.html#continuous-mapping-theorem-cmt"><i class="fa fa-check"></i><b>6.1.2</b> Continuous Mapping Theorem (CMT)</a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-tools-for-asymptotic-statistics.html"><a href="6.1-tools-for-asymptotic-statistics.html#slutsky-theorem"><i class="fa fa-check"></i><b>6.1.3</b> Slutsky Theorem</a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-tools-for-asymptotic-statistics.html"><a href="6.1-tools-for-asymptotic-statistics.html#law-of-large-numbers-lln-and-central-limit-theorem-clt"><i class="fa fa-check"></i><b>6.1.4</b> Law of Large Numbers (LLN) and Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-tools-for-asymptotic-statistics.html"><a href="6.1-tools-for-asymptotic-statistics.html#estimators-as-a-sequences-of-random-variables"><i class="fa fa-check"></i><b>6.1.5</b> Estimators as a Sequences of Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-asymptotics-under-the-classic-regression-model.html"><a href="6.2-asymptotics-under-the-classic-regression-model.html"><i class="fa fa-check"></i><b>6.2</b> Asymptotics under the Classic Regression Model</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="6.2-asymptotics-under-the-classic-regression-model.html"><a href="6.2-asymptotics-under-the-classic-regression-model.html#the-case-of-heteroscedasticity"><i class="fa fa-check"></i><b>6.2.1</b> The Case of Heteroscedasticity</a></li>
<li class="chapter" data-level="6.2.2" data-path="6.2-asymptotics-under-the-classic-regression-model.html"><a href="6.2-asymptotics-under-the-classic-regression-model.html#hypothesis-testing-and-confidence-intervals"><i class="fa fa-check"></i><b>6.2.2</b> Hypothesis Testing and Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6.3-robust-confidence-intervals.html"><a href="6.3-robust-confidence-intervals.html"><i class="fa fa-check"></i><b>6.3</b> Robust Confidence Intervals</a></li>
<li class="chapter" data-level="6.4" data-path="6.4-practice-large-sample-inference.html"><a href="6.4-practice-large-sample-inference.html"><i class="fa fa-check"></i><b>6.4</b> Practice: Large Sample Inference</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-practice-large-sample-inference.html"><a href="6.4-practice-large-sample-inference.html#normally-distributed-hatbeta-for-ntoinfty"><i class="fa fa-check"></i><b>6.4.1</b> Normally Distributed <span class="math inline">\(\hat\beta\)</span> for <span class="math inline">\(n\to\infty\)</span></a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-practice-large-sample-inference.html"><a href="6.4-practice-large-sample-inference.html#testing-multiple-and-single-parameters"><i class="fa fa-check"></i><b>6.4.2</b> Testing Multiple and Single Parameters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-maximum-likelihood.html"><a href="7-maximum-likelihood.html"><i class="fa fa-check"></i><b>7</b> Maximum Likelihood</a>
<ul>
<li class="chapter" data-level="7.1" data-path="7.1-likelihood-principle.html"><a href="7.1-likelihood-principle.html"><i class="fa fa-check"></i><b>7.1</b> Likelihood Principle</a></li>
<li class="chapter" data-level="7.2" data-path="7.2-properties-of-maximum-likelihood-estimators.html"><a href="7.2-properties-of-maximum-likelihood-estimators.html"><i class="fa fa-check"></i><b>7.2</b> Properties of Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="7.3" data-path="7.3-the-log-likelihood-function.html"><a href="7.3-the-log-likelihood-function.html"><i class="fa fa-check"></i><b>7.3</b> The (Log-)Likelihood Function</a></li>
<li class="chapter" data-level="7.4" data-path="7.4-optimization-non-analytical-solutions.html"><a href="7.4-optimization-non-analytical-solutions.html"><i class="fa fa-check"></i><b>7.4</b> Optimization: Non-Analytical Solutions</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-optimization-non-analytical-solutions.html"><a href="7.4-optimization-non-analytical-solutions.html#newton-raphson-optimization"><i class="fa fa-check"></i><b>7.4.1</b> Newton-Raphson Optimization</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-ols-estimation-as-ml-estimation.html"><a href="7.5-ols-estimation-as-ml-estimation.html"><i class="fa fa-check"></i><b>7.5</b> OLS-Estimation as ML-Estimation</a></li>
<li class="chapter" data-level="7.6" data-path="7.6-variance-of-ml-estimators-hatbeta_ml-and-s2_ml.html"><a href="7.6-variance-of-ml-estimators-hatbeta_ml-and-s2_ml.html"><i class="fa fa-check"></i><b>7.6</b> Variance of ML-Estimators <span class="math inline">\(\hat\beta_{ML}\)</span> and <span class="math inline">\(s^2_{ML}\)</span></a></li>
<li class="chapter" data-level="7.7" data-path="7.7-consistency-of-hatbeta_ml-and-s_ml2.html"><a href="7.7-consistency-of-hatbeta_ml-and-s_ml2.html"><i class="fa fa-check"></i><b>7.7</b> Consistency of <span class="math inline">\(\hat\beta_{ML}\)</span> and <span class="math inline">\(s_{ML}^2\)</span></a></li>
<li class="chapter" data-level="7.8" data-path="7.8-asymptotic-theory-of-maximum-likelihood-estimators.html"><a href="7.8-asymptotic-theory-of-maximum-likelihood-estimators.html"><i class="fa fa-check"></i><b>7.8</b> Asymptotic Theory of Maximum-Likelihood Estimators</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-ivr.html"><a href="8-ivr.html"><i class="fa fa-check"></i><b>8</b> Instrumental Variables Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="8.1-TIVEWASRAASI.html"><a href="8.1-TIVEWASRAASI.html"><i class="fa fa-check"></i><b>8.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-TIVEWASRAASI.html"><a href="8.1-TIVEWASRAASI.html#the-two-stage-least-squares-estimator"><i class="fa fa-check"></i><b>8.1.1</b> The Two-Stage Least Squares Estimator</a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-TIVEWASRAASI.html"><a href="8.1-TIVEWASRAASI.html#application-demand-for-cigarettes-12"><i class="fa fa-check"></i><b>8.1.2</b> Application: Demand For Cigarettes (1/2)</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-TGIVRM.html"><a href="8.2-TGIVRM.html"><i class="fa fa-check"></i><b>8.2</b> The General IV Regression Model</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-TGIVRM.html"><a href="8.2-TGIVRM.html#application-demand-for-cigarettes-22"><i class="fa fa-check"></i><b>8.2.1</b> Application: Demand for Cigarettes (2/2)</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-civ.html"><a href="8.3-civ.html"><i class="fa fa-check"></i><b>8.3</b> Checking Instrument Validity</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-civ.html"><a href="8.3-civ.html#instrument-relevance"><i class="fa fa-check"></i><b>8.3.1</b> Instrument Relevance</a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-civ.html"><a href="8.3-civ.html#instrument-validity"><i class="fa fa-check"></i><b>8.3.2</b> Instrument Validity</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-attdfc.html"><a href="8.4-attdfc.html"><i class="fa fa-check"></i><b>8.4</b> Application to the Demand for Cigarettes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics (M.Sc.)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability-theory" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Probability Theory</h2>
<p>Probability is the mathematical language for quantifying uncertainty. We can apply probability theory to a diverse set of problems, from coin flipping to the analysis of econometric problems. The starting point is to specify the <strong>sample space</strong>, that is, the set of possible outcomes.</p>
<div id="sample-spaces-and-events" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Sample Spaces and Events</h3>
<p>The <strong>sample space</strong> <span class="math inline">\(\Omega,\)</span> is the set of possible outcomes of an experiment. Points <span class="math inline">\(\omega\)</span> in <span class="math inline">\(\Omega\)</span> are called sample outcomes or realizations. <strong>Events</strong> are subsets of <span class="math inline">\(\Omega\)</span> Example 2.1 If we toss a coin twice then <span class="math inline">\(\Omega=\{H H, H T, T H, T T\} .\)</span> The event that the first toss is heads is <span class="math inline">\(A=\{H H, H T\}\)</span>.</p>
<p> Let <span class="math inline">\(\omega\)</span> be the outcome of a measurement of some physical quantity, for example, temperature. Then <span class="math inline">\(\Omega=\mathbb{R}=(-\infty, \infty).\)</span> The event that the measurement is larger than 10 but less than or equal to 23 is <span class="math inline">\(A=(10,23]\)</span>.</p>
<p> If we toss a coin forever then the sample space is the infinite set <span class="math inline">\(\Omega=\left\{\omega=\left(\omega_{1}, \omega_{2}, \omega_{3}, \ldots,\right)|\omega_{i} \in\{H, T\}\right\}\)</span> Let <span class="math inline">\(A\)</span> be the event that the first head appears on the third toss. Then
<span class="math inline">\(A=\left\{\left(\omega_{1}, \omega_{2}, \omega_{3}, \ldots,\right)| \omega_{1}=T, \omega_{2}=T, \omega_{3}=H, \omega_{i} \in\{H, T\} \text { for } i&gt;3\right\}\)</span>.</p>
<p>Given an event <span class="math inline">\(A,\)</span> let <span class="math inline">\(A^{c}=\{\omega \in \Omega ; \omega \notin A\}\)</span> denote the <strong>complement</strong> of <span class="math inline">\(A\)</span>. Informally, <span class="math inline">\(A^{c}\)</span> can be read as  The complement of <span class="math inline">\(\Omega\)</span> is the empty set <span class="math inline">\(\emptyset\)</span>. The <strong>union</strong> of events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is defined as
<span class="math display">\[
A\cup B=\{\omega \in \Omega|\omega\in A\text{ or }\omega \in B\text{ or }\omega\in\text{ both}\}
\]</span>
which can be thought of as  If <span class="math inline">\(A_{1}, A_{2}, \ldots\)</span> is a sequence of sets then
<span class="math display">\[
\bigcup_{i=1}^{\infty} A_{i}=\left\{\omega \in \Omega: \omega \in A_{i} \text { for at least one i }\right\}.
\]</span>
The <strong>intersection</strong> of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is defined as
<span class="math display">\[
A \cap B=\{\omega \in \Omega ; \omega \in A\text{ and }\omega \in B\}\]</span>
which reads as  Sometimes <span class="math inline">\(A \cap B\)</span> is also written shortly as <span class="math inline">\(AB\)</span>. If <span class="math inline">\(A_{1}, A_{2}, \ldots\)</span> is a sequence of sets then
<span class="math display">\[
\bigcap_{i=1}^{\infty} A_{i}=\left\{\omega \in \Omega: \omega \in A_{i} \text { for all i }\right\}.
\]</span>
If every element of <span class="math inline">\(A\)</span> is also contained in <span class="math inline">\(B\)</span> we write <span class="math inline">\(A \subset B\)</span> or, equivalently, <span class="math inline">\(B \supset A\)</span>. If <span class="math inline">\(A\)</span> is a finite set, let <span class="math inline">\(|A|\)</span> denote the number of elements in <span class="math inline">\(A .\)</span> We say that <span class="math inline">\(A_{1}, A_{2}, \ldots\)</span> are <strong>disjoint</strong> or <strong>mutually exclusive</strong> if <span class="math inline">\(A_{i} \cap A_{j}=\emptyset\)</span> whenever <span class="math inline">\(i \neq j\)</span>. For example, <span class="math inline">\(A_{1}=[0,1), A_{2}=[1,2), A_{3}=[2,3), \ldots\)</span> are disjoint. A <strong>partition</strong> of <span class="math inline">\(\Omega\)</span> is a sequence of disjoint sets <span class="math inline">\(A_{1}, A_{2}, \ldots\)</span> such that <span class="math inline">\(\bigcup_{i=1}^{\infty} A_{i}=\Omega\)</span>.</p>
<!-- \newpage -->


<!-- Given an event $A,$ define the indicator function of $A$ by -->
<!-- $$ -->
<!-- I_{A}(\omega)=I(\omega \in A)=\left\{\begin{array}{ll} -->
<!-- 1 & \text { if } \omega \in A \\ -->
<!-- 0 & \text { if } \omega \notin A -->
<!-- \end{array}\right. -->
<!-- $$ -->
<!-- A sequence of sets $A_{1}, A_{2}, \ldots$ is monotone increasing if $A_{1} \subset A_{2} \subset$ -->
<!-- $\cdots$ and we define $\lim _{n \rightarrow \infty} A_{n}=\bigcup_{i=1}^{\infty} A_{i} .$ A sequence of sets $A_{1}, A_{2}, \ldots$ is -->
<!-- monotone decreasing if $A_{1} \supset A_{2} \supset \cdots$ and then we define $\lim _{n \rightarrow \infty} A_{n}=$ -->
<!-- $\bigcap_{i=1}^{\infty} A_{i} .$ In either case, we will write $A_{n} \rightarrow A$ -->
<!-- Example 2.4 Let $\Omega=\mathbb{R}$ and let $A_{i}=[0,1 / i)$ for $i=1,2, \ldots$ Then $\bigcup_{i=1}^{\infty} A_{i}=$ [0,1) and $\bigcap_{i=1}^{\infty} A_{i}=\{0\} .$ If instead we define $A_{i}=(0,1 / i)$ then $\bigcup_{i=1}^{\infty} A_{i}=$ -->
<!-- (0,1) and $\bigcap_{i=1}^{\infty} A_{i}=\emptyset$ -->
</div>
<div id="probability" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Probability</h3>
We want to assign a real number <span class="math inline">\(P(A)\)</span> to every event <span class="math inline">\(A,\)</span> called the <strong>probability</strong> of <span class="math inline">\(A .\)</span> We also call <span class="math inline">\(P\)</span> a <strong>probability distribution</strong> or a <strong>probability measure</strong>. To qualify as a probability, <span class="math inline">\(P\)</span> has to satisfy three axioms. That is, a function <span class="math inline">\(P\)</span> that assigns a real number <span class="math inline">\(P(A)\in[0,1]\)</span> to each event <span class="math inline">\(A\)</span> is a  or a  if it satisfies the following three axioms:

<p> It is not always possible to assign a probability to every event <span class="math inline">\(A\)</span> if the sample space is large, such as, for instance, the whole real line, <span class="math inline">\(\Omega=\mathbb{R}\)</span>. In case of <span class="math inline">\(\Omega=\mathbb{R}\)</span> strange things can happen. There are pathological sets that simply break down the mathematics. An example of one of these pathological sets, also known as non-measurable sets because they literally can’t be measured (i.e. we cannot assign probabilities to them), are the Vitali sets. Therefore, in such cases like <span class="math inline">\(\Omega=\mathbb{R}\)</span>, we assign probabilities to a <em>limited</em> class of sets called a <strong><span class="math inline">\(\sigma\)</span>-field</strong> or <strong><span class="math inline">\(\sigma\)</span>-algebra</strong>. For <span class="math inline">\(\Omega=\mathbb{R}\)</span>, the canonical <strong><span class="math inline">\(\sigma\)</span>-algebra</strong> is the <strong>Borel <span class="math inline">\(\sigma\)</span>-algebra</strong>. The Borel <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\mathbb{R}\)</span> is generated by the collection of all open subsets of <span class="math inline">\(R\)</span>.</p>
One can derive many properties of <span class="math inline">\(P\)</span> from the axioms. Here are a few:
<p>A less obvious property is given in the following: For any events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> we have that,
<span class="math display">\[P(A \cup B)=P(A)+P(B)-P(A B).\]</span></p>
<p>Two coin tosses. Let <span class="math inline">\(H_{1}\)</span> be the event that heads occurs on toss 1 and let <span class="math inline">\(H_{2}\)</span> be the event that heads occurs on toss 2. If all outcomes are equally likely, that is, <span class="math inline">\(\mathrm{P}\left(\left\{H_{1}, H_{2}\right\}\right)=\mathrm{P}\left(\left\{H_{1}, T_{2}\right\}\right)=\mathrm{P}\left(\left\{T_{1}, H_{2}\right\}\right)=\mathrm{P}\left(\left\{T_{1}, T_{2}\right\}\right)=1 / 4\)</span>, then
<span class="math display">\[
\mathrm{P}\left(H_{1} \cup H_{2}\right)=\mathrm{P}\left(H_{1}\right)+\mathrm{P}\left(H_{2}\right)-\mathrm{P}\left(H_{1} H_{2}\right)=\frac{1}{2}+\frac{1}{2}-\frac{1}{4}=\frac{3}{4}.
\]</span></p>
<p>One can interpret <span class="math inline">\(P(A)\)</span> in terms of . That is, <span class="math inline">\(P(A)\)</span> is the (infinitely) long run proportion of times that <span class="math inline">\(A\)</span> is true in repetitions. For example, if we say that the probability of heads is <span class="math inline">\(1 / 2\)</span>, i.e <span class="math inline">\(P(H)=1/2\)</span> we mean that if we flip the coin many times then the proportion of times we get heads tends to <span class="math inline">\(1 / 2\)</span> as the number of tosses increases. An infinitely long, unpredictable sequence of tosses whose limiting proportion tends to a constant is an idealization, much like the idea of a straight line in geometry. 
The following  codes approximates the probability <span class="math inline">\(P(H)=1/2\)</span> using 1, 10 and 100,000 many (pseudo) random coin flips:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="2.1-probability-theory.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">869</span>)</span>
<span id="cb60-2"><a href="2.1-probability-theory.html#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 (fair) coin-flip:</span></span>
<span id="cb60-3"><a href="2.1-probability-theory.html#cb60-3" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="st">&quot;H&quot;</span>, <span class="st">&quot;T&quot;</span>), <span class="at">size =</span> <span class="dv">1</span>)</span>
<span id="cb60-4"><a href="2.1-probability-theory.html#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Relative frequency of &quot;H&quot; in 1 coin-flips</span></span>
<span id="cb60-5"><a href="2.1-probability-theory.html#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(results[results<span class="sc">==</span><span class="st">&quot;H&quot;</span>])<span class="sc">/</span><span class="dv">1</span></span>
<span id="cb60-6"><a href="2.1-probability-theory.html#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1</span></span>
<span id="cb60-7"><a href="2.1-probability-theory.html#cb60-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-8"><a href="2.1-probability-theory.html#cb60-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 10 (fair) coin-flips:</span></span>
<span id="cb60-9"><a href="2.1-probability-theory.html#cb60-9" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="st">&quot;H&quot;</span>, <span class="st">&quot;T&quot;</span>), <span class="at">size =</span> <span class="dv">10</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb60-10"><a href="2.1-probability-theory.html#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Relative frequency of &quot;H&quot; in 10 coin-flips</span></span>
<span id="cb60-11"><a href="2.1-probability-theory.html#cb60-11" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(results[results<span class="sc">==</span><span class="st">&quot;H&quot;</span>])<span class="sc">/</span><span class="dv">10</span></span>
<span id="cb60-12"><a href="2.1-probability-theory.html#cb60-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.3</span></span>
<span id="cb60-13"><a href="2.1-probability-theory.html#cb60-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-14"><a href="2.1-probability-theory.html#cb60-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 100000 (fair) coin-flips:</span></span>
<span id="cb60-15"><a href="2.1-probability-theory.html#cb60-15" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="st">&quot;H&quot;</span>, <span class="st">&quot;T&quot;</span>), <span class="at">size =</span> <span class="dv">100000</span>, <span class="at">replace =</span> T)</span>
<span id="cb60-16"><a href="2.1-probability-theory.html#cb60-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Relative frequency of &quot;H&quot; in 100000 coin-flips</span></span>
<span id="cb60-17"><a href="2.1-probability-theory.html#cb60-17" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(results[results<span class="sc">==</span><span class="st">&quot;H&quot;</span>])<span class="sc">/</span><span class="dv">100000</span></span>
<span id="cb60-18"><a href="2.1-probability-theory.html#cb60-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.50189</span></span></code></pre></div>
</div>
<div id="independent-events" class="section level3" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Independent Events</h3>
<p>If we flip a fair coin twice, then the probability of two heads is <span class="math inline">\(\frac{1}{2} \times \frac{1}{2}\)</span>. We multiply the probabilities because we regard the two tosses as independent. Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are called  if
<span class="math display">\[P(A B)=P(A) P(B).\]</span>
Or more generally, a whole set of events <span class="math inline">\(\{A_i|i\in I\}\)</span> is independent if
<span class="math display">\[
P\left(\bigcap_{i \in J} A_{i}\right)=\prod_{i \in J}P\left(A_{i}\right)
\]</span>
for every finite subset <span class="math inline">\(J\)</span> of <span class="math inline">\(I\)</span>, where <span class="math inline">\(I\)</span> denotes the not necessarily finite index set (e.g. <span class="math inline">\(I=\{1,2,\dots\}\)</span>).</p>
<p>Independence can arise in two distinct ways. Sometimes, we <strong>explicitly assume</strong> that two events are independent. For example, in tossing a coin twice, we usually assume the tosses are independent which reflects the fact that the coin has no memory of the first toss. 
In other instances, we <strong>derive</strong> independence by verifying that the definition of independence <span class="math inline">\(P(A B)=P(A)P(B)\)</span> holds. For example, in tossing a fair die , let <span class="math inline">\(A=\{2,4,6\}\)</span> be the event of observing an even number and let <span class="math inline">\(B=\{1,2,3,4\}\)</span> be the event of observing no <span class="math inline">\(5\)</span> and no <span class="math inline">\(6\)</span>. Then,
<span class="math inline">\(A \cap B=\{2,4\}\)</span> is the event of observing either a <span class="math inline">\(2\)</span> or a <span class="math inline">\(4\)</span>. Are the events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> independent?<br />
<span class="math display">\[
P(A B)=\frac{2}{6}=P(A)P(B)=\frac{1}{2}\cdot \frac{2}{3}
\]</span>
and so <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent. In this case, we didn’t assume that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent it just turned out that they were.</p>
<p>Suppose that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are  (i.e. <span class="math inline">\(AB=\emptyset\)</span>), each with positive probability (i.e. <span class="math inline">\(P(A)&gt;0\)</span> and <span class="math inline">\(P(B)&gt;0\)</span>). Can they be independent? No! This follows since
<span class="math display">\[
P(A B)=P(\emptyset)=0\neq P(A)P(B)&gt;0.
\]</span>
Except in this special case, there is no way to judge (in-)dependence by looking at the sets in a Venn diagram.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if <span class="math inline">\(P(A B)=P(A) P(B)\)</span>.</li>
<li>Independence is sometimes assumed and sometimes derived.</li>
<li>Disjoint events with positive probability are not independent.</li>
</ol>
</div>
<div id="conditional-probability" class="section level3" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Conditional Probability</h3>
If <span class="math inline">\(P(B)&gt;0\)</span> then the  of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> is
<span class="math display">\[
P(A \mid B)=\frac{P(A B)}{P(B)}.
\]</span>
Think of <span class="math inline">\(P(A \mid B)\)</span> as the fraction of times <span class="math inline">\(A\)</span> occurs among those in which <span class="math inline">\(B\)</span> occurs. Here are some facts about conditional probabilities:
<p>In general it is also  the case that <span class="math inline">\(P(A \mid B)=P(B \mid A)\)</span>. People get this confused all the time. For example, the probability of spots given you have measles is 1 but the probability that you have measles given that you have spots is not <span class="math inline">\(1 .\)</span> In this case, the difference between <span class="math inline">\(P(A \mid B)\)</span> and <span class="math inline">\(P(B \mid A)\)</span> is obvious but there are cases where it is less obvious. This mistake is made often enough in legal cases that it is sometimes called the .</p>
A medical test for a disease <span class="math inline">\(D\)</span> has outcomes <span class="math inline">\(+\)</span> and <span class="math inline">\(-\)</span>. The probabilities are:
<span class="math display">\[
\begin{array}{c|cc|c} 
&amp; D &amp; D^{c} \\
\hline
+ &amp; .0081 &amp; .0900 &amp;  .0981\\
- &amp; .0009 &amp; .9010 &amp;  .9019\\
\hline
  &amp; .0090 &amp; .9910 &amp;  1
\end{array}
\]</span>
From the definition of conditional probability, we have that:
<p>Apparently, the test is fairly accurate. Sick people yield a positive test result 90 percent of the time and healthy people yield a negative test result about 90 percent of the time. Suppose you go for a test and get a positive result. What is the probability you have the disease? Most people answer <span class="math inline">\(0.90=90\%\)</span>. The correct answer is <span class="math inline">\(P(D \mid+)=P(+\cap D) / P(+)=0.0081 /(0.0081+0.0900)=0.08\)</span>. The lesson here is that you need to compute the answer numerically. Don’t trust your intuition.</p>
<p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are  then
<span class="math display">\[
P(A \mid B)=\frac{P(A B)}{P(B)}=\frac{P(A) P(B)}{P(B)}=P(A)
\]</span>
So another  is that knowing <span class="math inline">\(B\)</span> doesn’t change the probability of <span class="math inline">\(A\)</span>.</p>
<p>From the definition of conditional probability we can write
<span class="math display">\[
P(A B)=P(A \mid B) P(B)\quad\text{and also}\quad P(A B)=P(B \mid A) P(A).
\]</span>
Often, these formulas give us a convenient way to compute <span class="math inline">\(P(A B)\)</span> when <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are not independent.</p>
<p>Note, sometimes <span class="math inline">\(P(A B)\)</span> is written as <span class="math inline">\(P(A,B)\)</span>.</p>
<p>Draw two cards from a deck, without replacement. Let <span class="math inline">\(A\)</span> be the event that the first draw is Ace of Clubs and let <span class="math inline">\(B\)</span> be the event that the second draw is Queen of Diamonds. Then <span class="math inline">\(P(A, B)=P(A) P(B \mid A)=(1 / 52) \times(1 / 51)\)</span></p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(P(B)&gt;0\)</span> then <span class="math inline">\(P(A \mid B)=P(A B)/P(B)\)</span></li>
<li><span class="math inline">\(P(\cdot \mid B)\)</span> satisfies the axioms of probability, for fixed <span class="math inline">\(B\)</span>. In general, <span class="math inline">\(P(A \mid \cdot)\)</span> does not satisfy the axioms of probability, for fixed <span class="math inline">\(A\)</span>.</li>
<li>In general, <span class="math inline">\(P(A \mid B) \neq P(B \mid A)\)</span>.</li>
<li><span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if and only if <span class="math inline">\(P(A \mid B)=P(A)\)</span>.</li>
</ol>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-review-probability-and-statistics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2.2-random-variables.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Script_Econometrics_MSc.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
