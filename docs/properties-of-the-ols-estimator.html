<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.3 Properties of the OLS Estimator | Econometrics (M.Sc.)</title>
  <meta name="description" content="3.3 Properties of the OLS Estimator | Econometrics (M.Sc.)" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="3.3 Properties of the OLS Estimator | Econometrics (M.Sc.)" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/mylogo.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.3 Properties of the OLS Estimator | Econometrics (M.Sc.)" />
  
  
  <meta name="twitter:image" content="/images/mylogo.png" />

<meta name="author" content="Prof. Dr. Dominik Liebl" />


<meta name="date" content="2021-11-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ordinary-least-squares-estimation.html"/>
<link rel="next" href="ch:MLR.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#organization-of-the-course"><i class="fa fa-check"></i>Organization of the Course</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#literature"><i class="fa fa-check"></i>Literature</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>1</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="short-glossary.html"><a href="short-glossary.html"><i class="fa fa-check"></i><b>1.1</b> Short Glossary</a></li>
<li class="chapter" data-level="1.2" data-path="first-steps.html"><a href="first-steps.html"><i class="fa fa-check"></i><b>1.2</b> First Steps</a></li>
<li class="chapter" data-level="1.3" data-path="further-data-objects.html"><a href="further-data-objects.html"><i class="fa fa-check"></i><b>1.3</b> Further Data Objects</a></li>
<li class="chapter" data-level="1.4" data-path="simple-regression-analysis-using-r.html"><a href="simple-regression-analysis-using-r.html"><i class="fa fa-check"></i><b>1.4</b> Simple Regression Analysis using R</a></li>
<li class="chapter" data-level="1.5" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>1.5</b> Programming in R</a></li>
<li class="chapter" data-level="1.6" data-path="r-packages.html"><a href="r-packages.html"><i class="fa fa-check"></i><b>1.6</b> R-packages</a></li>
<li class="chapter" data-level="1.7" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>1.7</b> Tidyverse</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="tidyverse.html"><a href="tidyverse.html#tidyverse-plotting-basics"><i class="fa fa-check"></i><b>1.7.1</b> Tidyverse: Plotting Basics</a></li>
<li class="chapter" data-level="1.7.2" data-path="tidyverse.html"><a href="tidyverse.html#tidyverse-data-wrangling-basics"><i class="fa fa-check"></i><b>1.7.2</b> Tidyverse: Data Wrangling Basics</a></li>
<li class="chapter" data-level="1.7.3" data-path="tidyverse.html"><a href="tidyverse.html#the-pipe-operator"><i class="fa fa-check"></i><b>1.7.3</b> The pipe operator <code>%&gt;%</code></a></li>
<li class="chapter" data-level="1.7.4" data-path="tidyverse.html"><a href="tidyverse.html#the-group_by-function"><i class="fa fa-check"></i><b>1.7.4</b> The <code>group_by()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="further-links.html"><a href="further-links.html"><i class="fa fa-check"></i><b>1.8</b> Further Links</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="further-links.html"><a href="further-links.html#further-r-intros"><i class="fa fa-check"></i><b>1.8.1</b> Further R-Intros</a></li>
<li class="chapter" data-level="1.8.2" data-path="further-links.html"><a href="further-links.html#version-control-gitgithub"><i class="fa fa-check"></i><b>1.8.2</b> Version Control (Git/GitHub)</a></li>
<li class="chapter" data-level="1.8.3" data-path="further-links.html"><a href="further-links.html#r-ladies"><i class="fa fa-check"></i><b>1.8.3</b> R-Ladies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="review-probability-and-statistics.html"><a href="review-probability-and-statistics.html"><i class="fa fa-check"></i><b>2</b> Review: Probability and Statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2.1</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="probability-theory.html"><a href="probability-theory.html#sample-spaces-and-elementary-events"><i class="fa fa-check"></i><b>2.1.1</b> Sample Spaces and (Elementary) Events</a></li>
<li class="chapter" data-level="2.1.2" data-path="probability-theory.html"><a href="probability-theory.html#probability"><i class="fa fa-check"></i><b>2.1.2</b> Probability</a></li>
<li class="chapter" data-level="2.1.3" data-path="probability-theory.html"><a href="probability-theory.html#independent-events"><i class="fa fa-check"></i><b>2.1.3</b> Independent Events</a></li>
<li class="chapter" data-level="2.1.4" data-path="probability-theory.html"><a href="probability-theory.html#conditional-probability"><i class="fa fa-check"></i><b>2.1.4</b> Conditional Probability</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>2.2</b> Random Variables</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="random-variables.html"><a href="random-variables.html#univariate-distribution-and-probability-functions"><i class="fa fa-check"></i><b>2.2.1</b> Univariate Distribution and Probability Functions</a></li>
<li class="chapter" data-level="2.2.2" data-path="random-variables.html"><a href="random-variables.html#multivariate-distribution-and-probability-functions"><i class="fa fa-check"></i><b>2.2.2</b> Multivariate Distribution and Probability Functions</a></li>
<li class="chapter" data-level="2.2.3" data-path="random-variables.html"><a href="random-variables.html#means-and-moments"><i class="fa fa-check"></i><b>2.2.3</b> Means and Moments</a></li>
<li class="chapter" data-level="2.2.4" data-path="random-variables.html"><a href="random-variables.html#unconditional-means"><i class="fa fa-check"></i><b>2.2.4</b> Unconditional Means</a></li>
<li class="chapter" data-level="2.2.5" data-path="random-variables.html"><a href="random-variables.html#conditional-means"><i class="fa fa-check"></i><b>2.2.5</b> Conditional Means</a></li>
<li class="chapter" data-level="2.2.6" data-path="random-variables.html"><a href="random-variables.html#means-of-transformed-random-variables-and-moments"><i class="fa fa-check"></i><b>2.2.6</b> Means of Transformed Random Variables and Moments</a></li>
<li class="chapter" data-level="2.2.7" data-path="random-variables.html"><a href="random-variables.html#independent-random-variables"><i class="fa fa-check"></i><b>2.2.7</b> Independent Random Variables</a></li>
<li class="chapter" data-level="2.2.8" data-path="random-variables.html"><a href="random-variables.html#i.i.d.-samples"><i class="fa fa-check"></i><b>2.2.8</b> I.I.D. Samples</a></li>
<li class="chapter" data-level="2.2.9" data-path="random-variables.html"><a href="random-variables.html#some-important-discrete-random-variables"><i class="fa fa-check"></i><b>2.2.9</b> Some Important Discrete Random Variables</a></li>
<li class="chapter" data-level="2.2.10" data-path="random-variables.html"><a href="random-variables.html#some-important-continuous-random-variables"><i class="fa fa-check"></i><b>2.2.10</b> Some Important Continuous Random Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch:SLR.html"><a href="ch:SLR.html"><i class="fa fa-check"></i><b>3</b> Review: Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-simple-linear-regression-model.html"><a href="the-simple-linear-regression-model.html"><i class="fa fa-check"></i><b>3.1</b> The Simple Linear Regression Model</a>
<ul>
<li class="chapter" data-level="" data-path="the-simple-linear-regression-model.html"><a href="the-simple-linear-regression-model.html#the-data-generating-process"><i class="fa fa-check"></i>The Data-Generating Process</a></li>
<li class="chapter" data-level="3.1.1" data-path="the-simple-linear-regression-model.html"><a href="the-simple-linear-regression-model.html#assumptions-about-the-error-term"><i class="fa fa-check"></i><b>3.1.1</b> Assumptions About the Error Term</a></li>
<li class="chapter" data-level="3.1.2" data-path="the-simple-linear-regression-model.html"><a href="the-simple-linear-regression-model.html#the-population-regression-line"><i class="fa fa-check"></i><b>3.1.2</b> The Population Regression Line</a></li>
<li class="chapter" data-level="3.1.3" data-path="the-simple-linear-regression-model.html"><a href="the-simple-linear-regression-model.html#terminology-estimates-versus-estimators"><i class="fa fa-check"></i><b>3.1.3</b> Terminology: Estimates versus Estimators</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ordinary-least-squares-estimation.html"><a href="ordinary-least-squares-estimation.html"><i class="fa fa-check"></i><b>3.2</b> Ordinary Least Squares Estimation</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ordinary-least-squares-estimation.html"><a href="ordinary-least-squares-estimation.html#terminology-sample-regression-line-prediction-and-residuals"><i class="fa fa-check"></i><b>3.2.1</b> Terminology: Sample Regression Line, Prediction and Residuals</a></li>
<li class="chapter" data-level="3.2.2" data-path="ordinary-least-squares-estimation.html"><a href="ordinary-least-squares-estimation.html#sec:SLROLS"><i class="fa fa-check"></i><b>3.2.2</b> Deriving the Expression of the OLS Estimator</a></li>
<li class="chapter" data-level="3.2.3" data-path="ordinary-least-squares-estimation.html"><a href="ordinary-least-squares-estimation.html#behavior-of-the-ols-estimates-for-resampled-data-conditionally-on-x_i"><i class="fa fa-check"></i><b>3.2.3</b> Behavior of the OLS Estimates for Resampled Data (conditionally on <span class="math inline">\(X_i\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="properties-of-the-ols-estimator.html"><a href="properties-of-the-ols-estimator.html"><i class="fa fa-check"></i><b>3.3</b> Properties of the OLS Estimator</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="properties-of-the-ols-estimator.html"><a href="properties-of-the-ols-estimator.html#mean-and-bias-of-the-ols-estimator"><i class="fa fa-check"></i><b>3.3.1</b> Mean and Bias of the OLS Estimator</a></li>
<li class="chapter" data-level="3.3.2" data-path="properties-of-the-ols-estimator.html"><a href="properties-of-the-ols-estimator.html#variance-of-the-ols-estimator"><i class="fa fa-check"></i><b>3.3.2</b> Variance of the OLS Estimator</a></li>
<li class="chapter" data-level="3.3.3" data-path="properties-of-the-ols-estimator.html"><a href="properties-of-the-ols-estimator.html#consistency-of-the-ols-estimator"><i class="fa fa-check"></i><b>3.3.3</b> Consistency of the OLS Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch:MLR.html"><a href="ch:MLR.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="assumptions.html"><a href="assumptions.html"><i class="fa fa-check"></i><b>4.1</b> Assumptions</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="assumptions.html"><a href="assumptions.html#some-implications-of-the-exogeneity-assumption"><i class="fa fa-check"></i><b>4.1.1</b> Some Implications of the Exogeneity Assumption</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="deriving-the-expression-of-the-ols-estimator.html"><a href="deriving-the-expression-of-the-ols-estimator.html"><i class="fa fa-check"></i><b>4.2</b> Deriving the Expression of the OLS Estimator</a></li>
<li class="chapter" data-level="4.3" data-path="some-quantities-of-interest.html"><a href="some-quantities-of-interest.html"><i class="fa fa-check"></i><b>4.3</b> Some Quantities of Interest</a></li>
<li class="chapter" data-level="4.4" data-path="method-of-moments-estimator.html"><a href="method-of-moments-estimator.html"><i class="fa fa-check"></i><b>4.4</b> Method of Moments Estimator</a></li>
<li class="chapter" data-level="4.5" data-path="unbiasedness-of-hatbetax-and-hatbeta.html"><a href="unbiasedness-of-hatbetax-and-hatbeta.html"><i class="fa fa-check"></i><b>4.5</b> Unbiasedness of <span class="math inline">\(\hat\beta|X\)</span> and <span class="math inline">\(\hat\beta\)</span></a></li>
<li class="chapter" data-level="4.6" data-path="ch:VarEstBeta.html"><a href="ch:VarEstBeta.html"><i class="fa fa-check"></i><b>4.6</b> Variance of <span class="math inline">\(\hat\beta|X\)</span></a></li>
<li class="chapter" data-level="4.7" data-path="the-gauss-markov-theorem.html"><a href="the-gauss-markov-theorem.html"><i class="fa fa-check"></i><b>4.7</b> The Gauss-Markov Theorem</a></li>
<li class="chapter" data-level="4.8" data-path="practice.html"><a href="practice.html"><i class="fa fa-check"></i><b>4.8</b> Practice</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="practice.html"><a href="practice.html#dummy-variables-and-contrast-codings"><i class="fa fa-check"></i><b>4.8.1</b> Dummy variables and contrast codings</a></li>
<li class="chapter" data-level="4.8.2" data-path="practice.html"><a href="practice.html#the-function"><i class="fa fa-check"></i><b>4.8.2</b> The function </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch:SSINF.html"><a href="ch:SSINF.html"><i class="fa fa-check"></i><b>5</b> Small Sample Inference</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ch:testmultp.html"><a href="ch:testmultp.html"><i class="fa fa-check"></i><b>5.1</b> Hypothesis Tests about Multiple Parameters</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ch:testmultp.html"><a href="ch:testmultp.html#the-test-statistic-and-its-null-distribution"><i class="fa fa-check"></i><b>5.1.1</b> The Test Statistic and its Null Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ch:testingsinglep.html"><a href="ch:testingsinglep.html"><i class="fa fa-check"></i><b>5.2</b> Tests about One Parameter</a></li>
<li class="chapter" data-level="5.3" data-path="testtheory.html"><a href="testtheory.html"><i class="fa fa-check"></i><b>5.3</b> Testtheory</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="testtheory.html"><a href="testtheory.html#significance-level"><i class="fa fa-check"></i><b>5.3.1</b> Significance Level</a></li>
<li class="chapter" data-level="5.3.2" data-path="testtheory.html"><a href="testtheory.html#critical-value-for-the-f-test"><i class="fa fa-check"></i><b>5.3.2</b> Critical Value for the <span class="math inline">\(F\)</span>-Test</a></li>
<li class="chapter" data-level="5.3.3" data-path="testtheory.html"><a href="testtheory.html#critical-values-for-the-t-test"><i class="fa fa-check"></i><b>5.3.3</b> Critical Value(s) for the <span class="math inline">\(t\)</span>-Test</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="type-ii-error-and-power.html"><a href="type-ii-error-and-power.html"><i class="fa fa-check"></i><b>5.4</b> Type II Error and Power</a></li>
<li class="chapter" data-level="5.5" data-path="p-value.html"><a href="p-value.html"><i class="fa fa-check"></i><b>5.5</b> <span class="math inline">\(p\)</span>-Value</a></li>
<li class="chapter" data-level="5.6" data-path="CIsmallsample.html"><a href="CIsmallsample.html"><i class="fa fa-check"></i><b>5.6</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.7" data-path="PSSI.html"><a href="PSSI.html"><i class="fa fa-check"></i><b>5.7</b> Practice: Small Sample Inference</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="PSSI.html"><a href="PSSI.html#normally-distributed-hatbetax"><i class="fa fa-check"></i><b>5.7.1</b> Normally Distributed <span class="math inline">\(\hat\beta|X\)</span></a></li>
<li class="chapter" data-level="5.7.2" data-path="PSSI.html"><a href="PSSI.html#testing-multiple-parameters"><i class="fa fa-check"></i><b>5.7.2</b> Testing Multiple Parameters</a></li>
<li class="chapter" data-level="5.7.3" data-path="PSSI.html"><a href="PSSI.html#dualty-of-confidence-intervals-and-hypothesis-tests"><i class="fa fa-check"></i><b>5.7.3</b> Dualty of Confidence Intervals and Hypothesis Tests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics (M.Sc.)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="properties-of-the-ols-estimator" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Properties of the OLS Estimator</h2>
Note that we derived the OLS estimator for a given realization <span class="math inline">\(((y_1,x_1),\dots,(y_n,x_n))\)</span> of the random sample <span class="math inline">\(((Y_1,X_1),\dots,(Y_n,X_n))\)</span>. So, why should OLS be a  estimator in general? Does it behave well when we consider all possible realization of the OLS estimator for conditional resamplings, given <span class="math inline">\(X_1,\dots,X_n\)</span>, from all possible data generating processes described by ?
<!-- not need to make any assumptions about the error term, and in particular, about its relationship to $X$ in order to derive the estimator.  We also did not make any assumption about the variance of $\varepsilon$ or about the relationship between the error terms $\varepsilon_i$ and $\varepsilon_j$ for $i\neq j$ in order to derive this estimator. So why is OLS a ``good'' estimator?  -->
In the following we consider the two questions:
<div id="mean-and-bias-of-the-ols-estimator" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Mean and Bias of the OLS Estimator</h3>
<p>Because the estimator
<span class="math display">\[\begin{align}
\hat{\beta}=
\begin{pmatrix}\hat\beta_0\\\hat\beta_1\end{pmatrix}=
\begin{pmatrix}
\bar Y - \hat\beta_1 \bar X\\
\dfrac{\sum_{i=1}^n (X_i- \bar X)Y_i}{\sum_{i=1}^n (X_i-\bar X)^2}
\end{pmatrix}
\label{eq:OLSslreg}
\end{align}\]</span>
is a function of the random variables in the random sample <span class="math inline">\(((Y_1,X_1),\dots,(Y_n,X_n))\)</span>, it is itself a random variable. Therefore, <span class="math inline">\(\hat{\beta}\)</span> has a probability distribution, and we can talk sensibly about its expectation <span class="math inline">\(E(\hat{\beta})\)</span>.</p>
<p>Note that the estimator <span class="math inline">\(\hat{\beta}\)</span> in  contains both the <span class="math inline">\(Y_i\)</span>’s and the <span class="math inline">\(X_i\)</span>’s. Therefore, under random designs, we need to be specific which mean (or variance) of the estimator we want to consider. For the random design, we consider <em>conditional</em> means (or variances) <em>given</em> the design matrix
<span class="math display">\[X=\begin{pmatrix}1&amp;X_1\\\vdots&amp;\vdots\\1&amp;X_n\end{pmatrix}.\]</span>
Using the conditional mean, <span class="math inline">\(E[\hat{\beta}|X]\)</span>, we can define the  bias:
<span class="math display">\[\begin{align*}
\operatorname{Bias}(\hat{\beta}|X)=E[\hat{\beta}|X]-\beta.
\end{align*}\]</span>
For fixed designs, conditioning on <span class="math inline">\(X\)</span> is not false but superfluous since <span class="math inline">\(X\)</span> remains anyways fixed in repeated samples and therefore <span class="math inline">\(\operatorname{Bias}(\hat{\beta}|X)=\operatorname{Bias}(\hat{\beta})=E[\hat{\beta}]-\beta\)</span>.</p>
<p>The OLS estimator is called , if its conditional expectation is equal to the true parameter, i.e. if
<span class="math display">\[\begin{align*}
E[\hat{\beta}|X]=\beta\quad
\Leftrightarrow\quad\operatorname{Bias}(\hat\beta|X)=0
\end{align*}\]</span>
That is, the conditional mean of the random estimator equals the true parameter value which we want to estimate. Put another way, unbiasedness means that the conditional distribution of the estimator is centered on the true value of the parameter. Of course, the probability of a single estimate being  equal to the truth in any particular sample is zero if the error terms <span class="math inline">\(\varepsilon_i\)</span> (and therefore the <span class="math inline">\(Y_i\)</span>) are continuous random variables.</p>
<p>To show that <span class="math inline">\(\hat\beta_1\)</span> is conditionally unbiased, we need to show that its conditional expectation is equal to <span class="math inline">\(\beta_1\)</span>:
<span class="math display">\[\begin{align*}
E[\hat\beta_1|X]&amp;=E\left[\dfrac{\sum_{i=1}^n(X_i - \bar X)Y_i}{\sum_{i=1}^n(X_i - \bar X)^2}\mid X\right] 
               =E\left[\dfrac{\sum_{i=1}^n(X_i - \bar X)(\beta_0 + \beta_1 X_i + \varepsilon_i)}{\sum_{i=1}^n(X_i - \bar X)^2}|X\right]
\end{align*}\]</span>
In case of a deterministic design, <span class="math inline">\(X\)</span> is fixed in repeated samples which makes the conditioning on <span class="math inline">\(X\)</span> superfluous since, for instance, <span class="math inline">\(\sum(X_i - \bar X)^2\)</span> is then a constant which allows us to bring it outside of the expectation; the same applies to all other terms that involve  deterministic quantities. So, the <em>conditioning</em> <span class="math inline">\(X\)</span> is actually only necessary under the random design, where it allows us then to bring all terms containing  <span class="math inline">\(X\)</span> variables (or deterministic quantities) outside of the expectation. However, interpretations then must consider the conditioning on <span class="math inline">\(X\)</span>.
<!-- Namely, we investigate the conditional random variable $\hat\beta_1|X$ where randomness is only due to the error terms $\varepsilon_1$ --></p>
So, we really only need to concern ourselves with the numerator, which we can write as:
<span class="math display">\[\begin{equation*}
\beta_0 \sum_{i=1}^n(X_i - \bar X)+ 
\beta_1 \sum_{i=1}^n(X_i - \bar X)X_i + 
E\left[\sum_{i=1}^n(X_i - \bar X)\varepsilon_i\mid X\right]
\end{equation*}\]</span>
For the first two terms, note that
<span class="math display">\[\begin{align*}
\beta_0 \sum_{i=1}^n(X_i - \bar X)    &amp;=\beta_0 \left[\left(\sum_{i=1}^n X_i \right) - n \bar X\right] = 0 \\
\beta_1 \sum_{i=1}^n(X_i - \bar X)X_i &amp;=\beta_1 \sum_{i=1}^n(X_i - \bar X)^2,
\end{align*}\]</span>
where the latter result follows again from the  that will be discussed in the exercises of this chapter. So we are left with
<span class="math display">\[E[\hat\beta_1|X]= \beta_1 + \dfrac{1}{\sum_{i=1}^n(X_i-\bar X)^2}E\left[\sum_{i=1}^n(X_i - \bar X)\varepsilon_i\mid X\right]\]</span>
Clearly, then, we want the last term here to be zero, so that <span class="math inline">\(\hat\beta_1\)</span> is conditionally unbiased. The last term is zero if the factor <span class="math inline">\(E\left[\sum(X_i - \bar X)\varepsilon_i|X\right]=0\)</span>. For fixed/random designs we have the following expressions:
<p>That is, the OLS estimator, <span class="math inline">\(\hat\beta_1\)</span>, is (conditionally) unbiased if the exogeneity assumption <span class="math inline">\(E[\varepsilon_i |X_i]=0\)</span> holds (or if <span class="math inline">\(E[\varepsilon_i]=0\)</span> in case of the fixed design). We return, therefore, to the importance of the exogeneity assumption <span class="math inline">\(E[\varepsilon | X_i]\)</span> since if this assumption does not hold, the estimator is biased and remains biased even asymptotically as <span class="math inline">\(n\to\infty\)</span>.</p>
<p>The unbiasedness of <span class="math inline">\(\hat\beta_1\)</span> implies that <span class="math inline">\(\hat\beta_0=\bar{Y}-\hat\beta_1\bar{X}\)</span> is also an unbiased estimator, since
<span class="math display">\[\begin{align*}
E[\hat\beta_0|X]
&amp;=E\left[\bar Y - \hat\beta_1 \bar X|X\right]\\
&amp;=E\left[\dfrac{1}{n}\sum_{i=1}^n (\beta_0 + \beta_1 X_i + \varepsilon_i) - \hat\beta_1 \bar X |X\right]\\
&amp;=\beta_0 + \beta_1\bar X + \dfrac{1}{n}\sum_{i=1}^n E[\varepsilon_i|X_i] - E[\hat\beta_1|X] \bar X \\
&amp;=\beta_0
\end{align*}\]</span>
where the last line follows from the exogeneity assumption <span class="math inline">\(E[\varepsilon_i|X]=0\)</span> and the unbiasedness of <span class="math inline">\(\hat\beta_1\)</span>.</p>
<p>Since the conditional means of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, given <span class="math inline">\(X\)</span>, do not depend on <span class="math inline">\(X\)</span>, we have that<br />
<span class="math display">\[\begin{align*}
E[\hat\beta_0|X]&amp;=\beta_0\quad\Rightarrow\quad E[E[\hat\beta_0|X]]=\beta_0\\
E[\hat\beta_1|X]&amp;=\beta_1\quad\Rightarrow\quad E[E[\hat\beta_1|X]]=\beta_1.
\end{align*}\]</span>
This implies that the conditional biases of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, given <span class="math inline">\(X\)</span>, also do not depend on <span class="math inline">\(X\)</span>,
<span class="math display">\[\begin{align*}
\operatorname{Bias}(\hat{\beta}_0|X)&amp;=E[\hat\beta_0|X]-\beta_0 =0\\
\operatorname{Bias}(\hat{\beta}_1|X)&amp;=E[\hat\beta_1|X]-\beta_1 =0.
\end{align*}\]</span>
Consequently, <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are also unbiased,  on <span class="math inline">\(X\)</span>,
<span class="math display">\[\begin{align*}
\underbrace{E[\operatorname{Bias}(\hat{\beta}_0|X)]}_{=\operatorname{Bias}(\hat{\beta}_0)}&amp;=E[E[\hat\beta_0|X]]-\beta_0 =0\\
\underbrace{E[\operatorname{Bias}(\hat{\beta}_1|X)]}_{=\operatorname{Bias}(\hat{\beta}_1)}&amp;=E[E[\hat\beta_1|X]]-\beta_1 =0.
\end{align*}\]</span></p>
<p>Note that the exogeneity assumption, <span class="math inline">\(E[\varepsilon_i|X]=0\)</span>, is the only distributional assumption on the error terms (and the <span class="math inline">\(X_i\)</span>’s) that is needed to show that the OLS estimator is unbiased. In fact, OLS remains unbiased if the error terms are heteroscedastic and/or autocorrelated. However, under the more restrictive assumption that the error terms are <em>homoscedastic</em> and <em>not autocorrelated</em>, one can show that the OLS estimator is the most efficient (lowest variance) estimator within the family of all unbiased estimators for the regression paraemters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. This result is known as the Gauss-Markov Theorem and we will consider this theorem in the context of the multiple regression model in more detail (see Chapter <a href="ch:MLR.html#ch:MLR">4</a>).</p>
</div>
<div id="variance-of-the-ols-estimator" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Variance of the OLS Estimator</h3>
<p>Remember: An unbiased estimator can still be a very imprecise estimator if it has a large variance. In order to derive the variance of the OLS estimator one needs more than only the exogeneity assumption. One gets different variance expressions for different distributional assumptions on the error terms <span class="math inline">\(\varepsilon_1,\dots,\varepsilon_n\)</span>. As we consider this in more detail for the multiple regression model, we consider in the following only the most simple case, namely, conditionally homoscedastic i.i.d. error terms. Under this simple scenario it can be shown that
<span class="math display">\[\begin{align}
\operatorname{Var}(\hat\beta_0|X)&amp;= \frac{1}{n} \sigma^2\cdot\frac{n^{-1}\sum_{i=1}^n X_i^2}{n^{-1}\sum_{i=1}^n\left(X_i-\bar{X} \right)^2}\label{eq:varbeta0}\\
\operatorname{Var}(\hat\beta_1|X)&amp;= \frac{1}{n} \sigma^2\cdot\frac{1}{n^{-1}\sum_{i=1}^n\left(X_i-\bar{X} \right)^2}, \label{eq:varbeta1}
\end{align}\]</span>
assuming that <span class="math inline">\(\operatorname{Var}(\varepsilon_i|X)=\sigma^2\)</span>. That is equations  and  are only sensible for the case of homoscedastic error terms. From equations  and  it follows that <span class="math inline">\(\operatorname{Var}(\hat\beta_0|X)\)</span> and <span class="math inline">\(\operatorname{Var}(\hat\beta_1|X)\)</span> become eventually small as <span class="math inline">\(n\to\infty\)</span> which is good since this means that the estimators become more precise as the sample size <span class="math inline">\(n\)</span> increases.</p>
<p>By contrast to the conditional biases, the conditional variances <span class="math inline">\(\operatorname{Var}(\hat\beta_0|X)\)</span> and <span class="math inline">\(\operatorname{Var}(\hat\beta_1|X)\)</span> depend on <span class="math inline">\(X\)</span>.</p>
<!-- \begin{align*} -->
<!-- \operatorname{Var}(\hat\beta_0|X)&\to_p 0 \quad\text{as}\quad n\to\infty\\ -->
<!-- \operatorname{Var}(\hat\beta_1|X)&\to_p 0 \quad\text{as}\quad n\to\infty -->
<!-- \end{align*} -->
<!-- Thus, generally  -->
<!-- \begin{align*} -->
<!-- \operatorname{Var}(\hat\beta_0)&=E[\operatorname{Var}(\hat\beta_0|X)]\neq \operatorname{Var}(\hat\beta_0|X)\\ -->
<!-- \operatorname{Var}(\hat\beta_1)&=E[\operatorname{Var}(\hat\beta_1|X)]\neq \operatorname{Var}(\hat\beta_1|X) -->
<!-- \end{align*} -->
<p>The variance expressions in  and  are not really useful in practice since usually we do not know the variance of the error terms <span class="math inline">\(\sigma^2\)</span>. Under the i.i.d. assumption we can replace the unknown <span class="math inline">\(\sigma^2\)</span> by plugging-in reasonable estimates such as
<span class="math display">\[\begin{align*}
s_{ML}^2&amp;=\frac{1}{n}\sum_{i=1}^n \hat\eps_i^2\qquad\text{or}\qquad s_{UB}^2=\frac{1}{n-2}\sum_{i=1}^n \hat\eps_i^2
\end{align*}\]</span>
where <span class="math inline">\(s^2_{UB}\)</span> is the unbiased estimator and <span class="math inline">\(s_{ML}^2\)</span> the maximum likelihood estimator of <span class="math inline">\(\sigma^2\)</span>.
This leads to the practically relevant variance estimators
<span class="math display">\[\begin{align*}
\widehat{\operatorname{Var}}(\hat\beta_0|X)&amp;= \frac{1}{n} s^2\cdot\frac{n^{-1}\sum_{i=1}^n X_i^2}{n^{-1}\sum_{i=1}^n\left(X_i-\bar{X} \right)^2}\\
\widehat{\operatorname{Var}}(\hat\beta_1|X)&amp;= \frac{1}{n} s^2\cdot\frac{1}{n^{-1}\sum_{i=1}^n\left(X_i-\bar{X} \right)^2},
\end{align*}\]</span>
where <span class="math inline">\(s^2=s_{ML}^2\)</span> or <span class="math inline">\(s^2=s^2_{UB}\)</span>.</p>
</div>
<div id="consistency-of-the-ols-estimator" class="section level3" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Consistency of the OLS Estimator</h3>
<p>Now, we have everything together in order to show that the OLS estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are <strong>consistent</strong> estimators of the unknown parameter values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Since the OLS estimator is unbiased, we do not need to bother with a possible bias-problem—provided the underlying assumptions of the simple linear regression model are true. Moreover, since the variances of the estimators <span class="math inline">\(\hat\beta_0\equiv\hat\beta_{0n}\)</span> and <span class="math inline">\(\hat\beta_1\equiv\hat\beta_{1n}\)</span> converge to zero as <span class="math inline">\(n\to\infty\)</span>, we have that the Mean Squared Error (MSE) converges to zero, i.e.
<span class="math display">\[\begin{align*}
\operatorname{MSE}(\hat\beta_{jn}|X)&amp;=E\left[(\hat\beta_{jn}-\beta_j)^2|X\right]\\
&amp;=\underbrace{\left(\operatorname{Bias}(\hat\beta_{jn}|X)\right)^2}_{=0}+\operatorname{Var}(\hat\beta_{jn}|X)\to 0\;\;\text{as}\;\;n\to\infty\;\;\text{for}\;\;j=0,1.
\end{align*}\]</span>
This means that the estimators <span class="math inline">\(\hat\beta_{0n}\)</span> and <span class="math inline">\(\hat\beta_{1n}\)</span>  as <span class="math inline">\(n\to\infty\)</span>. Fortunately, mean-square convergence implies  or short <span class="math inline">\(\hat\beta_{jn}\to_p\beta_j\)</span> as <span class="math inline">\(n\to\infty\)</span> for <span class="math inline">\(j=0,1\)</span>. The latter result is exactly what we mean by saying that an estimator is (weakly) .</p>
<p>Let <span class="math inline">\(\hat\theta\)</span> be some estimator of a univariate parameter <span class="math inline">\(\theta\)</span>, then we can decompose the MSE of <span class="math inline">\(\hat\theta\)</span> as following
<span class="math display">\[\begin{align*}
\operatorname{MSE}(\hat\theta)
&amp;=E\left[(\hat\theta-\theta)^2\right]\\
&amp;=E\left[\left(\big(\hat\theta\underbrace{-E(\hat\theta)\big)+\big(E(\hat\theta)}_{=0}-\theta\big)\right)^2\right]\\
&amp;=E\left[(\hat\theta-E(\hat\theta))^2+2(\hat\theta-E(\hat\theta))(E(\hat\theta)-\theta)+\overbrace{(E(\hat\theta)-\theta)^2}^{\text{deterministic}}\right]\\
&amp;=\underbrace{E\left[(\hat\theta-E(\hat\theta))^2\right]}_{=\V(\hat\theta)}+\underbrace{E\left[2(\hat\theta-E(\hat\theta))(E(\hat\theta)-\theta)\right]}_{=0,\;\text{\small since $E(\hat\theta)-E(\hat\theta)=0$}}+\underbrace{(E(\hat\theta)-\theta)^2}_{=(\operatorname{Bias}(\hat\theta))^2}\\
&amp;=\V(\hat\theta) + \left(\operatorname{Bias}(\hat\theta)\right)^2
\end{align*}\]</span></p>
<!-- ## Summary of Assumptions: The Simple Linear Regression Model {#sec:SLRM} -->
<!-- The simple linear regression model can be defined by the following set of assumptions: -->
<!-- \paragraph*{Assumption 1: The Linear Model Assumption.} -->
<!-- $$ -->
<!-- Y_i=\beta_0+\beta_1 X_{i}+\varepsilon_i, \quad i=1,\dots,n. -->
<!-- $$ -->
<!-- Plainly spoken: \say{the simple linear model is true} (let's hope it is). Moreover, we adopt the classical sampling assumption when analyzing data that was randomly sampled from a large population: -->
<!-- \begin{itemize} -->
<!-- \item In case of random designs, we assume that the random variables $(Y_i,X_i)$ are i.i.d. across observations $i=1,\dots,n$.   -->
<!-- \item In the case of fixed design, we assume that the random variables $Y_i$ are i.i.d. across observations $i=1,\dots,n$.  -->
<!-- \end{itemize} -->
<!-- \paragraph*{Assumption 2: Strict Exogeneity.} -->
<!-- $$E(\varepsilon_i|X_i)=0\quad\text{for all }i=1,\dots,n$$ -->
<!-- The means of the error terms $\varepsilon_1,\dots,\varepsilon_n$ are zero -- no matter the realizations of $X_1,\dots,X_n$.  -->
<!-- In case of fixed designs, it is assumed that $E(\eps_i)=0$ for all $i=1,\dots,n$.  -->
<!-- \paragraph*{Assumption 3: No perfect collinearity.} -->
<!-- $$\rank(X)=2\quad\text{a.s.}$$ -->
<!-- This assumption demands that the event of $(X_1,\dots,X_n)$ being linearly dependent of the constant vector $(1,\dots,1)$ occurs with a probability equal to zero. (This is the literal translation of the "almost surely (a.s.)" concept.) It implies that $n\geq 2$.  -->
<!-- Note: This assumption is not so critical in the simple linear regression model, but will be a critical assumption for the multiple linear regression model.  -->
<!-- \paragraph*{Assumption 4: Error distribution.} Depending on the context (estimation or hypothesis testing, small $n$ or large $n$) there are different more or less restrictive assumptions.  Some of the most common ones are the following: -->
<!-- \begin{itemize} -->
<!-- \item\textbf{i.i.d. Normal.} $\varepsilon_i\overset{\operatorname{i.i.d.}}{\sim}\mathcal{N}(0,\sigma)$ for all $i=1,\dots,n$. -->
<!-- \item\textbf{i.i.d.} The error terms $\varepsilon_1,\dots,\varepsilon_n$ are i.i.d. -->
<!-- \item \textbf{Spherical errors (\say{Gauss-Markov assumptions}).} The error terms $\varepsilon_1,\dots,\varepsilon_n$ might have different distributions, but their variances and covariances are such that -->
<!-- \begin{align*} -->
<!-- E(\varepsilon_i^2|X)        &=\sigma^2>0\quad\text{for all }i=1,\dots,n\\ -->
<!-- E(\varepsilon_i\varepsilon_j|X)&=0,\quad\quad i\neq j. -->
<!-- \end{align*} -->
<!-- \paragraph*{Technical Note.} When we write that $E(\varepsilon_i^2|X)=\sigma^2$, we implicitly assume that the second moment of $\varepsilon_i$ exists and that it is finite.  -->
<!-- \end{itemize} -->

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="ordinary-least-squares-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch:MLR.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Script_Econometrics_MSc.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed",
"download": "https://uni-bonn.sciebo.de/s/IZ2yeLeA4dRTFUY",
"search": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
