<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Econometrics with R</title>
  <meta name="description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Econometrics with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Econometrics with R" />
  
  <meta name="twitter:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2018-10-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="arosur.html">
<link rel="next" href="htaciitslrm.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<!-- font families -->

<link href="https://fonts.googleapis.com/css?family=PT+Sans|Pacifico|Source+Sans+Pro" rel="stylesheet">

<!-- function to adjust height of iframes automatically depending on content loaded -->

<script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';
  }
</script>

<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/default.js"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110299877-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110299877-1');
</script>

<!-- open review block -->

<script async defer src="https://hypothes.is/embed.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://www.oek.wiwi.uni-due.de/en/">Chair of Econometrics at UDE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#a-very-short-introduction-to-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> A Very Short Introduction to <tt>R</tt> and <em>RStudio</em></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="pt.html"><a href="pt.html#random-variables-and-probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#expected-value-mean-and-variance"><i class="fa fa-check"></i>Expected Value, Mean and Variance</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#thetdist"><i class="fa fa-check"></i>The Student t Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-f-distribution"><i class="fa fa-check"></i>The F Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pt.html"><a href="pt.html#RSATDOSA"><i class="fa fa-check"></i><b>2.2</b> Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="pt.html"><a href="pt.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arosur.html"><a href="arosur.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="3.1" data-path="arosur.html"><a href="arosur.html#estimation-of-the-population-mean"><i class="fa fa-check"></i><b>3.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="3.2" data-path="arosur.html"><a href="arosur.html#potsm"><i class="fa fa-check"></i><b>3.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="3.3" data-path="arosur.html"><a href="arosur.html#hypothesis-tests-concerning-the-population-mean"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests Concerning the Population Mean</a><ul>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-p-value"><i class="fa fa-check"></i>The p-Value</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-known"><i class="fa fa-check"></i>Calculating the p-Value when the Standard Deviation is Known</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#SVSSDASE"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-unknown"><i class="fa fa-check"></i>Calculating the p-value When the Standard Deviation is Unknown</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-t-statistic"><i class="fa fa-check"></i>The t-statistic</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="arosur.html"><a href="arosur.html#confidence-intervals-for-the-population-mean"><i class="fa fa-check"></i><b>3.4</b> Confidence Intervals for the Population Mean</a></li>
<li class="chapter" data-level="3.5" data-path="arosur.html"><a href="arosur.html#cmfdp"><i class="fa fa-check"></i><b>3.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="3.6" data-path="arosur.html"><a href="arosur.html#aattggoe"><i class="fa fa-check"></i><b>3.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="3.7" data-path="arosur.html"><a href="arosur.html#scatterplots-sample-covariance-and-sample-correlation"><i class="fa fa-check"></i><b>3.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
<li class="chapter" data-level="3.8" data-path="arosur.html"><a href="arosur.html#exercises-1"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="4.1" data-path="lrwor.html"><a href="lrwor.html#simple-linear-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="4.2" data-path="lrwor.html"><a href="lrwor.html#estimating-the-coefficients-of-the-linear-regression-model"><i class="fa fa-check"></i><b>4.2</b> Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lrwor.html"><a href="lrwor.html#measures-of-fit"><i class="fa fa-check"></i><b>4.3</b> Measures of Fit</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-coefficient-of-determination"><i class="fa fa-check"></i>The Coefficient of Determination</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-standard-error-of-the-regression"><i class="fa fa-check"></i>The Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lrwor.html"><a href="lrwor.html#tlsa"><i class="fa fa-check"></i><b>4.4</b> The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption 1: The Error Term has Conditional Mean of Zero</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-2-independently-and-identically-distributed-data"><i class="fa fa-check"></i>Assumption 2: Independently and Identically Distributed Data</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption 3: Large Outliers are Unlikely</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="lrwor.html"><a href="lrwor.html#tsdotoe"><i class="fa fa-check"></i><b>4.5</b> The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-1"><i class="fa fa-check"></i>Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-2"><i class="fa fa-check"></i>Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-3"><i class="fa fa-check"></i>Simulation Study 3</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="lrwor.html"><a href="lrwor.html#exercises-2"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="htaciitslrm.html"><a href="htaciitslrm.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li class="chapter" data-level="5.1" data-path="htaciitslrm.html"><a href="htaciitslrm.html#testing-two-sided-hypotheses-concerning-the-slope-coefficient"><i class="fa fa-check"></i><b>5.1</b> Testing Two-Sided Hypotheses Concerning the Slope Coefficient</a></li>
<li class="chapter" data-level="5.2" data-path="htaciitslrm.html"><a href="htaciitslrm.html#cifrc"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-confidence-intervals"><i class="fa fa-check"></i>Simulation Study: Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="htaciitslrm.html"><a href="htaciitslrm.html#rwxiabv"><i class="fa fa-check"></i><b>5.3</b> Regression when X is a Binary Variable</a></li>
<li class="chapter" data-level="5.4" data-path="htaciitslrm.html"><a href="htaciitslrm.html#hah"><i class="fa fa-check"></i><b>5.4</b> Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="htaciitslrm.html"><a href="htaciitslrm.html#the-gauss-markov-theorem"><i class="fa fa-check"></i><b>5.5</b> The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-blue-estimator"><i class="fa fa-check"></i>Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="htaciitslrm.html"><a href="htaciitslrm.html#using-the-t-statistic-in-regression-when-the-sample-size-is-small"><i class="fa fa-check"></i><b>5.6</b> Using the t-Statistic in Regression When the Sample Size Is Small</a></li>
<li class="chapter" data-level="5.7" data-path="htaciitslrm.html"><a href="htaciitslrm.html#exercises-3"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rmwmr.html"><a href="rmwmr.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="6.1" data-path="rmwmr.html"><a href="rmwmr.html#omitted-variable-bias"><i class="fa fa-check"></i><b>6.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="6.2" data-path="rmwmr.html"><a href="rmwmr.html#tmrm"><i class="fa fa-check"></i><b>6.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="rmwmr.html"><a href="rmwmr.html#mofimr"><i class="fa fa-check"></i><b>6.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="6.4" data-path="rmwmr.html"><a href="rmwmr.html#ols-assumptions-in-multiple-regression"><i class="fa fa-check"></i><b>6.4</b> OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#simulation-study-imperfect-multicollinearity"><i class="fa fa-check"></i>Simulation Study: Imperfect Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="rmwmr.html"><a href="rmwmr.html#the-distribution-of-the-ols-estimators-in-multiple-regression"><i class="fa fa-check"></i><b>6.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
<li class="chapter" data-level="6.6" data-path="rmwmr.html"><a href="rmwmr.html#exercises-4"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="htaciimr.html"><a href="htaciimr.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence Intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="htaciimr.html"><a href="htaciimr.html#hypothesis-tests-and-confidence-intervals-for-a-single-coefficient"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="htaciimr.html"><a href="htaciimr.html#an-application-to-test-scores-and-the-student-teacher-ratio"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="htaciimr.html"><a href="htaciimr.html#joint-hypothesis-testing-using-the-f-statistic"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the F-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="htaciimr.html"><a href="htaciimr.html#confidence-sets-for-multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-for-multiple-regression"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="htaciimr.html"><a href="htaciimr.html#analysis-of-the-test-score-data-set"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
<li class="chapter" data-level="7.7" data-path="htaciimr.html"><a href="htaciimr.html#exercises-5"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nrf.html"><a href="nrf.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="8.1" data-path="nrf.html"><a href="nrf.html#a-general-strategy-for-modelling-nonlinear-regression-functions"><i class="fa fa-check"></i><b>8.1</b> A General Strategy for Modelling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="8.2" data-path="nrf.html"><a href="nrf.html#nfoasiv"><i class="fa fa-check"></i><b>8.2</b> Nonlinear Functions of a Single Independent Variable</a><ul>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="nrf.html"><a href="nrf.html#interactions-between-independent-variables"><i class="fa fa-check"></i><b>8.3</b> Interactions Between Independent Variables</a></li>
<li class="chapter" data-level="8.4" data-path="nrf.html"><a href="nrf.html#nonlinear-effects-on-test-scores-of-the-student-teacher-ratio"><i class="fa fa-check"></i><b>8.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
<li class="chapter" data-level="8.5" data-path="nrf.html"><a href="nrf.html#exercises-6"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="asbomr.html"><a href="asbomr.html"><i class="fa fa-check"></i><b>9</b> Assessing Studies Based on Multiple Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity"><i class="fa fa-check"></i><b>9.1</b> Internal and External Validity</a></li>
<li class="chapter" data-level="9.2" data-path="asbomr.html"><a href="asbomr.html#ttivomra"><i class="fa fa-check"></i><b>9.2</b> Threats to Internal Validity of Multiple Regression Analysis</a></li>
<li class="chapter" data-level="9.3" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity-when-the-regression-is-used-for-forecasting"><i class="fa fa-check"></i><b>9.3</b> Internal and External Validity when the Regression is Used for Forecasting</a></li>
<li class="chapter" data-level="9.4" data-path="asbomr.html"><a href="asbomr.html#etsacs"><i class="fa fa-check"></i><b>9.4</b> Example: Test Scores and Class Size</a></li>
<li class="chapter" data-level="9.5" data-path="asbomr.html"><a href="asbomr.html#exercises-7"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="rwpd.html"><a href="rwpd.html"><i class="fa fa-check"></i><b>10</b> Regression with Panel Data</a><ul>
<li class="chapter" data-level="10.1" data-path="rwpd.html"><a href="rwpd.html#panel-data"><i class="fa fa-check"></i><b>10.1</b> Panel Data</a></li>
<li class="chapter" data-level="10.2" data-path="rwpd.html"><a href="rwpd.html#PDWTTP"><i class="fa fa-check"></i><b>10.2</b> Panel Data with Two Time Periods: “Before and After” Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="rwpd.html"><a href="rwpd.html#fixed-effects-regression"><i class="fa fa-check"></i><b>10.3</b> Fixed Effects Regression</a><ul>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#estimation-and-inference"><i class="fa fa-check"></i>Estimation and Inference</a></li>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#application-to-traffic-deaths"><i class="fa fa-check"></i>Application to Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="rwpd.html"><a href="rwpd.html#regression-with-time-fixed-effects"><i class="fa fa-check"></i><b>10.4</b> Regression with Time Fixed Effects</a></li>
<li class="chapter" data-level="10.5" data-path="rwpd.html"><a href="rwpd.html#tferaaseffer"><i class="fa fa-check"></i><b>10.5</b> The Fixed Effects Regression Assumptions and Standard Errors for Fixed Effects Regression</a></li>
<li class="chapter" data-level="10.6" data-path="rwpd.html"><a href="rwpd.html#drunk-driving-laws-and-traffic-deaths"><i class="fa fa-check"></i><b>10.6</b> Drunk Driving Laws and Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="rwabdv.html"><a href="rwabdv.html"><i class="fa fa-check"></i><b>11</b> Regression with a Binary Dependent Variable</a><ul>
<li class="chapter" data-level="11.1" data-path="rwabdv.html"><a href="rwabdv.html#binary-dependent-variables-and-the-linear-probability-model"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables and the Linear Probability Model</a></li>
<li class="chapter" data-level="11.2" data-path="rwabdv.html"><a href="rwabdv.html#palr"><i class="fa fa-check"></i><b>11.2</b> Probit and Logit Regression</a><ul>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#probit-regression"><i class="fa fa-check"></i>Probit Regression</a></li>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#logit-regression"><i class="fa fa-check"></i>Logit Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="rwabdv.html"><a href="rwabdv.html#estimation-and-inference-in-the-logit-and-probit-models"><i class="fa fa-check"></i><b>11.3</b> Estimation and Inference in the Logit and Probit Models</a></li>
<li class="chapter" data-level="11.4" data-path="rwabdv.html"><a href="rwabdv.html#application-to-the-boston-hmda-data"><i class="fa fa-check"></i><b>11.4</b> Application to the Boston HMDA Data</a></li>
<li class="chapter" data-level="11.5" data-path="rwabdv.html"><a href="rwabdv.html#exercises-8"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ivr.html"><a href="ivr.html"><i class="fa fa-check"></i><b>12</b> Instrumental Variables Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="ivr.html"><a href="ivr.html#TIVEWASRAASI"><i class="fa fa-check"></i><b>12.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a></li>
<li class="chapter" data-level="12.2" data-path="ivr.html"><a href="ivr.html#TGIVRM"><i class="fa fa-check"></i><b>12.2</b> The General IV Regression Model</a></li>
<li class="chapter" data-level="12.3" data-path="ivr.html"><a href="ivr.html#civ"><i class="fa fa-check"></i><b>12.3</b> Checking Instrument Validity</a></li>
<li class="chapter" data-level="12.4" data-path="ivr.html"><a href="ivr.html#attdfc"><i class="fa fa-check"></i><b>12.4</b> Application to the Demand for Cigarettes</a></li>
<li class="chapter" data-level="12.5" data-path="ivr.html"><a href="ivr.html#where-do-valid-instruments-come-from"><i class="fa fa-check"></i><b>12.5</b> Where Do Valid Instruments Come From?</a></li>
<li class="chapter" data-level="12.6" data-path="ivr.html"><a href="ivr.html#exercises-9"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="eaqe.html"><a href="eaqe.html"><i class="fa fa-check"></i><b>13</b> Experiments and Quasi-Experiments</a><ul>
<li class="chapter" data-level="13.1" data-path="eaqe.html"><a href="eaqe.html#potential-outcomes-causal-effects-and-idealized-experiments"><i class="fa fa-check"></i><b>13.1</b> Potential Outcomes, Causal Effects and Idealized Experiments</a></li>
<li class="chapter" data-level="13.2" data-path="eaqe.html"><a href="eaqe.html#threats-to-validity-of-experiments"><i class="fa fa-check"></i><b>13.2</b> Threats to Validity of Experiments</a></li>
<li class="chapter" data-level="13.3" data-path="eaqe.html"><a href="eaqe.html#experimental-estimates-of-the-effect-of-class-size-reductions"><i class="fa fa-check"></i><b>13.3</b> Experimental Estimates of the Effect of Class Size Reductions</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#experimental-design-and-the-data-set"><i class="fa fa-check"></i>Experimental Design and the Data Set</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#analysis-of-the-star-data"><i class="fa fa-check"></i>Analysis of the STAR Data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="eaqe.html"><a href="eaqe.html#quasi-experiments"><i class="fa fa-check"></i><b>13.4</b> Quasi Experiments</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#the-differences-in-differences-estimator"><i class="fa fa-check"></i>The Differences-in-Differences Estimator</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#regression-discontinuity-estimators"><i class="fa fa-check"></i>Regression Discontinuity Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ittsraf.html"><a href="ittsraf.html"><i class="fa fa-check"></i><b>14</b> Introduction to Time Series Regression and Forecasting</a><ul>
<li class="chapter" data-level="14.1" data-path="ittsraf.html"><a href="ittsraf.html#using-regression-models-for-forecasting"><i class="fa fa-check"></i><b>14.1</b> Using Regression Models for Forecasting</a></li>
<li class="chapter" data-level="14.2" data-path="ittsraf.html"><a href="ittsraf.html#tsdasc"><i class="fa fa-check"></i><b>14.2</b> Time Series Data and Serial Correlation</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#notation-lags-differences-logarithms-and-growth-rates"><i class="fa fa-check"></i>Notation, Lags, Differences, Logarithms and Growth Rates</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ittsraf.html"><a href="ittsraf.html#autoregressions"><i class="fa fa-check"></i><b>14.3</b> Autoregressions</a><ul>
<li><a href="ittsraf.html#autoregressive-models-of-order-p">Autoregressive Models of Order <span class="math inline">\(p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ittsraf.html"><a href="ittsraf.html#cybtmpi"><i class="fa fa-check"></i><b>14.4</b> Can You Beat the Market? (Part I)</a></li>
<li class="chapter" data-level="14.5" data-path="ittsraf.html"><a href="ittsraf.html#apatadlm"><i class="fa fa-check"></i><b>14.5</b> Additional Predictors and The ADL Model</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#forecast-uncertainty-and-forecast-intervals"><i class="fa fa-check"></i>Forecast Uncertainty and Forecast Intervals</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="ittsraf.html"><a href="ittsraf.html#llsuic"><i class="fa fa-check"></i><b>14.6</b> Lag Length Selection Using Information Criteria</a></li>
<li class="chapter" data-level="14.7" data-path="ittsraf.html"><a href="ittsraf.html#nit"><i class="fa fa-check"></i><b>14.7</b> Nonstationarity I: Trends</a></li>
<li class="chapter" data-level="14.8" data-path="ittsraf.html"><a href="ittsraf.html#niib"><i class="fa fa-check"></i><b>14.8</b> Nonstationarity II: Breaks</a></li>
<li class="chapter" data-level="14.9" data-path="ittsraf.html"><a href="ittsraf.html#can-you-beat-the-market-part-ii"><i class="fa fa-check"></i><b>14.9</b> Can You Beat the Market? (Part II)</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="eodce.html"><a href="eodce.html"><i class="fa fa-check"></i><b>15</b> Estimation of Dynamic Causal Effects</a><ul>
<li class="chapter" data-level="15.1" data-path="eodce.html"><a href="eodce.html#the-orange-juice-data"><i class="fa fa-check"></i><b>15.1</b> The Orange Juice Data</a></li>
<li class="chapter" data-level="15.2" data-path="eodce.html"><a href="eodce.html#dynamic-causal-effects"><i class="fa fa-check"></i><b>15.2</b> Dynamic Causal Effects</a></li>
<li class="chapter" data-level="15.3" data-path="eodce.html"><a href="eodce.html#dynamic-multipliers-and-cumulative-dynamic-multipliers"><i class="fa fa-check"></i><b>15.3</b> Dynamic Multipliers and Cumulative Dynamic Multipliers</a></li>
<li class="chapter" data-level="15.4" data-path="eodce.html"><a href="eodce.html#hac-standard-errors"><i class="fa fa-check"></i><b>15.4</b> HAC Standard Errors</a></li>
<li class="chapter" data-level="15.5" data-path="eodce.html"><a href="eodce.html#estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors"><i class="fa fa-check"></i><b>15.5</b> Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors</a></li>
<li class="chapter" data-level="15.6" data-path="eodce.html"><a href="eodce.html#orange-juice-prices-and-cold-weather"><i class="fa fa-check"></i><b>15.6</b> Orange Juice Prices and Cold Weather</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="atitsr.html"><a href="atitsr.html"><i class="fa fa-check"></i><b>16</b> Additional Topics in Time Series Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="atitsr.html"><a href="atitsr.html#vector-autoregressions"><i class="fa fa-check"></i><b>16.1</b> Vector Autoregressions</a></li>
<li class="chapter" data-level="16.2" data-path="atitsr.html"><a href="atitsr.html#ooiatdfglsurt"><i class="fa fa-check"></i><b>16.2</b> Orders of Integration and the DF-GLS Unit Root Test</a></li>
<li class="chapter" data-level="16.3" data-path="atitsr.html"><a href="atitsr.html#cointegration"><i class="fa fa-check"></i><b>16.3</b> Cointegration</a></li>
<li class="chapter" data-level="16.4" data-path="atitsr.html"><a href="atitsr.html#volatility-clustering-and-autoregressive-conditional-heteroskedasticity"><i class="fa fa-check"></i><b>16.4</b> Volatility Clustering and Autoregressive Conditional Heteroskedasticity</a><ul>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#arch-and-garch-models"><i class="fa fa-check"></i>ARCH and GARCH Models</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#application-to-stock-price-volatility"><i class="fa fa-check"></i>Application to Stock Price Volatility</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#summary-8"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Econometrics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div class = rmdreview>
This book is in <b>Open Review</b>. We want your feedback to make the book better for you and other students. You may annotate some text by <span style="background-color: #3297FD; color: white">selecting it with the cursor</span> and then click the <i class="h-icon-annotate"></i> on the pop-up menu. You can also see the annotations of others: click the <i class="h-icon-chevron-left"></i> in the upper right hand corner of the page <i class="fa fa-arrow-circle-right  fa-rotate-315" aria-hidden="true"></i>
</div>
<div id="lrwor" class="section level1">
<h1><span class="header-section-number">4</span> Linear Regression with One Regressor</h1>
<p>This chapter introduces the basics in linear regression and shows how to perform regression analysis in <tt>R</tt>. In linear regression, the aim is to model the relationship between a dependent variable <span class="math inline">\(Y\)</span> and one or more explanatory variables denoted by <span class="math inline">\(X_1, X_2, \dots, X_k\)</span>. Following the book we will focus on the concept of simple linear regression throughout the whole chapter. In simple linear regression, there is just one explanatory variable <span class="math inline">\(X_1\)</span>. <br> If, for example, a school cuts its class sizes by hiring new teachers, that is, the school lowers <span class="math inline">\(X_1\)</span>, the student-teacher ratios of its classes, how would this affect <span class="math inline">\(Y\)</span>, the performance of the students involved in a standardized test? With linear regression we can not only examine whether the student-teacher ratio <em>does have</em> an impact on the test results but we can also learn about the <em>direction</em> and the <em>strength</em> of this effect.</p>
<p>The following packages are needed for reproducing the code presented in this chapter:</p>
<ul>
<li><p><tt>AER</tt> - accompanies the Book <em>Applied Econometrics with R</em> <span class="citation">C. Kleiber &amp; Zeileis (<a href="#ref-kleiber2008">2008</a>)</span> and provides useful functions and data sets.</p></li>
<li><p><tt>MASS</tt> - a collection of functions for applied statistics.</p></li>
</ul>
<p>Make sure these are installed before you go ahead and try to replicate the examples. The safest way to do so is by checking whether the following code chunk executes without any errors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(AER)
<span class="kw">library</span>(MASS)</code></pre></div>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">4.1</span> Simple Linear Regression</h2>
<p>To start with an easy example, consider the following combinations of average test score and the average student-teacher ratio in some fictional school districts.</p>

<p>To work with these data in <tt>R</tt> we begin by generating two vectors: one for the student-teacher ratios (<tt>STR</tt>) and one for test scores (<tt>TestScore</tt>), both containing the data from the table above.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create sample data</span>
STR &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>, <span class="dv">17</span>, <span class="dv">19</span>, <span class="dv">20</span>, <span class="dv">22</span>, <span class="fl">23.5</span>, <span class="dv">25</span>)
TestScore &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">680</span>, <span class="dv">640</span>, <span class="dv">670</span>, <span class="dv">660</span>, <span class="dv">630</span>, <span class="dv">660</span>, <span class="dv">635</span>) 

<span class="co"># Print out sample data</span>
STR</code></pre></div>
<pre><code>## [1] 15.0 17.0 19.0 20.0 22.0 23.5 25.0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TestScore</code></pre></div>
<pre><code>## [1] 680 640 670 660 630 660 635</code></pre>
<p>In a simple linear regression model, we model the relationship between both variables by a straight line, formally <span class="math display">\[ Y = b \cdot X + a. \]</span> For now, let us suppose that the function which relates test score and student-teacher ratio to each other is <span class="math display">\[TestScore = 713 - 3 \times STR.\]</span></p>
<p>It is always a good idea to visualize the data you work with. Here, it is suitable to use <tt>plot()</tt> to produce a scatterplot with <tt>STR</tt> on the <span class="math inline">\(x\)</span>-axis and <tt>TestScore</tt> on the <span class="math inline">\(y\)</span>-axis. Just call <code>plot(y_variable ~ x_variable)</code> whereby <tt>y_variable</tt> and <tt>x_variable</tt> are placeholders for the vectors of observations we want to plot. Furthermore, we might want to add a systematic relationship to the plot. To draw a straight line, <tt>R</tt> provides the function <tt>abline()</tt>. We just have to call this function with arguments <tt>a</tt> (representing the intercept) and <tt>b</tt> (representing the slope) after executing <tt>plot()</tt> in order to add the line to our plot.</p>
<p>The following code reproduces Figure 4.1 from the textbook.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create a scatterplot of the data</span>
<span class="kw">plot</span>(TestScore <span class="op">~</span><span class="st"> </span>STR)

<span class="co"># add the systematic relationship to the plot</span>
<span class="kw">abline</span>(<span class="dt">a =</span> <span class="dv">713</span>, <span class="dt">b =</span> <span class="op">-</span><span class="dv">3</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-143-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>We find that the line does not touch any of the points although we claimed that it represents the systematic relationship. The reason for this is randomness. Most of the time there are additional influences which imply that there is no bivariate relationship between the two variables.</p>
<p>In order to account for these differences between observed data and the systematic relationship, we extend our model from above by an <em>error term</em> <span class="math inline">\(u\)</span> which captures additional random effects. Put differently, <span class="math inline">\(u\)</span> accounts for all the differences between the regression line and the actual observed data. Beside pure randomness, these deviations could also arise from measurement errors or, as will be discussed later, could be the consequence of leaving out other factors that are relevant in explaining the dependent variable.</p>
<p>Which other factors are plausible in our example? For one thing, the test scores might be driven by the teachers’ quality and the background of the students. It is also possible that in some classes, the students were lucky on the test days and thus achieved higher scores. For now, we will summarize such influences by an additive component:</p>
<p><span class="math display">\[ TestScore = \beta_0 + \beta_1 \times STR + \text{other factors} \]</span></p>
<p>Of course this idea is very general as it can be easily extended to other situations that can be described with a linear model. The basic linear regression function we will work with hence is</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 X_i + u_i. \]</span></p>
<p>Key Concept 4.1 summarizes the terminology of the simple linear regression model.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 4.1
</h3>
<h3 class="left">
Terminology for the Linear Regression Model with a Single Regressor
</h3>
<p>
<p>The linear regression model is</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_1 + u_i\]</span></p>
<p>where</p>
<ul>
<li>the index <span class="math inline">\(i\)</span> runs over the observations, <span class="math inline">\(i=1,\dots,n\)</span></li>
<li><span class="math inline">\(Y_i\)</span> is the <em>dependent variable</em>, the <em>regressand</em>, or simply the <em>left-hand variable</em></li>
<li><span class="math inline">\(X_i\)</span> is the <em>independent variable</em>, the <em>regressor</em>, or simply the <em>right-hand variable</em></li>
<li><span class="math inline">\(Y = \beta_0 + \beta_1 X\)</span> is the <em>population regression line</em> also called the <em>population regression function</em></li>
<li><span class="math inline">\(\beta_0\)</span> is the <em>intercept</em> of the population regression line</li>
<li><span class="math inline">\(\beta_1\)</span> is the <em>slope</em> of the population regression line</li>
<li><span class="math inline">\(u_i\)</span> is the <em>error term</em>.</li>
</ul>
</p>
</div>
</div>
<div id="estimating-the-coefficients-of-the-linear-regression-model" class="section level2">
<h2><span class="header-section-number">4.2</span> Estimating the Coefficients of the Linear Regression Model</h2>
<p>In practice, the intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span> of the population regression line are unknown. Therefore, we must employ data to estimate both unknown parameters. In the following, a real world example will be used to demonstrate how this is achieved. We want to relate test scores to student-teacher ratios measured in Californian schools. The test score is the district-wide average of reading and math scores for fifth graders. Again, the class size is measured as the number of students divided by the number of teachers (the student-teacher ratio). As for the data, the California School data set (<tt>CASchools</tt>) comes with an <tt>R</tt> package called <tt>AER</tt>, an acronym for <a href="https://cran.r-project.org/web/packages/AER/AER.pdf">Applied Econometrics with R</a> <span class="citation">(Christian Kleiber &amp; Zeileis, <a href="#ref-R-AER">2017</a>)</span>. After installing the package with <tt>install.packages(“AER”)</tt> and attaching it with <tt>library(AER)</tt> the data set can be loaded using the function <tt>data()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># install the AER package (once)</span>
<span class="kw">install.packages</span>(<span class="st">&quot;AER&quot;</span>)

<span class="co"># load the AER package </span>
<span class="kw">library</span>(AER)   

<span class="co"># load the the data set in the workspace</span>
<span class="kw">data</span>(CASchools) </code></pre></div>
<p>Once a package has been installed it is available for use at further occasions when invoked with <tt>library()</tt> — there is no need to run <tt>install.packages()</tt> again!</p>
<p>It is interesting to know what kind of object we are dealing with. <tt>class()</tt> returns the class of an object. Depending on the class of an object some functions (for example <tt>plot()</tt> and <tt>summary()</tt>) behave differently.</p>
<p>Let us check the class of the object <tt>CASchools</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">class</span>(CASchools)</code></pre></div>
<pre><code>## [1] &quot;data.frame&quot;</code></pre>
<p>It turns out that <tt>CASchools</tt> is of class <tt>data.frame</tt> which is a convenient format to work with, especially for performing regression analysis.</p>
<p>With help of <tt>head()</tt> we get a first overview of our data. This function shows only the first 6 rows of the data set which prevents an overcrowded console output.</p>

<div class="rmdnote">
Press <tt>ctrl + L</tt> to clear the console. This command deletes any code that has been typed in and executed by you or printed to the console by <tt>R</tt> functions. The good news is that anything else is left untouched. You neither loose defined variables etc. nor the code history. It is still possible to recall previously executed <tt>R</tt> commands using the up and down keys. If you are working in <em>RStudio</em>, press <tt>ctrl + Up</tt> on your keyboard (<tt>CMD + Up</tt> on a Mac) to review a list of previously entered commands.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(CASchools)</code></pre></div>
<pre><code>##   district                          school  county grades students
## 1    75119              Sunol Glen Unified Alameda  KK-08      195
## 2    61499            Manzanita Elementary   Butte  KK-08      240
## 3    61549     Thermalito Union Elementary   Butte  KK-08     1550
## 4    61457 Golden Feather Union Elementary   Butte  KK-08      243
## 5    61523        Palermo Union Elementary   Butte  KK-08     1335
## 6    62042         Burrel Union Elementary  Fresno  KK-08      137
##   teachers calworks   lunch computer expenditure    income   english  read
## 1    10.90   0.5102  2.0408       67    6384.911 22.690001  0.000000 691.6
## 2    11.15  15.4167 47.9167      101    5099.381  9.824000  4.583333 660.5
## 3    82.90  55.0323 76.3226      169    5501.955  8.978000 30.000002 636.3
## 4    14.00  36.4754 77.0492       85    7101.831  8.978000  0.000000 651.9
## 5    71.50  33.1086 78.4270      171    5235.988  9.080333 13.857677 641.8
## 6     6.40  12.3188 86.9565       25    5580.147 10.415000 12.408759 605.7
##    math
## 1 690.0
## 2 661.9
## 3 650.9
## 4 643.5
## 5 639.9
## 6 605.4</code></pre>
<p>We find that the data set consists of plenty of variables and that most of them are numeric.</p>
<p>By the way: an alternative to <tt>class()</tt> and <tt>head()</tt> is <tt>str()</tt> which is deduced from ‘structure’ and gives a comprehensive overview of the object. Try!</p>
<p>Turning back to <tt>CASchools</tt>, the two variables we are interested in (i.e., average test score and the student-teacher ratio) are <em>not</em> included. However, it is possible to calculate both from the provided data. To obtain the student-teacher ratios, we simply divide the number of students by the number of teachers. The average test score is the arithmetic mean of the test score for reading and the score of the math test. The next code chunk shows how the two variables can be constructed as vectors and how they are appended to <tt>CASchools</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute STR and append it to CASchools</span>
CASchools<span class="op">$</span>STR &lt;-<span class="st"> </span>CASchools<span class="op">$</span>students<span class="op">/</span>CASchools<span class="op">$</span>teachers 

<span class="co"># compute TestScore and append it to CASchools</span>
CASchools<span class="op">$</span>score &lt;-<span class="st"> </span>(CASchools<span class="op">$</span>read <span class="op">+</span><span class="st"> </span>CASchools<span class="op">$</span>math)<span class="op">/</span><span class="dv">2</span>     </code></pre></div>
<p>If we ran <tt>head(CASchools)</tt> again we would find the two variables of interest as additional columns named <tt>STR</tt> and <tt>score</tt> (check this!).</p>
<p>Table 4.1 from the textbook summarizes the distribution of test scores and student-teacher ratios. There are several functions which can be used to produce similar results, e.g.,</p>
<ul>
<li><p><tt>mean()</tt> (computes the arithmetic mean of the provided numbers),</p></li>
<li><p><tt>sd()</tt> (computes the sample standard deviation),</p></li>
<li><p><tt>quantile()</tt> (returns a vector of the specified sample quantiles for the data).</p></li>
</ul>
<p>The next code chunk shows how to achieve this. First, we compute summary statistics on the columns <tt>STR</tt> and <tt>score</tt> of <tt>CASchools</tt>. In order to get nice output we gather the measures in a <tt>data.frame</tt> named <tt>DistributionSummary</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute sample averages of STR and score</span>
avg_STR &lt;-<span class="st"> </span><span class="kw">mean</span>(CASchools<span class="op">$</span>STR) 
avg_score &lt;-<span class="st"> </span><span class="kw">mean</span>(CASchools<span class="op">$</span>score)

<span class="co"># compute sample standard deviations of STR and score</span>
sd_STR &lt;-<span class="st"> </span><span class="kw">sd</span>(CASchools<span class="op">$</span>STR) 
sd_score &lt;-<span class="st"> </span><span class="kw">sd</span>(CASchools<span class="op">$</span>score)

<span class="co"># set up a vector of percentiles and compute the quantiles </span>
quantiles &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.10</span>, <span class="fl">0.25</span>, <span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.75</span>, <span class="fl">0.9</span>)
quant_STR &lt;-<span class="st"> </span><span class="kw">quantile</span>(CASchools<span class="op">$</span>STR, quantiles)
quant_score &lt;-<span class="st"> </span><span class="kw">quantile</span>(CASchools<span class="op">$</span>score, quantiles)

<span class="co"># gather everything in a data.frame </span>
DistributionSummary &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Average =</span> <span class="kw">c</span>(avg_STR, avg_score), 
                                  <span class="dt">StandardDeviation =</span> <span class="kw">c</span>(sd_STR, sd_score), 
                                  <span class="dt">quantile =</span> <span class="kw">rbind</span>(quant_STR, quant_score))

<span class="co"># print the summary to the console</span>
DistributionSummary</code></pre></div>
<pre><code>##               Average StandardDeviation quantile.10. quantile.25.
## quant_STR    19.64043          1.891812      17.3486     18.58236
## quant_score 654.15655         19.053347     630.3950    640.05000
##             quantile.40. quantile.50. quantile.60. quantile.75.
## quant_STR       19.26618     19.72321      20.0783     20.87181
## quant_score    649.06999    654.45000     659.4000    666.66249
##             quantile.90.
## quant_STR       21.86741
## quant_score    678.85999</code></pre>
<p>As for the sample data, we use <tt>plot()</tt>. This allows us to detect characteristics of our data, such as outliers which are harder to discover by looking at mere numbers. This time we add some additional arguments to the call of <tt>plot()</tt>.</p>
<p>The first argument in our call of <tt>plot()</tt>, <tt>score ~ STR</tt>, is again a formula that states variables on the y- and the x-axis. However, this time the two variables are not saved in separate vectors but are columns of <tt>CASchools</tt>. Therefore, <tt>R</tt> would not find them without the argument <tt>data</tt> being correctly specified. <tt>data</tt> must be in accordance with the name of the <tt>data.frame</tt> to which the variables belong to, in this case <tt>CASchools</tt>. Further arguments are used to change the appearance of the plot: while <tt>main</tt> adds a title, <tt>xlab</tt> and <tt>ylab</tt> add custom labels to both axes.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(score <span class="op">~</span><span class="st"> </span>STR, 
     <span class="dt">data =</span> CASchools,
     <span class="dt">main =</span> <span class="st">&quot;Scatterplot of TestScore and STR&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;STR (X)&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Test Score (Y)&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-151-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>The plot (Figure 4.2 in the book) shows the scatterplot of all observations on the student-teacher ratio and test score. We see that the points are strongly scattered, and that the variables are negatively correlated. That is, we expect to observe lower test scores in bigger classes.</p>
<p>The function <tt>cor()</tt> (see <tt>?cor</tt> for further info) can be used to compute the correlation between two <em>numeric</em> vectors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(CASchools<span class="op">$</span>STR, CASchools<span class="op">$</span>score)</code></pre></div>
<pre><code>## [1] -0.2263627</code></pre>
<p>As the scatterplot already suggests, the correlation is negative but rather weak.</p>
<p>The task we are now facing is to find a line which best fits the data. Of course we could simply stick with graphical inspection and correlation analysis and then select the best fitting line by eyeballing. However, this would be rather subjective: different observers would draw different regression lines. On this account, we are interested in techniques that are less arbitrary. Such a technique is given by ordinary least squares (OLS) estimation.</p>
<div id="the-ordinary-least-squares-estimator" class="section level3 unnumbered">
<h3>The Ordinary Least Squares Estimator</h3>
<p>The OLS estimator chooses the regression coefficients such that the estimated regression line is as “close” as possible to the observed data points. Here, closeness is measured by the sum of the squared mistakes made in predicting <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. Let <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> be some estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Then the sum of squared estimation mistakes can be expressed as</p>
<p><span class="math display">\[ \sum^n_{i = 1} (Y_i - b_0 - b_1 X_i)^2. \]</span></p>
<p>The OLS estimator in the simple regression model is the pair of estimators for intercept and slope which minimizes the expression above. The derivation of the OLS estimators for both parameters are presented in Appendix 4.1 of the book. The results are summarized in Key Concept 4.2.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 4.2
</h3>
<h3 class="left">
The OLS Estimator, Predicted Values, and Residuals
</h3>
<p>
The OLS estimators of the slope <span class="math inline">\(\beta_1\)</span> and the intercept <span class="math inline">\(\beta_0\)</span> in the simple linear regression model are
<span class="math display">\[\begin{align}
  \hat\beta_1 &amp; = \frac{ \sum_{i = 1}^n (X_i - \overline{X})(Y_i - \overline{Y}) } { \sum_{i=1}^n (X_i - \overline{X})^2},  \\
  \\
  \hat\beta_0 &amp; =  \overline{Y} - \hat\beta_1 \overline{X}. 
\end{align}\]</span>
The OLS predicted values <span class="math inline">\(\widehat{Y}_i\)</span> and residuals <span class="math inline">\(\hat{u}_i\)</span> are
<span class="math display">\[\begin{align}
  \widehat{Y}_i &amp; =  \hat\beta_0 + \hat\beta_1 X_i,\\
  \\
  \hat{u}_i &amp; =  Y_i - \widehat{Y}_i. 
\end{align}\]</span>
The estimated intercept <span class="math inline">\(\hat{\beta}_0\)</span>, the slope parameter <span class="math inline">\(\hat{\beta}_1\)</span> and the residuals <span class="math inline">\(\left(\hat{u}_i\right)\)</span> are computed from a sample of <span class="math inline">\(n\)</span> observations of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(i\)</span>, <span class="math inline">\(...\)</span>, <span class="math inline">\(n\)</span>. These are <em>estimates</em> of the unknown population intercept <span class="math inline">\(\left(\beta_0 \right)\)</span>, slope <span class="math inline">\(\left(\beta_1\right)\)</span>, and error term <span class="math inline">\((u_i)\)</span>.
</p>
<p>The formulas presented above may not be very intuitive at first glance. The following interactive application aims to help you understand the mechanics of OLS. You can add observations by clicking into the coordinate system where the data are represented by points. Once two or more observations are available, the application computes a regression line using OLS and some statistics which are displayed in the right panel. The results are updated as you add further observations to the left panel. A double-click resets the application, i.e., all data are removed.</p>
<iframe height="410" width="900" frameborder="0" scrolling="no" src="SimpleRegression.html">
</iframe>
</div>
<p>There are many possible ways to compute <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> in <tt>R</tt>. For example, we could implement the formulas presented in Key Concept 4.2 with two of <tt>R</tt>’s most basic functions: <tt>mean()</tt> and <tt>sum()</tt>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">attach</span>(CASchools) <span class="co"># allows to use the variables contained in CASchools directly</span>

<span class="co"># compute beta_1_hat</span>
beta_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">sum</span>((STR <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(STR)) <span class="op">*</span><span class="st"> </span>(score <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(score))) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>((STR <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(STR))<span class="op">^</span><span class="dv">2</span>)

<span class="co"># compute beta_0_hat</span>
beta_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">mean</span>(score) <span class="op">-</span><span class="st"> </span>beta_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(STR)

<span class="co"># print the results to the console</span>
beta_<span class="dv">1</span></code></pre></div>
<pre><code>## [1] -2.279808</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">beta_<span class="dv">0</span></code></pre></div>
<pre><code>## [1] 698.9329</code></pre>
</div>
<p>Of course, there are even more manual ways to perform these tasks. With OLS being one of the most widely-used estimation techniques, <tt>R</tt> of course already contains a built-in function named <tt>lm()</tt> (<strong>l</strong>inear <strong>m</strong>odel) which can be used to carry out regression analysis.</p>
<p>The first argument of the function to be specified is, similar to <tt>plot()</tt>, the regression formula with the basic syntax <tt>y ~ x</tt> where <tt>y</tt> is the dependent variable and <tt>x</tt> the explanatory variable. The argument <tt>data</tt> determines the data set to be used in the regression. We now revisit the example from the book where the relationship between the test scores and the class sizes is analyzed. The following code uses <tt>lm()</tt> to replicate the results presented in figure 4.3 of the book.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the model and assign the result to linear_model</span>
linear_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>STR, <span class="dt">data =</span> CASchools)

<span class="co"># print the standard output of the estimated lm object to the console </span>
linear_model</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Coefficients:
## (Intercept)          STR  
##      698.93        -2.28</code></pre>
<p>Let us add the estimated regression line to the plot. This time we also enlarge the ranges of both axes by setting the arguments <tt>xlim</tt> and <tt>ylim</tt>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the data</span>
<span class="kw">plot</span>(score <span class="op">~</span><span class="st"> </span>STR, 
     <span class="dt">data =</span> CASchools,
     <span class="dt">main =</span> <span class="st">&quot;Scatterplot of TestScore and STR&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;STR (X)&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Test Score (Y)&quot;</span>,
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">30</span>),
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">600</span>, <span class="dv">720</span>))

<span class="co"># add the regression line</span>
<span class="kw">abline</span>(linear_model) </code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-157-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>Did you notice that this time, we did not pass the intercept and slope parameters to <tt>abline</tt>? If you call <tt>abline()</tt> on an object of class <tt>lm</tt> that only contains a single regressor, <tt>R</tt> draws the regression line automatically!</p>
</div>
</div>
<div id="measures-of-fit" class="section level2">
<h2><span class="header-section-number">4.3</span> Measures of Fit</h2>
<p>After fitting a linear regression model, a natural question is how well the model describes the data. Visually, this amounts to assessing whether the observations are tightly clustered around the regression line. Both the <em>coefficient of determination</em> and the <em>standard error of the regression</em> measure how well the OLS Regression line fits the data.</p>
<div id="the-coefficient-of-determination" class="section level3 unnumbered">
<h3>The Coefficient of Determination</h3>
<p><span class="math inline">\(R^2\)</span>, the <em>coefficient of determination</em>, is the fraction of the sample variance of <span class="math inline">\(Y_i\)</span> that is explained by <span class="math inline">\(X_i\)</span>. Mathematically, the <span class="math inline">\(R^2\)</span> can be written as the ratio of the explained sum of squares to the total sum of squares. The <em>explained sum of squares</em> (<span class="math inline">\(ESS\)</span>) is the sum of squared deviations of the predicted values <span class="math inline">\(\hat{Y_i}\)</span>, from the average of the <span class="math inline">\(Y_i\)</span>. The <em>total sum of squares</em> (<span class="math inline">\(TSS\)</span>) is the sum of squared deviations of the <span class="math inline">\(Y_i\)</span> from their average. Thus we have</p>
<span class="math display">\[\begin{align}
  ESS &amp; =  \sum_{i = 1}^n \left( \hat{Y_i} - \overline{Y} \right)^2,   \\
  TSS &amp; =  \sum_{i = 1}^n \left( Y_i - \overline{Y} \right)^2,   \\
  R^2 &amp; = \frac{ESS}{TSS}.
\end{align}\]</span>
<p>Since <span class="math inline">\(TSS = ESS + SSR\)</span> we can also write</p>
<p><span class="math display">\[ R^2 = 1- \frac{SSR}{TSS} \]</span></p>
<p>where <span class="math inline">\(SSR\)</span> is the sum of squared residuals, a measure for the errors made when predicting the <span class="math inline">\(Y\)</span> by <span class="math inline">\(X\)</span>. The <span class="math inline">\(SSR\)</span> is defined as</p>
<p><span class="math display">\[ SSR = \sum_{i=1}^n \hat{u}_i^2. \]</span></p>
<p><span class="math inline">\(R^2\)</span> lies between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. It is easy to see that a perfect fit, i.e., no errors made when fitting the regression line, implies <span class="math inline">\(R^2 = 1\)</span> since then we have <span class="math inline">\(SSR=0\)</span>. On the contrary, if our estimated regression line does not explain any variation in the <span class="math inline">\(Y_i\)</span>, we have <span class="math inline">\(ESS=0\)</span> and consequently <span class="math inline">\(R^2=0\)</span>.</p>
</div>
<div id="the-standard-error-of-the-regression" class="section level3 unnumbered">
<h3>The Standard Error of the Regression</h3>
<p>The <em>Standard Error of the Regression</em> (<span class="math inline">\(SER\)</span>) is an estimator of the standard deviation of the residuals <span class="math inline">\(\hat{u}_i\)</span>. As such it measures the magnitude of a typical deviation from the regression line, i.e., the magnitude of a typical residual.</p>
<p><span class="math display">\[ SER = s_{\hat{u}} = \sqrt{s_{\hat{u}}^2} \ \ \ \text{where} \ \ \ s_{\hat{u} }^2 = \frac{1}{n-2} \sum_{i = 1}^n \hat{u}^2_i = \frac{SSR}{n - 2} \]</span></p>
<p>Remember that the <span class="math inline">\(u_i\)</span> are <em>unobserved</em>. This is why we use their estimated counterparts, the residuals <span class="math inline">\(\hat{u}_i\)</span>, instead. See Chapter 4.3 of the book for a more detailed comment on the <span class="math inline">\(SER\)</span>.</p>
</div>
<div id="application-to-the-test-score-data" class="section level3 unnumbered">
<h3>Application to the Test Score Data</h3>
<p>Both measures of fit can be obtained by using the function <tt>summary()</tt> with an <tt>lm</tt> object provided as the only argument. While the function <tt>lm()</tt> only prints out the estimated coefficients to the console, <tt>summary()</tt> provides additional predefined information such as the regression’s <span class="math inline">\(R^2\)</span> and the <span class="math inline">\(SER\)</span>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_summary &lt;-<span class="st"> </span><span class="kw">summary</span>(linear_model)
mod_summary</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -47.727 -14.251   0.483  12.822  48.540 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 698.9329     9.4675  73.825  &lt; 2e-16 ***
## STR          -2.2798     0.4798  -4.751 2.78e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 18.58 on 418 degrees of freedom
## Multiple R-squared:  0.05124,    Adjusted R-squared:  0.04897 
## F-statistic: 22.58 on 1 and 418 DF,  p-value: 2.783e-06</code></pre>
</div>
<p>The <span class="math inline">\(R^2\)</span> in the output is called <em>Multiple R-squared</em> and has a value of <span class="math inline">\(0.051\)</span>. Hence, <span class="math inline">\(5.1 \%\)</span> of the variance of the dependent variable <span class="math inline">\(score\)</span> is explained by the explanatory variable <span class="math inline">\(STR\)</span>. That is, the regression explains little of the variance in <span class="math inline">\(score\)</span>, and much of the variation in test scores remains unexplained (cf. Figure 4.3 of the book).</p>
<p>The <span class="math inline">\(SER\)</span> is called <em>Residual standard error</em> and equals <span class="math inline">\(18.58\)</span>. The unit of the <span class="math inline">\(SER\)</span> is the same as the unit of the dependent variable. That is, on average the deviation of the actual achieved test score and the regression line is <span class="math inline">\(18.58\)</span> points.</p>
<p>Now, let us check whether <tt>summary()</tt> uses the same definitions for <span class="math inline">\(R^2\)</span> and <span class="math inline">\(SER\)</span> as we do when computing them manually.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute R^2 manually</span>
SSR &lt;-<span class="st"> </span><span class="kw">sum</span>(mod_summary<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)
TSS &lt;-<span class="st"> </span><span class="kw">sum</span>((score <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(score))<span class="op">^</span><span class="dv">2</span>)
R2 &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>SSR<span class="op">/</span>TSS

<span class="co"># print the value to the console</span>
R2</code></pre></div>
<pre><code>## [1] 0.05124009</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute SER manually</span>
n &lt;-<span class="st"> </span><span class="kw">nrow</span>(CASchools)
SER &lt;-<span class="st"> </span><span class="kw">sqrt</span>(SSR <span class="op">/</span><span class="st"> </span>(n<span class="op">-</span><span class="dv">2</span>))

<span class="co"># print the value to the console</span>
SER</code></pre></div>
<pre><code>## [1] 18.58097</code></pre>
<p>We find that the results coincide. Note that the values provided by <tt>summary()</tt> are rounded to two decimal places. Can you do so using <tt>R</tt>?</p>
</div>
</div>
<div id="tlsa" class="section level2">
<h2><span class="header-section-number">4.4</span> The Least Squares Assumptions</h2>
<p>OLS performs well under a quite broad variety of different circumstances. However, there are some assumptions which need to be satisfied in order to ensure that the estimates are normally distributed in large samples (we discuss this in Chapter @ref{tsdotoe}).</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 4.3
</h3>
<h3 class="left">
The Least Squares Assumptions
</h3>
<p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + u_i \text{, } i = 1,\dots,n\]</span> where</p>
<ol style="list-style-type: decimal">
<li>The error term <span class="math inline">\(u_i\)</span> has conditional mean zero given <span class="math inline">\(X_i\)</span>: <span class="math inline">\(E(u_i|X_i) = 0\)</span>.</li>
<li><span class="math inline">\((X_i,Y_i), i = 1,\dots,n\)</span> are independent and identically distributed (i.i.d.) draws from their joint distribution.</li>
<li>Large outliers are unlikely: <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> have nonzero finite fourth moments.</li>
</ol>
</p>
</div>
<div id="assumption-1-the-error-term-has-conditional-mean-of-zero" class="section level3 unnumbered">
<h3>Assumption 1: The Error Term has Conditional Mean of Zero</h3>
<p>This means that no matter which value we choose for <span class="math inline">\(X\)</span>, the error term <span class="math inline">\(u\)</span> must not show any systematic pattern and must have a mean of <span class="math inline">\(0\)</span>. Consider the case that, unconditionally, <span class="math inline">\(E(u) = 0\)</span>, but for low and high values of <span class="math inline">\(X\)</span>, the error term tends to be positive and for midrange values of <span class="math inline">\(X\)</span> the error tends to be negative. We can use R to construct such an example. To do so we generate our own data using <tt>R</tt>’s built-in random number generators.</p>
<p>We will use the following functions:</p>
<ul>
<li><tt>runif()</tt> - generates uniformly distributed random numbers</li>
<li><tt>rnorm()</tt> - generates normally distributed random numbers</li>
<li><tt>predict()</tt> - does predictions based on the results of model fitting functions like <tt>lm()</tt></li>
<li><tt>lines()</tt> - adds line segments to an existing plot</li>
</ul>
<p>We start by creating a vector containing values that are uniformly distributed on the interval <span class="math inline">\([-5,5]\)</span>. This can be done with the function <tt>runif()</tt>. We also need to simulate the error term. For this we generate normally distributed random numbers with a mean equal to <span class="math inline">\(0\)</span> and a variance of <span class="math inline">\(1\)</span> using <tt>rnorm()</tt>. The <span class="math inline">\(Y\)</span> values are obtained as a quadratic function of the <span class="math inline">\(X\)</span> values and the error.</p>
<p>After generating the data we estimate both a simple regression model and a quadratic model that also includes the regressor <span class="math inline">\(X^2\)</span> (this is a multiple regression model, see Chapter <a href="rmwmr.html#rmwmr">6</a>). Finally, we plot the simulated data and add a the estimated regression line of a simple regression model as well as the predictions made with a quadratic model to compare the fit graphically.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set a seed to make the results reproducible</span>
<span class="kw">set.seed</span>(<span class="dv">321</span>)

<span class="co"># simulate the data </span>
X &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">50</span>, <span class="dt">min =</span> <span class="op">-</span><span class="dv">5</span>, <span class="dt">max =</span> <span class="dv">5</span>)
u &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>, <span class="dt">sd =</span> <span class="dv">5</span>)  

<span class="co"># the true relation  </span>
Y &lt;-<span class="st"> </span>X<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span>u                

<span class="co"># estimate a simple regression model </span>
mod_simple &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X)

<span class="co"># predict using a quadratic model </span>
prediction &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(X<span class="op">^</span><span class="dv">2</span>)), <span class="kw">data.frame</span>(<span class="dt">X =</span> <span class="kw">sort</span>(X)))

<span class="co"># plot the results</span>
<span class="kw">plot</span>(Y <span class="op">~</span><span class="st"> </span>X)
<span class="kw">abline</span>(mod_simple, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
<span class="kw">lines</span>(<span class="kw">sort</span>(X), prediction)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-162-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>The plot shows what is meant by <span class="math inline">\(E(u_i|X_i) = 0\)</span> and why it does not hold for the linear model:</p>
<p>Using the quadratic model (represented by the black curve) we see that there are no systematic deviations of the observation from the predicted relation. It is credible that the assumption is not violated when such a model is employed. However, using a simple linear regression model we see that the assumption is probably violated as <span class="math inline">\(E(u_i|X_i)\)</span> varies with the <span class="math inline">\(X_i\)</span>.</p>
</div>
<div id="assumption-2-independently-and-identically-distributed-data" class="section level3 unnumbered">
<h3>Assumption 2: Independently and Identically Distributed Data</h3>
<p>Most sampling schemes used when collecting data from populations produce i.i.d.-samples. For example, we could use <tt>R</tt>’s random number generator to randomly select student IDs from a university’s enrollment list and record age <span class="math inline">\(X\)</span> and earnings <span class="math inline">\(Y\)</span> of the corresponding students. This is a typical example of simple random sampling and ensures that all the <span class="math inline">\((X_i, Y_i)\)</span> are drawn randomly from the same population.</p>
<p>A prominent example where the i.i.d. assumption is not fulfilled is time series data where we have observations on the same unit over time. For example, take <span class="math inline">\(X\)</span> as the number of workers in a production company over time. Due to business transformations, the company cuts job periodically by a specific share but there are also some non-deterministic influences that relate to economics, politics etc. Using <tt>R</tt> we can easily simulate such a process and plot it.</p>
<p>We start the series with a total of 5000 workers and simulate the reduction of employment with an autoregressive process that exhibits a downward movement in the long-run and has normally distributed errors:<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<span class="math display">\[ employment_t = -5 + 0.98 \cdot employment_{t-1} + u_t \]</span>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set seed</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="co"># generate a date vector</span>
Date &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">as.Date</span>(<span class="st">&quot;1951/1/1&quot;</span>), <span class="kw">as.Date</span>(<span class="st">&quot;2000/1/1&quot;</span>), <span class="st">&quot;years&quot;</span>)

<span class="co"># initialize the employment vector</span>
X &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">5000</span>, <span class="kw">rep</span>(<span class="ot">NA</span>, <span class="kw">length</span>(Date)<span class="op">-</span><span class="dv">1</span>))

<span class="co"># generate time series observations with random influences</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="kw">length</span>(Date)) {
  
    X[i] &lt;-<span class="st"> </span><span class="op">-</span><span class="dv">50</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.98</span> <span class="op">*</span><span class="st"> </span>X[i<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">200</span>)
    
}

<span class="co">#plot the results</span>
<span class="kw">plot</span>(<span class="dt">x =</span> Date, 
     <span class="dt">y =</span> X, 
     <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">ylab =</span> <span class="st">&quot;Workers&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;Time&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-163-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>It is evident that the observations on the number of employees cannot be independent in this example: the level of today’s employment is correlated with tomorrows employment level. Thus, the i.i.d. assumption is violated.</p>
</div>
<div id="assumption-3-large-outliers-are-unlikely" class="section level3 unnumbered">
<h3>Assumption 3: Large Outliers are Unlikely</h3>
<p>It is easy to come up with situations where extreme observations, i.e., observations that deviate considerably from the usual range of the data, may occur. Such observations are called outliers. Technically speaking, assumption 3 requires that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a finite kurtosis.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<p>Common cases where we want to exclude or (if possible) correct such outliers is when they are apparently typos, conversion errors or measurement errors. Even if it seems like extreme observations have been recorded correctly, it is advisable to exclude them before estimating a model since OLS suffers from <em>sensitivity to outliers</em>.</p>
<p>What does this mean? One can show that extreme observations receive heavy weighting in the estimation of the unknown regression coefficients when using OLS. Therefore, outliers can lead to strongly distorted estimates of regression coefficients. To get a better impression of this issue, consider the following application where we have placed some sample data on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> which are highly correlated. The relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> seems to be explained pretty good by the plotted regression line: all of the blue dots lie close to the red line and we have <span class="math inline">\(R^2=0.92\)</span>.</p>
<p>Now go ahead and add a further observation at, say, <span class="math inline">\((18,2)\)</span>. This observations clearly is an outlier. The result is quite striking: the estimated regression line differs greatly from the one we adjudged to fit the data well. The slope is heavily downward biased and <span class="math inline">\(R^2\)</span> decreased to a mere <span class="math inline">\(29\%\)</span>! <br> Double-click inside the coordinate system to reset the app. Feel free to experiment. Choose different coordinates for the outlier or add additional ones.</p>
<iframe height="410" width="900" frameborder="0" scrolling="no" src="Outlier.html">
</iframe>
<p>The following code roughly reproduces what is shown in figure 4.5 in the book. As done above we use sample data generated using <tt>R</tt>’s random number functions <tt>rnorm()</tt> and <tt>runif()</tt>. We estimate two simple regression models, one based on the original data set and another using a modified set where one observation is change to be an outlier and then plot the results. In order to understand the complete code you should be familiar with the function <tt>sort()</tt> which sorts the entries of a numeric vector in ascending order.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set seed</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="co"># generate the data</span>
X &lt;-<span class="st"> </span><span class="kw">sort</span>(<span class="kw">runif</span>(<span class="dv">10</span>, <span class="dt">min =</span> <span class="dv">30</span>, <span class="dt">max =</span> <span class="dv">70</span>))
Y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10</span> , <span class="dt">mean =</span> <span class="dv">200</span>, <span class="dt">sd =</span> <span class="dv">50</span>)
Y[<span class="dv">9</span>] &lt;-<span class="st"> </span><span class="dv">2000</span>

<span class="co"># fit model with outlier</span>
fit &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X)

<span class="co"># fit model without outlier</span>
fitWithoutOutlier &lt;-<span class="st"> </span><span class="kw">lm</span>(Y[<span class="op">-</span><span class="dv">9</span>] <span class="op">~</span><span class="st"> </span>X[<span class="op">-</span><span class="dv">9</span>])

<span class="co"># plot the results</span>
<span class="kw">plot</span>(Y <span class="op">~</span><span class="st"> </span>X)
<span class="kw">abline</span>(fit)
<span class="kw">abline</span>(fitWithoutOutlier, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-164-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="tsdotoe" class="section level2">
<h2><span class="header-section-number">4.5</span> The Sampling Distribution of the OLS Estimator</h2>
<p>Because <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are computed from a sample, the estimators themselves are random variables with a probability distribution — the so-called sampling distribution of the estimators — which describes the values they could take on over different samples. Although the sampling distribution of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> can be complicated when the sample size is small and generally changes with the number of observations, <span class="math inline">\(n\)</span>, it is possible, provided the assumptions discussed in the book are valid, to make certain statements about it that hold for all <span class="math inline">\(n\)</span>. In particular <span class="math display">\[ E(\hat{\beta}_0) = \beta_0 \ \ \text{and} \ \  E(\hat{\beta}_1) = \beta_1,\]</span> that is, <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are unbiased estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the true parameters. If the sample is sufficiently large, by the central limit theorem the <em>joint</em> sampling distribution of the estimators is well approximated by the bivariate normal distribution (<a href="#mjx-eqn-2.1">2.1</a>). This implies that the marginal distributions are also normal in large samples. Core facts on the large-sample distributions of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are presented in Key Concept 4.4.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 4.4
</h3>
<h3 class="left">
Large Sample Distribution of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>
</h3>
<p>
<p>If the least squares assumptions in Key Concept 4.3 hold, then in large samples <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> have a joint normal sampling distribution. The large sample normal distribution of <span class="math inline">\(\hat\beta_1\)</span> is <span class="math inline">\(\mathcal{N}(\beta_1, \sigma^2_{\hat\beta_1})\)</span>, where the variance of the distribution, <span class="math inline">\(\sigma^2_{\hat\beta_1}\)</span>, is</p>
<span class="math display" id="eq:olsvar1">\[\begin{align}
\sigma^2_{\hat\beta_1} = \frac{1}{n} \frac{Var \left[ \left(X_i - \mu_X \right) u_i  \right]}  {\left[  Var \left(X_i \right)  \right]^2}. \tag{4.1}
\end{align}\]</span>
<p>The large sample normal distribution of <span class="math inline">\(\hat\beta_0\)</span> is <span class="math inline">\(\mathcal{N}(\beta_0, \sigma^2_{\hat\beta_0})\)</span> with</p>
<span class="math display" id="eq:olsvar2">\[\begin{align}
\sigma^2_{\hat\beta_0} =  \frac{1}{n} \frac{Var \left( H_i u_i \right)}{ \left[  E \left(H_i^2  \right)  \right]^2 } \ , \ \text{where} \ \ H_i = 1 - \left[ \frac{\mu_X} {E \left( X_i^2\right)} \right] X_i. \tag{4.2}
\end{align}\]</span>
<p>The interactive simulation below continuously generates random samples <span class="math inline">\((X_i,Y_i)\)</span> of <span class="math inline">\(200\)</span> observations where <span class="math inline">\(E(Y\vert X) = 100 + 3X\)</span>, estimates a simple regression model, stores the estimate of the slope <span class="math inline">\(\beta_1\)</span> and visualizes the distribution of the <span class="math inline">\(\widehat{\beta}_1\)</span>s observed so far using a histogram. The idea here is that for a large number of <span class="math inline">\(\widehat{\beta}_1\)</span>s, the histogram gives a good approximation of the sampling distribution of the estimator. By decreasing the time between two sampling iterations, it becomes clear that the shape of the histogram approaches the characteristic bell shape of a normal distribution centered at the true slope of <span class="math inline">\(3\)</span>.</p>
<iframe height="470" width="700" frameborder="0" scrolling="no" src="SmallSampleDIstReg.html">
</iframe>
</p>
</div>
<div id="simulation-study-1" class="section level3 unnumbered">
<h3>Simulation Study 1</h3>
<p>Whether the statements of Key Concept 4.4 really hold can also be verified using <tt>R</tt>. For this we first we build our own population of <span class="math inline">\(100000\)</span> observations in total. To do this we need values for the independent variable <span class="math inline">\(X\)</span>, for the error term <span class="math inline">\(u\)</span>, and for the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. With these combined in a simple regression model, we compute the dependent variable <span class="math inline">\(Y\)</span>. <br> In our example we generate the numbers <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i = 1\)</span>, … ,<span class="math inline">\(100000\)</span> by drawing a random sample from a uniform distribution on the interval <span class="math inline">\([0,20]\)</span>. The realizations of the error terms <span class="math inline">\(u_i\)</span> are drawn from a standard normal distribution with parameters <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma^2 = 100\)</span> (note that <tt>rnorm()</tt> requires <span class="math inline">\(\sigma\)</span> as input for the argument <tt>sd</tt>, see <tt>?rnorm</tt>). Furthermore we chose <span class="math inline">\(\beta_0 = -2\)</span> and <span class="math inline">\(\beta_1 = 3.5\)</span> so the true model is</p>
<p><span class="math display">\[ Y_i = -2 + 3.5 \cdot X_i. \]</span></p>
<p>Finally, we store the results in a data.frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulate data</span>
N &lt;-<span class="st"> </span><span class="dv">100000</span>
X &lt;-<span class="st"> </span><span class="kw">runif</span>(N, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">20</span>)
u &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dt">sd =</span> <span class="dv">10</span>)

<span class="co"># population regression</span>
Y &lt;-<span class="st"> </span><span class="op">-</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="fl">3.5</span> <span class="op">*</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span>u
population &lt;-<span class="st"> </span><span class="kw">data.frame</span>(X, Y)</code></pre></div>
<p>From now on we will consider the previously generated data as the true population (which of course would be <em>unknown</em> in a real world application, otherwise there would be no reason to draw a random sample in the first place). The knowledge about the true population and the true relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> can be used to verify the statements made in Key Concept 4.4.</p>
<p>First, let us calculate the true variances <span class="math inline">\(\sigma^2_{\hat{\beta}_0}\)</span> and <span class="math inline">\(\sigma^2_{\hat{\beta}_1}\)</span> for a randomly drawn sample of size <span class="math inline">\(n = 100\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set sample size</span>
n &lt;-<span class="st"> </span><span class="dv">100</span>

<span class="co"># compute the variance of beta_hat_0</span>
H_i &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(X) <span class="op">/</span><span class="st"> </span><span class="kw">mean</span>(X<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>X
var_b0 &lt;-<span class="st"> </span><span class="kw">var</span>(H_i <span class="op">*</span><span class="st"> </span>u) <span class="op">/</span><span class="st"> </span>(n <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(H_i<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span> )

<span class="co"># compute the variance of hat_beta_1</span>
var_b1 &lt;-<span class="st"> </span><span class="kw">var</span>( ( X <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(X) ) <span class="op">*</span><span class="st"> </span>u ) <span class="op">/</span><span class="st"> </span>(<span class="dv">100</span> <span class="op">*</span><span class="st"> </span><span class="kw">var</span>(X)<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># print variances to the console</span>
var_b0</code></pre></div>
<pre><code>## [1] 4.045066</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">var_b1</code></pre></div>
<pre><code>## [1] 0.03018694</code></pre>
<p>Now let us assume that we do not know the true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and that it is not possible to observe the whole population. However, we can observe a random sample of <span class="math inline">\(n\)</span> observations. Then, it would not be possible to compute the true parameters but we could obtain estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> from the sample data using OLS. However, we know that these estimates are outcomes of random variables themselves since the observations are randomly sampled from the population. Key Concept 4.4 describes their distributions for large <span class="math inline">\(n\)</span>. When drawing a single sample of size <span class="math inline">\(n\)</span> it is not possible to make any statement about these distributions. Things change if we repeat the sampling scheme many times and compute the estimates for each sample: using this procedure we simulate outcomes of the respective distributions.</p>
<p>To achieve this in R, we employ the following approach:</p>
<ul>
<li>We assign the number of repetitions, say <span class="math inline">\(10000\)</span>, to <tt>reps</tt> and then initialize a matrix <tt>fit</tt> were the estimates obtained in each sampling iteration shall be stored row-wise. Thus <tt>fit</tt> has to be a matrix of dimensions <tt>reps</tt><span class="math inline">\(\times2\)</span>.</li>
<li>In the next step we draw <tt>reps</tt> random samples of size <tt>n</tt> from the population and obtain the OLS estimates for each sample. The results are stored as row entries in the outcome matrix <tt>fit</tt>. This is done using a <tt>for()</tt> loop.</li>
<li>At last, we estimate variances of both estimators using the sampled outcomes and plot histograms of the latter. We also add a plot of the density functions belonging to the distributions that follow from Key Concept 4.4. The function <tt>bquote()</tt> is used to obtain math expressions in the titles and labels of both plots. See <tt>?bquote</tt>.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set repetitions and sample size</span>
n &lt;-<span class="st"> </span><span class="dv">100</span>
reps &lt;-<span class="st"> </span><span class="dv">10000</span>

<span class="co"># initialize the matrix of outcomes</span>
fit &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> reps)

<span class="co"># loop sampling and estimation of the coefficients</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>reps){
  
 sample &lt;-<span class="st"> </span>population[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>N, n), ]
 fit[i, ] &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> sample)<span class="op">$</span>coefficients
 
}

<span class="co"># compute variance estimates using outcomes</span>
<span class="kw">var</span>(fit[, <span class="dv">1</span>])</code></pre></div>
<pre><code>## [1] 4.057089</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(fit[, <span class="dv">2</span>])</code></pre></div>
<pre><code>## [1] 0.03021784</code></pre>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># divide plotting area as 1-by-2 array</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))

<span class="co"># plot histograms of beta_0 estimates</span>
<span class="kw">hist</span>(fit[, <span class="dv">1</span>],
     <span class="dt">cex.main =</span> <span class="dv">1</span>,
     <span class="dt">main =</span> <span class="kw">bquote</span>(The <span class="op">~</span><span class="st"> </span>Distribution  <span class="op">~</span><span class="st"> </span>of <span class="op">~</span><span class="st"> </span><span class="dv">10000</span> <span class="op">~</span><span class="st"> </span>beta[<span class="dv">0</span>] <span class="op">~</span><span class="st"> </span>Estimates), 
     <span class="dt">xlab =</span> <span class="kw">bquote</span>(<span class="kw">hat</span>(beta)[<span class="dv">0</span>]), 
     <span class="dt">freq =</span> F)

<span class="co"># add true distribution to plot</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x, 
            <span class="op">-</span><span class="dv">2</span>, 
            <span class="kw">sqrt</span>(var_b0)), 
      <span class="dt">add =</span> T, 
      <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)

<span class="co"># plot histograms of beta_hat_1 </span>
<span class="kw">hist</span>(fit[, <span class="dv">2</span>],
    <span class="dt">cex.main =</span> <span class="dv">1</span>,
     <span class="dt">main =</span> <span class="kw">bquote</span>(The <span class="op">~</span><span class="st"> </span>Distribution  <span class="op">~</span><span class="st"> </span>of <span class="op">~</span><span class="st"> </span><span class="dv">10000</span> <span class="op">~</span><span class="st"> </span>beta[<span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>Estimates), 
     <span class="dt">xlab =</span> <span class="kw">bquote</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>]), 
     <span class="dt">freq =</span> F)

<span class="co"># add true distribution to plot</span>
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x, 
            <span class="fl">3.5</span>, 
            <span class="kw">sqrt</span>(var_b1)), 
      <span class="dt">add =</span> T, 
      <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-171-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>Our variance estimates support the statements made in Key Concept 4.4, coming close to the theoretical values. The histograms suggest that the distributions of the estimators can be well approximated by the respective theoretical normal distributions stated in Key Concept 4.4.</p>
</div>
<div id="simulation-study-2" class="section level3 unnumbered">
<h3>Simulation Study 2</h3>
<p>A further result implied by Key Concept 4.4 is that both estimators are consistent, i.e., they converge in probability to the true parameters we are interested in. This is because they are asymptotically unbiased and their variances converge to <span class="math inline">\(0\)</span> as <span class="math inline">\(n\)</span> increases. We can check this by repeating the simulation above for a sequence of increasing sample sizes. This means we no longer assign the sample size but a <em>vector</em> of sample sizes: <tt>n &lt;- c(…)</tt>. <br> Let us look at the distributions of <span class="math inline">\(\beta_1\)</span>. The idea here is to add an additional call of <tt>for()</tt> to the code. This is done in order to loop over the vector of sample sizes <tt>n</tt>. For each of the sample sizes we carry out the same simulation as before but plot a density estimate for the outcomes of each iteration over <tt>n</tt>. Notice that we have to change <tt>n</tt> to <tt>n[j]</tt> in the inner loop to ensure that the <tt>j</tt><span class="math inline">\(^{th}\)</span> element of <tt>n</tt> is used. In the simulation, we use sample sizes of <span class="math inline">\(100, 250, 1000\)</span> and <span class="math inline">\(3000\)</span>. Consequently we have a total of four distinct simulations using different sample sizes.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># set repetitions and the vector of sample sizes</span>
reps &lt;-<span class="st"> </span><span class="dv">1000</span>
n &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span>, <span class="dv">250</span>, <span class="dv">1000</span>, <span class="dv">3000</span>)

<span class="co"># initialize the matrix of outcomes</span>
fit &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> reps)

<span class="co"># divide the plot panel in a 2-by-2 array</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))

<span class="co"># loop sampling and plotting</span>

<span class="co"># outer loop over n</span>
<span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n)) {
  
  <span class="co"># inner loop: sampling and estimating of the coefficients</span>
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>reps){
    
    sample &lt;-<span class="st"> </span>population[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>N, n[j]), ]
    fit[i, ] &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> sample)<span class="op">$</span>coefficients
    
  }
  
  <span class="co"># draw density estimates</span>
  <span class="kw">plot</span>(<span class="kw">density</span>(fit[ ,<span class="dv">2</span>]), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="fl">2.5</span>, <span class="fl">4.5</span>), 
       <span class="dt">col =</span> j, 
       <span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&quot;n=&quot;</span>, n[j]), 
       <span class="dt">xlab =</span> <span class="kw">bquote</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>]))
  
}</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-172-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>We find that, as <span class="math inline">\(n\)</span> increases, the distribution of <span class="math inline">\(\hat\beta_1\)</span> concentrates around its mean, i.e., its variance decreases. Put differently, the likelihood of observing estimates close to the true value of <span class="math inline">\(\beta_1 = 3.5\)</span> grows as we increase the sample size. The same behavior can be observed if we analyze the distribution of <span class="math inline">\(\hat\beta_0\)</span> instead.</p>
</div>
<div id="simulation-study-3" class="section level3 unnumbered">
<h3>Simulation Study 3</h3>
<p>Furthermore, (4.1) reveals that the variance of the OLS estimator for <span class="math inline">\(\beta_1\)</span> decreases as the variance of the <span class="math inline">\(X_i\)</span> increases. In other words, as we increase the amount of information provided by the regressor, that is, increasing <span class="math inline">\(Var(X)\)</span>, which is used to estimate <span class="math inline">\(\beta_1\)</span>, we become more confident that the estimate is close to the true value (i.e., <span class="math inline">\(Var(\hat\beta_1)\)</span> decreases).<br> We can visualize this by reproducing Figure 4.6 from the book. To do this, we sample observations <span class="math inline">\((X_i,Y_i)\)</span>, <span class="math inline">\(i=1,\dots,100\)</span> from a bivariate normal distribution with</p>
<p><span class="math display">\[E(X)=E(Y)=5,\]</span> <span class="math display">\[Var(X)=Var(Y)=5\]</span> and <span class="math display">\[Cov(X,Y)=4.\]</span></p>
<p>Formally, this is written down as</p>
<span class="math display">\[\begin{align}
  \begin{pmatrix}
    X \\
    Y \\
  \end{pmatrix}
  \overset{i.i.d.}{\sim} &amp; \ \mathcal{N} 
  \left[
    \begin{pmatrix}
      5 \\
      5 \\
    \end{pmatrix}, \ 
    \begin{pmatrix}
      5 &amp; 4 \\
      4 &amp; 5 \\
    \end{pmatrix}
  \right]. \tag{4.3}
\end{align}\]</span>
<p>To carry out the random sampling, we make use of the function <tt>mvrnorm()</tt> from the package <tt>MASS</tt> <span class="citation">(Ripley, <a href="#ref-R-MASS">2018</a>)</span> which allows to draw random samples from multivariate normal distributions, see <code>?mvtnorm</code>. Next, we use <tt>subset()</tt> to split the sample into two subsets such that the first set, <tt>set1</tt>, consists of observations that fulfill the condition <span class="math inline">\(\lvert X - \overline{X} \rvert &gt; 1\)</span> and the second set, <tt>set2</tt>, includes the remainder of the sample. We then plot both sets and use different colors to distinguish the observations.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the MASS package</span>
<span class="kw">library</span>(MASS)

<span class="co"># set seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">4</span>)

<span class="co"># simulate bivarite normal data</span>
bvndata &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dv">100</span>, 
                <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">5</span>), 
                <span class="dt">Sigma =</span> <span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">4</span>), <span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">5</span>))) 

<span class="co"># assign column names / convert to data.frame</span>
<span class="kw">colnames</span>(bvndata) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X&quot;</span>, <span class="st">&quot;Y&quot;</span>)
bvndata &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(bvndata)

<span class="co"># subset the data</span>
set1 &lt;-<span class="st"> </span><span class="kw">subset</span>(bvndata, <span class="kw">abs</span>(<span class="kw">mean</span>(X) <span class="op">-</span><span class="st"> </span>X) <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>)
set2 &lt;-<span class="st"> </span><span class="kw">subset</span>(bvndata, <span class="kw">abs</span>(<span class="kw">mean</span>(X) <span class="op">-</span><span class="st"> </span>X) <span class="op">&lt;=</span><span class="st"> </span><span class="dv">1</span>)

<span class="co"># plot both data sets</span>
<span class="kw">plot</span>(set1, 
     <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>, 
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>, 
     <span class="dt">pch =</span> <span class="dv">19</span>)

<span class="kw">points</span>(set2, 
       <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
       <span class="dt">pch =</span> <span class="dv">19</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-173-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>It is clear that observations that are close to the sample average of the <span class="math inline">\(X_i\)</span> have less variance than those that are farther away. Now, if we were to draw a line as accurately as possible through either of the two sets it is intuitive that choosing the observations indicated by the black dots, i.e., using the set of observations which has larger variance than the blue ones, would result in a more precise line. Now, let us use OLS to estimate slope and intercept for both sets of observations. We then plot the observations along with both regression lines.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate both regression lines</span>
lm.set1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> set1)
lm.set2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> set2)

<span class="co"># plot observations</span>
<span class="kw">plot</span>(set1, <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>)
<span class="kw">points</span>(set2, <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>)

<span class="co"># add both lines to the plot</span>
<span class="kw">abline</span>(lm.set1, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>)
<span class="kw">abline</span>(lm.set2, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-174-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>Evidently, the green regression line does far better in describing data sampled from the bivariate normal distribution stated in (<a href="#mjx-eqn-4.3">4.3</a>) than the red line. This is a nice example for demonstrating why we are interested in a high variance of the regressor <span class="math inline">\(X\)</span>: more variance in the <span class="math inline">\(X_i\)</span> means more information from which the precision of the estimation benefits.</p>
</div>
</div>
<div id="exercises-2" class="section level2">
<h2><span class="header-section-number">4.6</span> Exercises</h2>
<div class="DCexercise">
<h4 id="class-sizes-and-test-scores" class="unnumbered">1. Class Sizes and Test Scores</h4>
<p>A researcher wants to analyze the relationship between class size (measured by the student-teacher ratio) and the average test score. Therefore he measures both variables in <span class="math inline">\(10\)</span> different classes and ends up with the following results.</p>


  <table>
      <tr>
        <td><b>Class Size</b></td>
        <td>23</td>
        <td>19</td>
        <td>30</td>
        <td>22</td>
        <td>23</td>
        <td>29</td>
        <td>35</td>
        <td>36</td>
        <td>33</td>
        <td>25</td>
      </tr>
      <tr>
        <td><b>Test Score</b></td>
        <td>430</td>
        <td>430</td>
        <td>333</td>
        <td>410</td>
        <td>390</td>
        <td>377</td>
        <td>325</td>
        <td>310</td>
        <td>328</td>
        <td>375</td>
      </tr>
    </table>


<p><strong>Instructions:</strong></p>
<ul>
<li><p>Create the vectors <tt>cs</tt> (the class size) and <tt>ts</tt> (the test score), containing the observations above.</p></li>
<li><p>Draw a scatterplot of the results using <tt>plot()</tt>.</p></li>
</ul>
<iframe src="DCL/ex4_1.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="mean-variance-covariance-and-correlation" class="unnumbered">2. Mean, Variance, Covariance and Correlation</h4>
<p>The vectors <tt>cs</tt> and <tt>ts</tt> are available in the working environment (you can check this: type their names into the console and press enter).</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Compute the mean, the sample variance and the sample standard deviation of <tt>ts</tt>.</p></li>
<li><p>Compute the covariance and the correlation coefficient for <tt>ts</tt> and <tt>cs</tt>.</p></li>
</ul>
<iframe src="DCL/ex4_2.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hint:</strong> Use the <tt>R</tt> functions presented in this chapter: <tt>mean()</tt>, <tt>sd()</tt>, <tt>cov()</tt>, <tt>cor()</tt> and <tt>var()</tt>.</p>
</div>
<div class="DCexercise">
<h4 id="simple-linear-regression-1" class="unnumbered">3. Simple Linear Regression</h4>
<p>The vectors <tt>cs</tt> and <tt>ts</tt> are available in the working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>The function <tt>lm()</tt> is part of the package <tt>AER</tt>. Attach the package using <tt>library()</tt>.</p></li>
<li><p>Use <tt>lm()</tt> to estimate the regression model <span class="math display">\[TestScore_i = \beta_0 + \beta_1 STR_i + u_i.\]</span> Assign the result to <tt>mod</tt>.</p></li>
<li><p>Obtain a statistical summary of the model.</p></li>
</ul>
<iframe src="DCL/ex4_3.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="the-model-object" class="unnumbered">4. The Model Object</h4>
<p>Let us see how an object of class <tt>lm</tt> is structured.</p>
<p>The vectors <tt>cs</tt> and <tt>ts</tt> as well as the model object <tt>mod</tt> from the previous exercise are available in your workspace.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Use <tt>class()</tt> to learn about the class of the object <tt>mod</tt>.</li>
<li><tt>mod</tt> is an object of type <tt>list</tt> with named entries. Check this using the function <tt>is.list()</tt>.</li>
<li>See what information you can obtain from <tt>mod</tt> using <tt>names()</tt>.</li>
<li>Read out an arbitrary entry of the object <tt>mod</tt> using the <tt>$</tt> operator.</li>
</ul>
<iframe src="DCL/ex4_4.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="plotting-the-regression-line" class="unnumbered">5. Plotting the Regression Line</h4>
<p>You are provided with the code for the scatterplot in <tt>script.R</tt></p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Add the regression line to the scatterplot from a few exercises before.</p></li>
<li><p>The object <tt>mod</tt> is available in your working environment.</p></li>
</ul>
<iframe src="DCL/ex4_5.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hint:</strong> Use the function <tt>abline()</tt>.</p>
</div>
<div class="DCexercise">
<h4 id="summary-of-a-model-object" class="unnumbered">6. Summary of a Model Object</h4>
<p>Now read out and store some of the information that is contained in the output of <tt>summary()</tt>.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Assign the output of <tt>summary(mod)</tt> to the variable <tt>s</tt>.</p></li>
<li><p>Check entry names of the object <tt>s</tt>.</p></li>
<li><p>Create a new variable <tt>R2</tt> and assign the <span class="math inline">\(R^2\)</span> of the regression.</p></li>
</ul>
<p>The object <tt>mod</tt> is available in your working environment.</p>
<iframe src="DCL/ex4_6.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="estimated-coefficients" class="unnumbered">7. Estimated Coefficients</h4>
<p>The function <tt>summary()</tt> also provides information on the statistical significance of the estimated coefficients.</p>
<p><strong>Instructions:</strong></p>
<p>Extract the named <span class="math inline">\(2\times4\)</span> matrix with estimated coefficients, standard errors, <span class="math inline">\(t\)</span>-statistics and corresponding <span class="math inline">\(p\)</span>-values from the model summary <tt>s</tt>. Save this matrix in an object named <tt>coefs</tt>.</p>
<p>The objects <tt>mod</tt> and <tt>s</tt> are available in your working environment.</p>
<iframe src="DCL/ex4_7.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="dropping-the-intercept" class="unnumbered">8. Dropping the Intercept</h4>
<p>So far, we have estimated regression models consisting of an intercept and a single regressor. In this exercise you will learn how to specify and how to estimate regression a model without intercept.</p>
<p>Note that excluding the intercept from a regression model might be a dodgy practice in some applications as this imposes the conditional expectation function of the dependent variable to be zero if the regressor is zero.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Figure out how the <tt>formula</tt> argument must be specified for a regression of <tt>ts</tt> solely on <tt>cs</tt>, i.e., a regression without intercept. Google is your friend!</p></li>
<li><p>Estimate the regression model without intercept and store the result in <tt>mod_ni</tt>.</p></li>
</ul>
<p>The vectors <tt>cs</tt>, <tt>ts</tt> and the model object <tt>mod</tt> from previous exercises are available in the working environment.</p>
<iframe src="DCL/ex4_8.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="regression-output-no-constant-case" class="unnumbered">9. Regression Output: No Constant Case</h4>
<p>In Exercise 8 you have estimated a model without intercept. The estimated regression function is</p>
<p><span class="math display">\[\widehat{TestScore} = \underset{(1.36)}{12.65} \times STR.\]</span></p>
<p><strong>Instructions:</strong></p>
<p>Convince yourself that everything is as stated above: extract the coefficient matrix from the summary of <tt>mod_ni</tt> and store it in a variable named <tt>coef</tt>.</p>
<p>The vectors <tt>cs</tt>, <tt>ts</tt> as well as the model object <tt>mod_ni</tt> from the previous exercise are available in your working environment.</p>
<iframe src="DCL/ex4_9.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hint:</strong> An entry of a named list can be accessed using the <tt>$</tt> operator.</p>
</div>
<div class="DCexercise">
<h4 id="regression-output-no-constant-case-ctd." class="unnumbered">10. Regression Output: No Constant Case — Ctd.</h4>
<p>In Exercises 8 and 9 you have dealt with a model without intercept. The estimated regression function was</p>
<p><span class="math display">\[\widehat{TestScore_i} = \underset{(1.36)}{12.65} \times STR_i.\]</span></p>
<p>The coefficient matrix <tt>coef</tt> from Exercise 9 contains the estimated coefficient on <span class="math inline">\(STR\)</span>, its standard error, the <span class="math inline">\(t\)</span>-statistic of the significance test and the corresponding <span class="math inline">\(p\)</span>-value.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Print the contents of <tt>coef</tt> to the console.</li>
<li>Convince yourself that the reported <span class="math inline">\(t\)</span>-statistic is correct: use the entries of <tt>coef</tt> to compute the <span class="math inline">\(t\)</span>-statistic and save it to <tt>t_stat</tt>.</li>
</ul>
<p>The matrix <tt>coef</tt> from the previous exercise is available in your working environment.</p>
<iframe src="DCL/ex4_10.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p><tt>X[a,b]</tt> returns the <tt>[a,b]</tt> element of the matrix <tt>X</tt>.</p></li>
<li><p>The <span class="math inline">\(t\)</span>-statistic for a test of the hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> is computed as <span class="math display">\[t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)}.\]</span></p></li>
</ul>
</div>
<div class="DCexercise">
<h4 id="two-regressions-one-plot" class="unnumbered">11. Two Regressions, One Plot</h4>
<p>The two estimated regression models from the previous exercises are</p>
<p><span class="math display">\[\widehat{TestScore_i} = \underset{(1.36)}{12.65} \times STR_i\]</span></p>
<p>and</p>
<p><span class="math display">\[\widehat{TestScore_i} = \underset{(23.96)}{567.4272} \underset{(0.85)}{-7.1501} \times STR_i.\]</span></p>
<p>You are provided with the code line <tt>plot(cs, ts)</tt> which creates a scatterplot of <tt>ts</tt> and <tt>cs</tt>. Note that this line must be executed before calling <tt>abline()</tt>! You may color the regression lines by using, e.g., <tt>col = “red”</tt> or <tt>col = “blue”</tt> as an additional argument to <tt>abline()</tt> for better distinguishability.</p>
<p>The vectors <tt>cs</tt> and <tt>ts</tt> as well as the list objects <tt>mod</tt> and <tt>mod_ni</tt> from previous exercises are availabe in your working environment.</p>
<p><strong>Instructions:</strong></p>
<p>Generate a scatterplot of <tt>ts</tt> and <tt>cs</tt> and add the estimated regression lines of <tt>mod</tt> and <tt>mod_ni</tt>.</p>
<iframe src="DCL/ex4_11.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="tss-and-ssr" class="unnumbered">12. <span class="math inline">\(TSS\)</span> and <span class="math inline">\(SSR\)</span></h4>
<p>If graphical inspection does not help, researchers resort to analytic techniques in order to detect if a model fits the data at hand good or better than another model.</p>
<p>Let us go back to the simple regression model including an intercept. The estimated regression line for <tt>mod</tt> was</p>
<p><span class="math display">\[\widehat{TestScore_i} = 567.43 - 7.15 \times STR_i, \, R^2 = 0.8976, \, SER=15.19.\]</span></p>
<p>You can check this as <tt>mod</tt> and the vectors <tt>cs</tt> and <tt>ts</tt> are available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Compute <span class="math inline">\(SSR\)</span>, the sum of squared residuals, and save it to <tt>ssr</tt>.</li>
<li>Compute <span class="math inline">\(TSS\)</span>, the total sum of squares, and save it to <tt>tss</tt>.</li>
</ul>
<iframe src="DCL/ex4_12.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="the-r2-of-a-regression-model" class="unnumbered">13. The <span class="math inline">\(R^2\)</span> of a Regression Model</h4>
<p>The <span class="math inline">\(R^2\)</span> of the regression saved in <tt>mod</tt> is <span class="math inline">\(0.8976\)</span>. You can check this by executing <tt>summary(mod)$r.squared</tt> in the console below.</p>
<p>Remember the formula of <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[R^2 = \frac{ESS}{TSS} = 1 - \frac{SSR}{TSS}\]</span></p>
<p>The objects <tt>mod</tt>, <tt>tss</tt> and <tt>ssr</tt> from the previous exercise are available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Use <tt>ssr</tt> and <tt>tss</tt> to compute <span class="math inline">\(R^2\)</span> manually. <em>Round</em> the result to <em>four</em> decimal places and save it to <tt>R2</tt>.</li>
<li>Use the logical operator <tt>==</tt> to check whether your result matches the value mentioned above.</li>
</ul>
<iframe src="DCL/ex4_13.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints:</strong></p>
<p>You may round numeric values using the function <tt>round()</tt>.</p>
</div>
<div class="DCexercise">
<h4 id="the-standard-error-of-the-regression-1" class="unnumbered">14. The Standard Error of The Regression</h4>
<p>The standard error of the Regression in the simple regression model is <span class="math display">\[SER = \frac{1}{n-2} \sum_{i=1}^n \widehat{u}_i^2 =\sqrt{\frac{SSR}{n-2}}.\]</span> <span class="math inline">\(SER\)</span> measures the size of an average residual which is an estimate of the magnitude of a typical regression error.</p>
<p>The model object <tt>mod</tt> and the vectors <tt>cs</tt> and </tt>ts</tt> are available in your workspace.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Use <tt>summary()</tt> to obtain the <span class="math inline">\(SER\)</span> for the regression of <tt>ts</tt> on <tt>cs</tt> saved in the model object <tt>mod</tt>. Save the result in the variable <tt>SER</tt>.</p></li>
<li><p>Use <tt>SER</tt> to compute the <span class="math inline">\(SSR\)</span> and store it in <tt>SSR</tt>.</p></li>
<li><p>Check that <tt>SSR</tt> is indeed the <span class="math inline">\(SSR\)</span> by comparing <tt>SSR</tt> to the result of <tt>sum(mod$residuals^2)</tt></p></li>
</ul>
<iframe src="DCL/ex4_14.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="the-estimated-covariance-matrix" class="unnumbered">15. The Estimated Covariance Matrix</h4>
<p>As has been discussed in Chapter <a href="lrwor.html#tlsa">4.4</a>, the OLS estimators <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span> are functions of the random error term. Therefore, they are random variables themselves. For two or more random variables, their covariances and variances are summarized by a <em>variance-covariance matrix</em> (which is often simply called the <em>covariance matrix</em>). Taking the square root of the diagonal elements of the estimated covariance matrix obtains <span class="math inline">\(SE(\widehat\beta_0)\)</span> and <span class="math inline">\(SE(\widehat\beta_1)\)</span>, the standard errors of <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span>.</p>
<p><tt>summary()</tt> computes an estimate of this matrix. The respective entry in the output of summary (remember that <tt>summary()</tt> produces a list) is called <tt>cov.unscaled</tt>. The model object <tt>mod</tt> is available in your workspace.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Use <tt>summary()</tt> to obtain the covariance matrix estimate for the regression of test scores on student-teacher ratios stored in the model object <tt>mod</tt>. Save the result to <tt>cov_matrix</tt>.</p></li>
<li><p>Obtain the diagonal elements of <tt>cov_matrix</tt>, compute their square root and assign the result to the variable <tt>SEs</tt>.</p></li>
</ul>
<iframe src="DCL/ex4_15.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hint:</strong> <tt>diag(A)</tt> returns a vector containing the diagonal elements of the matrix <tt>A</tt>.</p>
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-kleiber2008">
<p>Kleiber, C., &amp; Zeileis, A. (2008). <em>Applied Econometrics with R</em>. Springer.</p>
</div>
<div id="ref-R-AER">
<p>Kleiber, C., &amp; Zeileis, A. (2017). AER: Applied Econometrics with R (Version 1.2-5). Retrieved from <a href="https://CRAN.R-project.org/package=AER" class="uri">https://CRAN.R-project.org/package=AER</a></p>
</div>
<div id="ref-R-MASS">
<p>Ripley, B. (2018). MASS: Support Functions and Datasets for Venables and Ripley’s MASS (Version 7.3-50). Retrieved from <a href="https://CRAN.R-project.org/package=MASS" class="uri">https://CRAN.R-project.org/package=MASS</a></p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>See Chapter <a href="ittsraf.html#ittsraf">14</a> for more on autoregressive processes and time series analysis in general.<a href="lrwor.html#fnref4">↩</a></p></li>
<li id="fn5"><p>See Chapter 4.4 of the book.<a href="lrwor.html#fnref5">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="arosur.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="htaciitslrm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": true,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 1
},
"edit": null,
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
