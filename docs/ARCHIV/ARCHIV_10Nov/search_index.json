[["2-review-simple-linear-regression.html", "2 Review: Simple Linear Regression ", " 2 Review: Simple Linear Regression "],["2-1-the-simple-linear-regression-model.html", "2.1 The Simple Linear Regression Model", " 2.1 The Simple Linear Regression Model The Data-Generating Process We assume some outcome, \\(Y_i\\), and the explanatory variable, \\(X_i\\), are related to each other via the following linear model: \\[\\begin{align} Y_i &amp;=\\beta_0 + \\beta_1 X_{i} + \\epsilon_i,\\label{eq:slreg} \\end{align}\\] where \\(i=1,\\dots,n\\) denotes the individual observation and \\(n\\) denotes the sample size. 2.1.1 Assumptions About the Error Term The Distribution of \\(\\epsilon_i\\) One common distributional assumption on the error terms is the i.i.d. assumption. Under this assumption each \\(\\epsilon_i\\) is drawn from the same (but unknown) distribution and two error terms \\(\\epsilon_i\\) and \\(\\epsilon_j\\) are independent of each other for all \\(i\\neq j\\). Sometimes we need to be even more specific, for instance, by assuming that the error terms \\(\\epsilon_1,\\dots,\\epsilon_n\\) are i.i.d. normal, i.e. \\[\\epsilon_i \\stackrel{\\textrm{i.i.d.}}{\\sim}{\\mathcal N} (0, \\sigma^2)\\quad\\text{for all}\\quad i=1,\\dots,n,\\] where \\(\\sigma^2\\) is an additional parameter – namely, the variance of \\(\\epsilon_i\\) – which in the i.i.d. case is the same for all \\(\\epsilon\\)’s. The literature often distinguishes between the following general scenarios for the variances of the error terms and the covariances between the error terms (auto-covariances): For the rest of this chapter, we will assume that the i.i.d. assumption holds which implies homoscedastic and non-autocorrelated error terms. The Exogeneity Assumption \\(E[\\epsilon_i|X_i]=0\\) For the simple linear regression model in to be well-defined, we need further assumptions about the random error term \\[\\epsilon_i = Y_i - \\beta_0 - \\beta_1 X_i.\\] An assumption of central importance is that the conditional mean of \\(\\epsilon_i\\) given \\(X_i\\) is zero, i.e. \\(E[\\epsilon_i|X_i]=0\\), for all \\(i=1,\\dots,n\\). In other words, on average, the non-systematic determinants of \\(Y_i\\) are zero – no matter the value of \\(X_i\\). That is, this assumption is also a statement about the relationship between \\(\\epsilon_i\\) and \\(X_i\\). It turns out that the assumption \\(E[\\epsilon_i|X_i]=0\\) is one of the most important assumptions we make in econometrics. When it is violated it causes fundamental econometric problems. The assumption \\(E[\\epsilon_i | X_i]=0\\) is known as the orthogonality assumption or the exogeneity assumption. Implications of the orthogonality assumption: The assumption that \\(E[\\epsilon_i|X_i]\\) implies that the correlation between \\(X_i\\) and \\(\\epsilon_i\\) is zero, although the converse is not true. The assumption that \\(E[\\epsilon_i|X_i]\\) implies also that \\(E[\\epsilon_i]=0\\). Under a fixed design the assumption \\(E[\\epsilon_i|X_i]=0\\) is not so critical since \\(E[\\epsilon_i|X_i]=E[\\epsilon_i]\\). That is, for fixed designs we only need to assume that the unconditional mean of \\(\\epsilon_i\\) is zero, i.e. \\(E[\\epsilon_i]=0\\). Terminology: The (unknown) Population Regression Line We now have enough information to think about the relationship between \\(Y_i\\) and \\(X_i\\). In particular, note that we can write \\[\\begin{align*} E[Y_i | X_i ]&amp;= E[\\beta_0 + \\beta_1 X_i + \\epsilon_i | X_i] \\\\ &amp;= \\beta_0 + \\beta_1 X_i + E[\\epsilon_i | X_i]. \\end{align*}\\] Then, by our orthogonality assumption, \\(E[\\epsilon_i|X_i]=0\\), we have that \\[E[Y_i | X_i ] = \\beta_0 + \\beta_1 X_i\\] This conditional expectation of \\(Y_i\\) given \\(X_i\\) defines the population regression line. Terminology: Estimates versus Estimators Our goal is to devise some way to estimate the unknown parameters \\(\\beta_0\\in\\mathbb{R}\\) and \\(\\beta_1\\mathbb{R}\\) which define the population regression line. We call our method or formula for estimating these parameters an estimator. An estimator is a function of the random sample \\(((Y_1,X_1),\\dots,(Y_n,X_n))\\) and, therefore, is itself a random variable that generally has different realizations in repeated samples. The results of applying our estimator to data (data \\(=\\) a given realization of the random sample \\(((Y_1,X_1),\\dots,(Y_n,X_n))\\)) are called the estimates. Let us consider the following simple estimation problem: estimating the population mean, \\(\\mu\\), of a random variable, \\(Y\\), on the basis of an i.i.d. random sample \\(Y_1,\\dots,Y_n\\stackrel{\\textrm{i.i.d.}}{\\sim}Y\\). The observed data, \\(y_1,\\dots y_n\\), are a realization of the i.i.d. random variables \\(Y_1,\\dots,Y_n\\). A sensible estimator is the (random) sample mean, \\(\\bar Y=n^{-1}\\sum_{i=1}^n Y_i\\). We denote a result of the estimator, the estimate \\(\\bar y=n^{-1}\\sum_{i=1}^n y_i\\). We will discuss later what we mean by a or estimator. set.seed(123) n &lt;- 100 # sample size mu &lt;- 5 # mean (usually unknown) var &lt;- 2 # variance (usually unknown) # simulate a realization of an i.i.d. random # sample (Y_1,...,Y_n), where Y_i ~ N(0,1) for all i=1,...,n Y_data &lt;- rnorm(n = n, mean = mu, sd = sqrt(var)) # compute estimate of mu mean(Y_data) ## [1] 5.127853 This example is actually a special case of the simple linear regression model with \\(Y_i=\\beta_0+\\epsilon_i\\), where \\(\\beta_0=\\mu\\). We will not always notionally distinguish between a random variable, \\(Y_i\\) say, and its realization, \\(y_i\\) say; sometimes can denote the random variable or its realization. However, the context will allow you to distinguish between these two cases. "],["2-2-ordinary-least-squares-estimation.html", "2.2 Ordinary Least Squares Estimation", " 2.2 Ordinary Least Squares Estimation There are many ways to estimate the parameters \\(\\beta_0\\) and \\(\\beta_1\\) of the simple linear regression model in . In the following, we will derive the Ordinary Least Squares (OLS) estimator using a ``mechanical’’ approach. In the next chapter, we will use an alternative approach () that is particularly interesting in econometrics. The properties of estimators for \\(\\beta_0\\) and \\(\\beta_1\\) crucially depend on the assumptions we need to make about \\(\\epsilon\\) and \\(X\\). 2.2.1 Population vs. Sample Regression, Residual vs. Error Let us define some necessary terms. Call \\(\\hat\\beta_0\\) our estimate of \\(\\beta_0\\) and \\(\\hat\\beta_1\\) our estimate of \\(\\beta_1\\). Now, define the predicted value, \\(\\hat Y_i\\), of the dependent variable, \\(Y_i\\), to be \\[\\begin{align} \\hat Y_i&amp;= \\hat\\beta_0 + \\hat\\beta_1 X_i\\label{eq:fitted_y} \\end{align}\\] This is just the prediction of the dependent variable, \\(Y_i\\), given the value of \\(X_i\\) and the estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\). Equation defines the sample regression line. Define the , \\(e_i\\), as the difference between the observed value, \\(Y_i\\), and the predicted value, \\(\\hat Y_i\\): \\[\\begin{align*} e_i&amp;= Y_i - \\hat Y_i \\\\ &amp;= Y_i - \\hat\\beta_0 - \\hat\\beta_1 Y_i \\end{align*}\\] The residual, \\(e_i\\), is the vertical distance between the observed value and the sample regression line. We must make an important distinction between the residuals, \\(e_i\\), and the errors \\(\\epsilon_i\\). \\[\\begin{align*} e_i &amp;= Y_i - \\hat\\beta_0 - \\hat\\beta_i X_i \\\\ \\epsilon_i &amp;= Y_i - \\beta_0 - \\beta_1 X_i \\end{align*}\\] Because \\(\\beta_0\\) and \\(\\beta_1\\) are unknown, we can never know the value of the error terms \\(\\epsilon_i\\). However, because we actually come up with the estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\), and because we observe \\(X_i\\), we can calculate the residual \\(e_i\\) for each observation. This distinction is important to keep in mind. Note that we can write \\[\\begin{align*} \\epsilon_i &amp;= Y_i - \\beta_0 + \\beta_1 X_i\\\\ &amp;= Y_i - E[Y_i | X_i]. \\end{align*}\\] But for this to make sense, we needed to impose the orthogonality assumption that \\(E[\\epsilon_i | X_i]=0\\), since only then we can identify the population regression line \\(\\beta_0 + \\beta_1 X_i\\) using the conditional mean of \\(Y_i\\) given \\(X_i\\). 2.2.2 Deriving the OLS estimate The method of () estimation has a long history and was first described by Legendre in 1805 – although Karl Friedrich Gauss claimed to use OLS since 1795. In 1809 Gauss published his work on OLS which extended the work of Legendre. The idea of OLS is to choose parameter values that for a given data set. These minimizing parameters are then the estimates of the unknown population parameters. It turns out that the OLS estimator is equipped with several desirable properties. In a sense, OLS is a purely method. We will see that it is equivalent to an alternative estimation method called methods of moments which have a more profound econometric motivation, given a certain set of assumptions. Deriving the OLS estimate: Our objective is to find the parameter values \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) that minimize the sum of squared residuals \\[\\begin{align*} S_n(b_0, b_1)&amp;=\\sum_{i=1}^n e_i^2 \\\\ &amp;=\\sum_{i=1}^n (y_i - \\hat y_i)^2 \\\\ &amp;= \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2 \\\\ &amp;= \\sum_{i=1}^n (y_i^2 - 2 b_0 y_i - 2b_1 y_i x_i + b_0^2 + 2b_0 b_1 x_i + b_1^2 x_i^2) \\end{align*}\\] Now partially differentiate the last line with respect to \\(b_0\\) and \\(b_1\\), respectively. \\[\\begin{align*} \\dfrac{\\partial S_n(b_0,b_1)}{\\partial b_0}&amp;= \\sum_{i=1}^n \\left(-2y_i + 2b_0 +2 b_1 x_i\\right)\\\\ \\dfrac{\\partial S_n(b_0,b_1)}{\\partial b_1}&amp;= \\sum_{i=1}^n\\left(-2y_i x_i + 2 b_0 x_i + 2 b_1 x_i^2\\right) \\end{align*}\\] Setting the two partial derivatives equal to zero gives to equations and two unknown parameters which allows us to derive the minimizing arguments \\((\\hat\\beta_0,\\hat\\beta_1)&#39;=\\min\\arg_{(b_0,b_1)\\in\\mathbb{R}^2}S_n(b_0,b_1)\\): \\[\\begin{align*} n\\hat\\beta_0 - \\sum_{i=1}^n y_i+ \\hat\\beta_1 \\sum_{i=1}^n x_i &amp;=0\\\\ \\sum_{i=1}^n\\left(-y_i x_i + \\hat\\beta_0 x_i +\\hat\\beta_1 x_i^2\\right)&amp;=0 % -\\left(\\sum_{i=1}^n y_i x_i\\right) + \\hat\\beta_1 \\left( \\sum_{i=1}^n x_i^2\\right)&amp;=0 \\end{align*}\\] These equations are known as the . It is easy to see from the first normal equation that the OLS estimators of \\(\\beta_0\\) is \\[\\begin{equation} \\hat\\beta_0 = \\bar y - \\hat\\beta_1 \\bar x \\end{equation}\\] Substituting \\(\\hat\\beta_0\\) into the second normal equation gives \\[\\begin{align*} 0&amp;=\\sum_{i=1}^n\\left(-y_i x_i + ( \\bar y - \\hat\\beta_1 \\bar x) x_i + \\hat\\beta_1 x_i^2\\right) \\\\ &amp;= \\sum_{i=1}^n\\left(-x_i (y_i - \\bar y)+ \\hat\\beta_1 x_i(x_i - \\bar x)\\right)\\\\ &amp;=-\\left(\\sum_{i=1}^n x_i (y_i - \\bar y)\\right) + \\hat\\beta_1 \\left(\\sum_{i=1}^n x_i (x_i - \\bar x)\\right)\\\\ \\end{align*}\\] Solving for \\(\\hat\\beta_1\\) gives \\[\\begin{align*} \\hat\\beta_1&amp;=\\dfrac{\\sum_{i=1}^n (y_i - \\bar y) x_i}{\\sum_{i=1}^n (x_i-\\bar x)x_i}\\\\\\notag &amp;=\\dfrac{\\sum_{i=1}^n (y_i - \\bar y) (x_i- \\bar x)}{\\sum_{i=1}^n (x_i-\\bar x)(x_i - \\bar x)}\\\\\\notag &amp;=\\dfrac{\\sum_{i=1}^n (x_i- \\bar x)y_i}{\\sum_{i=1}^n (x_i-\\bar x)^2} \\end{align*}\\] The last two lines follow from the that will be discussed in the exercises of this chapter. ## simulate data set.seed(3) n &lt;- 25 # sample size X &lt;- runif(n, min = 1, max = 10) error &lt;- rt(n, df = 3) beta0 &lt;- 1 beta1 &lt;- 2 Y &lt;- beta0 + beta1 * X + error ## save simulated data as data frame data_sim &lt;- data.frame(&quot;Y&quot; = Y, &quot;X&quot; = X) ## OLS fit lm_obj &lt;- lm(Y~X, data = data_sim) ## ## Plot op &lt;- par(family = &quot;serif&quot;) plot(x = data_sim$X, y = data_sim$Y, main=&quot;&quot;, axes=FALSE, pch = 16, cex = 0.8, xlab = &quot;X&quot;, ylab = &quot;Y&quot;) axis(1, tick = FALSE) axis(2, tick = FALSE, las = 2) abline(lm_obj, lty=2, lwd = 1.3, col=&quot;darkorange&quot;) abline(a = beta0, b = beta1, lwd=1.3, col=&quot;darkblue&quot;) legend(&quot;topleft&quot;, col=c(&quot;darkorange&quot;, &quot;darkblue&quot;), legend = c(&quot;Sample Regression Line&quot;, &quot;Population Regression Line&quot;), lwd=1.3, lty=c(2,1), bty=&quot;n&quot;) ## Estimates coef(lm_obj) ## (Intercept) X ## 1.721012 1.861660 Interpretation of the Results The coefficients have the usual intercept and slope interpretation. In particular, \\(\\hat\\beta_1\\) in this case is the estimated of a one unit change in \\(X\\) on \\(Y\\): \\[\\begin{align*} E[Y | X] &amp;= \\beta_0 + \\beta_1 X \\\\ \\dfrac{\\partial E[Y | X]}{\\partial X} &amp;= \\beta_1 \\end{align*}\\] We do not know the true value of \\(\\partial E[Y | X]/\\partial X\\), but \\(\\hat\\beta_1\\) gives us an estimate \\[\\begin{align*} \\widehat{\\dfrac{\\partial E[Y | X]}{\\partial X}} &amp;= \\hat\\beta_1. \\end{align*}\\] "],["2-3-properties-of-the-ols-estimator.html", "2.3 Properties of the OLS estimator", " 2.3 Properties of the OLS estimator Note that we did not need to make any assumptions about the error term, and in particular, its relationship to \\(X\\) in order to derive the estimator. We also did not make any assumption about the variance of \\(\\epsilon\\) or about the relationship between the error terms \\(\\epsilon_i\\) and \\(\\epsilon_j\\) for \\(i\\neq j\\) in order to derive this estimator. So why is OLS a ``good’’ estimator? 2.3.1 Mean and Bias of the OLS Estimator Because the estimator \\[\\begin{align} \\hat{\\boldsymbol{\\beta}}= \\begin{pmatrix}\\hat\\beta_0\\\\\\hat\\beta_1\\end{pmatrix}= \\begin{pmatrix} \\bar Y - \\hat\\beta_1 \\bar X\\\\ \\dfrac{\\sum_{i=1}^n (X_i- \\bar X)Y_i}{\\sum_{i=1}^n (X_i-\\bar X)^2} \\end{pmatrix} \\label{eq:OLSslreg} \\end{align}\\] is a function of the random variables in the random sample \\(((Y_1,X_1),\\dots,(Y_n,X_n))\\), it is itself a random variable. It therefore has a probability distribution, and we can talk sensibly about its expectation \\(E(\\hat{\\boldsymbol{\\beta}})\\). Note that the estimator \\(\\hat{\\boldsymbol{\\beta}}\\) in contains both the \\(Y_i\\)’s and the \\(X_i\\)’s. Therefore, under random designs, we need to be specific which mean (or variance) of the estimator we want to consider. For the random design, we consider conditional means (and later variances) given the design matrix \\(X=\\begin{pmatrix}1&amp;X_1\\\\\\vdots&amp;\\vdots\\\\1&amp;X_n\\end{pmatrix}\\). That is, under random designs, we consider \\(E[\\hat{\\boldsymbol{\\beta}}|X]\\) which leads to the definition of the conditional bias: \\[\\begin{align*} \\operatorname{Bias}(\\hat{\\boldsymbol{\\beta}}|X)=E[\\hat{\\boldsymbol{\\beta}}|X]-\\boldsymbol{\\beta}, \\end{align*}\\] For fixed designs, conditioning on \\(X\\) is not false but superfluous since \\(X\\) remains fixed in repeated samples and therefore \\(\\operatorname{Bias}(\\hat{\\boldsymbol{\\beta}}|X)=\\operatorname{Bias}(\\hat{\\boldsymbol{\\beta}})=E[\\hat{\\boldsymbol{\\beta}}]-\\boldsymbol{\\beta}\\). Since random designs are the more relevant scenarios in economics we will use conditional means (and variances) which just simply under a fixed design. The OLS estimator is called , if its conditional expectation is equal to the true parameter, i.e. if \\[\\begin{align*} E[\\hat{\\boldsymbol{\\beta}}|X]=\\boldsymbol{\\beta}\\quad \\Leftrightarrow\\quad\\operatorname{Bias}(\\boldsymbol{\\beta}|X)=0 \\end{align*}\\] That is, the conditional mean of the random estimator equals the true parameter value which we want to estimate. Put another way, unbiasedness means that the conditional distribution of the estimator is centered on the true value of the parameter. Of course, the probability that a estimate is equal to the truth in any particular sample is zero if the error terms \\(\\epsilon_i\\) (and therefore the \\(Y_i\\)) are continuous random variables. To show that \\(\\hat\\beta_1\\) is conditionally unbiased, we want to show that its conditional expectation is equal to \\(\\beta_1\\): \\[\\begin{align*} E[\\hat\\beta_1|X]&amp;=E\\left[\\dfrac{\\sum_{i=1}^n(X_i - \\bar X)Y_i}{\\sum_{i=1}^n(X_i - \\bar X)^2}|X\\right] =E\\left[\\dfrac{\\sum_{i=1}^n(X_i - \\bar X)(\\beta_0 + \\beta_1 X_i + \\epsilon_i)}{\\sum_{i=1}^n(X_i - \\bar X)^2}|X\\right] \\end{align*}\\] In case of a deterministic design, \\(X\\) is fixed in repeated samples which makes the conditioning on \\(X\\) superfluous since, for instance, \\(\\sum(X_i - \\bar X)^2\\) is then a constant which allows us to bring it outside of the expectation; the same applies to all other terms that involve \\(X\\) variables. In order to do this under a random design, it is important to condition on the observed design matrix \\(X\\). That is, in case of a random design, conditioning \\(X\\) also allows us to bring all terms containing \\(X\\) variables outside of the expectation. However, interpretations then must consider the conditioning on \\(X\\). So, we really only need to concern ourselves with the numerator, which we can write as: \\[\\begin{equation*} \\beta_0 \\sum_{i=1}^n(X_i - \\bar X)+ \\beta_1 \\sum_{i=1}^n(X_i - \\bar X)X_i + E\\left[\\sum_{i=1}^n(X_i - \\bar X)\\epsilon_i\\right] \\end{equation*}\\] For the first two terms, note that \\[\\begin{align*} \\beta_0 \\sum_{i=1}^n(X_i - \\bar X) &amp;=\\beta_0 \\left[\\left(\\sum_{i=1}^n X_i \\right) - n \\bar X\\right] = 0 \\\\ \\beta_1 \\sum_{i=1}^n(X_i - \\bar X)X_i &amp;=\\beta_1 \\sum_{i=1}^n(X_i - \\bar X)^2. \\end{align*}\\] So we are left with \\[E[\\hat\\beta_1|X]= \\beta_1 + \\dfrac{1}{\\sum_{i=1}^n(X_i-\\bar X)^2}E\\left[\\sum_{i=1}^n(X_i - \\bar X)\\epsilon_i\\right|X]\\] Clearly, then, we want the last term here to be zero, so that \\(\\hat\\beta_1\\) is conditionally unbiased. The last term is zero if the factor \\(E\\left[\\sum(X_i - \\bar X)\\epsilon_i|X\\right]=0\\). For fixed/random designs we have the following expressions: That is, the OLS estimator, \\(\\hat\\beta_1\\), is (conditionally) unbiased if the orthogonality assumption \\(E[\\epsilon_i |X_i]=0\\) holds, which simplifies under the fixed design to \\(E[\\epsilon_i]=0\\). We return, therefore, to the importance of the orthogonality assumption \\(E[\\epsilon | X_i]\\) since if this assumption does not hold, the estimator is biased and remains biased even asymptotically as \\(n\\to\\infty\\). The unbiasedness of \\(\\hat\\beta_1\\) implies that \\(\\hat\\beta_0=\\bar{Y}-\\hat\\beta_1\\bar{X}\\) since \\[\\begin{align*} E[\\hat\\beta_0|X] &amp;=E\\left[\\bar Y - \\hat\\beta_1 \\bar X|X\\right]\\\\ &amp;=E\\left[\\dfrac{1}{n}\\sum_{i=1}^n (\\beta_0 + \\beta_1 X_i + \\epsilon_i) - \\hat\\beta_1 \\bar X |X\\right]\\\\ &amp;=\\beta_0 + \\beta_1\\bar X + \\dfrac{1}{n}\\sum_{i=1}^n E[\\epsilon_i|X_i] - E[\\hat\\beta_1|X] \\bar X \\\\ &amp;=\\beta_0 \\end{align*}\\] where the last line follows from the exogeneity (or orthogonality) assumption \\(E[\\epsilon_i|X]=0\\) and the unbiasedness of \\(\\hat\\beta_1\\). Note that the exogeneity assumption, \\(E[\\epsilon_i|X]=0\\), is the only distributional assumption on the error terms (and the \\(X_i\\)’s) that is needed to show that the OLS estimator is unbiased. In fact, OLS remains unbiased if the error terms are heteroscedastic and/or autocorrelated. However, under the more restrictive assumption that the error terms are homoscedastic and not autocorrelated, one can show that the OLS estimator is the most efficient (lowest variance) estimator within the family of all unbiased estimators. This result is known as the Gauss-Markov Theorem and we will consider this theorem in the context of the multiple regression model in more detail. 2.3.2 Variance of the OLS Estimator Remember: An unbiased estimator can still be a very imprecise (inefficient) estimator if it has a large variance. In order to derive the variance of the OLS estimator one needs more than only the exogeneity assumption. One gets different variance expressions for different distributional assumptions on the error terms \\(\\epsilon_1,\\dots,\\epsilon_n\\). As we consider this in more detail for the multiple regression model, we consider in the following the most simple case of i.i.d. error terms which implies homoscedastic and non-autocorrelated error terms. Under this simple scenario it can be shown that \\[\\begin{align} \\operatorname{Var}(\\hat\\beta_0|X)&amp;= \\frac{1}{n} \\sigma^2\\cdot\\frac{n^{-1}\\sum_{i=1}^n X_i^2}{n^{-1}\\sum_{i=1}^n\\left(X_i-\\bar{X} \\right)^2}\\\\ \\operatorname{Var}(\\hat\\beta_1|X)&amp;= \\frac{1}{n} \\sigma^2\\cdot\\frac{1}{n^{-1}\\sum_{i=1}^n\\left(X_i-\\bar{X} \\right)^2}, \\label{eq:varbeta} \\end{align}\\] where \\(\\operatorname{Var}(\\epsilon_i)=\\sigma^2\\). That is \\[\\begin{align*} \\operatorname{Var}(\\hat\\beta_0|X)&amp;\\to 0 \\quad\\text{as}\\quad n\\to\\infty\\\\ \\operatorname{Var}(\\hat\\beta_1|X)&amp;\\to 0 \\quad\\text{as}\\quad n\\to\\infty \\end{align*}\\] which is good since this means that the estimators become more precise as the sample size \\(n\\) (i.e. more information) increases. Note that variance expressions in are not really useful in practice since usually we do not know the variance of the error terms \\(\\sigma^2\\). Under the i.i.d. assumption we can replace the unknown \\(\\sigma^2\\) by plugging-in reasonable estimates such as \\[\\begin{align*} s_{ML}^2&amp;=\\frac{1}{n}\\sum_{i=1}^n e_i^2\\qquad\\text{or}\\qquad s^2=\\frac{1}{n-1}\\sum_{i=1}^n e_i^2 \\end{align*}\\] where \\(s^2\\) is the unbiased estimator and \\(s_{ML}^2\\) the aximum likelihood estimator of \\(\\sigma^2\\). This leads to the practically relevant variance estimators \\[\\begin{align*} \\widehat{\\operatorname{Var}}(\\hat\\beta_0|X)&amp;= \\frac{1}{n} s^2\\cdot\\frac{n^{-1}\\sum_{i=1}^n X_i^2}{n^{-1}\\sum_{i=1}^n\\left(X_i-\\bar{X} \\right)^2}\\\\ \\widehat{\\operatorname{Var}}(\\hat\\beta_1|X)&amp;= \\frac{1}{n} s^2\\cdot\\frac{1}{n^{-1}\\sum_{i=1}^n\\left(X_i-\\bar{X} \\right)^2}, \\label{eq:varbeta} \\end{align*}\\] 2.3.3 Consistency of the OLS Estimator Now, we have everything together in order to show that the OLS estimators \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are consistent estimators of the unknown parameter values \\(\\beta_0\\) and \\(\\beta_1\\). Since the OLS estimator is unbiased, we do not need to bother with a possible bias-problem – provided the assumptions of the simple linear regression model are true. Moreover, since the variances of the estimators \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) converge to zero as \\(n\\to\\infty\\), we have that the Mean Squared Error (MSE) converges to zero, i.e. \\[\\begin{align*} \\operatorname{MSE}(\\hat\\beta_j|X)&amp;=E\\left[(\\hat\\beta_j-\\beta_j|X)^2\\right]\\\\ &amp;=\\underbrace{\\left(\\operatorname{Bias}(\\hat\\beta_j|X)\\right)^2}_{=0}+\\operatorname{Var}(\\beta_j|X)^2\\to 0\\;\\;\\text{as}\\;\\;n\\to\\infty\\;\\text{for}\\;j=1,2. \\end{align*}\\] This means that "]]
