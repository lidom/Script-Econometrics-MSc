<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Econometrics with R</title>
  <meta name="description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Econometrics with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Econometrics with R" />
  
  <meta name="twitter:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2018-10-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ittsraf.html">
<link rel="next" href="atitsr.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<!-- font families -->

<link href="https://fonts.googleapis.com/css?family=PT+Sans|Pacifico|Source+Sans+Pro" rel="stylesheet">

<!-- function to adjust height of iframes automatically depending on content loaded -->

<script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';
  }
</script>

<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/default.js"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110299877-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110299877-1');
</script>

<!-- open review block -->

<script async defer src="https://hypothes.is/embed.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://www.oek.wiwi.uni-due.de/en/">Chair of Econometrics at UDE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#a-very-short-introduction-to-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> A Very Short Introduction to <tt>R</tt> and <em>RStudio</em></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="pt.html"><a href="pt.html#random-variables-and-probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#expected-value-mean-and-variance"><i class="fa fa-check"></i>Expected Value, Mean and Variance</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#thetdist"><i class="fa fa-check"></i>The Student t Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-f-distribution"><i class="fa fa-check"></i>The F Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pt.html"><a href="pt.html#RSATDOSA"><i class="fa fa-check"></i><b>2.2</b> Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="pt.html"><a href="pt.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arosur.html"><a href="arosur.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="3.1" data-path="arosur.html"><a href="arosur.html#estimation-of-the-population-mean"><i class="fa fa-check"></i><b>3.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="3.2" data-path="arosur.html"><a href="arosur.html#potsm"><i class="fa fa-check"></i><b>3.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="3.3" data-path="arosur.html"><a href="arosur.html#hypothesis-tests-concerning-the-population-mean"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests Concerning the Population Mean</a><ul>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-p-value"><i class="fa fa-check"></i>The p-Value</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-known"><i class="fa fa-check"></i>Calculating the p-Value when the Standard Deviation is Known</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#SVSSDASE"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-unknown"><i class="fa fa-check"></i>Calculating the p-value When the Standard Deviation is Unknown</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-t-statistic"><i class="fa fa-check"></i>The t-statistic</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="arosur.html"><a href="arosur.html#confidence-intervals-for-the-population-mean"><i class="fa fa-check"></i><b>3.4</b> Confidence Intervals for the Population Mean</a></li>
<li class="chapter" data-level="3.5" data-path="arosur.html"><a href="arosur.html#cmfdp"><i class="fa fa-check"></i><b>3.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="3.6" data-path="arosur.html"><a href="arosur.html#aattggoe"><i class="fa fa-check"></i><b>3.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="3.7" data-path="arosur.html"><a href="arosur.html#scatterplots-sample-covariance-and-sample-correlation"><i class="fa fa-check"></i><b>3.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
<li class="chapter" data-level="3.8" data-path="arosur.html"><a href="arosur.html#exercises-1"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="4.1" data-path="lrwor.html"><a href="lrwor.html#simple-linear-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="4.2" data-path="lrwor.html"><a href="lrwor.html#estimating-the-coefficients-of-the-linear-regression-model"><i class="fa fa-check"></i><b>4.2</b> Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lrwor.html"><a href="lrwor.html#measures-of-fit"><i class="fa fa-check"></i><b>4.3</b> Measures of Fit</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-coefficient-of-determination"><i class="fa fa-check"></i>The Coefficient of Determination</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-standard-error-of-the-regression"><i class="fa fa-check"></i>The Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lrwor.html"><a href="lrwor.html#tlsa"><i class="fa fa-check"></i><b>4.4</b> The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption 1: The Error Term has Conditional Mean of Zero</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-2-independently-and-identically-distributed-data"><i class="fa fa-check"></i>Assumption 2: Independently and Identically Distributed Data</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption 3: Large Outliers are Unlikely</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="lrwor.html"><a href="lrwor.html#tsdotoe"><i class="fa fa-check"></i><b>4.5</b> The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-1"><i class="fa fa-check"></i>Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-2"><i class="fa fa-check"></i>Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-3"><i class="fa fa-check"></i>Simulation Study 3</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="lrwor.html"><a href="lrwor.html#exercises-2"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="htaciitslrm.html"><a href="htaciitslrm.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li class="chapter" data-level="5.1" data-path="htaciitslrm.html"><a href="htaciitslrm.html#testing-two-sided-hypotheses-concerning-the-slope-coefficient"><i class="fa fa-check"></i><b>5.1</b> Testing Two-Sided Hypotheses Concerning the Slope Coefficient</a></li>
<li class="chapter" data-level="5.2" data-path="htaciitslrm.html"><a href="htaciitslrm.html#cifrc"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-confidence-intervals"><i class="fa fa-check"></i>Simulation Study: Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="htaciitslrm.html"><a href="htaciitslrm.html#rwxiabv"><i class="fa fa-check"></i><b>5.3</b> Regression when X is a Binary Variable</a></li>
<li class="chapter" data-level="5.4" data-path="htaciitslrm.html"><a href="htaciitslrm.html#hah"><i class="fa fa-check"></i><b>5.4</b> Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="htaciitslrm.html"><a href="htaciitslrm.html#the-gauss-markov-theorem"><i class="fa fa-check"></i><b>5.5</b> The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-blue-estimator"><i class="fa fa-check"></i>Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="htaciitslrm.html"><a href="htaciitslrm.html#using-the-t-statistic-in-regression-when-the-sample-size-is-small"><i class="fa fa-check"></i><b>5.6</b> Using the t-Statistic in Regression When the Sample Size Is Small</a></li>
<li class="chapter" data-level="5.7" data-path="htaciitslrm.html"><a href="htaciitslrm.html#exercises-3"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rmwmr.html"><a href="rmwmr.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="6.1" data-path="rmwmr.html"><a href="rmwmr.html#omitted-variable-bias"><i class="fa fa-check"></i><b>6.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="6.2" data-path="rmwmr.html"><a href="rmwmr.html#tmrm"><i class="fa fa-check"></i><b>6.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="rmwmr.html"><a href="rmwmr.html#mofimr"><i class="fa fa-check"></i><b>6.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="6.4" data-path="rmwmr.html"><a href="rmwmr.html#ols-assumptions-in-multiple-regression"><i class="fa fa-check"></i><b>6.4</b> OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#simulation-study-imperfect-multicollinearity"><i class="fa fa-check"></i>Simulation Study: Imperfect Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="rmwmr.html"><a href="rmwmr.html#the-distribution-of-the-ols-estimators-in-multiple-regression"><i class="fa fa-check"></i><b>6.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
<li class="chapter" data-level="6.6" data-path="rmwmr.html"><a href="rmwmr.html#exercises-4"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="htaciimr.html"><a href="htaciimr.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence Intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="htaciimr.html"><a href="htaciimr.html#hypothesis-tests-and-confidence-intervals-for-a-single-coefficient"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="htaciimr.html"><a href="htaciimr.html#an-application-to-test-scores-and-the-student-teacher-ratio"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="htaciimr.html"><a href="htaciimr.html#joint-hypothesis-testing-using-the-f-statistic"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the F-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="htaciimr.html"><a href="htaciimr.html#confidence-sets-for-multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-for-multiple-regression"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="htaciimr.html"><a href="htaciimr.html#analysis-of-the-test-score-data-set"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
<li class="chapter" data-level="7.7" data-path="htaciimr.html"><a href="htaciimr.html#exercises-5"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nrf.html"><a href="nrf.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="8.1" data-path="nrf.html"><a href="nrf.html#a-general-strategy-for-modelling-nonlinear-regression-functions"><i class="fa fa-check"></i><b>8.1</b> A General Strategy for Modelling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="8.2" data-path="nrf.html"><a href="nrf.html#nfoasiv"><i class="fa fa-check"></i><b>8.2</b> Nonlinear Functions of a Single Independent Variable</a><ul>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="nrf.html"><a href="nrf.html#interactions-between-independent-variables"><i class="fa fa-check"></i><b>8.3</b> Interactions Between Independent Variables</a></li>
<li class="chapter" data-level="8.4" data-path="nrf.html"><a href="nrf.html#nonlinear-effects-on-test-scores-of-the-student-teacher-ratio"><i class="fa fa-check"></i><b>8.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
<li class="chapter" data-level="8.5" data-path="nrf.html"><a href="nrf.html#exercises-6"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="asbomr.html"><a href="asbomr.html"><i class="fa fa-check"></i><b>9</b> Assessing Studies Based on Multiple Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity"><i class="fa fa-check"></i><b>9.1</b> Internal and External Validity</a></li>
<li class="chapter" data-level="9.2" data-path="asbomr.html"><a href="asbomr.html#ttivomra"><i class="fa fa-check"></i><b>9.2</b> Threats to Internal Validity of Multiple Regression Analysis</a></li>
<li class="chapter" data-level="9.3" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity-when-the-regression-is-used-for-forecasting"><i class="fa fa-check"></i><b>9.3</b> Internal and External Validity when the Regression is Used for Forecasting</a></li>
<li class="chapter" data-level="9.4" data-path="asbomr.html"><a href="asbomr.html#etsacs"><i class="fa fa-check"></i><b>9.4</b> Example: Test Scores and Class Size</a></li>
<li class="chapter" data-level="9.5" data-path="asbomr.html"><a href="asbomr.html#exercises-7"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="rwpd.html"><a href="rwpd.html"><i class="fa fa-check"></i><b>10</b> Regression with Panel Data</a><ul>
<li class="chapter" data-level="10.1" data-path="rwpd.html"><a href="rwpd.html#panel-data"><i class="fa fa-check"></i><b>10.1</b> Panel Data</a></li>
<li class="chapter" data-level="10.2" data-path="rwpd.html"><a href="rwpd.html#PDWTTP"><i class="fa fa-check"></i><b>10.2</b> Panel Data with Two Time Periods: “Before and After” Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="rwpd.html"><a href="rwpd.html#fixed-effects-regression"><i class="fa fa-check"></i><b>10.3</b> Fixed Effects Regression</a><ul>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#estimation-and-inference"><i class="fa fa-check"></i>Estimation and Inference</a></li>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#application-to-traffic-deaths"><i class="fa fa-check"></i>Application to Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="rwpd.html"><a href="rwpd.html#regression-with-time-fixed-effects"><i class="fa fa-check"></i><b>10.4</b> Regression with Time Fixed Effects</a></li>
<li class="chapter" data-level="10.5" data-path="rwpd.html"><a href="rwpd.html#tferaaseffer"><i class="fa fa-check"></i><b>10.5</b> The Fixed Effects Regression Assumptions and Standard Errors for Fixed Effects Regression</a></li>
<li class="chapter" data-level="10.6" data-path="rwpd.html"><a href="rwpd.html#drunk-driving-laws-and-traffic-deaths"><i class="fa fa-check"></i><b>10.6</b> Drunk Driving Laws and Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="rwabdv.html"><a href="rwabdv.html"><i class="fa fa-check"></i><b>11</b> Regression with a Binary Dependent Variable</a><ul>
<li class="chapter" data-level="11.1" data-path="rwabdv.html"><a href="rwabdv.html#binary-dependent-variables-and-the-linear-probability-model"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables and the Linear Probability Model</a></li>
<li class="chapter" data-level="11.2" data-path="rwabdv.html"><a href="rwabdv.html#palr"><i class="fa fa-check"></i><b>11.2</b> Probit and Logit Regression</a><ul>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#probit-regression"><i class="fa fa-check"></i>Probit Regression</a></li>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#logit-regression"><i class="fa fa-check"></i>Logit Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="rwabdv.html"><a href="rwabdv.html#estimation-and-inference-in-the-logit-and-probit-models"><i class="fa fa-check"></i><b>11.3</b> Estimation and Inference in the Logit and Probit Models</a></li>
<li class="chapter" data-level="11.4" data-path="rwabdv.html"><a href="rwabdv.html#application-to-the-boston-hmda-data"><i class="fa fa-check"></i><b>11.4</b> Application to the Boston HMDA Data</a></li>
<li class="chapter" data-level="11.5" data-path="rwabdv.html"><a href="rwabdv.html#exercises-8"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ivr.html"><a href="ivr.html"><i class="fa fa-check"></i><b>12</b> Instrumental Variables Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="ivr.html"><a href="ivr.html#TIVEWASRAASI"><i class="fa fa-check"></i><b>12.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a></li>
<li class="chapter" data-level="12.2" data-path="ivr.html"><a href="ivr.html#TGIVRM"><i class="fa fa-check"></i><b>12.2</b> The General IV Regression Model</a></li>
<li class="chapter" data-level="12.3" data-path="ivr.html"><a href="ivr.html#civ"><i class="fa fa-check"></i><b>12.3</b> Checking Instrument Validity</a></li>
<li class="chapter" data-level="12.4" data-path="ivr.html"><a href="ivr.html#attdfc"><i class="fa fa-check"></i><b>12.4</b> Application to the Demand for Cigarettes</a></li>
<li class="chapter" data-level="12.5" data-path="ivr.html"><a href="ivr.html#where-do-valid-instruments-come-from"><i class="fa fa-check"></i><b>12.5</b> Where Do Valid Instruments Come From?</a></li>
<li class="chapter" data-level="12.6" data-path="ivr.html"><a href="ivr.html#exercises-9"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="eaqe.html"><a href="eaqe.html"><i class="fa fa-check"></i><b>13</b> Experiments and Quasi-Experiments</a><ul>
<li class="chapter" data-level="13.1" data-path="eaqe.html"><a href="eaqe.html#potential-outcomes-causal-effects-and-idealized-experiments"><i class="fa fa-check"></i><b>13.1</b> Potential Outcomes, Causal Effects and Idealized Experiments</a></li>
<li class="chapter" data-level="13.2" data-path="eaqe.html"><a href="eaqe.html#threats-to-validity-of-experiments"><i class="fa fa-check"></i><b>13.2</b> Threats to Validity of Experiments</a></li>
<li class="chapter" data-level="13.3" data-path="eaqe.html"><a href="eaqe.html#experimental-estimates-of-the-effect-of-class-size-reductions"><i class="fa fa-check"></i><b>13.3</b> Experimental Estimates of the Effect of Class Size Reductions</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#experimental-design-and-the-data-set"><i class="fa fa-check"></i>Experimental Design and the Data Set</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#analysis-of-the-star-data"><i class="fa fa-check"></i>Analysis of the STAR Data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="eaqe.html"><a href="eaqe.html#quasi-experiments"><i class="fa fa-check"></i><b>13.4</b> Quasi Experiments</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#the-differences-in-differences-estimator"><i class="fa fa-check"></i>The Differences-in-Differences Estimator</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#regression-discontinuity-estimators"><i class="fa fa-check"></i>Regression Discontinuity Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ittsraf.html"><a href="ittsraf.html"><i class="fa fa-check"></i><b>14</b> Introduction to Time Series Regression and Forecasting</a><ul>
<li class="chapter" data-level="14.1" data-path="ittsraf.html"><a href="ittsraf.html#using-regression-models-for-forecasting"><i class="fa fa-check"></i><b>14.1</b> Using Regression Models for Forecasting</a></li>
<li class="chapter" data-level="14.2" data-path="ittsraf.html"><a href="ittsraf.html#tsdasc"><i class="fa fa-check"></i><b>14.2</b> Time Series Data and Serial Correlation</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#notation-lags-differences-logarithms-and-growth-rates"><i class="fa fa-check"></i>Notation, Lags, Differences, Logarithms and Growth Rates</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ittsraf.html"><a href="ittsraf.html#autoregressions"><i class="fa fa-check"></i><b>14.3</b> Autoregressions</a><ul>
<li><a href="ittsraf.html#autoregressive-models-of-order-p">Autoregressive Models of Order <span class="math inline">\(p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ittsraf.html"><a href="ittsraf.html#cybtmpi"><i class="fa fa-check"></i><b>14.4</b> Can You Beat the Market? (Part I)</a></li>
<li class="chapter" data-level="14.5" data-path="ittsraf.html"><a href="ittsraf.html#apatadlm"><i class="fa fa-check"></i><b>14.5</b> Additional Predictors and The ADL Model</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#forecast-uncertainty-and-forecast-intervals"><i class="fa fa-check"></i>Forecast Uncertainty and Forecast Intervals</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="ittsraf.html"><a href="ittsraf.html#llsuic"><i class="fa fa-check"></i><b>14.6</b> Lag Length Selection Using Information Criteria</a></li>
<li class="chapter" data-level="14.7" data-path="ittsraf.html"><a href="ittsraf.html#nit"><i class="fa fa-check"></i><b>14.7</b> Nonstationarity I: Trends</a></li>
<li class="chapter" data-level="14.8" data-path="ittsraf.html"><a href="ittsraf.html#niib"><i class="fa fa-check"></i><b>14.8</b> Nonstationarity II: Breaks</a></li>
<li class="chapter" data-level="14.9" data-path="ittsraf.html"><a href="ittsraf.html#can-you-beat-the-market-part-ii"><i class="fa fa-check"></i><b>14.9</b> Can You Beat the Market? (Part II)</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="eodce.html"><a href="eodce.html"><i class="fa fa-check"></i><b>15</b> Estimation of Dynamic Causal Effects</a><ul>
<li class="chapter" data-level="15.1" data-path="eodce.html"><a href="eodce.html#the-orange-juice-data"><i class="fa fa-check"></i><b>15.1</b> The Orange Juice Data</a></li>
<li class="chapter" data-level="15.2" data-path="eodce.html"><a href="eodce.html#dynamic-causal-effects"><i class="fa fa-check"></i><b>15.2</b> Dynamic Causal Effects</a></li>
<li class="chapter" data-level="15.3" data-path="eodce.html"><a href="eodce.html#dynamic-multipliers-and-cumulative-dynamic-multipliers"><i class="fa fa-check"></i><b>15.3</b> Dynamic Multipliers and Cumulative Dynamic Multipliers</a></li>
<li class="chapter" data-level="15.4" data-path="eodce.html"><a href="eodce.html#hac-standard-errors"><i class="fa fa-check"></i><b>15.4</b> HAC Standard Errors</a></li>
<li class="chapter" data-level="15.5" data-path="eodce.html"><a href="eodce.html#estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors"><i class="fa fa-check"></i><b>15.5</b> Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors</a></li>
<li class="chapter" data-level="15.6" data-path="eodce.html"><a href="eodce.html#orange-juice-prices-and-cold-weather"><i class="fa fa-check"></i><b>15.6</b> Orange Juice Prices and Cold Weather</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="atitsr.html"><a href="atitsr.html"><i class="fa fa-check"></i><b>16</b> Additional Topics in Time Series Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="atitsr.html"><a href="atitsr.html#vector-autoregressions"><i class="fa fa-check"></i><b>16.1</b> Vector Autoregressions</a></li>
<li class="chapter" data-level="16.2" data-path="atitsr.html"><a href="atitsr.html#ooiatdfglsurt"><i class="fa fa-check"></i><b>16.2</b> Orders of Integration and the DF-GLS Unit Root Test</a></li>
<li class="chapter" data-level="16.3" data-path="atitsr.html"><a href="atitsr.html#cointegration"><i class="fa fa-check"></i><b>16.3</b> Cointegration</a></li>
<li class="chapter" data-level="16.4" data-path="atitsr.html"><a href="atitsr.html#volatility-clustering-and-autoregressive-conditional-heteroskedasticity"><i class="fa fa-check"></i><b>16.4</b> Volatility Clustering and Autoregressive Conditional Heteroskedasticity</a><ul>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#arch-and-garch-models"><i class="fa fa-check"></i>ARCH and GARCH Models</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#application-to-stock-price-volatility"><i class="fa fa-check"></i>Application to Stock Price Volatility</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#summary-8"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Econometrics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div class = rmdreview>
This book is in <b>Open Review</b>. We want your feedback to make the book better for you and other students. You may annotate some text by <span style="background-color: #3297FD; color: white">selecting it with the cursor</span> and then click the <i class="h-icon-annotate"></i> on the pop-up menu. You can also see the annotations of others: click the <i class="h-icon-chevron-left"></i> in the upper right hand corner of the page <i class="fa fa-arrow-circle-right  fa-rotate-315" aria-hidden="true"></i>
</div>
<div id="eodce" class="section level1">
<h1><span class="header-section-number">15</span> Estimation of Dynamic Causal Effects</h1>
<p>It sometimes is of interest to know the size of current and future reaction of <span class="math inline">\(Y\)</span> to a change in <span class="math inline">\(X\)</span>. This is called the <em>dynamic causal effect</em> on <span class="math inline">\(Y\)</span> of a change in <span class="math inline">\(X\)</span>. This Chapter we discusses how to estimate dynamic causal effects in <tt>R</tt> applications, where we investigate the dynamic effect of cold weather in Florida on the price of orange juice concentrate.</p>
<p>The discussion covers:</p>
<ul>
<li>estimation of distributed lag models</li>
<li>heteroskedasticity- and autocorrelation-consistent (HAC) standard errors</li>
<li>generalized least squares (GLS) estimation of ADL models</li>
</ul>
<p>To reproduce code examples, install the <tt>R</tt> packages listed below beforehand and make sure that the subsequent code chunk executes without any errors.</p>
<ul>
<li><tt>AER</tt> <span class="citation">(Christian Kleiber &amp; Zeileis, <a href="#ref-R-AER">2017</a>)</span></li>
<li><tt>dynlm</tt> <span class="citation">(Zeileis, <a href="#ref-R-dynlm">2016</a>)</span></li>
<li><tt>nlme</tt> <span class="citation">(Pinheiro, Bates, &amp; R-core, <a href="#ref-R-nlme">2018</a>)</span></li>
<li><tt>orcutt</tt> <span class="citation">(Spada, <a href="#ref-R-orcutt">2017</a>)</span></li>
<li><tt>quantmod</tt> <span class="citation">(Ryan &amp; Ulrich, <a href="#ref-R-quantmod">2018</a>)</span></li>
<li><tt>stargazer</tt> <span class="citation">(Hlavac, <a href="#ref-R-stargazer">2018</a>)</span></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(AER)
<span class="kw">library</span>(quantmod)
<span class="kw">library</span>(dynlm)
<span class="kw">library</span>(orcutt)
<span class="kw">library</span>(nlme)
<span class="kw">library</span>(stargazer)</code></pre></div>
<div id="the-orange-juice-data" class="section level2">
<h2><span class="header-section-number">15.1</span> The Orange Juice Data</h2>
<p>The largest cultivation region for oranges in the U.S. is located in Florida which usually has ideal climate for the fruit growth. It thus is the source of almost all frozen juice concentrate produced in the country. However, from time to time and depending on their severeness, cold snaps cause a loss of harvests such that the supply of oranges decreases and consequently the price of frozen juice concentrate rises. The timing of the price increases is complicated: a cut in today’s supply of concentrate influences not just today’s price but also future prices because supply in future periods will decrease, too. Clearly, the magnitude of today’s and future prices increases due to freeze is an empirical question that can be investigated using a distributed lag model — a time series model that relates price changes to weather conditions.</p>
<p>To begin with the analysis, we reproduce Figure 15.1 of the book which displays plots of the price index for frozen concentrated orange juice, percentage changes in the price as well as monthly freezing degree days in Orlando, the center of Florida’s orange-growing region.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the frozen orange juice data set</span>
<span class="kw">data</span>(<span class="st">&quot;FrozenJuice&quot;</span>)

<span class="co"># compute the price index for frozen concentrated juice</span>
FOJCPI &lt;-<span class="st"> </span>FrozenJuice[, <span class="st">&quot;price&quot;</span>]<span class="op">/</span>FrozenJuice[, <span class="st">&quot;ppi&quot;</span>]
FOJC_pctc &lt;-<span class="st"> </span><span class="dv">100</span> <span class="op">*</span><span class="st"> </span><span class="kw">diff</span>(<span class="kw">log</span>(FOJCPI))
FDD &lt;-<span class="st"> </span>FrozenJuice[, <span class="st">&quot;fdd&quot;</span>]</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># convert series to xts objects</span>
FOJCPI_xts &lt;-<span class="st"> </span><span class="kw">as.xts</span>(FOJCPI)
FDD_xts &lt;-<span class="st"> </span><span class="kw">as.xts</span>(FrozenJuice[, <span class="dv">3</span>])

<span class="co"># Plot orange juice price index</span>
<span class="kw">plot</span>(<span class="kw">as.zoo</span>(FOJCPI),
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">lwd =</span> <span class="dv">2</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Date&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Price index&quot;</span>, 
     <span class="dt">main =</span> <span class="st">&quot;Frozen Concentrated Orange Juice&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-664-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># divide plotting area</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>))

<span class="co"># Plot percentage changes in prices</span>
<span class="kw">plot</span>(<span class="kw">as.zoo</span>(FOJC_pctc),
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">lwd =</span> <span class="dv">2</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Date&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Percent&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Monthly Changes in the Price of Frozen Conentrated Orange Juice&quot;</span>)

<span class="co"># plot freezing degree days</span>
<span class="kw">plot</span>(<span class="kw">as.zoo</span>(FDD),
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">lwd =</span> <span class="dv">2</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Date&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Freezing degree days&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Monthly Freezing Degree Days in Orlando, FL&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-665-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Periods with a high amount of freezing degree days are followed by large month-to-month price changes. These coinciding movements motivate a simple regression of price changes (<span class="math inline">\(\%ChgOJC_t\)</span>) on freezing degree days (<span class="math inline">\(FDD_t\)</span>) to estimate the effect of an additional freezing degree day one the price in the current month. For this, as for all other regressions in this chapter, we use <span class="math inline">\(T=611\)</span> observations (January 1950 to December 2000).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simple regression of percentage changes on freezing degree days</span>
orange_SR &lt;-<span class="st"> </span><span class="kw">dynlm</span>(FOJC_pctc <span class="op">~</span><span class="st"> </span>FDD)
<span class="kw">coeftest</span>(orange_SR, <span class="dt">vcov. =</span> vcovHAC)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) -0.42095    0.18683 -2.2531 0.0246064 *  
## FDD          0.46724    0.13385  3.4906 0.0005167 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Notice that the standard errors are computed using a “HAC” estimator of the variance-covariance matrix, see Chapter <a href="ittsraf.html#apatadlm">14.5</a> for a discussion of this estimator.</p>
<span class="math display">\[\begin{align*}
  \widehat{\%ChgOJC_t} = -\underset{(0.19)}{0.42} + \underset{(0.13)}{0.47} FDD_t
\end{align*}\]</span>
<p>The estimated coefficient on <span class="math inline">\(FDD_t\)</span> has the following interpretation: an additional freezing degree day in month <span class="math inline">\(t\)</span> leads to a price increase 0f <span class="math inline">\(0.47\)</span> percentage points in the same month.</p>
<p>To consider effects of cold snaps on the orange juice price over the subsequent periods, we include lagged values of <span class="math inline">\(FDD_t\)</span> in our model which leads to a <em>distributed lag regression model</em>. We estimate a specification using a contemporaneous and six lagged values of <span class="math inline">\(FDD_t\)</span> as regressors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># distributed lag model with 6 lags of freezing degree days</span>
orange_DLM &lt;-<span class="st"> </span><span class="kw">dynlm</span>(FOJC_pctc <span class="op">~</span><span class="st"> </span>FDD <span class="op">+</span><span class="st"> </span><span class="kw">L</span>(FDD, <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>))
<span class="kw">coeftest</span>(orange_DLM, <span class="dt">vcov. =</span> vcovHAC)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##               Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept)  -0.692961   0.212445 -3.2618 0.0011700 ** 
## FDD           0.471433   0.135195  3.4871 0.0005242 ***
## L(FDD, 1:6)1  0.145021   0.081557  1.7782 0.0758853 .  
## L(FDD, 1:6)2  0.058364   0.058911  0.9907 0.3222318    
## L(FDD, 1:6)3  0.074166   0.047143  1.5732 0.1162007    
## L(FDD, 1:6)4  0.036304   0.029335  1.2376 0.2163670    
## L(FDD, 1:6)5  0.048756   0.031370  1.5543 0.1206535    
## L(FDD, 1:6)6  0.050246   0.045129  1.1134 0.2659919    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>As the result we obtain</p>
<span class="math display" id="eq:orangemod1">\[\begin{align}
  \begin{split}
  \widehat{\%ChgOJC_t} =&amp; -\underset{(0.21)}{0.69} + \underset{(0.14)}{0.47} FDD_t + \underset{(0.08)}{0.15} FDD_{t-1} + \underset{(0.06)}{0.06} FDD_{t-2} + \underset{(0.05)}{0.07} FDD_{t-3} \\ &amp;+ \underset{(0.03)}{0.04} FDD_{t-4} + \underset{(0.03)}{0.05} FDD_{t-5} + \underset{(0.05)}{0.05} FDD_{t-6},
  \end{split}
  \tag{15.1}
\end{align}\]</span>
<p>where the coefficient on <span class="math inline">\(FDD_{t-1}\)</span> estimates the price increase in period <span class="math inline">\(t\)</span> caused by an additional freezing degree day in the preceding month, the coefficient on <span class="math inline">\(FDD_{t-2}\)</span> estimates the effect of an additional freezing degree day two month ago and so on. Consequently, the coefficients in <a href="eodce.html#eq:orangemod1">(15.1)</a> can be interpreted as price changes in current and future periods due to a unit increase in the current month’ freezing degree days.</p>
</div>
<div id="dynamic-causal-effects" class="section level2">
<h2><span class="header-section-number">15.2</span> Dynamic Causal Effects</h2>
<p>This section of the book describes the general idea of a dynamic causal effect and how the concept of a randomized controlled experiment can be translated to time series applications, using several examples.</p>
<p>In general, for empirical attempts to measure a dynamic causal effect, the assumptions of stationarity (see Key Concept 14.5) and exogeneity must hold. In time series applications up until here we have assumed that the model error term has conditional mean zero given current and past values of the regressors. For estimation of a dynamic causal effect using a distributed lag model, assuming a stronger form termed <em>strict exogeneity</em> may be useful. Strict exogeneity states that the error term has mean zero conditional on past, present <em>and future</em> values of the independent variables.</p>
<p>The two concepts of exogeneity and the distributed lag model are summarized in Key Concept 15.1.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 15.1
</h3>
<h3 class="left">
The Distributed Lag Model and Exogeneity
</h3>
<p>
The general distributed lag model is
<span class="math display" id="eq:dlm">\[\begin{align}
  Y_t = \beta_0 + \beta_1 X_t + \beta_2 X_{t-1} + \beta_3 X_{t-2} + \dots + \beta_{r+1} X_{t-r} + u_t, \tag{15.2}
\end{align}\]</span>
<p>where it is assumed that</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(X\)</span> is an exogenous variable, <span class="math display">\[E(u_t\vert X_t, X_{t-1}, X_{t-2},\dots) = 0.\]</span></p></li>
<li><ul>
<li>a <span class="math inline">\(X_t,Y_t\)</span> have a stationary distribution.</li>
<li>b <span class="math inline">\((Y_t,X_t)\)</span> and <span class="math inline">\((Y_{t-j},X_{t-j})\)</span> become independently distributed as <span class="math inline">\(j\)</span> gets large.</li>
</ul></li>
<li><p>Large outliers are unlikely. In particular, we need that all the variables have more than eight nonzero and finite moments — a stronger assumption than before (four finite nonzero moments) that is required for computation of the HAC covariance matrix estimator.</p></li>
<li><p>There is no perfect multicollinearity.</p></li>
</ol>
<p>The distributed lag model may be extended to include contemporaneous and past values of additional regressors.</p>
<p><strong>On the assumption of exogeneity</strong></p>
<ul>
<li><p>There is another form of exogeneity termed <em>strict exogeneity</em> which assumes <span class="math display">\[E(u_t\vert \dots, X_{t+2},X_{t+1},X_t,X_{t-1},X_{t-2},\dots)=0,\]</span> that is the error term has mean zero conditional on past, present and future values of <span class="math inline">\(X\)</span>. Strict exogeneity implies exogeneity (as defined in 1. above) but not the other way around. From this point on we will therefore distinguish between exogeneity and strict exogeneity.</p></li>
<li>Exogeneity as in 1. suffices for the OLS estimators of the coefficient in distributed lag models to be consistent. However, if the the assumption of strict exogeneity is valid, more efficient estimators can be applied.</li>
</ul>
</p>
</div>
</div>
<div id="dynamic-multipliers-and-cumulative-dynamic-multipliers" class="section level2">
<h2><span class="header-section-number">15.3</span> Dynamic Multipliers and Cumulative Dynamic Multipliers</h2>
<p>The following terminology regarding the coefficients in the distributed lag model <a href="eodce.html#eq:dlm">(15.2)</a> is useful for upcoming applications:</p>
<ul>
<li><p>The dynamic causal effect is also called the <em>dynamic multiplier</em>. <span class="math inline">\(\beta_{h+1}\)</span> in <a href="eodce.html#eq:dlm">(15.2)</a> is the <span class="math inline">\(h\)</span>-period dynamic multiplier.</p></li>
<li><p>The contemporaneous effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>, <span class="math inline">\(\beta_1\)</span>, is termed the <em>impact effect</em>.</p></li>
<li><p>The <span class="math inline">\(h\)</span>-period <em>cumulative dynamic multiplier</em> of a unit change in <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined as the cumulative sum of the dynamic multipliers. In particular, <span class="math inline">\(\beta_1\)</span> is the zero-period cumulative dynamic multiplier, <span class="math inline">\(\beta_1 + \beta_2\)</span> is the one-period cumulative dynamic multiplier and so forth.</p>
The cumulative dynamic multipliers of the distributed lag model <a href="eodce.html#eq:dlm">(15.2)</a> are the coefficients <span class="math inline">\(\delta_1,\delta_2,\dots,\delta_r,\delta_{r+1}\)</span> in the modified regression
<span class="math display" id="eq:DCMreg">\[\begin{align}
  Y_t =&amp; \, \delta_0 + \delta_1 \Delta X_t + \delta_2 \Delta X_{t-1} + \dots + \delta_r \Delta X_{t-r+1} + \delta_{r+1} X_{t-r} + u_t \tag{15.3}
\end{align}\]</span>
<p>and thus can be directly estimated using OLS which makes it convenient to compute their HAC standard errors. <span class="math inline">\(\delta_{r+1}\)</span> is called the <em>long-run cumulative dynamic multiplier</em>.</p></li>
</ul>
<p>It is straightforward to compute the cumulative dynamic multipliers for <a href="eodce.html#eq:orangemod1">(15.1)</a>, the estimated distributed lag regression of changes in orange juice concentrate prices on freezing degree days, using the corresponding model object <tt>orange_DLM</tt> and the function <tt>cumsum()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute cumulative multipliers</span>
cum_mult &lt;-<span class="kw">cumsum</span>(orange_DLM<span class="op">$</span>coefficients[<span class="op">-</span><span class="dv">1</span>])

<span class="co"># rename entries</span>
<span class="kw">names</span>(cum_mult) &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">6</span>, <span class="dt">sep =</span> <span class="st">&quot;-&quot;</span>, <span class="st">&quot;period CDM&quot;</span>)

cum_mult</code></pre></div>
<pre><code>## 0-period CDM 1-period CDM 2-period CDM 3-period CDM 4-period CDM 
##    0.4714329    0.6164542    0.6748177    0.7489835    0.7852874 
## 5-period CDM 6-period CDM 
##    0.8340436    0.8842895</code></pre>
<p>Translating the distributed lag model with six lags of <span class="math inline">\(FDD\)</span> to <a href="eodce.html#eq:DCMreg">(15.3)</a>, we see that the OLS coefficient estimates in this model coincide with the multipliers stored in <tt>cum_mult</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate cumulative dynamic multipliers using the modified regression</span>
cum_mult_reg &lt;-<span class="kw">dynlm</span>(FOJC_pctc <span class="op">~</span><span class="st"> </span><span class="kw">d</span>(FDD) <span class="op">+</span><span class="st"> </span><span class="kw">d</span>(<span class="kw">L</span>(FDD,<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">L</span>(FDD,<span class="dv">6</span>))
<span class="kw">coef</span>(cum_mult_reg)[<span class="op">-</span><span class="dv">1</span>]</code></pre></div>
<pre><code>##          d(FDD) d(L(FDD, 1:5))1 d(L(FDD, 1:5))2 d(L(FDD, 1:5))3 
##       0.4714329       0.6164542       0.6748177       0.7489835 
## d(L(FDD, 1:5))4 d(L(FDD, 1:5))5       L(FDD, 6) 
##       0.7852874       0.8340436       0.8842895</code></pre>
<p>As noted above, using a model specification as in <a href="eodce.html#eq:DCMreg">(15.3)</a> allows to easily obtain standard errors for the estimated dynamic cumulative multipliers.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># obtain coefficient summary that reports HAC standard errors</span>
<span class="kw">coeftest</span>(cum_mult_reg, <span class="dt">vcov. =</span> vcovHAC)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##                 Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept)     -0.69296    0.23668 -2.9278 0.0035431 ** 
## d(FDD)           0.47143    0.13583  3.4709 0.0005562 ***
## d(L(FDD, 1:5))1  0.61645    0.13145  4.6896 3.395e-06 ***
## d(L(FDD, 1:5))2  0.67482    0.16009  4.2151 2.882e-05 ***
## d(L(FDD, 1:5))3  0.74898    0.17263  4.3387 1.682e-05 ***
## d(L(FDD, 1:5))4  0.78529    0.17351  4.5260 7.255e-06 ***
## d(L(FDD, 1:5))5  0.83404    0.18236  4.5737 5.827e-06 ***
## L(FDD, 6)        0.88429    0.19303  4.5810 5.634e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="hac-standard-errors" class="section level2">
<h2><span class="header-section-number">15.4</span> HAC Standard Errors</h2>
<p>The error term <span class="math inline">\(u_t\)</span> in the distributed lag model <a href="eodce.html#eq:dlm">(15.2)</a> may be serially correlated due to serially correlated determinants of <span class="math inline">\(Y_t\)</span> that are not included as regressors. When these factors are not correlated with the regressors included in the model, serially correlated errors do not violate the assumption of exogeneity such that the OLS estimator remains unbiased and consistent.</p>
<p>However, autocorrelated standard errors render the usual homoskedasticity-only <em>and</em> heteroskedasticity-robust standard errors invalid and may cause misleading inference. HAC errors are a remedy.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 15.2
</h3>
<h3 class="left">
HAC Standard errors
</h3>
<p>
<p><strong>Problem</strong>:</p>
<p>If the error term <span class="math inline">\(u_t\)</span> in the distributed lag model <a href="eodce.html#eq:dlm">(15.2)</a> is serially correlated, statistical inference that rests on usual (heteroskedasticity-robust) standard errors can be strongly misleading.</p>
<p><strong>Solution</strong>:</p>
<p>Heteroskedasticity- and autocorrelation-consistent (HAC) estimators of the variance-covariance matrix circumvent this issue. There are <tt>R</tt> functions like <tt>vcovHAC()</tt> from the package <tt>sandwich</tt> which are convenient for computation of such estimators.</p>
The package <tt>sandwich</tt> also contains the function <tt>NeweyWest()</tt>, an implementation of the HAC variance-covariance estimator proposed by <span class="citation">Newey &amp; West (<a href="#ref-newey1987">1987</a>)</span>.
</p>
</div>
Consider the distributed lag regression model with no lags and a single regressor <span class="math inline">\(X_t\)</span>
<span class="math display">\[\begin{align*}
  Y_t = \beta_0 + \beta_1 X_t + u_t.
\end{align*}\]</span>
with autocorrelated errors. A brief derivation of
<span class="math display" id="eq:nwhac">\[\begin{align}
  \overset{\sim}{\sigma}^2_{\widehat{\beta}_1} = \widehat{\sigma}^2_{\widehat{\beta}_1} \widehat{f}_t \tag{15.4}
\end{align}\]</span>
the so-called <em>Newey-West variance estimator</em> for the variance of the OLS estimator of <span class="math inline">\(\beta_1\)</span> is presented in Chapter 15.4 of the book. <span class="math inline">\(\widehat{\sigma}^2_{\widehat{\beta}_1}\)</span> in <a href="eodce.html#eq:nwhac">(15.4)</a> is the heteroskedasticity-robust variance estimate of <span class="math inline">\(\widehat{\beta}_1\)</span> and
<span class="math display" id="eq:nwhacf">\[\begin{align}
  \widehat{f}_t = 1 + 2 \sum_{j=1}^{m-1} \left(\frac{m-j}{m}\right) \overset{\sim}{\rho}_j \tag{15.5}
\end{align}\]</span>
<p>is a correction factor that adjusts for serially correlated errors and involves estimates of <span class="math inline">\(m-1\)</span> autocorrelation coefficients <span class="math inline">\(\overset{\sim}{\rho}_j\)</span>. As it turns out, using the sample autocorrelation as implemented in <tt>acf()</tt> to estimate the autocorrelation coefficients renders <a href="eodce.html#eq:nwhac">(15.4)</a> inconsistent, see pp. 650-651 of the book for a detailed argument. Therefore, we use a somewhat different estimator. For a time series <span class="math inline">\(X\)</span> we have <span class="math display">\[ \ \overset{\sim}{\rho}_j = \frac{\sum_{t=j+1}^T \hat v_t \hat v_{t-j}}{\sum_{t=1}^T \hat v_t^2}, \ \text{with} \ \hat v= (X_t-\overline{X}) \hat u_t. \]</span> We implement this estimator in the function <tt>acf_c()</tt> below.</p>
<span class="math inline">\(m\)</span> in <a href="eodce.html#eq:nwhacf">(15.5)</a> is a truncation parameter to be chosen. A rule of thumb for choosing <span class="math inline">\(m\)</span> is
<span class="math display" id="eq:hactruncrot">\[\begin{align}
  m = \left \lceil{0.75 \cdot T^{1/3}}\right\rceil. \tag{15.6}
\end{align}\]</span>
<p>We simulate a time series that, as stated above, follows a distributed lag model with autocorrelated errors and then show how to compute the Newey-West HAC estimate of <span class="math inline">\(SE(\widehat{\beta}_1)\)</span> using <tt>R</tt>. This is done via two separate but, as we will see, identical approaches: at first we follow the derivation presented in the book step-by-step and compute the estimate “manually”. We then show that the result is exactly the estimate obtained when using the function <tt>NeweyWest()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># function that computes rho tilde</span>
acf_c &lt;-<span class="st"> </span><span class="cf">function</span>(x, j) {
  <span class="kw">return</span>(
    <span class="kw">t</span>(x[<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>j)]) <span class="op">%*%</span><span class="st"> </span><span class="kw">na.omit</span>(<span class="kw">Lag</span>(x, j)) <span class="op">/</span><span class="st"> </span><span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>x
  )
}

<span class="co"># simulate time series with serially correlated errors</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

N &lt;-<span class="st"> </span><span class="dv">100</span>

eps &lt;-<span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> N, <span class="dt">model =</span> <span class="kw">list</span>(<span class="dt">ma =</span> <span class="fl">0.5</span>))
X &lt;-<span class="st"> </span><span class="kw">runif</span>(N, <span class="dv">1</span>, <span class="dv">10</span>)
Y &lt;-<span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span>eps

<span class="co"># compute OLS residuals</span>
res &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X)<span class="op">$</span>res

<span class="co"># compute v</span>
v &lt;-<span class="st"> </span>(X <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(X)) <span class="op">*</span><span class="st"> </span>res

<span class="co"># compute robust estimate of beta_1 variance</span>
var_beta_hat &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>N <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>(N<span class="op">-</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>((X <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(X))<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>res<span class="op">^</span><span class="dv">2</span>) ) <span class="op">/</span><span class="st"> </span>
<span class="st">                        </span>(<span class="dv">1</span><span class="op">/</span>N <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>((X <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(X))<span class="op">^</span><span class="dv">2</span>))<span class="op">^</span><span class="dv">2</span>

<span class="co"># rule of thumb truncation parameter</span>
m &lt;-<span class="st"> </span><span class="kw">floor</span>(<span class="fl">0.75</span> <span class="op">*</span><span class="st"> </span>N<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>))

<span class="co"># compute correction factor</span>
f_hat_T &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(
  (m <span class="op">-</span><span class="st"> </span><span class="dv">1</span><span class="op">:</span>(m<span class="op">-</span><span class="dv">1</span>))<span class="op">/</span>m <span class="op">*</span><span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>(m <span class="op">-</span><span class="st"> </span><span class="dv">1</span>), <span class="cf">function</span>(i) <span class="kw">acf_c</span>(<span class="dt">x =</span> v, <span class="dt">j =</span> i))
  ) 

<span class="co"># compute Newey-West HAC estimate of the standard error </span>
<span class="kw">sqrt</span>(var_beta_hat <span class="op">*</span><span class="st"> </span>f_hat_T)</code></pre></div>
<pre><code>## [1] 0.04036208</code></pre>
<p>For the code to be reusable in other applications, we use <tt>sapply()</tt> to estimate the <span class="math inline">\(m-1\)</span> autocorrelations <span class="math inline">\(\overset{\sim}{\rho}_j\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Using NeweyWest():</span>
NW_VCOV &lt;-<span class="st"> </span><span class="kw">NeweyWest</span>(<span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X), 
              <span class="dt">lag =</span> m <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">prewhite =</span> F, 
              <span class="dt">adjust =</span> T)

<span class="co"># compute standard error</span>
<span class="kw">sqrt</span>(<span class="kw">diag</span>(NW_VCOV))[<span class="dv">2</span>]</code></pre></div>
<pre><code>##          X 
## 0.04036208</code></pre>
<p>By choosing <tt>lag = m-1</tt> we ensure that the maximum order of autocorrelations used is <span class="math inline">\(m-1\)</span> — just as in equation <a href="eodce.html#eq:nwhacf">(15.5)</a>. Notice that we set the arguments <tt>prewhite = F</tt> and <tt>adjust = T</tt> to ensure that the formula <a href="eodce.html#eq:nwhac">(15.4)</a> is used and finite sample adjustments are made.</p>
<p>We find that the computed standard errors coincide. Of course, a variance-covariance matrix estimate as computed by <tt>NeweyWest()</tt> can be supplied as the argument <tt>vcov</tt> in <tt>coeftest()</tt> such that HAC <span class="math inline">\(t\)</span>-statistics and <span class="math inline">\(p\)</span>-values are provided by the latter.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">example_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X)
<span class="kw">coeftest</span>(example_mod, <span class="dt">vcov =</span> NW_VCOV)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.542310   0.235423  2.3036  0.02336 *  
## X           0.423305   0.040362 10.4877  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors" class="section level2">
<h2><span class="header-section-number">15.5</span> Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors</h2>
<p>In general, the errors in a distributed lag model are correlated which necessitates usage of HAC standard errors for valid inference. If, however, the assumption of exogeneity (the first assumption stated in Key Concept 15.1) is replaced by strict exogeneity, that is <span class="math display">\[E(u_t\vert \dots, X_{t+1}, X_{t}, X_{t-1}, \dots) = 0,\]</span> more efficient approaches than OLS estimation of the coefficients are available. For a general distributed lag model with <span class="math inline">\(r\)</span> lags and AR(<span class="math inline">\(p\)</span>) errors, these approaches are summarized in Key Concept 15.4.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 15.4
</h3>
<h3 class="left">
Estimation of Dynamic Multipliers Under Strict Exogeneity
</h3>
<p>
Consider the general distributed lag model with <span class="math inline">\(r\)</span> lags and errors following an AR(<span class="math inline">\(p\)</span>) process,
<span class="math display" id="eq:dlmarerrors" id="eq:dlmar">\[\begin{align}
  Y_t =&amp; \, \beta_0 + \beta_1 X_t + \beta_2 X_{t-1} + \dots + \beta_{r+1} X_{t-r} + u_t \tag{15.7} \\
  u_t =&amp; \, \phi_1 u_{t-1} + \phi u_{t-2} + \dots + \phi_p u_{t-p} + \overset{\sim}{u}_t. \tag{15.8}
\end{align}\]</span>
Under strict exogeneity of <span class="math inline">\(X_t\)</span>, one may rewrite the above model in the ADL specification
<span class="math display">\[\begin{align*}
  Y_t =&amp; \, \alpha_0 + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} \\
      &amp;+ \, \delta_0 X_t + \delta_1 X_{t-1} + \dots + \delta_q X_{t-q} + \overset{\sim}{u}_t
\end{align*}\]</span>
<p>where <span class="math inline">\(q=r+p\)</span> and compute estimates of the dynamic multipliers <span class="math inline">\(\beta_1, \beta_2, \dots, \beta_{r+1}\)</span> using OLS estimates of <span class="math inline">\(\phi_1, \phi_2, \dots, \phi_p, \delta_0, \delta_1, \dots, \delta_q\)</span>.</p>
<p>An alternative is to estimate the dynamic multipliers using feasible GLS, that is to apply the OLS estimator to a quasi-difference specification of <a href="eodce.html#eq:dlmar">(15.7)</a>. Under strict exogeneity, the feasible GLS approach is the BLUE estimator for the dynamic multipliers in large samples.</p>
On the one hand, as demonstrated in Chapter 15.5 of the book, OLS estimation of the ADL representation can be beneficial for estimation of the dynamic multipliers in large distributed lag models because it allows for a more parsimonious model that may be a good approximation to the large model. On the other hand, the GLS approach is more efficient than the ADL estimator if the sample size is large.
</p>
</div>
<p>We shortly review how the different representations of a small distributed lag model can be obtained and show how this specification can be estimated by OLS and GLS using <tt>R</tt>.</p>
The model is
<span class="math display" id="eq:dldynamic">\[\begin{align}
  Y_t = \beta_0 + \beta_1 X_t + \beta_2 X_{t-1} + u_t, \tag{15.9}
\end{align}\]</span>
<p>so a change in <span class="math inline">\(X\)</span> is modeled to effect <span class="math inline">\(Y\)</span> contemporaneously (<span class="math inline">\(\beta_1\)</span>) and in the next period (<span class="math inline">\(\beta_2\)</span>). The error term <span class="math inline">\(u_t\)</span> is assumed to follow an AR(<span class="math inline">\(1\)</span>) process,<span class="math display">\[u_t = \phi_1 u_{t-1} + \overset{\sim}{u_t},\]</span> where <span class="math inline">\(\overset{\sim}{u_t}\)</span> is serially uncorrelated.</p>
One can show that the ADL representation of this model is
<span class="math display" id="eq:adl21dynamic">\[\begin{align}
  Y_t = \alpha_0 + \phi_1 Y_{t-1} + \delta_0 X_t + \delta_1 X_{t-1} + \delta_2 X_{t-2} + \overset{\sim}{u}_t, \tag{15.10}
\end{align}\]</span>
with the restrictions
<span class="math display">\[\begin{align*}
  \beta_1 =&amp; \, \delta_0, \\
  \beta_2 =&amp; \, \delta_1 + \phi_1 \delta_0,
\end{align*}\]</span>
<p>see p. 657 of the book.</p>
<div id="quasi-differences" class="section level4 unnumbered">
<h4>Quasi-Differences</h4>
Another way of writing the ADL(<span class="math inline">\(1\)</span>,<span class="math inline">\(2\)</span>) representation <a href="eodce.html#eq:adl21dynamic">(15.10)</a> is the <em>quasi-difference model</em>
<span class="math display" id="eq:qdm">\[\begin{align}
  \overset{\sim}{Y}_t = \alpha_0 + \beta_1 \overset{\sim}{X}_t + \beta_2   \overset{\sim}{X}_{t-1} + \overset{\sim}{u}_t, \tag{15.11}
\end{align}\]</span>
<p>where <span class="math inline">\(\overset{\sim}{Y}_t = Y_t - \phi_1 Y_{t-1}\)</span> and <span class="math inline">\(\overset{\sim}{X}_t = X_t - \phi_1 X_{t-1}\)</span>. Notice that the error term <span class="math inline">\(\overset{\sim}{u}_t\)</span> is uncorrelated in both models and, as shown in Chapter 15.5 of the book, <span class="math display">\[E(u_t\vert X_{t+1}, X_t, X_{t-1}, \dots) = 0,\]</span> which is implied by the assumption of strict exogeneity.</p>
<p>We continue by simulating a time series of <span class="math inline">\(500\)</span> observations using the model <a href="eodce.html#eq:dldynamic">(15.9)</a> with <span class="math inline">\(\beta_1 = 0.1\)</span>, <span class="math inline">\(\beta_2 = 0.25\)</span>, <span class="math inline">\(\phi = 0.5\)</span> and <span class="math inline">\(\overset{\sim}{u}_t \sim \mathcal{N}(0,1)\)</span> and estimate the different representations, starting with the distributed lag model <a href="eodce.html#eq:dldynamic">(15.9)</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># simulate a time series with serially correlated errors</span>
obs &lt;-<span class="st"> </span><span class="dv">501</span>
eps &lt;-<span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> obs<span class="op">-</span><span class="dv">1</span> , <span class="dt">model =</span> <span class="kw">list</span>(<span class="dt">ar =</span> <span class="fl">0.5</span>))
X &lt;-<span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> obs, <span class="dt">model =</span> <span class="kw">list</span>(<span class="dt">ar =</span> <span class="fl">0.25</span>))
Y &lt;-<span class="st"> </span><span class="fl">0.1</span> <span class="op">*</span><span class="st"> </span>X[<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="fl">0.25</span> <span class="op">*</span><span class="st"> </span>X[<span class="op">-</span>obs] <span class="op">+</span><span class="st"> </span>eps
X &lt;-<span class="st"> </span><span class="kw">ts</span>(X[<span class="op">-</span><span class="dv">1</span>])

<span class="co"># estimate the distributed lag model</span>
dlm &lt;-<span class="st"> </span><span class="kw">dynlm</span>(Y <span class="op">~</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span><span class="kw">L</span>(X))</code></pre></div>
<p>Let us check that the residuals of this model exhibit autocorrelation using <tt>acf()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># check that the residuals are serially correlated</span>
<span class="kw">acf</span>(<span class="kw">residuals</span>(dlm))</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-681-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>In particular, the pattern reveals that the residuals follow an autoregressive process, as the sample autocorrelation function decays quickly for the first few lags and is probably zero for higher lag orders. In any case, HAC standard errors should be used.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># coefficient summary using the Newey-West SE estimates</span>
<span class="kw">coeftest</span>(dlm, <span class="dt">vcov =</span> NeweyWest, <span class="dt">prewhite =</span> F, <span class="dt">adjust =</span> T)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 0.038340   0.073411  0.5223  0.601717    
## X           0.123661   0.046710  2.6474  0.008368 ** 
## L(X)        0.247406   0.046377  5.3347 1.458e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="ols-estimation-of-the-adl-model" class="section level4 unnumbered">
<h4>OLS Estimation of the ADL Model</h4>
<p>Next, we estimate the ADL(<span class="math inline">\(1\)</span>,<span class="math inline">\(2\)</span>) model <a href="eodce.html#eq:adl21dynamic">(15.10)</a> using OLS. The errors are uncorrelated in this representation of the model. This statement is supported by a plot of the sample autocorrelation function of the residual series.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the ADL(2,1) representation of the distributed lag model</span>
adl21_dynamic &lt;-<span class="st"> </span><span class="kw">dynlm</span>(Y <span class="op">~</span><span class="st"> </span><span class="kw">L</span>(Y) <span class="op">+</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span><span class="kw">L</span>(X, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>))

<span class="co"># plot the sample autocorrelaltions of residuals</span>
<span class="kw">acf</span>(adl21_dynamic<span class="op">$</span>residuals)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-683-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The estimated coefficients of <code>adl21_dynamic$coefficients</code> are <em>not</em> the dynamic multipliers we are interested in, but instead can be computed according to the restrictions in <a href="eodce.html#eq:adl21dynamic">(15.10)</a>, where the true coefficients are replaced by the OLS estimates.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute estimated dynamic effects using coefficient restrictions</span>
<span class="co"># in the ADL(2,1) representation</span>
t &lt;-<span class="st"> </span>adl21_dynamic<span class="op">$</span>coefficients

<span class="kw">c</span>(<span class="st">&quot;hat_beta_1&quot;</span> =<span class="st"> </span>t[<span class="dv">3</span>],
  <span class="st">&quot;hat_beta_2&quot;</span> =<span class="st"> </span>t[<span class="dv">4</span>] <span class="op">+</span><span class="st"> </span>t[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>t[<span class="dv">2</span>])</code></pre></div>
<pre><code>##          hat_beta_1.X hat_beta_2.L(X, 1:2)1 
##             0.1176425             0.2478484</code></pre>
</div>
<div id="gls-estimation" class="section level4 unnumbered">
<h4>GLS Estimation</h4>
<p>Strict exogeneity allows for OLS estimation of the quasi-difference model <a href="eodce.html#eq:qdm">(15.11)</a>. The idea of applying the OLS estimator to a model where the variables are linearly transformed, such that the model errors are uncorrelated and homoskedastic, is called <em>generalized least squares</em> (GLS).</p>
<p>The OLS estimator in <a href="eodce.html#eq:qdm">(15.11)</a> is called the <em>infeasible GLS</em> estimator because <span class="math inline">\(\overset{\sim}{Y}\)</span> and <span class="math inline">\(\overset{\sim}{X}\)</span> cannot be computed without knowing <span class="math inline">\(\phi_1\)</span>, the autoregressive coefficient in the error AR(<span class="math inline">\(1\)</span>) model, which is generally unknown in practice.</p>
<p>Assume we knew that <span class="math inline">\(\phi = 0.5\)</span>. We then may obtain the infeasible GLS estimates of the dynamic multipliers in <a href="eodce.html#eq:dldynamic">(15.9)</a> by applying OLS to the transformed data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># GLS: estimate quasi-differenced specification by OLS</span>
iGLS_dynamic &lt;-<span class="st"> </span><span class="kw">dynlm</span>(<span class="kw">I</span>(Y<span class="op">-</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">L</span>(Y)) <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(X <span class="op">-</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">L</span>(X)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">L</span>(X) <span class="op">-</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">L</span>(X, <span class="dv">2</span>)))

<span class="kw">summary</span>(iGLS_dynamic)</code></pre></div>
<pre><code>## 
## Time series regression with &quot;ts&quot; data:
## Start = 3, End = 500
## 
## Call:
## dynlm(formula = I(Y - 0.5 * L(Y)) ~ I(X - 0.5 * L(X)) + I(L(X) - 
##     0.5 * L(X, 2)))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0325 -0.6375 -0.0499  0.6658  3.7724 
## 
## Coefficients:
##                         Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)              0.01620    0.04564   0.355  0.72273    
## I(X - 0.5 * L(X))        0.12000    0.04237   2.832  0.00481 ** 
## I(L(X) - 0.5 * L(X, 2))  0.25266    0.04237   5.963 4.72e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.017 on 495 degrees of freedom
## Multiple R-squared:  0.07035,    Adjusted R-squared:  0.0666 
## F-statistic: 18.73 on 2 and 495 DF,  p-value: 1.442e-08</code></pre>
<p>The <em>feasible GLS</em> estimator uses preliminary estimation of the coefficients in the presumed error term model, computes the quasi-differenced data and then estimates the model using OLS. This idea was introduced by <span class="citation">Cochrane &amp; Orcutt (<a href="#ref-cochrane1949">1949</a>)</span> and can be extended by continuing this process iteratively. Such a procedure is implemented in the function <tt>cochrane.orcutt()</tt> from the package <tt>orcutt</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X_t &lt;-<span class="st"> </span><span class="kw">c</span>(X[<span class="op">-</span><span class="dv">1</span>])
<span class="co"># create first lag</span>
X_l1 &lt;-<span class="st"> </span><span class="kw">c</span>(X[<span class="op">-</span><span class="dv">500</span>])
Y_t &lt;-<span class="st"> </span><span class="kw">c</span>(Y[<span class="op">-</span><span class="dv">1</span>])

<span class="co"># iterated cochrane-orcutt procedure</span>
<span class="kw">summary</span>(<span class="kw">cochrane.orcutt</span>(<span class="kw">lm</span>(Y_t <span class="op">~</span><span class="st"> </span>X_t <span class="op">+</span><span class="st"> </span>X_l1)))</code></pre></div>
<pre><code>## Call:
## lm(formula = Y_t ~ X_t + X_l1)
## 
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 0.032885   0.085163   0.386   0.69956    
## X_t         0.120128   0.042534   2.824   0.00493 ** 
## X_l1        0.252406   0.042538   5.934 5.572e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.0165 on 495 degrees of freedom
## Multiple R-squared:  0.0704 ,  Adjusted R-squared:  0.0666
## F-statistic: 18.7 on 2 and 495 DF,  p-value: &lt; 1.429e-08
## 
## Durbin-Watson statistic 
## (original):    1.06907 , p-value: 1.05e-25
## (transformed): 1.98192 , p-value: 4.246e-01</code></pre>
<p>Some more sophisticated methods for GLS estimation are provided with the package <tt>nlme</tt>. The function <tt>gls()</tt> can be used to fit linear models by maximum likelihood estimation algorithms and allows to specify a correlation structure for the error term.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># feasible GLS maximum likelihood estimation procedure</span>
<span class="kw">summary</span>(<span class="kw">gls</span>(Y_t <span class="op">~</span><span class="st"> </span>X_t <span class="op">+</span><span class="st"> </span>X_l1, <span class="dt">correlation =</span> <span class="kw">corAR1</span>()))</code></pre></div>
<pre><code>## Generalized least squares fit by REML
##   Model: Y_t ~ X_t + X_l1 
##   Data: NULL 
##        AIC     BIC    logLik
##   1451.847 1472.88 -720.9235
## 
## Correlation Structure: AR(1)
##  Formula: ~1 
##  Parameter estimate(s):
##       Phi 
## 0.4668343 
## 
## Coefficients:
##                  Value  Std.Error  t-value p-value
## (Intercept) 0.03929124 0.08530544 0.460595  0.6453
## X_t         0.11986994 0.04252270 2.818963  0.0050
## X_l1        0.25287471 0.04252497 5.946500  0.0000
## 
##  Correlation: 
##      (Intr) X_t  
## X_t  0.039       
## X_l1 0.037  0.230
## 
## Standardized residuals:
##         Min          Q1         Med          Q3         Max 
## -3.00075518 -0.64255522 -0.05400347  0.69101814  3.28555793 
## 
## Residual standard error: 1.14952 
## Degrees of freedom: 499 total; 496 residual</code></pre>
<p>Notice that in this example, the coefficient estimates produced by GLS are somewhat closer to their true values and that the standard errors are the smallest for the GLS estimator.</p>
</div>
</div>
<div id="orange-juice-prices-and-cold-weather" class="section level2">
<h2><span class="header-section-number">15.6</span> Orange Juice Prices and Cold Weather</h2>
<p>This section investigates the following two questions using the time series regression methods discussed here:</p>
<ul>
<li><p>How persistent is the effect of a single freeze on orange juice concentrate prices?</p></li>
<li><p>Has the effect been stable over the whole time span?</p></li>
</ul>
<p>We start by estimating dynamic causal effects with a distributed lag model where <span class="math inline">\(\%ChgOJC_t\)</span> is regressed on <span class="math inline">\(FDD_t\)</span> and 18 lags. A second model specification considers a transformation of the the distributed lag model which allows to estimate the 19 cumulative dynamic multipliers using OLS. The third model, adds 11 binary variables (one for each of the months from February to December) to adjust for a possible omitted variable bias arising from correlation of <span class="math inline">\(FDD_t\)</span> and seasons by adding <tt>season(FDD)</tt> to the right hand side of the formula of the second model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate distributed lag models of frozen orange juice price changes</span>
FOJC_mod_DM &lt;-<span class="st"> </span><span class="kw">dynlm</span>(FOJC_pctc <span class="op">~</span><span class="st"> </span><span class="kw">L</span>(FDD, <span class="dv">0</span><span class="op">:</span><span class="dv">18</span>))
FOJC_mod_CM1 &lt;-<span class="st"> </span><span class="kw">dynlm</span>(FOJC_pctc <span class="op">~</span><span class="st"> </span><span class="kw">L</span>(<span class="kw">d</span>(FDD), <span class="dv">0</span><span class="op">:</span><span class="dv">17</span>) <span class="op">+</span><span class="st"> </span><span class="kw">L</span>(FDD, <span class="dv">18</span>))
FOJC_mod_CM2 &lt;-<span class="st"> </span><span class="kw">dynlm</span>(FOJC_pctc <span class="op">~</span><span class="st"> </span><span class="kw">L</span>(<span class="kw">d</span>(FDD), <span class="dv">0</span><span class="op">:</span><span class="dv">17</span>) <span class="op">+</span><span class="st"> </span><span class="kw">L</span>(FDD, <span class="dv">18</span>) <span class="op">+</span><span class="st"> </span><span class="kw">season</span>(FDD))</code></pre></div>
<p>The above models include a large number of lags with default labels that correspond to the degree of differencing and the lag orders which makes it somewhat cumbersome to read the output. The regressor labels of a model object may be altered by overriding the attribute <tt>names</tt> of the coefficient section using the function <tt>attr()</tt>. Thus, for better readability we use the lag orders as regressor labels.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set lag orders as regressor labels </span>
<span class="kw">attr</span>(FOJC_mod_DM<span class="op">$</span>coefficients, <span class="st">&quot;names&quot;</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">20</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;(Intercept)&quot;</span>, <span class="kw">as.character</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">18</span>))
<span class="kw">attr</span>(FOJC_mod_CM1<span class="op">$</span>coefficients, <span class="st">&quot;names&quot;</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">20</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;(Intercept)&quot;</span>, <span class="kw">as.character</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">18</span>))
<span class="kw">attr</span>(FOJC_mod_CM2<span class="op">$</span>coefficients, <span class="st">&quot;names&quot;</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">20</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;(Intercept)&quot;</span>, <span class="kw">as.character</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">18</span>))</code></pre></div>
<p>Next, we compute HAC standard errors standard errors for each model using <tt>NeweyWest()</tt> and gather the results in a list which is then supplied as the argument <tt>se</tt> to the function <tt>stargazer()</tt>, see below. The sample consists of 612 observations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(FDD)</code></pre></div>
<pre><code>## [1] 612</code></pre>
<p>According to <a href="eodce.html#eq:hactruncrot">(15.6)</a>, the rule of thumb for choosing the HAC standard error truncation parameter <span class="math inline">\(m\)</span>, we choose <span class="math display">\[m = \left\lceil0.75 \cdot 612^{1/3} \right\rceil = \lceil6.37\rceil = 7.\]</span> To check for sensitivity of the standard errors to different choices of the truncation parameter in the model that is used to estimate the cumulative multipliers, we also compute the Newey-West estimator for <span class="math inline">\(m=14\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># gather HAC standard error errors in a list</span>
SEs &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">NeweyWest</span>(FOJC_mod_DM, <span class="dt">lag =</span> <span class="dv">7</span>, <span class="dt">prewhite =</span> F))), 
            <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">NeweyWest</span>(FOJC_mod_CM1, <span class="dt">lag =</span> <span class="dv">7</span>, <span class="dt">prewhite =</span> F))), 
            <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">NeweyWest</span>(FOJC_mod_CM1, <span class="dt">lag =</span> <span class="dv">14</span>, <span class="dt">prewhite =</span> F))),
            <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">NeweyWest</span>(FOJC_mod_CM2, <span class="dt">lag =</span> <span class="dv">7</span>, <span class="dt">prewhite =</span> F))))</code></pre></div>
<p>The results are then used to reproduce the outcomes presented in Table 15.1 of the book.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stargazer</span>(FOJC_mod_DM , FOJC_mod_CM1, FOJC_mod_CM1, FOJC_mod_CM2,
  <span class="dt">title =</span> <span class="st">&quot;Dynamic Effects of a Freezing Degree Day on the Price of Orange Juice&quot;</span>,
  <span class="dt">header =</span> <span class="ot">FALSE</span>, 
  <span class="dt">digits =</span> <span class="dv">3</span>, 
  <span class="dt">column.labels =</span> <span class="kw">c</span>(<span class="st">&quot;Dynamic Multipliers&quot;</span>, <span class="kw">rep</span>(<span class="st">&quot;Dynamic Cumulative Multipliers&quot;</span>, <span class="dv">3</span>)),
  <span class="dt">dep.var.caption  =</span> <span class="st">&quot;Dependent Variable: Monthly Percentage Change in Orange Juice Price&quot;</span>,
  <span class="dt">dep.var.labels.include =</span> <span class="ot">FALSE</span>,
  <span class="dt">covariate.labels =</span> <span class="kw">as.character</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">18</span>),
  <span class="dt">omit =</span> <span class="st">&quot;season&quot;</span>,
  <span class="dt">se =</span> SEs,
  <span class="dt">no.space =</span> T,
  <span class="dt">add.lines =</span> <span class="kw">list</span>(<span class="kw">c</span>(<span class="st">&quot;Monthly indicators?&quot;</span>,<span class="st">&quot;no&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>),
                   <span class="kw">c</span>(<span class="st">&quot;HAC truncation&quot;</span>,<span class="st">&quot;7&quot;</span>, <span class="st">&quot;7&quot;</span>, <span class="st">&quot;14&quot;</span>, <span class="st">&quot;7&quot;</span>)),
  <span class="dt">omit.stat =</span> <span class="kw">c</span>(<span class="st">&quot;rsq&quot;</span>, <span class="st">&quot;f&quot;</span>,<span class="st">&quot;ser&quot;</span>)) </code></pre></div>



<table style="text-align:center"><tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"></td><td colspan="4">Dependent Variable: Monthly Percentage Change in Orange Juice Price</td></tr>
<tr><td></td><td colspan="4" style="border-bottom: 1px solid black"></td></tr>
<tr><td style="text-align:left"></td><td>Dynamic Multipliers</td><td>Dynamic Cumulative Multipliers</td><td>Dynamic Cumulative Multipliers</td><td>Dynamic Cumulative Multipliers</td></tr>
<tr><td style="text-align:left"></td><td>(1)</td><td>(2)</td><td>(3)</td><td>(4)</td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">0</td><td>0.508<sup>***</sup></td><td>0.508<sup>***</sup></td><td>0.508<sup>***</sup></td><td>0.524<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.137)</td><td>(0.137)</td><td>(0.139)</td><td>(0.142)</td></tr>
<tr><td style="text-align:left">1</td><td>0.172<sup>**</sup></td><td>0.680<sup>***</sup></td><td>0.680<sup>***</sup></td><td>0.720<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.088)</td><td>(0.134)</td><td>(0.130)</td><td>(0.142)</td></tr>
<tr><td style="text-align:left">2</td><td>0.068</td><td>0.748<sup>***</sup></td><td>0.748<sup>***</sup></td><td>0.781<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.060)</td><td>(0.165)</td><td>(0.162)</td><td>(0.173)</td></tr>
<tr><td style="text-align:left">3</td><td>0.070</td><td>0.819<sup>***</sup></td><td>0.819<sup>***</sup></td><td>0.861<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.044)</td><td>(0.181)</td><td>(0.181)</td><td>(0.190)</td></tr>
<tr><td style="text-align:left">4</td><td>0.022</td><td>0.841<sup>***</sup></td><td>0.841<sup>***</sup></td><td>0.892<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.031)</td><td>(0.183)</td><td>(0.184)</td><td>(0.194)</td></tr>
<tr><td style="text-align:left">5</td><td>0.027</td><td>0.868<sup>***</sup></td><td>0.868<sup>***</sup></td><td>0.904<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.030)</td><td>(0.189)</td><td>(0.189)</td><td>(0.199)</td></tr>
<tr><td style="text-align:left">6</td><td>0.031</td><td>0.900<sup>***</sup></td><td>0.900<sup>***</sup></td><td>0.922<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.047)</td><td>(0.202)</td><td>(0.208)</td><td>(0.210)</td></tr>
<tr><td style="text-align:left">7</td><td>0.015</td><td>0.915<sup>***</sup></td><td>0.915<sup>***</sup></td><td>0.939<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.015)</td><td>(0.205)</td><td>(0.210)</td><td>(0.212)</td></tr>
<tr><td style="text-align:left">8</td><td>-0.042</td><td>0.873<sup>***</sup></td><td>0.873<sup>***</sup></td><td>0.904<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.034)</td><td>(0.214)</td><td>(0.218)</td><td>(0.219)</td></tr>
<tr><td style="text-align:left">9</td><td>-0.010</td><td>0.862<sup>***</sup></td><td>0.862<sup>***</sup></td><td>0.884<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.051)</td><td>(0.236)</td><td>(0.245)</td><td>(0.239)</td></tr>
<tr><td style="text-align:left">10</td><td>-0.116<sup>*</sup></td><td>0.746<sup>***</sup></td><td>0.746<sup>***</sup></td><td>0.752<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.069)</td><td>(0.257)</td><td>(0.262)</td><td>(0.259)</td></tr>
<tr><td style="text-align:left">11</td><td>-0.067</td><td>0.680<sup>**</sup></td><td>0.680<sup>**</sup></td><td>0.677<sup>**</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.052)</td><td>(0.266)</td><td>(0.272)</td><td>(0.267)</td></tr>
<tr><td style="text-align:left">12</td><td>-0.143<sup>*</sup></td><td>0.537<sup>**</sup></td><td>0.537<sup>**</sup></td><td>0.551<sup>**</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.076)</td><td>(0.268)</td><td>(0.271)</td><td>(0.272)</td></tr>
<tr><td style="text-align:left">13</td><td>-0.083<sup>*</sup></td><td>0.454<sup>*</sup></td><td>0.454<sup>*</sup></td><td>0.491<sup>*</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.043)</td><td>(0.267)</td><td>(0.273)</td><td>(0.275)</td></tr>
<tr><td style="text-align:left">14</td><td>-0.057</td><td>0.397</td><td>0.397</td><td>0.427</td></tr>
<tr><td style="text-align:left"></td><td>(0.035)</td><td>(0.273)</td><td>(0.284)</td><td>(0.278)</td></tr>
<tr><td style="text-align:left">15</td><td>-0.032</td><td>0.366</td><td>0.366</td><td>0.406</td></tr>
<tr><td style="text-align:left"></td><td>(0.028)</td><td>(0.276)</td><td>(0.287)</td><td>(0.280)</td></tr>
<tr><td style="text-align:left">16</td><td>-0.005</td><td>0.360</td><td>0.360</td><td>0.408</td></tr>
<tr><td style="text-align:left"></td><td>(0.055)</td><td>(0.283)</td><td>(0.293)</td><td>(0.286)</td></tr>
<tr><td style="text-align:left">17</td><td>0.003</td><td>0.363</td><td>0.363</td><td>0.395</td></tr>
<tr><td style="text-align:left"></td><td>(0.018)</td><td>(0.287)</td><td>(0.294)</td><td>(0.290)</td></tr>
<tr><td style="text-align:left">18</td><td>0.003</td><td>0.366</td><td>0.366</td><td>0.386</td></tr>
<tr><td style="text-align:left"></td><td>(0.017)</td><td>(0.293)</td><td>(0.301)</td><td>(0.295)</td></tr>
<tr><td style="text-align:left">Constant</td><td>-0.343</td><td>-0.343</td><td>-0.343</td><td>-0.241</td></tr>
<tr><td style="text-align:left"></td><td>(0.269)</td><td>(0.269)</td><td>(0.256)</td><td>(0.934)</td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">Monthly indicators?</td><td>no</td><td>no</td><td>no</td><td>yes</td></tr>
<tr><td style="text-align:left">HAC truncation</td><td>7</td><td>7</td><td>14</td><td>7</td></tr>
<tr><td style="text-align:left">Observations</td><td>594</td><td>594</td><td>594</td><td>594</td></tr>
<tr><td style="text-align:left">Adjusted R<sup>2</sup></td><td>0.109</td><td>0.109</td><td>0.109</td><td>0.101</td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"><em>Note:</em></td><td colspan="4" style="text-align:right"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>
</table>
<caption><p style='text-align:center'><span id="tab:deoafddotpooj">Table 15.1: </span> Dynamic Effects of a Freezing Degree Day on the Price of Orange Juice</p></caption>


<p>According to column (1) of Table <a href="eodce.html#tab:deoafddotpooj">15.1</a>, the contemporaneous effect of a freezing degree day is an increase of <span class="math inline">\(0.5\%\)</span> in orange juice prices. The estimated effect is only <span class="math inline">\(0.17\%\)</span> for the next month and close to zero for subsequent months. In fact, for all lags larger than 1, we cannot reject the null hypotheses that the respective coefficients are zero using individual <span class="math inline">\(t\)</span>-tests. The model <tt>FOJC_mod_DM</tt> only explains little of the variation in the dependent variable (<span class="math inline">\(\bar{R}^2 = 0.11\)</span>).</p>
<p>Columns (2) and (3) present estimates of the dynamic cumulative multipliers of model <tt>FOJC_mod_CM1</tt>. Apparently, it does not matter whether we choose <span class="math inline">\(m=7\)</span> or <span class="math inline">\(m=14\)</span> when computing HAC standard errors so we stick with <span class="math inline">\(m=7\)</span> and the standard errors reported in column (2).</p>
<p>If the demand for orange juice is higher in winter, <span class="math inline">\(FDD_t\)</span> would be correlated with the error term since freezes occur rather in winter so we would face omitted variable bias. The third model estimate, <tt>FOJC_mod_CM2</tt>, accounts for this possible issue by using an additional set of 11 monthly dummies. For brevity, estimates of the dummy coefficients are excluded from the output produced by stargazer (this is achieved by setting <tt>omit = ‘season’</tt>). We may check that the dummy for January was omitted to prevent perfect multicollinearity.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimates on mothly dummies</span>
FOJC_mod_CM2<span class="op">$</span>coefficients[<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">20</span>)]</code></pre></div>
<pre><code>## season(FDD)Feb season(FDD)Mar season(FDD)Apr season(FDD)May season(FDD)Jun 
##     -0.9565759     -0.6358007      0.5006770     -1.0801764      0.3195624 
## season(FDD)Jul season(FDD)Aug season(FDD)Sep season(FDD)Oct season(FDD)Nov 
##      0.1951113      0.3644312     -0.4130969     -0.1566622      0.3116534 
## season(FDD)Dec 
##      0.1481589</code></pre>
<p>A comparison of the estimates presented in columns (3) and (4) indicates that adding monthly dummies has a negligible effect. Further evidence for this comes from a joint test of the hypothesis that the 11 dummy coefficients are zero. Instead of using <tt>linearHypothesis()</tt>, we use the function <tt>waldtest()</tt> and supply two model objects instead: <tt>unres_model</tt>, the unrestricted model object which is the same as <tt>FOJC_mod_CM2</tt> (except for the coefficient names since we have modified them above) and <tt>res_model</tt>, the model where the restriction that all dummy coefficients are zero is imposed. <tt>res_model</tt> is conveniently obtained using the function <tt>update()</tt>. It extracts the argument <tt>formula</tt> of a model object, updates it as specified and then re-fits the model. By setting <tt>formula = . ~ . - season(FDD)</tt> we impose that the monthly dummies do not enter the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># test if coefficients on monthly dummies are zero</span>
unres_model &lt;-<span class="st"> </span><span class="kw">dynlm</span>(FOJC_pctc <span class="op">~</span><span class="st"> </span><span class="kw">L</span>(<span class="kw">d</span>(FDD), <span class="dv">0</span><span class="op">:</span><span class="dv">17</span>) <span class="op">+</span><span class="st"> </span><span class="kw">L</span>(FDD, <span class="dv">18</span>) <span class="op">+</span><span class="st"> </span><span class="kw">season</span>(FDD))

res_model &lt;-<span class="st"> </span><span class="kw">update</span>(unres_model, <span class="dt">formula =</span> . <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span><span class="kw">season</span>(FDD))

<span class="kw">waldtest</span>(unres_model, 
         res_model, 
         <span class="dt">vcov =</span> <span class="kw">NeweyWest</span>(unres_model, <span class="dt">lag =</span> <span class="dv">7</span>, <span class="dt">prewhite =</span> F))</code></pre></div>
<pre><code>## Wald test
## 
## Model 1: FOJC_pctc ~ L(d(FDD), 0:17) + L(FDD, 18) + season(FDD)
## Model 2: FOJC_pctc ~ L(d(FDD), 0:17) + L(FDD, 18)
##   Res.Df  Df      F Pr(&gt;F)
## 1    563                  
## 2    574 -11 0.9683 0.4743</code></pre>
<p>The <span class="math inline">\(p\)</span>-value is <span class="math inline">\(0.47\)</span> so we cannot reject the hypothesis that the coefficients on the monthly dummies are zero, even at the <span class="math inline">\(10\%\)</span> level. We conclude that the seasonal fluctuations in demand for orange juice do not pose a serious threat to internal validity of the model.</p>
<p>It is convenient to use plots of dynamic multipliers and cumulative dynamic multipliers. The following two code chunks reproduce Figures 15.2 (a) and 15.2 (b) of the book which display point estimates of dynamic and cumulative multipliers along with upper and lower bounds of their <span class="math inline">\(95\%\)</span> confidence intervals computed using the above HAC standard errors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 95% CI bounds</span>
point_estimates &lt;-<span class="st"> </span>FOJC_mod_DM<span class="op">$</span>coefficients

CI_bounds &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="st">&quot;lower&quot;</span> =<span class="st"> </span>point_estimates <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>SEs[[<span class="dv">1</span>]],
                   <span class="st">&quot;upper&quot;</span> =<span class="st"> </span>point_estimates <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>SEs[[<span class="dv">1</span>]])[<span class="op">-</span><span class="dv">1</span>, ]

<span class="co"># plot the estimated dynamic multipliers</span>
<span class="kw">plot</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">18</span>, point_estimates[<span class="op">-</span><span class="dv">1</span>], 
     <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, 
     <span class="dt">lwd =</span> <span class="dv">2</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.4</span>, <span class="dv">1</span>),
     <span class="dt">xlab =</span> <span class="st">&quot;Lag&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Dynamic multiplier&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Dynamic Effect of FDD on Orange Juice Price&quot;</span>)

<span class="co"># add a dashed line at 0</span>
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>)

<span class="co"># add CI bounds</span>
<span class="kw">lines</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">18</span>, CI_bounds[,<span class="dv">1</span>], <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)
<span class="kw">lines</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">18</span>, CI_bounds[,<span class="dv">2</span>], <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:DM"></span>
<img src="ITER_files/figure-html/DM-1.png" alt="Dynamic Multipliers" width="672" />
<p class="caption">
Figure 15.1: Dynamic Multipliers
</p>
</div>
<p>The <span class="math inline">\(95\%\)</span> confidence intervals plotted in Figure <a href="eodce.html#fig:DM">15.1</a> indeed include zero for lags larger than 1 such that the null of a zero multiplier cannot be rejected for these lags.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 95% CI bounds</span>
point_estimates &lt;-<span class="st"> </span>FOJC_mod_CM1<span class="op">$</span>coefficients

CI_bounds &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="st">&quot;lower&quot;</span> =<span class="st"> </span>point_estimates <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>SEs[[<span class="dv">2</span>]],
                   <span class="st">&quot;upper&quot;</span> =<span class="st"> </span>point_estimates <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>SEs[[<span class="dv">2</span>]])[<span class="op">-</span><span class="dv">1</span>,]


<span class="co"># plot estimated dynamic multipliers</span>
<span class="kw">plot</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">18</span>, point_estimates[<span class="op">-</span><span class="dv">1</span>], 
     <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, 
     <span class="dt">lwd =</span> <span class="dv">2</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.4</span>, <span class="fl">1.6</span>),
     <span class="dt">xlab =</span> <span class="st">&quot;Lag&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Cumulative dynamic multiplier&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Cumulative Dynamic Effect of FDD on Orange Juice Price&quot;</span>)

<span class="co"># add dashed line at 0</span>
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>)

<span class="co"># add CI bounds</span>
<span class="kw">lines</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">18</span>, CI_bounds[, <span class="dv">1</span>], <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)
<span class="kw">lines</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">18</span>, CI_bounds[, <span class="dv">2</span>], <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:DCM"></span>
<img src="ITER_files/figure-html/DCM-1.png" alt="Dynamic Cumulative Multipliers" width="672" />
<p class="caption">
Figure 15.2: Dynamic Cumulative Multipliers
</p>
</div>
<p>As can be seen from Figure <a href="eodce.html#fig:DCM">15.2</a>, the estimated dynamic cumulative multipliers grow until the seventh month up to a price increase of about <span class="math inline">\(0.91\%\)</span> and then decrease slightly to the estimated long-run cumulative multiplier of <span class="math inline">\(0.37\%\)</span> which, however, is not significantly different from zero at the <span class="math inline">\(5\%\)</span> level.</p>
<p>Have the dynamic multipliers been stable over time? One way to see this is to estimate these multipliers for different subperiods of the sample span. For example, consider periods 1950 - 1966, 1967 - 1983 and 1984 - 2000. If the multipliers are the same for all three periods the estimates should be close and thus the estimated cumulative multipliers should be similar, too. We investigate this by re-estimating <tt>FOJC_mod_CM1</tt> for the three different time spans and then plot the estimated cumulative dynamic multipliers for the comparison.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate cumulative multiplieres using different sample periods</span>
FOJC_mod_CM1950 &lt;-<span class="st"> </span><span class="kw">update</span>(FOJC_mod_CM1, <span class="dt">start =</span> <span class="kw">c</span>(<span class="dv">1950</span>, <span class="dv">1</span>), <span class="dt">end =</span> <span class="kw">c</span>(<span class="dv">1966</span>, <span class="dv">12</span>))

FOJC_mod_CM1967 &lt;-<span class="st"> </span><span class="kw">update</span>(FOJC_mod_CM1, <span class="dt">start =</span> <span class="kw">c</span>(<span class="dv">1967</span>, <span class="dv">1</span>), <span class="dt">end =</span> <span class="kw">c</span>(<span class="dv">1983</span>, <span class="dv">12</span>))

FOJC_mod_CM1984 &lt;-<span class="st"> </span><span class="kw">update</span>(FOJC_mod_CM1, <span class="dt">start =</span> <span class="kw">c</span>(<span class="dv">1984</span>, <span class="dv">1</span>), <span class="dt">end =</span> <span class="kw">c</span>(<span class="dv">2000</span>, <span class="dv">12</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot estimated dynamic cumulative multipliers (1950-1966)</span>
<span class="kw">plot</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">18</span>, FOJC_mod_CM1950<span class="op">$</span>coefficients[<span class="op">-</span><span class="dv">1</span>], 
     <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, 
     <span class="dt">lwd =</span> <span class="dv">2</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">20</span>),
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="dv">2</span>),
     <span class="dt">xlab =</span> <span class="st">&quot;Lag&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Cumulative dynamic multiplier&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Cumulative Dynamic Effect for Different Sample Periods&quot;</span>)

<span class="co"># plot estimated dynamic multipliers (1967-1983)</span>
<span class="kw">lines</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">18</span>, FOJC_mod_CM1967<span class="op">$</span>coefficients[<span class="op">-</span><span class="dv">1</span>], <span class="dt">lwd =</span> <span class="dv">2</span>)

<span class="co"># plot estimated dynamic multipliers (1984-2000)</span>
<span class="kw">lines</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">18</span>, FOJC_mod_CM1984<span class="op">$</span>coefficients[<span class="op">-</span><span class="dv">1</span>], <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>)

<span class="co"># add dashed line at 0</span>
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>)

<span class="co"># add annotations</span>
<span class="kw">text</span>(<span class="dv">18</span>, <span class="op">-</span><span class="fl">0.24</span>, <span class="st">&quot;1984 - 2000&quot;</span>)
<span class="kw">text</span>(<span class="dv">18</span>, <span class="fl">0.6</span>, <span class="st">&quot;1967 - 1983&quot;</span>)
<span class="kw">text</span>(<span class="dv">18</span>, <span class="fl">1.2</span>, <span class="st">&quot;1950 - 1966&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-698-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Clearly, the cumulative dynamic multipliers have changed considerably over time. The effect of a freeze was stronger and more persistent in the 1950s and 1960s. For the 1970s the magnitude of the effect was lower but still highly persistent. We observe an even lower magnitude for the final third of the sample span (1984 - 2000) where the long-run effect is much less persistent and essentially zero after a year.</p>
<p>A QLR test for a break in the distributed lag regression of column (1) in Table @ref{tab:deoafddotpooj} with <span class="math inline">\(15\%\)</span> trimming using a HAC variance-covariance matrix estimate supports the conjecture that the population regression coefficients have changed over time.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set up a range of possible break dates</span>
tau &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">window</span>(<span class="kw">time</span>(FDD), 
                <span class="kw">time</span>(FDD)[<span class="kw">round</span>(<span class="dv">612</span><span class="op">/</span><span class="dv">100</span><span class="op">*</span><span class="dv">15</span>)], 
                <span class="kw">time</span>(FDD)[<span class="kw">round</span>(<span class="dv">612</span><span class="op">/</span><span class="dv">100</span><span class="op">*</span><span class="dv">85</span>)]))

<span class="co"># initialize the vector of F-statistics</span>
Fstats &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="kw">length</span>(tau))

<span class="co"># the restricted model</span>
res_model &lt;-<span class="st"> </span><span class="kw">dynlm</span>(FOJC_pctc <span class="op">~</span><span class="st"> </span><span class="kw">L</span>(FDD, <span class="dv">0</span><span class="op">:</span><span class="dv">18</span>))

<span class="co"># estimation, loop over break dates</span>
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(tau)) {
  
  <span class="co"># set up dummy variable</span>
  D &lt;-<span class="st"> </span><span class="kw">time</span>(FOJC_pctc) <span class="op">&gt;</span><span class="st"> </span>tau[i]
  
  <span class="co"># estimate DL model with intercations</span>
  unres_model &lt;-<span class="st"> </span><span class="kw">dynlm</span>(FOJC_pctc <span class="op">~</span><span class="st"> </span>D <span class="op">*</span><span class="st"> </span><span class="kw">L</span>(FDD, <span class="dv">0</span><span class="op">:</span><span class="dv">18</span>))
                 
  <span class="co"># compute and save F-statistic</span>
  Fstats[i] &lt;-<span class="st"> </span><span class="kw">waldtest</span>(res_model, 
                        unres_model, 
                        <span class="dt">vcov =</span> <span class="kw">NeweyWest</span>(unres_model, <span class="dt">lag =</span> <span class="dv">7</span>, <span class="dt">prewhite =</span> F))<span class="op">$</span>F[<span class="dv">2</span>]
    
}</code></pre></div>
<p>Note that this code takes a couple of seconds to run since a total of <tt>length(tau)</tt> regressions with <span class="math inline">\(40\)</span> model coefficients each are estimated.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># QLR test statistic</span>
<span class="kw">max</span>(Fstats)</code></pre></div>
<pre><code>## [1] 36.76819</code></pre>
<p>The QLR statistic is <span class="math inline">\(36.77\)</span>. From Table 14.5 of the book we see that the <span class="math inline">\(1\%\)</span> critical value for the QLR test with <span class="math inline">\(15\%\)</span> trimming and <span class="math inline">\(q=20\)</span> restrictions is <span class="math inline">\(2.43\)</span>. Since this is a right-sided test, the QLR statistic clearly lies in the region of rejection so we can discard the null hypothesis of no break in the population regression function.</p>
<p>See Chapter 15.7 of the book for a discussion of empirical examples where it is questionable whether the assumption of (past and present) exogeneity of the regressors is plausible.</p>
<div id="summary-7" class="section level4 unnumbered">
<h4>Summary</h4>
<ul>
<li><p>We have seen how <tt>R</tt> can be used to estimate the time path of the effect on <span class="math inline">\(Y\)</span> of a change in <span class="math inline">\(X\)</span> (the dynamic causal effect on <span class="math inline">\(Y\)</span> of a change in <span class="math inline">\(X\)</span>) using time series data on both. The corresponding model is called the distributed lag model. Distributed lag models are conveniently estimated using the function <tt>dynlm()</tt> from the package <tt>dynlm</tt>.</p></li>
<li><p>The regression error in distributed lag models is often serially correlated such that standard errors which are robust to heteroskedasticity <em>and</em> autocorrelation should be used to obtain valid inference. The package <tt>sandwich</tt> provides functions for computation of so-called HAC covariance matrix estimators, for example <tt>vcovHAC()</tt> and <tt>NeweyWest()</tt>.</p></li>
<li><p>When <span class="math inline">\(X\)</span> is <em>strictly exogeneous</em>, more efficient estimates can be obtained using an ADL model or by GLS estimation. Feasible GLS algorithms can be found in the <tt>R</tt> packages <tt>orcutt</tt> and <tt>nlme</tt>. Chapter 15.7 of the book emphasizes that the assumption of strict exogeneity is often implausible in empirical applications.</p></li>
</ul>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-AER">
<p>Kleiber, C., &amp; Zeileis, A. (2017). AER: Applied Econometrics with R (Version 1.2-5). Retrieved from <a href="https://CRAN.R-project.org/package=AER" class="uri">https://CRAN.R-project.org/package=AER</a></p>
</div>
<div id="ref-R-dynlm">
<p>Zeileis, A. (2016). dynlm: Dynamic Linear Regression (Version 0.3-5). Retrieved from <a href="https://CRAN.R-project.org/package=dynlm" class="uri">https://CRAN.R-project.org/package=dynlm</a></p>
</div>
<div id="ref-R-nlme">
<p>Pinheiro, J., Bates, D., &amp; R-core. (2018). nlme: Linear and Nonlinear Mixed Effects Models (Version 3.1-137). Retrieved from <a href="https://CRAN.R-project.org/package=nlme" class="uri">https://CRAN.R-project.org/package=nlme</a></p>
</div>
<div id="ref-R-orcutt">
<p>Spada, S. (2017). orcutt: Estimate Procedure in Case of First Order Autocorrelation (Version 2.2). Retrieved from <a href="https://CRAN.R-project.org/package=orcutt" class="uri">https://CRAN.R-project.org/package=orcutt</a></p>
</div>
<div id="ref-R-quantmod">
<p>Ryan, J. A., &amp; Ulrich, J. M. (2018). quantmod: Quantitative Financial Modelling Framework (Version 0.4-13). Retrieved from <a href="https://CRAN.R-project.org/package=quantmod" class="uri">https://CRAN.R-project.org/package=quantmod</a></p>
</div>
<div id="ref-R-stargazer">
<p>Hlavac, M. (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables (Version 5.2.2). Retrieved from <a href="https://CRAN.R-project.org/package=stargazer" class="uri">https://CRAN.R-project.org/package=stargazer</a></p>
</div>
<div id="ref-newey1987">
<p>Newey, W. K., &amp; West, K. D. (1987). A Simple, Positive Semi-definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix. <em>Econometrica</em>, <em>55</em>(3), 703–08.</p>
</div>
<div id="ref-cochrane1949">
<p>Cochrane, D., &amp; Orcutt, G. H. (1949). Application of Least Squares Regression to Relationships Containing Auto-Correlated Error Terms. <em>Journal of the American Statistical Association</em>, <em>44</em>(245), 32–61. doi:<a href="https://doi.org/10.1080/01621459.1949.10483290">10.1080/01621459.1949.10483290</a></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ittsraf.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="atitsr.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": true,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 1
},
"edit": null,
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
