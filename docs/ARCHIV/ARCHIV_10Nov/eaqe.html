<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Econometrics with R</title>
  <meta name="description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Econometrics with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Econometrics with R" />
  
  <meta name="twitter:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2018-10-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ivr.html">
<link rel="next" href="ittsraf.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<!-- font families -->

<link href="https://fonts.googleapis.com/css?family=PT+Sans|Pacifico|Source+Sans+Pro" rel="stylesheet">

<!-- function to adjust height of iframes automatically depending on content loaded -->

<script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';
  }
</script>

<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/default.js"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110299877-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110299877-1');
</script>

<!-- open review block -->

<script async defer src="https://hypothes.is/embed.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://www.oek.wiwi.uni-due.de/en/">Chair of Econometrics at UDE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#a-very-short-introduction-to-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> A Very Short Introduction to <tt>R</tt> and <em>RStudio</em></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="pt.html"><a href="pt.html#random-variables-and-probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#expected-value-mean-and-variance"><i class="fa fa-check"></i>Expected Value, Mean and Variance</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#thetdist"><i class="fa fa-check"></i>The Student t Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-f-distribution"><i class="fa fa-check"></i>The F Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pt.html"><a href="pt.html#RSATDOSA"><i class="fa fa-check"></i><b>2.2</b> Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="pt.html"><a href="pt.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arosur.html"><a href="arosur.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="3.1" data-path="arosur.html"><a href="arosur.html#estimation-of-the-population-mean"><i class="fa fa-check"></i><b>3.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="3.2" data-path="arosur.html"><a href="arosur.html#potsm"><i class="fa fa-check"></i><b>3.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="3.3" data-path="arosur.html"><a href="arosur.html#hypothesis-tests-concerning-the-population-mean"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests Concerning the Population Mean</a><ul>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-p-value"><i class="fa fa-check"></i>The p-Value</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-known"><i class="fa fa-check"></i>Calculating the p-Value when the Standard Deviation is Known</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#SVSSDASE"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-unknown"><i class="fa fa-check"></i>Calculating the p-value When the Standard Deviation is Unknown</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-t-statistic"><i class="fa fa-check"></i>The t-statistic</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="arosur.html"><a href="arosur.html#confidence-intervals-for-the-population-mean"><i class="fa fa-check"></i><b>3.4</b> Confidence Intervals for the Population Mean</a></li>
<li class="chapter" data-level="3.5" data-path="arosur.html"><a href="arosur.html#cmfdp"><i class="fa fa-check"></i><b>3.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="3.6" data-path="arosur.html"><a href="arosur.html#aattggoe"><i class="fa fa-check"></i><b>3.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="3.7" data-path="arosur.html"><a href="arosur.html#scatterplots-sample-covariance-and-sample-correlation"><i class="fa fa-check"></i><b>3.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
<li class="chapter" data-level="3.8" data-path="arosur.html"><a href="arosur.html#exercises-1"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="4.1" data-path="lrwor.html"><a href="lrwor.html#simple-linear-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="4.2" data-path="lrwor.html"><a href="lrwor.html#estimating-the-coefficients-of-the-linear-regression-model"><i class="fa fa-check"></i><b>4.2</b> Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lrwor.html"><a href="lrwor.html#measures-of-fit"><i class="fa fa-check"></i><b>4.3</b> Measures of Fit</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-coefficient-of-determination"><i class="fa fa-check"></i>The Coefficient of Determination</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-standard-error-of-the-regression"><i class="fa fa-check"></i>The Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lrwor.html"><a href="lrwor.html#tlsa"><i class="fa fa-check"></i><b>4.4</b> The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption 1: The Error Term has Conditional Mean of Zero</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-2-independently-and-identically-distributed-data"><i class="fa fa-check"></i>Assumption 2: Independently and Identically Distributed Data</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption 3: Large Outliers are Unlikely</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="lrwor.html"><a href="lrwor.html#tsdotoe"><i class="fa fa-check"></i><b>4.5</b> The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-1"><i class="fa fa-check"></i>Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-2"><i class="fa fa-check"></i>Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-3"><i class="fa fa-check"></i>Simulation Study 3</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="lrwor.html"><a href="lrwor.html#exercises-2"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="htaciitslrm.html"><a href="htaciitslrm.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li class="chapter" data-level="5.1" data-path="htaciitslrm.html"><a href="htaciitslrm.html#testing-two-sided-hypotheses-concerning-the-slope-coefficient"><i class="fa fa-check"></i><b>5.1</b> Testing Two-Sided Hypotheses Concerning the Slope Coefficient</a></li>
<li class="chapter" data-level="5.2" data-path="htaciitslrm.html"><a href="htaciitslrm.html#cifrc"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-confidence-intervals"><i class="fa fa-check"></i>Simulation Study: Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="htaciitslrm.html"><a href="htaciitslrm.html#rwxiabv"><i class="fa fa-check"></i><b>5.3</b> Regression when X is a Binary Variable</a></li>
<li class="chapter" data-level="5.4" data-path="htaciitslrm.html"><a href="htaciitslrm.html#hah"><i class="fa fa-check"></i><b>5.4</b> Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="htaciitslrm.html"><a href="htaciitslrm.html#the-gauss-markov-theorem"><i class="fa fa-check"></i><b>5.5</b> The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-blue-estimator"><i class="fa fa-check"></i>Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="htaciitslrm.html"><a href="htaciitslrm.html#using-the-t-statistic-in-regression-when-the-sample-size-is-small"><i class="fa fa-check"></i><b>5.6</b> Using the t-Statistic in Regression When the Sample Size Is Small</a></li>
<li class="chapter" data-level="5.7" data-path="htaciitslrm.html"><a href="htaciitslrm.html#exercises-3"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rmwmr.html"><a href="rmwmr.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="6.1" data-path="rmwmr.html"><a href="rmwmr.html#omitted-variable-bias"><i class="fa fa-check"></i><b>6.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="6.2" data-path="rmwmr.html"><a href="rmwmr.html#tmrm"><i class="fa fa-check"></i><b>6.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="rmwmr.html"><a href="rmwmr.html#mofimr"><i class="fa fa-check"></i><b>6.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="6.4" data-path="rmwmr.html"><a href="rmwmr.html#ols-assumptions-in-multiple-regression"><i class="fa fa-check"></i><b>6.4</b> OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#simulation-study-imperfect-multicollinearity"><i class="fa fa-check"></i>Simulation Study: Imperfect Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="rmwmr.html"><a href="rmwmr.html#the-distribution-of-the-ols-estimators-in-multiple-regression"><i class="fa fa-check"></i><b>6.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
<li class="chapter" data-level="6.6" data-path="rmwmr.html"><a href="rmwmr.html#exercises-4"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="htaciimr.html"><a href="htaciimr.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence Intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="htaciimr.html"><a href="htaciimr.html#hypothesis-tests-and-confidence-intervals-for-a-single-coefficient"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="htaciimr.html"><a href="htaciimr.html#an-application-to-test-scores-and-the-student-teacher-ratio"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="htaciimr.html"><a href="htaciimr.html#joint-hypothesis-testing-using-the-f-statistic"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the F-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="htaciimr.html"><a href="htaciimr.html#confidence-sets-for-multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-for-multiple-regression"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="htaciimr.html"><a href="htaciimr.html#analysis-of-the-test-score-data-set"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
<li class="chapter" data-level="7.7" data-path="htaciimr.html"><a href="htaciimr.html#exercises-5"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nrf.html"><a href="nrf.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="8.1" data-path="nrf.html"><a href="nrf.html#a-general-strategy-for-modelling-nonlinear-regression-functions"><i class="fa fa-check"></i><b>8.1</b> A General Strategy for Modelling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="8.2" data-path="nrf.html"><a href="nrf.html#nfoasiv"><i class="fa fa-check"></i><b>8.2</b> Nonlinear Functions of a Single Independent Variable</a><ul>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="nrf.html"><a href="nrf.html#interactions-between-independent-variables"><i class="fa fa-check"></i><b>8.3</b> Interactions Between Independent Variables</a></li>
<li class="chapter" data-level="8.4" data-path="nrf.html"><a href="nrf.html#nonlinear-effects-on-test-scores-of-the-student-teacher-ratio"><i class="fa fa-check"></i><b>8.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
<li class="chapter" data-level="8.5" data-path="nrf.html"><a href="nrf.html#exercises-6"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="asbomr.html"><a href="asbomr.html"><i class="fa fa-check"></i><b>9</b> Assessing Studies Based on Multiple Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity"><i class="fa fa-check"></i><b>9.1</b> Internal and External Validity</a></li>
<li class="chapter" data-level="9.2" data-path="asbomr.html"><a href="asbomr.html#ttivomra"><i class="fa fa-check"></i><b>9.2</b> Threats to Internal Validity of Multiple Regression Analysis</a></li>
<li class="chapter" data-level="9.3" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity-when-the-regression-is-used-for-forecasting"><i class="fa fa-check"></i><b>9.3</b> Internal and External Validity when the Regression is Used for Forecasting</a></li>
<li class="chapter" data-level="9.4" data-path="asbomr.html"><a href="asbomr.html#etsacs"><i class="fa fa-check"></i><b>9.4</b> Example: Test Scores and Class Size</a></li>
<li class="chapter" data-level="9.5" data-path="asbomr.html"><a href="asbomr.html#exercises-7"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="rwpd.html"><a href="rwpd.html"><i class="fa fa-check"></i><b>10</b> Regression with Panel Data</a><ul>
<li class="chapter" data-level="10.1" data-path="rwpd.html"><a href="rwpd.html#panel-data"><i class="fa fa-check"></i><b>10.1</b> Panel Data</a></li>
<li class="chapter" data-level="10.2" data-path="rwpd.html"><a href="rwpd.html#PDWTTP"><i class="fa fa-check"></i><b>10.2</b> Panel Data with Two Time Periods: “Before and After” Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="rwpd.html"><a href="rwpd.html#fixed-effects-regression"><i class="fa fa-check"></i><b>10.3</b> Fixed Effects Regression</a><ul>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#estimation-and-inference"><i class="fa fa-check"></i>Estimation and Inference</a></li>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#application-to-traffic-deaths"><i class="fa fa-check"></i>Application to Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="rwpd.html"><a href="rwpd.html#regression-with-time-fixed-effects"><i class="fa fa-check"></i><b>10.4</b> Regression with Time Fixed Effects</a></li>
<li class="chapter" data-level="10.5" data-path="rwpd.html"><a href="rwpd.html#tferaaseffer"><i class="fa fa-check"></i><b>10.5</b> The Fixed Effects Regression Assumptions and Standard Errors for Fixed Effects Regression</a></li>
<li class="chapter" data-level="10.6" data-path="rwpd.html"><a href="rwpd.html#drunk-driving-laws-and-traffic-deaths"><i class="fa fa-check"></i><b>10.6</b> Drunk Driving Laws and Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="rwabdv.html"><a href="rwabdv.html"><i class="fa fa-check"></i><b>11</b> Regression with a Binary Dependent Variable</a><ul>
<li class="chapter" data-level="11.1" data-path="rwabdv.html"><a href="rwabdv.html#binary-dependent-variables-and-the-linear-probability-model"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables and the Linear Probability Model</a></li>
<li class="chapter" data-level="11.2" data-path="rwabdv.html"><a href="rwabdv.html#palr"><i class="fa fa-check"></i><b>11.2</b> Probit and Logit Regression</a><ul>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#probit-regression"><i class="fa fa-check"></i>Probit Regression</a></li>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#logit-regression"><i class="fa fa-check"></i>Logit Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="rwabdv.html"><a href="rwabdv.html#estimation-and-inference-in-the-logit-and-probit-models"><i class="fa fa-check"></i><b>11.3</b> Estimation and Inference in the Logit and Probit Models</a></li>
<li class="chapter" data-level="11.4" data-path="rwabdv.html"><a href="rwabdv.html#application-to-the-boston-hmda-data"><i class="fa fa-check"></i><b>11.4</b> Application to the Boston HMDA Data</a></li>
<li class="chapter" data-level="11.5" data-path="rwabdv.html"><a href="rwabdv.html#exercises-8"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ivr.html"><a href="ivr.html"><i class="fa fa-check"></i><b>12</b> Instrumental Variables Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="ivr.html"><a href="ivr.html#TIVEWASRAASI"><i class="fa fa-check"></i><b>12.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a></li>
<li class="chapter" data-level="12.2" data-path="ivr.html"><a href="ivr.html#TGIVRM"><i class="fa fa-check"></i><b>12.2</b> The General IV Regression Model</a></li>
<li class="chapter" data-level="12.3" data-path="ivr.html"><a href="ivr.html#civ"><i class="fa fa-check"></i><b>12.3</b> Checking Instrument Validity</a></li>
<li class="chapter" data-level="12.4" data-path="ivr.html"><a href="ivr.html#attdfc"><i class="fa fa-check"></i><b>12.4</b> Application to the Demand for Cigarettes</a></li>
<li class="chapter" data-level="12.5" data-path="ivr.html"><a href="ivr.html#where-do-valid-instruments-come-from"><i class="fa fa-check"></i><b>12.5</b> Where Do Valid Instruments Come From?</a></li>
<li class="chapter" data-level="12.6" data-path="ivr.html"><a href="ivr.html#exercises-9"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="eaqe.html"><a href="eaqe.html"><i class="fa fa-check"></i><b>13</b> Experiments and Quasi-Experiments</a><ul>
<li class="chapter" data-level="13.1" data-path="eaqe.html"><a href="eaqe.html#potential-outcomes-causal-effects-and-idealized-experiments"><i class="fa fa-check"></i><b>13.1</b> Potential Outcomes, Causal Effects and Idealized Experiments</a></li>
<li class="chapter" data-level="13.2" data-path="eaqe.html"><a href="eaqe.html#threats-to-validity-of-experiments"><i class="fa fa-check"></i><b>13.2</b> Threats to Validity of Experiments</a></li>
<li class="chapter" data-level="13.3" data-path="eaqe.html"><a href="eaqe.html#experimental-estimates-of-the-effect-of-class-size-reductions"><i class="fa fa-check"></i><b>13.3</b> Experimental Estimates of the Effect of Class Size Reductions</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#experimental-design-and-the-data-set"><i class="fa fa-check"></i>Experimental Design and the Data Set</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#analysis-of-the-star-data"><i class="fa fa-check"></i>Analysis of the STAR Data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="eaqe.html"><a href="eaqe.html#quasi-experiments"><i class="fa fa-check"></i><b>13.4</b> Quasi Experiments</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#the-differences-in-differences-estimator"><i class="fa fa-check"></i>The Differences-in-Differences Estimator</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#regression-discontinuity-estimators"><i class="fa fa-check"></i>Regression Discontinuity Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ittsraf.html"><a href="ittsraf.html"><i class="fa fa-check"></i><b>14</b> Introduction to Time Series Regression and Forecasting</a><ul>
<li class="chapter" data-level="14.1" data-path="ittsraf.html"><a href="ittsraf.html#using-regression-models-for-forecasting"><i class="fa fa-check"></i><b>14.1</b> Using Regression Models for Forecasting</a></li>
<li class="chapter" data-level="14.2" data-path="ittsraf.html"><a href="ittsraf.html#tsdasc"><i class="fa fa-check"></i><b>14.2</b> Time Series Data and Serial Correlation</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#notation-lags-differences-logarithms-and-growth-rates"><i class="fa fa-check"></i>Notation, Lags, Differences, Logarithms and Growth Rates</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ittsraf.html"><a href="ittsraf.html#autoregressions"><i class="fa fa-check"></i><b>14.3</b> Autoregressions</a><ul>
<li><a href="ittsraf.html#autoregressive-models-of-order-p">Autoregressive Models of Order <span class="math inline">\(p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ittsraf.html"><a href="ittsraf.html#cybtmpi"><i class="fa fa-check"></i><b>14.4</b> Can You Beat the Market? (Part I)</a></li>
<li class="chapter" data-level="14.5" data-path="ittsraf.html"><a href="ittsraf.html#apatadlm"><i class="fa fa-check"></i><b>14.5</b> Additional Predictors and The ADL Model</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#forecast-uncertainty-and-forecast-intervals"><i class="fa fa-check"></i>Forecast Uncertainty and Forecast Intervals</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="ittsraf.html"><a href="ittsraf.html#llsuic"><i class="fa fa-check"></i><b>14.6</b> Lag Length Selection Using Information Criteria</a></li>
<li class="chapter" data-level="14.7" data-path="ittsraf.html"><a href="ittsraf.html#nit"><i class="fa fa-check"></i><b>14.7</b> Nonstationarity I: Trends</a></li>
<li class="chapter" data-level="14.8" data-path="ittsraf.html"><a href="ittsraf.html#niib"><i class="fa fa-check"></i><b>14.8</b> Nonstationarity II: Breaks</a></li>
<li class="chapter" data-level="14.9" data-path="ittsraf.html"><a href="ittsraf.html#can-you-beat-the-market-part-ii"><i class="fa fa-check"></i><b>14.9</b> Can You Beat the Market? (Part II)</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="eodce.html"><a href="eodce.html"><i class="fa fa-check"></i><b>15</b> Estimation of Dynamic Causal Effects</a><ul>
<li class="chapter" data-level="15.1" data-path="eodce.html"><a href="eodce.html#the-orange-juice-data"><i class="fa fa-check"></i><b>15.1</b> The Orange Juice Data</a></li>
<li class="chapter" data-level="15.2" data-path="eodce.html"><a href="eodce.html#dynamic-causal-effects"><i class="fa fa-check"></i><b>15.2</b> Dynamic Causal Effects</a></li>
<li class="chapter" data-level="15.3" data-path="eodce.html"><a href="eodce.html#dynamic-multipliers-and-cumulative-dynamic-multipliers"><i class="fa fa-check"></i><b>15.3</b> Dynamic Multipliers and Cumulative Dynamic Multipliers</a></li>
<li class="chapter" data-level="15.4" data-path="eodce.html"><a href="eodce.html#hac-standard-errors"><i class="fa fa-check"></i><b>15.4</b> HAC Standard Errors</a></li>
<li class="chapter" data-level="15.5" data-path="eodce.html"><a href="eodce.html#estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors"><i class="fa fa-check"></i><b>15.5</b> Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors</a></li>
<li class="chapter" data-level="15.6" data-path="eodce.html"><a href="eodce.html#orange-juice-prices-and-cold-weather"><i class="fa fa-check"></i><b>15.6</b> Orange Juice Prices and Cold Weather</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="atitsr.html"><a href="atitsr.html"><i class="fa fa-check"></i><b>16</b> Additional Topics in Time Series Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="atitsr.html"><a href="atitsr.html#vector-autoregressions"><i class="fa fa-check"></i><b>16.1</b> Vector Autoregressions</a></li>
<li class="chapter" data-level="16.2" data-path="atitsr.html"><a href="atitsr.html#ooiatdfglsurt"><i class="fa fa-check"></i><b>16.2</b> Orders of Integration and the DF-GLS Unit Root Test</a></li>
<li class="chapter" data-level="16.3" data-path="atitsr.html"><a href="atitsr.html#cointegration"><i class="fa fa-check"></i><b>16.3</b> Cointegration</a></li>
<li class="chapter" data-level="16.4" data-path="atitsr.html"><a href="atitsr.html#volatility-clustering-and-autoregressive-conditional-heteroskedasticity"><i class="fa fa-check"></i><b>16.4</b> Volatility Clustering and Autoregressive Conditional Heteroskedasticity</a><ul>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#arch-and-garch-models"><i class="fa fa-check"></i>ARCH and GARCH Models</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#application-to-stock-price-volatility"><i class="fa fa-check"></i>Application to Stock Price Volatility</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#summary-8"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Econometrics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div class = rmdreview>
This book is in <b>Open Review</b>. We want your feedback to make the book better for you and other students. You may annotate some text by <span style="background-color: #3297FD; color: white">selecting it with the cursor</span> and then click the <i class="h-icon-annotate"></i> on the pop-up menu. You can also see the annotations of others: click the <i class="h-icon-chevron-left"></i> in the upper right hand corner of the page <i class="fa fa-arrow-circle-right  fa-rotate-315" aria-hidden="true"></i>
</div>
<div id="eaqe" class="section level1">
<h1><span class="header-section-number">13</span> Experiments and Quasi-Experiments</h1>
<p>This chapter discusses statistical tools that are commonly applied in program evaluation, where interest lies in measuring the causal effects of programs, policies or other interventions. An optimal research design for this purpose is what statisticians call an ideal randomized controlled experiment. The basic idea is to randomly assign subjects to two different groups, one that receives the treatment (the treatment group) and one that does not (the control group) and to compare outcomes for both groups in order to get an estimate of the average treatment effect.</p>
<p>Such <em>experimental</em> data is fundamentally different from <em>observational</em> data. For example, one might use a randomized controlled experiment to measure how much the performance of students in a standardized test differs between two classes where one has a “regular”&quot; student-teacher ratio and the other one has fewer students. The data produced by such an experiment would be different from, e.g., the observed cross-section data on the students’ performance used throughout Chapters <a href="lrwor.html#lrwor">4</a> to <a href="nrf.html#nrf">8</a> where class sizes are not randomly assigned to students but instead are the results of an economic decision where educational objectives and budgetary aspects were balanced.</p>
<p>For economists, randomized controlled experiments are often difficult or even indefeasible to implement. For example, due to ethic, moral and legal reasons it is practically impossible for a business owner to estimate the causal effect on the productivity of workers of setting them under psychological stress using an experiment where workers are randomly assigned either to the treatment group that is under time pressure or to the control group where work is under regular conditions, at best without knowledge of being in an experiment (see the box <em>The Hawthorne Effect</em> on p. 528 of the book).</p>
<p>However, sometimes external circumstances produce what is called a <em>quasi-experiment</em> or <em>natural experiment</em>. This “as if” randomness allows for estimation of causal effects that are of interest for economists using tools which are very similar to those valid for ideal randomized controlled experiments. These tools draw heavily on the theory of multiple regression and also on IV regression (see Chapter <a href="ivr.html#ivr">12</a>). We will review the core aspects of these methods and demonstrate how to apply them in R using the STAR data set (see the <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/10766">description</a> of the data set).</p>
<p>The following packages and their dependencies are needed for reproduction of the code chunks presented throughout this chapter:</p>
<ul>
<li><tt>AER</tt> <span class="citation">(Christian Kleiber &amp; Zeileis, <a href="#ref-R-AER">2017</a>)</span></li>
<li><tt>dplyr</tt> <span class="citation">(Wickham et al., <a href="#ref-R-dplyr">2018</a>)</span></li>
<li><tt>MASS</tt> <span class="citation">(Ripley, <a href="#ref-R-MASS">2018</a>)</span></li>
<li><tt>mvtnorm</tt> <span class="citation">(Genz et al., <a href="#ref-R-mvtnorm">2018</a>)</span></li>
<li><tt>rddtools</tt> <span class="citation">(Stigler &amp; Quast, <a href="#ref-R-rddtools">2015</a>)</span></li>
<li><tt>scales</tt> <span class="citation">(Wickham, <a href="#ref-R-scales">2017</a>)</span></li>
<li><tt>stargazer</tt><span class="citation">(Hlavac, <a href="#ref-R-stargazer">2018</a>)</span></li>
<li><tt>tidyr</tt> <span class="citation">(Wickham &amp; Henry, <a href="#ref-R-tidyr">2018</a>)</span></li>
</ul>
<p>Make sure the following code chunk runs without any errors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(AER)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(MASS)
<span class="kw">library</span>(mvtnorm)
<span class="kw">library</span>(rddtools)
<span class="kw">library</span>(scales)
<span class="kw">library</span>(stargazer)
<span class="kw">library</span>(tidyr)</code></pre></div>
<div id="potential-outcomes-causal-effects-and-idealized-experiments" class="section level2">
<h2><span class="header-section-number">13.1</span> Potential Outcomes, Causal Effects and Idealized Experiments</h2>
<p>We now briefly recap the idea of the average causal effect and how it can be estimated using the <em>differences estimator</em>. We advise you to work through Chapter 13.1 of the book for a better understanding.</p>
<div id="potential-outcomes-and-the-average-causal-effect" class="section level4 unnumbered">
<h4>Potential Outcomes and the average causal effect</h4>
<p>A <em>potential outcome</em> is the outcome for an individual under a potential treatment. For this individual, the causal effect of the treatment is the difference between the potential outcome if the individual receives the treatment and the potential outcome if she does not. Since this causal effect may be different for different individuals and it is not possible to measure the causal effect for a single individual, one is interested in studying the <em>average causal effect</em> of the treatment, hence also called the <em>average treatment effect</em>.</p>
<p>In an ideal randomized controlled experiment the following conditions are fulfilled:</p>
<ol style="list-style-type: decimal">
<li>The subjects are selected at random from the population.</li>
<li>The subjects are randomly assigned to treatment and control group.</li>
</ol>
<p>Condition 1 guarantees that the subjects’ potential outcomes are drawn randomly from the same distribution such that the expected value of the causal effect in the sample is equal to the average causal effect in the distribution. Condition 2 ensures that the receipt of treatment is independent from the subjects’ potential outcomes. If both conditions are fulfilled, the expected causal effect is the expected outcome in the treatment group minus the expected outcome in the control group. Using conditional expectations we have <span class="math display">\[\text{Average causal effect} =  E(Y_i\vert X_i=1) -  E(Y_i\vert X_i=0),\]</span> where <span class="math inline">\(X_i\)</span> is a binary treatment indicator.</p>
The average causal effect can be estimated using the <em>differences estimator</em>, which is nothing but the OLS estimator in the simple regression model
<span class="math display" id="eq:diffest">\[\begin{align}
  Y_i = \beta_0 + \beta_1 X_i + u_i \ \ , \ \ i=1,\dots,n, \tag{13.1}
\end{align}\]</span>
<p>where random assignment ensures that <span class="math inline">\(E(u_i\vert X_i) = 0\)</span>.</p>
The OLS estimator in the regression model
<span class="math display" id="eq:diffestwar">\[\begin{align}
  Y_i = \beta_0 + \beta_1 X_i + \beta_2 W_{1i} + \dots + \beta_{1+r} W_{ri} + u_i \ \ , \ \ i=1,\dots,n \tag{13.2}
\end{align}\]</span>
<p>with additional regressors <span class="math inline">\(W_1,\dots,W_r\)</span> is called the <em>differences estimator with additional regressors</em>. It is assumed that treatment <span class="math inline">\(X_i\)</span> is randomly assigned so that it is independent of the the pretreatment characteristic <span class="math inline">\(W_i\)</span>. This is assumption is called <em>conditional mean independence</em> and implies <span class="math display">\[E(u_i\vert X_i , W_i) = E(u_i\vert W_i) = 0,\]</span> so the conditional expectation of the error <span class="math inline">\(u_i\)</span> given the treatment indicator <span class="math inline">\(X_i\)</span> and the pretreatment characteristic <span class="math inline">\(W_i\)</span> does not depend on the <span class="math inline">\(X_i\)</span>. Conditional mean independence replaces the first least squares assumption in Key Concept 6.4 and thus ensures that the differences estimator of <span class="math inline">\(\beta_1\)</span> is unbiased. The <em>differences estimator with additional regressors</em> is more efficient than the <em>differences estimator</em> if the additional regressors explain some of the variation in the <span class="math inline">\(Y_i\)</span>.</p>
</div>
</div>
<div id="threats-to-validity-of-experiments" class="section level2">
<h2><span class="header-section-number">13.2</span> Threats to Validity of Experiments</h2>
<p>The concepts of internal and external validity discussed in Key Concept 9.1 are also applicable for studies based on experimental and quasi-experimental data. Chapter 13.2 of the book provides a thorough explanation of the particular threats to internal and external validity of experiments including examples. We limit ourselves to a short repetition of the threats listed there. Consult the book for a more detailed explanation.</p>
<div id="threats-to-internal-validity-1" class="section level4 unnumbered">
<h4>Threats to Internal Validity</h4>
<ol style="list-style-type: decimal">
<li><p><strong>Failure to Randomize</strong></p>
<p>If the subjects are not randomly assigned to the treatment group, then the outcomes will be contaminated with the effect of the subjects’ individual characteristics or preferences and it is not possible to obtain an unbiased estimate of the treatment effect. One can test for nonrandom assignment using a significance test (<span class="math inline">\(F\)</span>-Test) on the coefficients in the regression model <span class="math display">\[X_i = \beta_0 + \beta_1 W_{1i} + \dots +\beta_2 W_{ri} + u_i \ \ , \ \ i=1,\dots,n.\]</span></p></li>
<li><p><strong>Failure to Follow the Treatment Protocol</strong></p>
<p>If subjects do not follow the treatment protocol, i.e., some subjects in the treatment group manage to avoid receiving the treatment and/or some subjects in the control group manage to receive the treatment (<em>partial compliance</em>), there is correlation between <span class="math inline">\(X_i\)</span> und <span class="math inline">\(u_i\)</span> such that the OLS estimator of the average treatment effect will be biased. If there are data on <em>both</em> treatment actually recieved (<span class="math inline">\(X_i\)</span>) and initial random assignment (<span class="math inline">\(Z_i\)</span>), IV regression of the models <a href="eaqe.html#eq:diffest">(13.1)</a> and <a href="eaqe.html#eq:diffestwar">(13.2)</a> is a remedy.</p></li>
<li><p><strong>Attrition</strong></p>
<p>Attrition may result in a nonrandomly selected sample. If subjects systematically drop out of the study after beeing assigned to the control or the treatment group (systematic means that the reason of the dropout is related to the treatment) there will be correlation between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(u_i\)</span> and hence bias in the OLS estimator of the treatment effect.</p></li>
<li><p><strong>Experimental Effects</strong></p>
<p>If human subjects in treatment group and/or control group know that they are in an experiment, they might adapt their behaviour in a way that prevents unbiased estimation of the treatment effect.</p></li>
<li><p><strong>Small Sample Sizes</strong></p>
<p>As we know from the theory of linear regression, small sample sizes lead to imprecise estimation of the coefficients and thus imply imprecise estimation of the causal effect. Furthermore, confidence intervals and hypothesis test may produce wrong inference when the sample size is small.</p></li>
</ol>
</div>
<div id="threats-to-external-validity-1" class="section level4 unnumbered">
<h4>Threats to External Validity</h4>
<ol style="list-style-type: decimal">
<li><p><strong>Nonrepresentative Sample</strong></p>
<p>If the population studied and the population of interest are not sufficiently similar, there is no justification in generalizing the results.</p></li>
<li><p><strong>Nonrepresentative Program or Policy</strong></p>
<p>If the program or policy for the population studied differs considerably from the program (to be) applied to population(s) of interest, the results cannot be generalized. For example, a small-scale programm with low funding might have different effects than a widely available scaled-up program that is actually implemented. There are other factors like duration and the extent of monitoring that should be considered here.</p></li>
<li><p><strong>General Equilibrium Effects</strong></p>
<p>If market and/or environmental conditions cannot be kept constant when an internally valid program is implemented broadly, external validity may be doubtful.</p></li>
</ol>
</div>
</div>
<div id="experimental-estimates-of-the-effect-of-class-size-reductions" class="section level2">
<h2><span class="header-section-number">13.3</span> Experimental Estimates of the Effect of Class Size Reductions</h2>
<div id="experimental-design-and-the-data-set" class="section level3 unnumbered">
<h3>Experimental Design and the Data Set</h3>
<p>The Project <em>Student-Teacher Achievement Ratio</em> (STAR) was a large randomized controlled experiment with the aim of asserting whether a class size reduction is effective in improving education outcomes. It has been conducted in 80 Tennessee elementary schools over a period of four years during the 1980s by the State Department of Education.</p>
<p>In the first year, about 6400 students were randomly assigned into one of three interventions: small class (13 to 17 students per teacher), regular class (22 to 25 students per teacher), and regular-with-aide class (22 to 25 students with a full-time teacher’s aide). Teachers were also randomly assigned to the classes they taught. The interventions were initiated as the students entered school in kindergarten and continued through to third grade. Control and treatment groups across grades are summarized in Table <a href="eaqe.html#tab:starstructure">13.1</a>.</p>
<table>
<caption><span id="tab:starstructure">Table 13.1: </span> Control and treatment groups in the STAR experiment</caption>
<colgroup>
<col width="16%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>K</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Treatment 1</td>
<td>Small class</td>
<td>Small class</td>
<td>Small class</td>
<td>Small class</td>
</tr>
<tr class="even">
<td>Treatment 2</td>
<td>Regular class + aide</td>
<td>Regular class + aide</td>
<td>Regular class + aide</td>
<td>Regular class + aide</td>
</tr>
<tr class="odd">
<td>Control</td>
<td>Regular class</td>
<td>Regular class</td>
<td>Regular class</td>
<td>Regular class</td>
</tr>
</tbody>
</table>
<p>Each year, the students’ learning progress was assessed using the sum of the points scored on the math and reading parts of a standardized test (the <a href="https://en.wikipedia.org/wiki/Stanford_Achievement_Test_Series">Stanford Achievement Test</a>).</p>
<p>The STAR data set is part of the package <tt>AER</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the package AER and the STAR dataset</span>
<span class="kw">library</span>(AER)
<span class="kw">data</span>(STAR)</code></pre></div>
<p><tt>head(STAR)</tt> shows that there is a variety of factor variables that describe student and teacher characteristics as well as various school indicators, all of which are separately recorded for the four different grades. The data is in <em>wide format</em>. That is, each variable has its own column and for each student, the rows contain observations on these variables. Using <tt>dim(STAR)</tt> we find that there are a total of 11598 observations on 47 variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get an overview</span>
<span class="kw">head</span>(STAR, <span class="dv">2</span>)</code></pre></div>
<pre><code>##      gender ethnicity   birth stark star1 star2   star3 readk read1 read2
## 1122 female      afam 1979 Q3  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; regular    NA    NA    NA
## 1137 female      cauc 1980 Q1 small small small   small   447   507   568
##      read3 mathk math1 math2 math3   lunchk lunch1   lunch2 lunch3 schoolk
## 1122   580    NA    NA    NA   564     &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;   free    &lt;NA&gt;
## 1137   587   473   538   579   593 non-free   free non-free   free   rural
##      school1 school2  school3  degreek  degree1  degree2  degree3 ladderk
## 1122    &lt;NA&gt;    &lt;NA&gt; suburban     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt; bachelor    &lt;NA&gt;
## 1137   rural   rural    rural bachelor bachelor bachelor bachelor  level1
##      ladder1    ladder2    ladder3 experiencek experience1 experience2
## 1122    &lt;NA&gt;       &lt;NA&gt;     level1          NA          NA          NA
## 1137  level1 apprentice apprentice           7           7           3
##      experience3 tethnicityk tethnicity1 tethnicity2 tethnicity3 systemk
## 1122          30        &lt;NA&gt;        &lt;NA&gt;        &lt;NA&gt;        cauc    &lt;NA&gt;
## 1137           1        cauc        cauc        cauc        cauc      30
##      system1 system2 system3 schoolidk schoolid1 schoolid2 schoolid3
## 1122    &lt;NA&gt;    &lt;NA&gt;      22      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;        54
## 1137      30      30      30        63        63        63        63</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(STAR)</code></pre></div>
<pre><code>## [1] 11598    47</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get variable names</span>
<span class="kw">names</span>(STAR)</code></pre></div>
<pre><code>##  [1] &quot;gender&quot;      &quot;ethnicity&quot;   &quot;birth&quot;       &quot;stark&quot;       &quot;star1&quot;      
##  [6] &quot;star2&quot;       &quot;star3&quot;       &quot;readk&quot;       &quot;read1&quot;       &quot;read2&quot;      
## [11] &quot;read3&quot;       &quot;mathk&quot;       &quot;math1&quot;       &quot;math2&quot;       &quot;math3&quot;      
## [16] &quot;lunchk&quot;      &quot;lunch1&quot;      &quot;lunch2&quot;      &quot;lunch3&quot;      &quot;schoolk&quot;    
## [21] &quot;school1&quot;     &quot;school2&quot;     &quot;school3&quot;     &quot;degreek&quot;     &quot;degree1&quot;    
## [26] &quot;degree2&quot;     &quot;degree3&quot;     &quot;ladderk&quot;     &quot;ladder1&quot;     &quot;ladder2&quot;    
## [31] &quot;ladder3&quot;     &quot;experiencek&quot; &quot;experience1&quot; &quot;experience2&quot; &quot;experience3&quot;
## [36] &quot;tethnicityk&quot; &quot;tethnicity1&quot; &quot;tethnicity2&quot; &quot;tethnicity3&quot; &quot;systemk&quot;    
## [41] &quot;system1&quot;     &quot;system2&quot;     &quot;system3&quot;     &quot;schoolidk&quot;   &quot;schoolid1&quot;  
## [46] &quot;schoolid2&quot;   &quot;schoolid3&quot;</code></pre>
<p>A majority of the variable names contain a suffix (<tt>k</tt>, <tt>1</tt>, <tt>2</tt> or <tt>3</tt>) stating the grade which the respective variable is referring to. This facilitates regression analysis because it allows to adjust the <tt>formula</tt> argument in <tt>lm()</tt> for each grade by simply changing the variables’ suffixes accordingly.</p>
<p>The outcome produced by <tt>head()</tt> shows that some values recorded are <tt>NA</tt> and <tt><NA></tt>, i.e., there is no data on this variable for the student under consideration. This lies in the nature of the data: for example, take the first observation <code>STAR[1,]</code>.</p>
<p>In the output of <code>head(STAR, 2)</code> we find that the student entered the experiment in third grade in a regular class, which is why the class size is recorded in <tt>star3</tt> and the other class type indicator variables are <tt><NA></tt>. For the same reason there are no recordings of her math and reading score but for the third grade. It is straightforward to only get her non-<tt>NA</tt>/<tt><NA></tt> recordings: simply drop the <tt>NA</tt>s using <tt>!is.na()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># drop NA recordings for the first observation and print to the console</span>
STAR[<span class="dv">1</span>, <span class="op">!</span><span class="kw">is.na</span>(STAR[<span class="dv">1</span>, ])]</code></pre></div>
<pre><code>##      gender ethnicity   birth   star3 read3 math3 lunch3  school3  degree3
## 1122 female      afam 1979 Q3 regular   580   564   free suburban bachelor
##      ladder3 experience3 tethnicity3 system3 schoolid3
## 1122  level1          30        cauc      22        54</code></pre>
<p><code>is.na(STAR[1, ])</code> returns a logical vector with <tt>TRUE</tt> at positions that correspond to <tt><NA></tt> entries for the first observation. The <tt>!</tt> operator is used to invert the result such that we obtain only non-<tt><NA></tt> entries for the first observations.</p>
<p>In general it is not necessary to remove rows with missing data because <tt>lm()</tt> does so by default. Missing data may imply a small sample size and thus may lead to imprecise estimation and wrong inference This is, however, not an issue for the study at hand since, as we will see below, sample sizes lie beyond 5000 observations for each regression conducted.</p>
</div>
<div id="analysis-of-the-star-data" class="section level3 unnumbered">
<h3>Analysis of the STAR Data</h3>
As can be seen from Table <a href="eaqe.html#tab:starstructure">13.1</a> there are two treatment groups in each grade, small classes with only 13 to 17 students and regular classes with 22 to 25 students and a teaching aide. Thus, two binary variables, each being an indicator for the respective treatment group, are introduced for the differences estimator to capture the treatment effect for each treatment group separately. This yields the population regression model
<span class="math display" id="eq:starpopreg">\[\begin{align}
  Y_i = \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + u_i, \tag{13.3}
\end{align}\]</span>
<p>with test score <span class="math inline">\(Y_i\)</span>, the small class indicator <span class="math inline">\(SmallClass_i\)</span> and <span class="math inline">\(RegAide_i\)</span>, the indicator for a regular class with aide.</p>
<p>We reproduce the results presented in Table 13.1 of the book by performing the regression <a href="eaqe.html#eq:starpopreg">(13.3)</a> for each grade separately. For each student, the dependent variable is simply the sum of the points scored in the math and reading parts, constructed using <tt>I()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute differences Estimates for each grades</span>
fmk &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">I</span>(readk <span class="op">+</span><span class="st"> </span>mathk) <span class="op">~</span><span class="st"> </span>stark, <span class="dt">data =</span> STAR)
fm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">I</span>(read1 <span class="op">+</span><span class="st"> </span>math1) <span class="op">~</span><span class="st"> </span>star1, <span class="dt">data =</span> STAR)
fm2 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">I</span>(read2 <span class="op">+</span><span class="st"> </span>math2) <span class="op">~</span><span class="st"> </span>star2, <span class="dt">data =</span> STAR)
fm3 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">I</span>(read3 <span class="op">+</span><span class="st"> </span>math3) <span class="op">~</span><span class="st"> </span>star3, <span class="dt">data =</span> STAR)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># obtain coefficient matrix using robust standard errors</span>
<span class="kw">coeftest</span>(fmk, <span class="dt">vcov =</span> vcovHC, <span class="dt">type=</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##                    Estimate Std. Error  t value  Pr(&gt;|t|)    
## (Intercept)       918.04289    1.63339 562.0473 &lt; 2.2e-16 ***
## starksmall         13.89899    2.45409   5.6636 1.554e-08 ***
## starkregular+aide   0.31394    2.27098   0.1382    0.8901    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coeftest</span>(fm1, <span class="dt">vcov =</span> vcovHC, <span class="dt">type=</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##                    Estimate Std. Error  t value  Pr(&gt;|t|)    
## (Intercept)       1039.3926     1.7846 582.4321 &lt; 2.2e-16 ***
## star1small          29.7808     2.8311  10.5190 &lt; 2.2e-16 ***
## star1regular+aide   11.9587     2.6520   4.5093  6.62e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coeftest</span>(fm2, <span class="dt">vcov =</span> vcovHC, <span class="dt">type=</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##                    Estimate Std. Error  t value  Pr(&gt;|t|)    
## (Intercept)       1157.8066     1.8151 637.8820 &lt; 2.2e-16 ***
## star2small          19.3944     2.7117   7.1522  9.55e-13 ***
## star2regular+aide    3.4791     2.5447   1.3672    0.1716    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coeftest</span>(fm3, <span class="dt">vcov =</span> vcovHC, <span class="dt">type=</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##                     Estimate Std. Error  t value  Pr(&gt;|t|)    
## (Intercept)       1228.50636    1.68001 731.2483 &lt; 2.2e-16 ***
## star3small          15.58660    2.39604   6.5051 8.393e-11 ***
## star3regular+aide   -0.29094    2.27271  -0.1280    0.8981    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We gather the results and present them in a table using <tt>stargazer()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute robust standard errors for each model and gather them in a list</span>
rob_se_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(fmk, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
                 <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(fm1, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
                 <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(fm2, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
                 <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(fm2, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(stargazer)

<span class="kw">stargazer</span>(fmk,fm1,fm2,fm3,
  <span class="dt">title =</span> <span class="st">&quot;Project STAR: Differences Estimates&quot;</span>,
  <span class="dt">header =</span> <span class="ot">FALSE</span>, 
  <span class="dt">type =</span> <span class="st">&quot;latex&quot;</span>,
  <span class="dt">model.numbers =</span> F,
  <span class="dt">omit.table.layout =</span> <span class="st">&quot;n&quot;</span>,
  <span class="dt">digits =</span> <span class="dv">3</span>, 
  <span class="dt">column.labels =</span> <span class="kw">c</span>(<span class="st">&quot;K&quot;</span>, <span class="st">&quot;1&quot;</span>, <span class="st">&quot;2&quot;</span>, <span class="st">&quot;3&quot;</span>),
  <span class="dt">dep.var.caption  =</span> <span class="st">&quot;Dependent Variable: Grade&quot;</span>,
  <span class="dt">dep.var.labels.include =</span> <span class="ot">FALSE</span>,
  <span class="dt">se =</span> rob_se_<span class="dv">1</span>)</code></pre></div>



<table style="text-align:center"><tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"></td><td colspan="4">Dependent Variable: Grade</td></tr>
<tr><td></td><td colspan="4" style="border-bottom: 1px solid black"></td></tr>
<tr><td style="text-align:left"></td><td>K</td><td>1</td><td>2</td><td>3</td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">starksmall</td><td>13.90<sup>***</sup></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td>(2.45)</td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">starkregular+aide</td><td>0.31</td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td>(2.27)</td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">star1small</td><td></td><td>29.78<sup>***</sup></td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td>(2.83)</td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">star1regular+aide</td><td></td><td>11.96<sup>***</sup></td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td>(2.65)</td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">star2small</td><td></td><td></td><td>19.39<sup>***</sup></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td>(2.71)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">star2regular+aide</td><td></td><td></td><td>3.48</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td>(2.54)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">star3small</td><td></td><td></td><td></td><td>15.59</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">star3regular+aide</td><td></td><td></td><td></td><td>-0.29</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">Constant</td><td>918.04<sup>***</sup></td><td>1,039.39<sup>***</sup></td><td>1,157.81<sup>***</sup></td><td>1,228.51<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(1.63)</td><td>(1.78)</td><td>(1.82)</td><td>(1.82)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">Observations</td><td>5,786</td><td>6,379</td><td>6,049</td><td>5,967</td></tr>
<tr><td style="text-align:left">R<sup>2</sup></td><td>0.01</td><td>0.02</td><td>0.01</td><td>0.01</td></tr>
<tr><td style="text-align:left">Adjusted R<sup>2</sup></td><td>0.01</td><td>0.02</td><td>0.01</td><td>0.01</td></tr>
<tr><td style="text-align:left">Residual Std. Error</td><td>73.49 (df = 5783)</td><td>90.50 (df = 6376)</td><td>83.69 (df = 6046)</td><td>72.91 (df = 5964)</td></tr>
<tr><td style="text-align:left">F Statistic</td><td>21.26<sup>***</sup> (df = 2; 5783)</td><td>56.34<sup>***</sup> (df = 2; 6376)</td><td>28.71<sup>***</sup> (df = 2; 6046)</td><td>30.25<sup>***</sup> (df = 2; 5964)</td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr></table>
<caption><p style='text-align:center'><span id="tab:psde">Table 13.2: </span> Project STAR - Differences Estimates</p></caption>


<p>The estimates presented in Table <a href="eaqe.html#tab:psde">13.2</a> suggest that the class size reduction improves student performance. Except for grade 1, the estimates of the coefficient on <span class="math inline">\(SmallClass\)</span> are roughly of the same magnitude (the estimates lie between 13.90 and 19.39 points) and they are statistically significant at <span class="math inline">\(1\%\)</span>. Furthermore, a teaching aide has little, possibly zero, effect on the performance of the students.</p>
<p>Following the book, we augment the regression model <a href="eaqe.html#eq:starpopreg">(13.3)</a> by different sets of regressors for two reasons:</p>
<ol style="list-style-type: decimal">
<li>If the additional regressors explain some of the observed variation in the dependent variable, we obtain more efficient estimates of the coefficients of interest.</li>
<li>If the treatment is not received at random due to failures to follow the treatment protocol (see Chapter 13.3 of the book), the estimates obtained using <a href="eaqe.html#eq:starpopreg">(13.3)</a> may be biased. Adding additional regressors may solve or mitigate this problem.</li>
</ol>
<p>In particular, we consider the following student and teacher characteristics</p>
<ul>
<li><span class="math inline">\(experience\)</span> — Teacher’s years of experience</li>
<li><span class="math inline">\(boy\)</span> — Student is a boy (dummy)</li>
<li><span class="math inline">\(lunch\)</span> — Free lunch eligibility (dummy)</li>
<li><span class="math inline">\(black\)</span> — Student is African-American (dummy)</li>
<li><span class="math inline">\(race\)</span> — Student’s race is other than black or white (dummy)</li>
<li><span class="math inline">\(\text{schoolid}\)</span> — School indicator variables</li>
</ul>
in the four population regression specifications
<span class="math display" id="eq:augstarpopreg3" id="eq:augstarpopreg2" id="eq:augstarpopreg1">\[\begin{align}
Y_i =&amp; \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + u_i, \tag{13.4} \\
Y_i =&amp; \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + \beta_3 experience_i + u_i, \tag{13.5} \\
Y_i =&amp; \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + \beta_3 experience_i + schoolid + u_i, \tag{13.6}
\end{align}\]</span>
and
<span class="math display" id="eq:augstarpopreg4">\[\begin{align}
Y_i =&amp; \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + \beta_3 experience_i + \beta_4 boy + \beta_5 lunch \\ 
&amp; + \beta_6 black + \beta_7 race + schoolid + u_i. \tag{13.7}
\end{align}\]</span>
<p>Prior to estimation, we perform some subsetting and data wrangling using functions from the packages <tt>dplyr</tt> and <tt>tidyr</tt>. These are both part of <tt>tidyverse</tt>, a collection of <tt>R</tt> packages designed for data science and handling big datasets (see the <a href="https://www.tidyverse.org/">official site</a> for more on <tt>tidyverse</tt> packages). The functions <tt>%&gt;%</tt>, <tt>transmute()</tt> and <tt>mutate()</tt> are sufficient for us here:</p>
<ul>
<li><tt>%&gt;%</tt> allows to chain function calls.</li>
<li><tt>transmute()</tt> allows to subset the data set by naming the variables to be kept.</li>
<li><tt>mutate()</tt> is convenient for adding new variables based on existing ones while preserving the latter.</li>
</ul>
<p>The regression models <a href="eaqe.html#eq:augstarpopreg1">(13.4)</a> to <a href="eaqe.html#eq:augstarpopreg4">(13.7)</a> require the variables <tt>gender</tt>, <tt>ethnicity</tt>, <tt>stark</tt>, <tt>readk</tt>, <tt>mathk</tt>, <tt>lunchk</tt>, <tt>experiencek</tt> and <tt>schoolidk</tt>. After dropping the remaining variables using <tt>transmute()</tt>, we use <tt>mutate()</tt> to add three additional binary variables which are derivatives of existing ones: <tt>black</tt>, <tt>race</tt> and <tt>boy</tt>. They are generated using logical statements within the function <tt>ifelse()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load packages &#39;dplyr&#39; and &#39;tidyr&#39; for data wrangling functionalities</span>
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(tidyr)

<span class="co"># generate subset with kindergarten data</span>
STARK &lt;-<span class="st"> </span>STAR <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">transmute</span>(gender,
                ethnicity,
                stark,
                readk,
                mathk,
                lunchk,
                experiencek,
                schoolidk) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">mutate</span>(<span class="dt">black =</span> <span class="kw">ifelse</span>(ethnicity <span class="op">==</span><span class="st"> &quot;afam&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>),
             <span class="dt">race =</span> <span class="kw">ifelse</span>(ethnicity <span class="op">==</span><span class="st"> &quot;afam&quot;</span> <span class="op">|</span><span class="st"> </span>ethnicity <span class="op">==</span><span class="st"> &quot;cauc&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>),
             <span class="dt">boy =</span> <span class="kw">ifelse</span>(gender <span class="op">==</span><span class="st"> &quot;male&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the models </span>
gradeK1 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">I</span>(mathk <span class="op">+</span><span class="st"> </span>readk) <span class="op">~</span><span class="st"> </span>stark <span class="op">+</span><span class="st"> </span>experiencek, 
              <span class="dt">data =</span> STARK)

gradeK2 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">I</span>(mathk <span class="op">+</span><span class="st"> </span>readk) <span class="op">~</span><span class="st"> </span>stark <span class="op">+</span><span class="st"> </span>experiencek <span class="op">+</span><span class="st"> </span>schoolidk, 
              <span class="dt">data =</span> STARK)

gradeK3 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">I</span>(mathk <span class="op">+</span><span class="st"> </span>readk) <span class="op">~</span><span class="st"> </span>stark <span class="op">+</span><span class="st"> </span>experiencek <span class="op">+</span><span class="st"> </span>boy <span class="op">+</span><span class="st"> </span>lunchk 
              <span class="op">+</span><span class="st"> </span>black <span class="op">+</span><span class="st"> </span>race <span class="op">+</span><span class="st"> </span>schoolidk, 
              <span class="dt">data =</span> STARK)</code></pre></div>
<p>For brevity, we exclude the coefficients for the indicator dummies in <tt>coeftest()</tt>’s output by subsetting the matrices.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># obtain robust inference on the significance of coefficients</span>
<span class="kw">coeftest</span>(gradeK1, <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##                    Estimate Std. Error  t value  Pr(&gt;|t|)    
## (Intercept)       904.72124    2.22235 407.1020 &lt; 2.2e-16 ***
## starksmall         14.00613    2.44704   5.7237 1.095e-08 ***
## starkregular+aide  -0.60058    2.25430  -0.2664    0.7899    
## experiencek         1.46903    0.16929   8.6778 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coeftest</span>(gradeK2, <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, ]</code></pre></div>
<pre><code>##                      Estimate Std. Error     t value     Pr(&gt;|t|)
## (Intercept)       925.6748750  7.6527218 120.9602155 0.000000e+00
## starksmall         15.9330822  2.2411750   7.1092540 1.310324e-12
## starkregular+aide   1.2151960  2.0353415   0.5970477 5.504993e-01
## experiencek         0.7431059  0.1697619   4.3773429 1.222880e-05</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coeftest</span>(gradeK3, <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>, ]</code></pre></div>
<pre><code>##                      Estimate Std. Error     t value     Pr(&gt;|t|)
## (Intercept)       937.6831330 14.3726687  65.2407117 0.000000e+00
## starksmall         15.8900507  2.1551817   7.3729516 1.908960e-13
## starkregular+aide   1.7869378  1.9614592   0.9110247 3.623211e-01
## experiencek         0.6627251  0.1659298   3.9940097 6.578846e-05
## boy               -12.0905123  1.6726331  -7.2284306 5.533119e-13
## lunchkfree        -34.7033021  1.9870366 -17.4648529 1.437931e-66
## black             -25.4305130  3.4986918  -7.2685776 4.125252e-13</code></pre>
<p>We now use <tt>stargazer()</tt> to gather all relevant information in a structured table.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute robust standard errors for each model and gather them in a list</span>
rob_se_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(fmk, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
                 <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(gradeK1, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
                 <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(gradeK2, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
                 <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(gradeK3, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stargazer</span>(fmk, fm1, fm2, fm3,
  <span class="dt">title =</span> <span class="st">&quot;Project STAR - Differences Estimates with </span>
<span class="st">  Additional Regressors for Kindergarten&quot;</span>,
  <span class="dt">header =</span> <span class="ot">FALSE</span>, 
  <span class="dt">type =</span> <span class="st">&quot;latex&quot;</span>,
  <span class="dt">model.numbers =</span> F,
  <span class="dt">omit.table.layout =</span> <span class="st">&quot;n&quot;</span>,
  <span class="dt">digits =</span> <span class="dv">3</span>, 
  <span class="dt">column.labels =</span> <span class="kw">c</span>(<span class="st">&quot;(1)&quot;</span>, <span class="st">&quot;(2)&quot;</span>, <span class="st">&quot;(3)&quot;</span>, <span class="st">&quot;(4)&quot;</span>),
  <span class="dt">dep.var.caption  =</span> <span class="st">&quot;Dependent Variable: Test Score in Kindergarten&quot;</span>,
  <span class="dt">dep.var.labels.include =</span> <span class="ot">FALSE</span>,
  <span class="dt">se =</span> rob_se_<span class="dv">2</span>) </code></pre></div>



<table style="text-align:center"><tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"></td><td colspan="4">Dependent Variable: Test Score in Kindergarten</td></tr>
<tr><td></td><td colspan="4" style="border-bottom: 1px solid black"></td></tr>
<tr><td style="text-align:left"></td><td>(1)</td><td>(2)</td><td>(3)</td><td>(4)</td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">starksmall</td><td>13.899<sup>***</sup></td><td>14.006<sup>***</sup></td><td>15.933<sup>***</sup></td><td>15.890<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(2.454)</td><td>(2.447)</td><td>(2.241)</td><td>(2.155)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">starkregular+aide</td><td>0.314</td><td>-0.601</td><td>1.215</td><td>1.787</td></tr>
<tr><td style="text-align:left"></td><td>(2.271)</td><td>(2.254)</td><td>(2.035)</td><td>(1.961)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">experiencek</td><td></td><td>1.469<sup>***</sup></td><td>0.743<sup>***</sup></td><td>0.663<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td>(0.169)</td><td>(0.170)</td><td>(0.166)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">boy</td><td></td><td></td><td></td><td>-12.091<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td>(1.673)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">lunchkfree</td><td></td><td></td><td></td><td>-34.703<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td>(1.987)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">black</td><td></td><td></td><td></td><td>-25.431<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td>(3.499)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">race</td><td></td><td></td><td></td><td>8.501</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td>(12.520)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">Constant</td><td>918.043<sup>***</sup></td><td>904.721<sup>***</sup></td><td>925.675<sup>***</sup></td><td>937.683<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(1.633)</td><td>(2.222)</td><td>(7.653)</td><td>(14.373)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">School indicators?</td><td>no</td><td>no</td><td>yes</td><td>yes</td></tr>
<tr><td style="text-align:left">Observations</td><td>5,786</td><td>5,766</td><td>5,766</td><td>5,748</td></tr>
<tr><td style="text-align:left">R<sup>2</sup></td><td>0.007</td><td>0.020</td><td>0.234</td><td>0.291</td></tr>
<tr><td style="text-align:left">Adjusted R<sup>2</sup></td><td>0.007</td><td>0.020</td><td>0.223</td><td>0.281</td></tr>
<tr><td style="text-align:left">Residual Std. Error</td><td>73.490 (df = 5783)</td><td>73.085 (df = 5762)</td><td>65.075 (df = 5684)</td><td>62.663 (df = 5662)</td></tr>
<tr><td style="text-align:left">F Statistic</td><td>21.263<sup>***</sup> (df = 2; 5783)</td><td>39.861<sup>***</sup> (df = 3; 5762)</td><td>21.413<sup>***</sup> (df = 81; 5684)</td><td>27.364<sup>***</sup> (df = 85; 5662)</td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr></table>
<caption><p style='text-align:center'><span id="tab:psdewarfk">Table 13.3: </span> Project STAR - Differences Estimates with Additional Regressors for Kindergarten</p></caption>


<p>The results in column (1) of Table <a href="eaqe.html#tab:psdewarfk">13.3</a> just a repeat the results obtained for <a href="eaqe.html#eq:starpopreg">(13.3)</a>. Columns (2) to (4) reveal that adding student characteristics and school fixed effects does not lead to substantially different estimates of the treatment effects. This result makes it more plausible that the estimates of the effects obtained using model <a href="eaqe.html#eq:starpopreg">(13.3)</a> do not suffer from failure of random assignment. There is some decrease in the standard errors and some increase in <span class="math inline">\(\bar{R}^2\)</span>, implying that the estimates are more precise.</p>
<p>Because teachers were randomly assigned to classes, inclusion of school fixed effect allows us to estimate the causal effect of a teacher’s experience on test scores of students in kindergarten. Regression (3) predicts the average effect of 10 years experience on test scores to be <span class="math inline">\(10\cdot 0.74=7.4\)</span> points. Be aware that the other estimates on student characteristics in regression (4) <em>do not</em> have causal interpretation due to nonrandom assignment (see Chapter 13.3 of the book for a detailed discussion).</p>
<p>Are the estimated effects presented in Table <a href="eaqe.html#tab:psdewarfk">13.3</a> large or small in a practical sense? Let us translate the predicted changes in test scores to units of standard deviation in order to allow for a comparison (see Section <a href="asbomr.html#etsacs">9.4</a> for a similar argument).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the sample standard deviations of test scores</span>
SSD &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;K&quot;</span> =<span class="st"> </span><span class="kw">sd</span>(<span class="kw">na.omit</span>(STAR<span class="op">$</span>readk <span class="op">+</span><span class="st"> </span>STAR<span class="op">$</span>mathk)),
         <span class="st">&quot;1&quot;</span> =<span class="st"> </span><span class="kw">sd</span>(<span class="kw">na.omit</span>(STAR<span class="op">$</span>read1 <span class="op">+</span><span class="st"> </span>STAR<span class="op">$</span>math1)),
         <span class="st">&quot;2&quot;</span> =<span class="st"> </span><span class="kw">sd</span>(<span class="kw">na.omit</span>(STAR<span class="op">$</span>read2 <span class="op">+</span><span class="st"> </span>STAR<span class="op">$</span>math2)),
         <span class="st">&quot;3&quot;</span> =<span class="st"> </span><span class="kw">sd</span>(<span class="kw">na.omit</span>(STAR<span class="op">$</span>read3 <span class="op">+</span><span class="st"> </span>STAR<span class="op">$</span>math3)))

<span class="co"># translate the effects of small classes to standard deviations</span>
Small &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;K&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">coef</span>(fmk)[<span class="dv">2</span>]<span class="op">/</span>SSD[<span class="dv">1</span>]),
           <span class="st">&quot;1&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">coef</span>(fm1)[<span class="dv">2</span>]<span class="op">/</span>SSD[<span class="dv">2</span>]),
           <span class="st">&quot;2&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">coef</span>(fm2)[<span class="dv">2</span>]<span class="op">/</span>SSD[<span class="dv">3</span>]),
           <span class="st">&quot;3&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">coef</span>(fm3)[<span class="dv">2</span>]<span class="op">/</span>SSD[<span class="dv">4</span>]))

<span class="co"># adjust the standard errors</span>
SmallSE &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;K&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(rob_se_<span class="dv">1</span>[[<span class="dv">1</span>]][<span class="dv">2</span>]<span class="op">/</span>SSD[<span class="dv">1</span>]),
             <span class="st">&quot;1&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(rob_se_<span class="dv">1</span>[[<span class="dv">2</span>]][<span class="dv">2</span>]<span class="op">/</span>SSD[<span class="dv">2</span>]),
             <span class="st">&quot;2&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(rob_se_<span class="dv">1</span>[[<span class="dv">3</span>]][<span class="dv">2</span>]<span class="op">/</span>SSD[<span class="dv">3</span>]),
             <span class="st">&quot;3&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(rob_se_<span class="dv">1</span>[[<span class="dv">4</span>]][<span class="dv">2</span>]<span class="op">/</span>SSD[<span class="dv">4</span>]))

<span class="co"># translate the effects of regular classes with aide to standard deviations</span>
RegAide&lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;K&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">coef</span>(fmk)[<span class="dv">3</span>]<span class="op">/</span>SSD[<span class="dv">1</span>]),
            <span class="st">&quot;1&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">coef</span>(fm1)[<span class="dv">3</span>]<span class="op">/</span>SSD[<span class="dv">2</span>]),
            <span class="st">&quot;2&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">coef</span>(fm2)[<span class="dv">3</span>]<span class="op">/</span>SSD[<span class="dv">3</span>]),
            <span class="st">&quot;3&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">coef</span>(fm3)[<span class="dv">3</span>]<span class="op">/</span>SSD[<span class="dv">4</span>]))

<span class="co"># adjust the standard errors</span>
RegAideSE &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;K&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(rob_se_<span class="dv">1</span>[[<span class="dv">1</span>]][<span class="dv">3</span>]<span class="op">/</span>SSD[<span class="dv">1</span>]),
               <span class="st">&quot;1&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(rob_se_<span class="dv">1</span>[[<span class="dv">2</span>]][<span class="dv">3</span>]<span class="op">/</span>SSD[<span class="dv">2</span>]),
               <span class="st">&quot;2&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(rob_se_<span class="dv">1</span>[[<span class="dv">3</span>]][<span class="dv">3</span>]<span class="op">/</span>SSD[<span class="dv">3</span>]),
               <span class="st">&quot;3&quot;</span> =<span class="st"> </span><span class="kw">as.numeric</span>(rob_se_<span class="dv">1</span>[[<span class="dv">4</span>]][<span class="dv">3</span>]<span class="op">/</span>SSD[<span class="dv">4</span>]))

<span class="co"># gather the results in a data.frame and round</span>
df &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">round</span>(<span class="kw">data.frame</span>(
                        Small, SmallSE, RegAide, RegAideSE, SSD),
                        <span class="dt">digits =</span>  <span class="dv">2</span>))</code></pre></div>
<p>It is fairly easy to turn the <tt>data.frame</tt> <tt>df</tt> into a table.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate a simple table using stargazer</span>
<span class="kw">stargazer</span>(df,
          <span class="dt">title =</span> <span class="st">&quot;Estimated Class Size Effects </span>
<span class="st">          (in Units of Standard Deviations)&quot;</span>,
          <span class="dt">type =</span> <span class="st">&quot;html&quot;</span>, 
          <span class="dt">summary =</span> <span class="ot">FALSE</span>,
          <span class="dt">header =</span> <span class="ot">FALSE</span>
          )</code></pre></div>



<table style="text-align:center"><tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"></td><td>K</td><td>1</td><td>2</td><td>3</td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">Small</td><td>0.190</td><td>0.330</td><td>0.230</td><td>0.210</td></tr>
<tr><td style="text-align:left">SmallSE</td><td>0.030</td><td>0.030</td><td>0.030</td><td>0.040</td></tr>
<tr><td style="text-align:left">RegAide</td><td>0</td><td>0.130</td><td>0.040</td><td>0</td></tr>
<tr><td style="text-align:left">RegAideSE</td><td>0.030</td><td>0.030</td><td>0.030</td><td>0.030</td></tr>
<tr><td style="text-align:left">SSD</td><td>73.750</td><td>91.280</td><td>84.080</td><td>73.270</td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr></table>
<caption><p style='text-align:center'><span id="tab:ecse">Table 13.4: </span> Estimated Class Size Effects 
          (in Units of Standard Deviations)</p></caption>


<p>The estimated effect of a small classes is largest for grade 1. As pointed out in the book, this is probably because students in the control group for grade 1 did poorly on the test for some unknown reason or simply due to random variation. The difference between the estimated effect of being in a small class and being in a regular classes with an aide is roughly 0.2 standard deviations for all grades. This leads to the conclusion that the effect of being in a regular sized class with an aide is zero and the effect of being in a small class is roughly the same for all grades.</p>
<p>The remainder of Chapter 13.3 in the book discusses to what extent these experimental estimates are comparable with observational estimates obtained using data on school districts in California and Massachusetts in Chapter <a href="asbomr.html#asbomr">9</a>. It turns out that the estimates are indeed very similar. Please refer to the aforementioned section in the book for a more detailed discussion.</p>
</div>
</div>
<div id="quasi-experiments" class="section level2">
<h2><span class="header-section-number">13.4</span> Quasi Experiments</h2>
<p>In quasi-experiments, “as if” randomness is exploited to use methods similar to those that have been discussed in the previous chapter. There are two types of quasi-experiments:<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<ol style="list-style-type: decimal">
<li><p>Random variations in individual circumstances allow to view the treatment “as if” it was randomly determined.</p></li>
<li><p>The treatment is only partially determined by “as if” random variation.</p></li>
</ol>
<p>The former allows to estimate the effect using either model <a href="eaqe.html#eq:diffestwar">(13.2)</a>, i.e., the <em>difference estimator with additional regressors</em>, or, if there is doubt that the “as if” randomness does not entirely ensure that there are no systematic differences between control and treatment group, using the <em>differences-in-differences</em> (DID) estimator. In the latter case, an IV approach for estimation of a model like <a href="eaqe.html#eq:diffestwar">(13.2)</a> which uses the source of “as if” randomness in treatment assignment as the instrument may be applied.</p>
<p>Some more advanced techniques that are helpful in settings where the treatment assignment is (partially) determined by a threshold in a so-called running variable are <em>sharp regression discontinuity design</em> (RDD) and <em>fuzzy regression discontinuity design</em> (FRDD).</p>
<p>We briefly review these techniques and, since the book does not provide any empirical examples in this section, we will use our own simulated data in a minimal example to discuss how DID, RDD and FRDD can be applied in <tt>R</tt>.</p>
<div id="the-differences-in-differences-estimator" class="section level3 unnumbered">
<h3>The Differences-in-Differences Estimator</h3>
<p>In quasi-experiments the source of “as if” randomness in treatment assignment can often not entirely prevent systematic differences between control and treatment groups. This problem was encountered by <span class="citation">Card &amp; Krueger (<a href="#ref-card1994">1994</a>)</span> who use geography as the “as if” random treatment assignment to study the effect on employment in fast-food restaurants caused by an increase in the state minimum wage in New Jersey in the year of 1992. Their idea was to use the fact that the increase in minimum wage applied to employees in New Jersey (treatment group) but not to those living in neighboring Pennsylvania (control group).</p>
<p>It is quite conceivable that such a wage hike is not correlated with other determinants of employment. However, there still might be some state-specific differences and thus differences between control and treatment group. This would render the <em>differences estimator</em> biased and inconsistent. <span class="citation">Card &amp; Krueger (<a href="#ref-card1994">1994</a>)</span> solved this by using a DID estimator: they collected data in February 1992 (before the treatment) and November 1992 (after the treatment) for the same restaurants and estimated the effect of the wage hike by analyzing differences in the differences in employment for New Jersey and Pennsylvania before and after the increase.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> The DID estimator is</p>
<span class="math display" id="eq:DID">\[\begin{align}
  \widehat{\beta}_1^{\text{diffs-in-diffs}} =&amp; \, (\overline{Y}^{\text{treatment,after}} - \overline{Y}^{\text{treatment,before}}) - (\overline{Y}^{\text{control,after}} - \overline{Y}^{\text{control,before}}) \\
  =&amp; \Delta \overline{Y}^{\text{treatment}} - \Delta \overline{Y}^{\text{control}} \tag{13.8}
\end{align}\]</span>
<p>with</p>
<ul>
<li><p><span class="math inline">\(\overline{Y}^{\text{treatment,before}}\)</span> - the sample average in the treatment group before the treatment</p></li>
<li><p><span class="math inline">\(\overline{Y}^{\text{treatment,after}}\)</span> - the sample average in the treatment group after the treatment</p></li>
<li><p><span class="math inline">\(\overline{Y}^{\text{treatment,before}}\)</span> - the sample average in the control group before the treatment</p></li>
<li><p><span class="math inline">\(\overline{Y}^{\text{treatment,after}}\)</span> - the sample average in the control group after the treatment.</p></li>
</ul>
<p>We now use <tt>R</tt> to reproduce Figure 13.1 of the book.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize plot and add control group</span>
<span class="kw">plot</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">6</span>, <span class="dv">8</span>), 
     <span class="dt">type =</span> <span class="st">&quot;p&quot;</span>,
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">12</span>),
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.3</span>, <span class="fl">1.3</span>),
     <span class="dt">main =</span> <span class="st">&quot;The Differences-in-Differences Estimator&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Period&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">xaxt =</span> <span class="st">&quot;n&quot;</span>,
     <span class="dt">yaxt =</span> <span class="st">&quot;n&quot;</span>)

<span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;before&quot;</span>, <span class="st">&quot;after&quot;</span>))
<span class="kw">axis</span>(<span class="dv">2</span>, <span class="dt">at =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">13</span>))

<span class="co"># add treatment group</span>
<span class="kw">points</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">11</span>), 
       <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>,
       <span class="dt">pch =</span> <span class="dv">20</span>)

<span class="co"># add line segments</span>
<span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">11</span>), <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)
<span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">6</span>, <span class="dv">8</span>), <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>)
<span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">9</span>), <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">9</span>, <span class="dv">11</span>), <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)

<span class="co"># add annotations</span>
<span class="kw">text</span>(<span class="dv">1</span>, <span class="dv">10</span>, <span class="kw">expression</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>]<span class="op">^</span>{DID}), <span class="dt">cex =</span> <span class="fl">0.8</span>, <span class="dt">pos =</span> <span class="dv">4</span>)
<span class="kw">text</span>(<span class="dv">0</span>, <span class="fl">5.5</span>, <span class="st">&quot;s. mean control&quot;</span>, <span class="dt">cex =</span> <span class="fl">0.8</span> , <span class="dt">pos =</span> <span class="dv">4</span>)
<span class="kw">text</span>(<span class="dv">0</span>, <span class="fl">6.8</span>, <span class="st">&quot;s. mean treatment&quot;</span>, <span class="dt">cex =</span> <span class="fl">0.8</span> , <span class="dt">pos =</span> <span class="dv">4</span>)
<span class="kw">text</span>(<span class="dv">1</span>, <span class="fl">7.9</span>, <span class="st">&quot;s. mean control&quot;</span>, <span class="dt">cex =</span> <span class="fl">0.8</span> , <span class="dt">pos =</span> <span class="dv">4</span>)
<span class="kw">text</span>(<span class="dv">1</span>, <span class="fl">11.1</span>, <span class="st">&quot;s. mean treatment&quot;</span>, <span class="dt">cex =</span> <span class="fl">0.8</span> , <span class="dt">pos =</span> <span class="dv">4</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-548-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
The DID estimator <a href="eaqe.html#eq:DID">(13.8)</a> can also be written in regression notation: <span class="math inline">\(\widehat{\beta}_1^{\text{DID}}\)</span> is the OLS estimator of <span class="math inline">\(\beta_1\)</span> in
<span class="math display" id="eq:did">\[\begin{align}
  \Delta Y_i = \beta_0 + \beta_1 X_i + u_i, \tag{13.9}
\end{align}\]</span>
<p>where <span class="math inline">\(\Delta Y_i\)</span> denotes the difference in pre- and post-treatment outcomes of individual <span class="math inline">\(i\)</span> and <span class="math inline">\(X_i\)</span> is the treatment indicator.</p>
Adding additional regressors that measure pre-treatment characteristics to <a href="eaqe.html#eq:did">(13.9)</a> we obtain
<span class="math display" id="eq:didwar">\[\begin{align}
  \Delta Y_i = \beta_0 + \beta_1 X_i + \beta_2 W_{1i} + \dots + \beta_{1+r} W_{ri} + u_i, \tag{13.10}
\end{align}\]</span>
<p>the <em>difference-in-differences estimator</em> with additional regressors. The additional regressors may lead to a more precise estimate of <span class="math inline">\(\beta_1\)</span>.</p>
<p>We keep things simple and focus on estimation of the treatment effect using DID in the simplest case, that is a control and a treatment group observed for two time periods — one before and one after the treatment. In particular, we will see that there are three different ways to proceed.</p>
<p>First, we simulate pre- and post-treatment data using <tt>R</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set sample size</span>
n &lt;-<span class="st"> </span><span class="dv">200</span>

<span class="co"># define treatment effect</span>
TEffect &lt;-<span class="st"> </span><span class="dv">4</span>

<span class="co"># generate treatment dummy</span>
TDummy &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">/</span><span class="dv">2</span>), <span class="kw">rep</span>(<span class="dv">1</span>, n<span class="op">/</span><span class="dv">2</span>))

<span class="co"># simulate pre- and post-treatment values of the dependent variable</span>
y_pre &lt;-<span class="st"> </span><span class="dv">7</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n)
y_pre[<span class="dv">1</span><span class="op">:</span>n<span class="op">/</span><span class="dv">2</span>] &lt;-<span class="st"> </span>y_pre[<span class="dv">1</span><span class="op">:</span>n<span class="op">/</span><span class="dv">2</span>] <span class="op">-</span><span class="st"> </span><span class="dv">1</span>
y_post &lt;-<span class="st"> </span><span class="dv">7</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>TEffect <span class="op">*</span><span class="st"> </span>TDummy <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n)
y_post[<span class="dv">1</span><span class="op">:</span>n<span class="op">/</span><span class="dv">2</span>] &lt;-<span class="st"> </span>y_post[<span class="dv">1</span><span class="op">:</span>n<span class="op">/</span><span class="dv">2</span>] <span class="op">-</span><span class="st"> </span><span class="dv">1</span> </code></pre></div>
<p>Next plot the data. The function <tt>jitter()</tt> is used to add some artificial dispersion in the horizontal component of the points so that there is less overplotting. The function <tt>alpha()</tt> from the package <tt>scales</tt> allows to adjust the opacity of colors used in plots.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(scales)

pre &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(y_pre[TDummy<span class="op">==</span><span class="dv">0</span>]))
post &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">length</span>(y_pre[TDummy<span class="op">==</span><span class="dv">0</span>]))

<span class="co"># plot control group in t=1</span>
<span class="kw">plot</span>(<span class="kw">jitter</span>(pre, <span class="fl">0.6</span>), 
     y_pre[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">0</span>], 
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">16</span>), 
     <span class="dt">col =</span> <span class="kw">alpha</span>(<span class="st">&quot;steelblue&quot;</span>, <span class="fl">0.3</span>),
     <span class="dt">pch =</span> <span class="dv">20</span>, 
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>),
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Period&quot;</span>,
     <span class="dt">xaxt =</span> <span class="st">&quot;n&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Artificial Data for DID Estimation&quot;</span>)

<span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;before&quot;</span>, <span class="st">&quot;after&quot;</span>))

<span class="co"># add treatment group in t=1</span>
<span class="kw">points</span>(<span class="kw">jitter</span>(pre, <span class="fl">0.6</span>), 
       y_pre[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">1</span>], 
       <span class="dt">col =</span> <span class="kw">alpha</span>(<span class="st">&quot;darkred&quot;</span>, <span class="fl">0.3</span>), 
       <span class="dt">pch =</span> <span class="dv">20</span>)

<span class="co"># add control group in t=2</span>
<span class="kw">points</span>(<span class="kw">jitter</span>(post, <span class="fl">0.6</span>),
       y_post[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">0</span>], 
       <span class="dt">col =</span> <span class="kw">alpha</span>(<span class="st">&quot;steelblue&quot;</span>, <span class="fl">0.5</span>),
       <span class="dt">pch =</span> <span class="dv">20</span>)

<span class="co"># add treatment group in t=2</span>
<span class="kw">points</span>(<span class="kw">jitter</span>(post, <span class="fl">0.6</span>), 
       y_post[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">1</span>], 
       <span class="dt">col =</span> <span class="kw">alpha</span>(<span class="st">&quot;darkred&quot;</span>, <span class="fl">0.5</span>),
       <span class="dt">pch =</span> <span class="dv">20</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-550-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>Observations from both the control and treatment group have a higher mean after the treatment but that the increase is stronger for the treatment group. Using DID we may estimate how much of that difference is due to the treatment.</p>
<p>It is straightforward to compute the DID estimate in the fashion of <a href="eaqe.html#eq:DID">(13.8)</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the DID estimator for the treatment effect &#39;by hand&#39;</span>
<span class="kw">mean</span>(y_post[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">1</span>]) <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(y_pre[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">1</span>]) <span class="op">-</span><span class="st"> </span>
(<span class="kw">mean</span>(y_post[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">0</span>]) <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(y_pre[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">0</span>]))</code></pre></div>
<pre><code>## [1] 3.960268</code></pre>
<p>Notice that the estimate is close to <span class="math inline">\(4\)</span>, the value chosen as the treatment effect <tt>TEffect</tt> above. Since <a href="eaqe.html#eq:did">(13.9)</a> is a simple linear model, we may perform OLS estimation of this regression specification using <tt>lm()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the DID estimator using a linear model</span>
<span class="kw">lm</span>(<span class="kw">I</span>(y_post <span class="op">-</span><span class="st"> </span>y_pre) <span class="op">~</span><span class="st"> </span>TDummy)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = I(y_post - y_pre) ~ TDummy)
## 
## Coefficients:
## (Intercept)       TDummy  
##       2.104        3.960</code></pre>
We find that the estimates coincide. Furthermore, one can show that the DID estimate obtained by estimating specification <a href="eaqe.html#eq:did">(13.9)</a> OLS is the same as the OLS estimate of <span class="math inline">\(\beta_{TE}\)</span> in
<span class="math display" id="eq:DIDint">\[\begin{align}
  Y_i =&amp; \beta_0 + \beta_1 D_i + \beta_2 Period_i + \beta_{TE} (Period_i \times D_i) \tag{13.11}
\end{align}\]</span>
<p>where <span class="math inline">\(D_i\)</span> is the binary treatment indicator, <span class="math inline">\(Period_i\)</span> is a binary indicator for the after-treatment period and the <span class="math inline">\(Period_i \times D_i\)</span> is the interaction of both.</p>
<p>As for <a href="eaqe.html#eq:did">(13.9)</a>, estimation of <a href="eaqe.html#eq:DIDint">(13.11)</a> using <tt>R</tt> is straightforward. See Chapter <a href="nrf.html#nrf">8</a> for a discussion of interaction terms.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># prepare data for DID regression using the interaction term </span>
d &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="st">&quot;Y&quot;</span> =<span class="st"> </span><span class="kw">c</span>(y_pre,y_post),
                <span class="st">&quot;Treatment&quot;</span> =<span class="st"> </span>TDummy, 
                <span class="st">&quot;Period&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;1&quot;</span>, n), <span class="kw">rep</span>(<span class="st">&quot;2&quot;</span>, n)))

<span class="co"># estimate the model</span>
<span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>Treatment <span class="op">*</span><span class="st"> </span>Period, <span class="dt">data =</span> d)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ Treatment * Period, data = d)
## 
## Coefficients:
##       (Intercept)          Treatment            Period2  
##             5.858              1.197              2.104  
## Treatment:Period2  
##             3.960</code></pre>
<p>As expected, the estimate of the coefficient on the interaction of the treatment dummy and the time dummy coincide with the estimates obtained using <a href="eaqe.html#eq:DID">(13.8)</a> and OLS estimation of <a href="eaqe.html#eq:did">(13.9)</a>.</p>
</div>
<div id="regression-discontinuity-estimators" class="section level3 unnumbered">
<h3>Regression Discontinuity Estimators</h3>
Consider the model
<span class="math display" id="eq:SRDDsetting">\[\begin{align}
  Y_i =&amp; \beta_0 + \beta_1 X_i + \beta_2 W_i + u_i \tag{13.12}
\end{align}\]</span>
and let
<span class="math display">\[\begin{align*}
X_i =&amp; 
  \begin{cases}
    1, &amp; W_i \geq c \\
    0, &amp; W_i &lt; c
  \end{cases}
\end{align*}\]</span>
<p>so that the receipt of treatment, <span class="math inline">\(X_i\)</span>, is determined by some threshold <span class="math inline">\(c\)</span> of a continuous variable <span class="math inline">\(W_i\)</span>, the so called running variable. The idea of <em>regression discontinuity design</em> is to use observations with a <span class="math inline">\(W_i\)</span> close to <span class="math inline">\(c\)</span> for estimation of <span class="math inline">\(\beta_1\)</span>. <span class="math inline">\(\beta_1\)</span> is the average treatment effect for individuals with <span class="math inline">\(W_i = c\)</span> which is assumed to be a good approximation to the treatment effect in the population. <a href="eaqe.html#eq:SRDDsetting">(13.12)</a> is called a <em>sharp regression discontinuity design</em> because treatment assignment is deterministic and discontinuous at the cutoff: all observations with <span class="math inline">\(W_i &lt; c\)</span> do not receive treatment and all observations where <span class="math inline">\(W_i \geq c\)</span> are treated.</p>
<p>The subsequent code chunks show how to estimate a linear SRDD using <tt>R</tt> and how to produce plots in the way of Figure 13.2 of the book.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate some sample data</span>
W &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1000</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)
y &lt;-<span class="st"> </span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>W <span class="op">+</span><span class="st"> </span><span class="dv">10</span> <span class="op">*</span><span class="st"> </span>(W<span class="op">&gt;=</span><span class="dv">0</span>) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>)</code></pre></div>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the package &#39;rddtools&#39;</span>
<span class="kw">library</span>(rddtools)

<span class="co"># construct rdd_data </span>
data &lt;-<span class="st"> </span><span class="kw">rdd_data</span>(y, W, <span class="dt">cutpoint =</span> <span class="dv">0</span>)

<span class="co"># plot the sample data</span>
<span class="kw">plot</span>(data,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">cex =</span> <span class="fl">0.35</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;W&quot;</span>, 
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-555-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>The argument <tt>nbins</tt> sets the number of bins the running variable is divided into for aggregation. The dots represent bin averages of the outcome variable.</p>
<p>We may use the function <tt>rdd_reg_lm()</tt> to estimate the treatment effect using model <a href="eaqe.html#eq:SRDDsetting">(13.12)</a> for the artificial data generated above. By choosing <tt>slope = “same”</tt> we restrict the slopes of the estimated regression function to be the same on both sides of the jump at the cutpoint <span class="math inline">\(W=0\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the sharp RDD model</span>
rdd_mod &lt;-<span class="st"> </span><span class="kw">rdd_reg_lm</span>(<span class="dt">rdd_object =</span> data, 
                      <span class="dt">slope =</span> <span class="st">&quot;same&quot;</span>)
<span class="kw">summary</span>(rdd_mod)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ ., data = dat_step1, weights = weights)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2361 -0.6779 -0.0039  0.7113  3.0096 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.93889    0.07082   41.50   &lt;2e-16 ***
## D           10.12692    0.12631   80.18   &lt;2e-16 ***
## x            1.88249    0.11074   17.00   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.019 on 997 degrees of freedom
## Multiple R-squared:  0.972,  Adjusted R-squared:  0.972 
## F-statistic: 1.732e+04 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The coefficient estimate of interest is labeled <tt>D</tt>. The estimate is very close to the treatment effect chosen in the DGP above.</p>
<p>It is easy to visualize the result: simply call <tt>plot()</tt> on the estimated model object.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the RDD model along with binned observations</span>
<span class="kw">plot</span>(rdd_mod,
     <span class="dt">cex =</span> <span class="fl">0.35</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;W&quot;</span>, 
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-557-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>As above, the dots represent averages of binned observations.</p>
<p>So far we assumed that crossing of the threshold determines receipt of treatment so that the jump of the population regression functions at the threshold can be regarded as the causal effect of the treatment.</p>
<p>When crossing the threshold <span class="math inline">\(c\)</span> is not the only cause for receipt of the treatment, treatment is not a deterministic function of <span class="math inline">\(W_i\)</span>. Instead, it is useful to think of <span class="math inline">\(c\)</span> as a threshold where the <em>probability</em> of receiving the treatment jumps.</p>
This jump may be due to unobservable variables that have impact on the probability of being treated. Thus, <span class="math inline">\(X_i\)</span> in <a href="eaqe.html#eq:SRDDsetting">(13.12)</a> will be correlated with the error <span class="math inline">\(u_i\)</span> and it becomes more difficult to consistently estimate the treatment effect. In this setting, using a <em>fuzzy regression discontinuity design</em> which is based an IV approach may be a remedy: take the binary variable <span class="math inline">\(Z_i\)</span> as an indicator for crossing of the threshold,
<span class="math display">\[\begin{align*}
  Z_i = \begin{cases}
    1, &amp; W_i \geq c \\
    0, &amp; W_i &lt; c,
  \end{cases}
\end{align*}\]</span>
<p>and assume that <span class="math inline">\(Z_i\)</span> relates to <span class="math inline">\(Y_i\)</span> only through the treatment indicator <span class="math inline">\(X_i\)</span>. Then <span class="math inline">\(Z_i\)</span> and <span class="math inline">\(u_i\)</span> are uncorrelated but <span class="math inline">\(Z_i\)</span> influences receipt of treatment so it is correlated with <span class="math inline">\(X_i\)</span>. Thus, <span class="math inline">\(Z_i\)</span> is a valid instrument for <span class="math inline">\(X_i\)</span> and <a href="eaqe.html#eq:SRDDsetting">(13.12)</a> can be estimated using TSLS.</p>
<p>The following code chunk generates sample data where observations with a value of the running variable <span class="math inline">\(W_i\)</span> below the cutoff <span class="math inline">\(c=0\)</span> do not receive treatment and observations with <span class="math inline">\(W_i \geq 0\)</span> do receive treatment with a probability of <span class="math inline">\(80\%\)</span> so that treatment status is only partially determined by the running variable and the cutoff. Treatment leads to an increase in <span class="math inline">\(Y\)</span> by <span class="math inline">\(2\)</span> units. Observations with <span class="math inline">\(W_i \geq 0\)</span> that do not receive treatment are called <em>no-shows</em>: think of an individual that was assigned to receive the treatment but somehow manages to avoid it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)

<span class="co"># generate sample data</span>
mu &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>)
sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">0.7</span>, <span class="fl">0.7</span>, <span class="dv">1</span>), <span class="dt">ncol =</span> <span class="dv">2</span>)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
d &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">mvrnorm</span>(<span class="dv">2000</span>, mu, sigma))
<span class="kw">colnames</span>(d) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;W&quot;</span>, <span class="st">&quot;Y&quot;</span>)

<span class="co"># introduce fuzziness</span>
d<span class="op">$</span>treatProb &lt;-<span class="st"> </span><span class="kw">ifelse</span>(d<span class="op">$</span>W <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.8</span>)

fuzz &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dt">X =</span> d<span class="op">$</span>treatProb, <span class="dt">FUN =</span> <span class="cf">function</span>(x) <span class="kw">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">prob =</span> x))

<span class="co"># treatment effect</span>
d<span class="op">$</span>Y &lt;-<span class="st"> </span>d<span class="op">$</span>Y <span class="op">+</span><span class="st"> </span>fuzz <span class="op">*</span><span class="st"> </span><span class="dv">2</span></code></pre></div>
<p><tt>sapply()</tt> applies the function provided to <tt>FUN</tt> to every element of the argument <tt>X</tt>. Here, <tt>d$treatProb</tt> is a vector and the result is a vector, too.</p>
<p>We plot all observations and use blue color to mark individuals that did not receive the treatment and use red color for those who received the treatment.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate a colored plot of treatment and control group</span>
<span class="kw">plot</span>(d<span class="op">$</span>W, d<span class="op">$</span>Y,
     <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;steelblue&quot;</span>, <span class="st">&quot;darkred&quot;</span>)[<span class="kw">factor</span>(fuzz)], 
     <span class="dt">pch=</span> <span class="dv">20</span>, 
     <span class="dt">cex =</span> <span class="fl">0.5</span>,
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>),
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="dv">5</span>),
     <span class="dt">xlab =</span> <span class="st">&quot;W&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)

<span class="co"># add a dashed vertical line at cutoff</span>
<span class="kw">abline</span>(<span class="dt">v =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-559-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>Obviously, receipt of treatment is no longer a deterministic function of the running variable <span class="math inline">\(W\)</span>. Some observations with <span class="math inline">\(W\geq0\)</span> <em>did not</em> receive the treatment. We may estimate a FRDD by additionally setting <tt>treatProb</tt> as the assignment variable <tt>z</tt> in <tt>rdd_data()</tt>. Then <tt>rdd_reg_lm()</tt> applies the following TSLS procedure: treatment is predicted using <span class="math inline">\(W_i\)</span> and the cutoff dummy <span class="math inline">\(Z_i\)</span>, the instrumental variable, in the first stage regression. The fitted values from the first stage regression are used to obtain a consistent estimate of the treatment effect using the second stage where the outcome <span class="math inline">\(Y\)</span> is regressed on the fitted values and the running variable <span class="math inline">\(W\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the Fuzzy RDD</span>
data &lt;-<span class="st"> </span><span class="kw">rdd_data</span>(d<span class="op">$</span>Y, d<span class="op">$</span>W, 
                 <span class="dt">cutpoint =</span> <span class="dv">0</span>, 
                 <span class="dt">z =</span> d<span class="op">$</span>treatProb)

frdd_mod &lt;-<span class="st"> </span><span class="kw">rdd_reg_lm</span>(<span class="dt">rdd_object =</span> data, 
                       <span class="dt">slope =</span> <span class="st">&quot;same&quot;</span>)
frdd_mod</code></pre></div>
<pre><code>## ### RDD regression: parametric ###
##  Polynomial order:  1 
##  Slopes:  same 
##  Number of obs: 2000 (left: 999, right: 1001)
## 
##  Coefficient:
##   Estimate Std. Error t value  Pr(&gt;|t|)    
## D 1.981297   0.084696  23.393 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The estimate is close to <span class="math inline">\(2\)</span>, the population treatment effect. We may call <tt>plot()</tt> on the model object to obtain a figure consisting of binned data and the estimated regression function.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot estimated FRDD function</span>
<span class="kw">plot</span>(frdd_mod, 
     <span class="dt">cex =</span> <span class="fl">0.5</span>, 
     <span class="dt">lwd =</span> <span class="fl">0.4</span>,
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>),
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="dv">5</span>),
     <span class="dt">xlab =</span> <span class="st">&quot;W&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-561-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>What if we used a SRDD instead, thereby ignoring the fact that treatment is not perfectly determined by the cutoff in <span class="math inline">\(W\)</span>? We may get an impression of the consequences by estimating an SRDD using the previously simulated data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate SRDD</span>
data &lt;-<span class="st"> </span><span class="kw">rdd_data</span>(d<span class="op">$</span>Y, 
                 d<span class="op">$</span>W, 
                 <span class="dt">cutpoint =</span> <span class="dv">0</span>)

srdd_mod &lt;-<span class="st"> </span><span class="kw">rdd_reg_lm</span>(<span class="dt">rdd_object =</span> data, 
                       <span class="dt">slope =</span> <span class="st">&quot;same&quot;</span>)
srdd_mod</code></pre></div>
<pre><code>## ### RDD regression: parametric ###
##  Polynomial order:  1 
##  Slopes:  same 
##  Number of obs: 2000 (left: 999, right: 1001)
## 
##  Coefficient:
##   Estimate Std. Error t value  Pr(&gt;|t|)    
## D 1.585038   0.067756  23.393 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The estimate obtained using a SRDD is suggestive of a substantial downward bias. In fact, this procedure is inconsistent for the true causal effect so increasing the sample would not alleviate the bias.</p>
<p>The book continues with a discussion of potential problems with quasi-experiments. As for all empirical studies, these potential problems are related to internal and external validity. This part is followed by a technical discussion of treatment effect estimation when the causal effect of treatment is heterogeneous in the population. We encourage you to work on these sections on your own.</p>
<div id="summary-5" class="section level4 unnumbered">
<h4>Summary</h4>
<p>This chapter has introduced the concept of causal effects in randomized controlled experiments and quasi-experiments where variations in circumstances or accidents of nature are treated as sources of “as if” random assignment to treatment. We have also discussed methods that allow for consistent estimation of these effects in both settings. These included the <em>differences estimator</em>, the <em>differences-in-differences estimator</em> as well as <em>sharp</em> and <em>fuzzy regression discontinuity design</em> estimators. It was shown how to apply these estimation techniques in <tt>R</tt>.</p>
<p>In an empirical application we have shown how to replicate the results of the analysis of the STAR data presented in Chapter 13.3 of the book using <tt>R</tt>. This study uses a randomized controlled experiment to assess whether smaller classes improve students’ performance on standardized tests. Being related to a randomized controlled experiment, the data of this study is fundamentally different to those used in the cross-section studies in Chapters <a href="lrwor.html#lrwor">4</a> to <a href="nrf.html#nrf">8</a>. We therefore have motivated usage of a <em>differences estimator</em>.</p>
<p>Chapter <a href="ivr.html#attdfc">12.4</a> demonstrated how estimates of treatment effects can be obtained when the design of the study is a quasi-experiment that allows for <em>differences-in-differences</em> or <em>regression discontinuity design</em> estimators. In particular, we have introduced functions of the package <tt>rddtools</tt> that are convenient for estimation as well as graphical analysis when estimating a regression discontinuity design.</p>

</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-AER">
<p>Kleiber, C., &amp; Zeileis, A. (2017). AER: Applied Econometrics with R (Version 1.2-5). Retrieved from <a href="https://CRAN.R-project.org/package=AER" class="uri">https://CRAN.R-project.org/package=AER</a></p>
</div>
<div id="ref-R-dplyr">
<p>Wickham, H., François, R., Henry, L., &amp; Müller, K. (2018). dplyr: A Grammar of Data Manipulation (Version 0.7.5). Retrieved from <a href="https://CRAN.R-project.org/package=dplyr" class="uri">https://CRAN.R-project.org/package=dplyr</a></p>
</div>
<div id="ref-R-MASS">
<p>Ripley, B. (2018). MASS: Support Functions and Datasets for Venables and Ripley’s MASS (Version 7.3-50). Retrieved from <a href="https://CRAN.R-project.org/package=MASS" class="uri">https://CRAN.R-project.org/package=MASS</a></p>
</div>
<div id="ref-R-mvtnorm">
<p>Genz, A., Bretz, F., Miwa, T., Mi, X., &amp; Hothorn, T. (2018). mvtnorm: Multivariate Normal and t Distributions (Version 1.0-8). Retrieved from <a href="https://CRAN.R-project.org/package=mvtnorm" class="uri">https://CRAN.R-project.org/package=mvtnorm</a></p>
</div>
<div id="ref-R-rddtools">
<p>Stigler, M., &amp; Quast, B. (2015). rddtools: Toolbox for Regression Discontinuity Design (’RDD’) (Version 0.4.0). Retrieved from <a href="https://CRAN.R-project.org/package=rddtools" class="uri">https://CRAN.R-project.org/package=rddtools</a></p>
</div>
<div id="ref-R-scales">
<p>Wickham, H. (2017). scales: Scale Functions for Visualization (Version 0.5.0). Retrieved from <a href="https://CRAN.R-project.org/package=scales" class="uri">https://CRAN.R-project.org/package=scales</a></p>
</div>
<div id="ref-R-stargazer">
<p>Hlavac, M. (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables (Version 5.2.2). Retrieved from <a href="https://CRAN.R-project.org/package=stargazer" class="uri">https://CRAN.R-project.org/package=stargazer</a></p>
</div>
<div id="ref-R-tidyr">
<p>Wickham, H., &amp; Henry, L. (2018). tidyr: Easily Tidy Data with ’spread()’ and ’gather()’ Functions (Version 0.8.1). Retrieved from <a href="https://CRAN.R-project.org/package=tidyr" class="uri">https://CRAN.R-project.org/package=tidyr</a></p>
</div>
<div id="ref-card1994">
<p>Card, D., &amp; Krueger, A. B. (1994). Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania. <em>The American Economic Review</em>, <em>84</em>(4), 772–793.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="9">
<li id="fn9"><p>See Chapter 13.4 of the book for some example studies that are based on quasi-experiments.<a href="eaqe.html#fnref9">↩</a></p></li>
<li id="fn10"><p>Also see the box <em>What is the Effect on Employment of the Minimum Wage?</em> in Chapter 13.4 of the book.<a href="eaqe.html#fnref10">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ivr.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ittsraf.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": true,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 1
},
"edit": null,
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
