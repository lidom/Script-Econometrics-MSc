<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.5 The Sampling Distribution of the OLS Estimator | Advanced Regression Analysis</title>
  <meta name="description" content="3.5 The Sampling Distribution of the OLS Estimator | Advanced Regression Analysis" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="3.5 The Sampling Distribution of the OLS Estimator | Advanced Regression Analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/mylogo.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.5 The Sampling Distribution of the OLS Estimator | Advanced Regression Analysis" />
  
  
  <meta name="twitter:image" content="images/mylogo.png" />

<meta name="author" content="Dominik Liebl" />


<meta name="date" content="2020-09-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3-4-tlsa.html"/>
<link rel="next" href="3-6-exercises-4.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.46.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.46.1/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><center><a href="https://www.example.com/"><img src="images/mylogo.png" alt="logo" width="100%" height="100%"style="margin: 15px 0 0 0"></a></center></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="0-1-a-very-short-introduction-to-r-and-rstudio.html"><a href="0-1-a-very-short-introduction-to-r-and-rstudio.html"><i class="fa fa-check"></i><b>0.1</b> A Very Short Introduction to <tt>R</tt> and <em>RStudio</em></a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-pt.html"><a href="1-pt.html"><i class="fa fa-check"></i><b>1</b> Probability Theory</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-random-variables-and-probability-distributions.html"><a href="1-1-random-variables-and-probability-distributions.html"><i class="fa fa-check"></i><b>1.1</b> Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="1-1-random-variables-and-probability-distributions.html"><a href="1-1-random-variables-and-probability-distributions.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="1-1-random-variables-and-probability-distributions.html"><a href="1-1-random-variables-and-probability-distributions.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="1-1-random-variables-and-probability-distributions.html"><a href="1-1-random-variables-and-probability-distributions.html#expected-value-mean-and-variance"><i class="fa fa-check"></i>Expected Value, Mean and Variance</a></li>
<li class="chapter" data-level="" data-path="1-1-random-variables-and-probability-distributions.html"><a href="1-1-random-variables-and-probability-distributions.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a></li>
<li class="chapter" data-level="" data-path="1-1-random-variables-and-probability-distributions.html"><a href="1-1-random-variables-and-probability-distributions.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="1-1-random-variables-and-probability-distributions.html"><a href="1-1-random-variables-and-probability-distributions.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="" data-path="1-1-random-variables-and-probability-distributions.html"><a href="1-1-random-variables-and-probability-distributions.html#thetdist"><i class="fa fa-check"></i>The Student t Distribution</a></li>
<li class="chapter" data-level="" data-path="1-1-random-variables-and-probability-distributions.html"><a href="1-1-random-variables-and-probability-distributions.html#the-f-distribution"><i class="fa fa-check"></i>The F Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-RSATDOSA.html"><a href="1-2-RSATDOSA.html"><i class="fa fa-check"></i><b>1.2</b> Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="1-2-RSATDOSA.html"><a href="1-2-RSATDOSA.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="1-2-RSATDOSA.html"><a href="1-2-RSATDOSA.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-exercises-2.html"><a href="1-3-exercises-2.html"><i class="fa fa-check"></i><b>1.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-arosur.html"><a href="2-arosur.html"><i class="fa fa-check"></i><b>2</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-estimation-of-the-population-mean.html"><a href="2-1-estimation-of-the-population-mean.html"><i class="fa fa-check"></i><b>2.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-potsm.html"><a href="2-2-potsm.html"><i class="fa fa-check"></i><b>2.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="2.3" data-path="2-3-hypothesis-tests-concerning-the-population-mean.html"><a href="2-3-hypothesis-tests-concerning-the-population-mean.html"><i class="fa fa-check"></i><b>2.3</b> Hypothesis Tests Concerning the Population Mean</a><ul>
<li class="chapter" data-level="" data-path="2-3-hypothesis-tests-concerning-the-population-mean.html"><a href="2-3-hypothesis-tests-concerning-the-population-mean.html#the-p-value"><i class="fa fa-check"></i>The p-Value</a></li>
<li class="chapter" data-level="" data-path="2-3-hypothesis-tests-concerning-the-population-mean.html"><a href="2-3-hypothesis-tests-concerning-the-population-mean.html#calculating-the-p-value-when-the-standard-deviation-is-known"><i class="fa fa-check"></i>Calculating the p-Value when the Standard Deviation is Known</a></li>
<li class="chapter" data-level="" data-path="2-3-hypothesis-tests-concerning-the-population-mean.html"><a href="2-3-hypothesis-tests-concerning-the-population-mean.html#SVSSDASE"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li class="chapter" data-level="" data-path="2-3-hypothesis-tests-concerning-the-population-mean.html"><a href="2-3-hypothesis-tests-concerning-the-population-mean.html#calculating-the-p-value-when-the-standard-deviation-is-unknown"><i class="fa fa-check"></i>Calculating the p-value When the Standard Deviation is Unknown</a></li>
<li class="chapter" data-level="" data-path="2-3-hypothesis-tests-concerning-the-population-mean.html"><a href="2-3-hypothesis-tests-concerning-the-population-mean.html#the-t-statistic"><i class="fa fa-check"></i>The t-statistic</a></li>
<li class="chapter" data-level="" data-path="2-3-hypothesis-tests-concerning-the-population-mean.html"><a href="2-3-hypothesis-tests-concerning-the-population-mean.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="2-3-hypothesis-tests-concerning-the-population-mean.html"><a href="2-3-hypothesis-tests-concerning-the-population-mean.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-confidence-intervals-for-the-population-mean.html"><a href="2-4-confidence-intervals-for-the-population-mean.html"><i class="fa fa-check"></i><b>2.4</b> Confidence Intervals for the Population Mean</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-cmfdp.html"><a href="2-5-cmfdp.html"><i class="fa fa-check"></i><b>2.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="2.6" data-path="2-6-aattggoe.html"><a href="2-6-aattggoe.html"><i class="fa fa-check"></i><b>2.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="2.7" data-path="2-7-scatterplots-sample-covariance-and-sample-correlation.html"><a href="2-7-scatterplots-sample-covariance-and-sample-correlation.html"><i class="fa fa-check"></i><b>2.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
<li class="chapter" data-level="2.8" data-path="2-8-exercises-3.html"><a href="2-8-exercises-3.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-lrwor.html"><a href="3-lrwor.html"><i class="fa fa-check"></i><b>3</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-simple-linear-regression.html"><a href="3-1-simple-linear-regression.html"><i class="fa fa-check"></i><b>3.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-estimating-the-coefficients-of-the-linear-regression-model.html"><a href="3-2-estimating-the-coefficients-of-the-linear-regression-model.html"><i class="fa fa-check"></i><b>3.2</b> Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="3-2-estimating-the-coefficients-of-the-linear-regression-model.html"><a href="3-2-estimating-the-coefficients-of-the-linear-regression-model.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-measures-of-fit.html"><a href="3-3-measures-of-fit.html"><i class="fa fa-check"></i><b>3.3</b> Measures of Fit</a><ul>
<li class="chapter" data-level="" data-path="3-3-measures-of-fit.html"><a href="3-3-measures-of-fit.html#the-coefficient-of-determination"><i class="fa fa-check"></i>The Coefficient of Determination</a></li>
<li class="chapter" data-level="" data-path="3-3-measures-of-fit.html"><a href="3-3-measures-of-fit.html#the-standard-error-of-the-regression"><i class="fa fa-check"></i>The Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="3-3-measures-of-fit.html"><a href="3-3-measures-of-fit.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-4-tlsa.html"><a href="3-4-tlsa.html"><i class="fa fa-check"></i><b>3.4</b> The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="3-4-tlsa.html"><a href="3-4-tlsa.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption 1: The Error Term has Conditional Mean of Zero</a></li>
<li class="chapter" data-level="" data-path="3-4-tlsa.html"><a href="3-4-tlsa.html#assumption-2-independently-and-identically-distributed-data"><i class="fa fa-check"></i>Assumption 2: Independently and Identically Distributed Data</a></li>
<li class="chapter" data-level="" data-path="3-4-tlsa.html"><a href="3-4-tlsa.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption 3: Large Outliers are Unlikely</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-5-tsdotoe.html"><a href="3-5-tsdotoe.html"><i class="fa fa-check"></i><b>3.5</b> The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="3-5-tsdotoe.html"><a href="3-5-tsdotoe.html#simulation-study-1"><i class="fa fa-check"></i>Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="3-5-tsdotoe.html"><a href="3-5-tsdotoe.html#simulation-study-2"><i class="fa fa-check"></i>Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="3-5-tsdotoe.html"><a href="3-5-tsdotoe.html#simulation-study-3"><i class="fa fa-check"></i>Simulation Study 3</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-6-exercises-4.html"><a href="3-6-exercises-4.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-htaciitslrm.html"><a href="4-htaciitslrm.html"><i class="fa fa-check"></i><b>4</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-testing-two-sided-hypotheses-concerning-the-slope-coefficient.html"><a href="4-1-testing-two-sided-hypotheses-concerning-the-slope-coefficient.html"><i class="fa fa-check"></i><b>4.1</b> Testing Two-Sided Hypotheses Concerning the Slope Coefficient</a></li>
<li class="chapter" data-level="4.2" data-path="4-2-cifrc.html"><a href="4-2-cifrc.html"><i class="fa fa-check"></i><b>4.2</b> Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="4-2-cifrc.html"><a href="4-2-cifrc.html#simulation-study-confidence-intervals"><i class="fa fa-check"></i>Simulation Study: Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-3-rwxiabv.html"><a href="4-3-rwxiabv.html"><i class="fa fa-check"></i><b>4.3</b> Regression when X is a Binary Variable</a></li>
<li class="chapter" data-level="4.4" data-path="4-4-hah.html"><a href="4-4-hah.html"><i class="fa fa-check"></i><b>4.4</b> Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="4-4-hah.html"><a href="4-4-hah.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="4-4-hah.html"><a href="4-4-hah.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="4-4-hah.html"><a href="4-4-hah.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-5-the-gauss-markov-theorem.html"><a href="4-5-the-gauss-markov-theorem.html"><i class="fa fa-check"></i><b>4.5</b> The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="4-5-the-gauss-markov-theorem.html"><a href="4-5-the-gauss-markov-theorem.html#simulation-study-blue-estimator"><i class="fa fa-check"></i>Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4-6-using-the-t-statistic-in-regression-when-the-sample-size-is-small.html"><a href="4-6-using-the-t-statistic-in-regression-when-the-sample-size-is-small.html"><i class="fa fa-check"></i><b>4.6</b> Using the t-Statistic in Regression When the Sample Size Is Small</a></li>
<li class="chapter" data-level="4.7" data-path="4-7-exercises-5.html"><a href="4-7-exercises-5.html"><i class="fa fa-check"></i><b>4.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-rmwmr.html"><a href="5-rmwmr.html"><i class="fa fa-check"></i><b>5</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-omitted-variable-bias.html"><a href="5-1-omitted-variable-bias.html"><i class="fa fa-check"></i><b>5.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="5.2" data-path="5-2-tmrm.html"><a href="5-2-tmrm.html"><i class="fa fa-check"></i><b>5.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="5.3" data-path="5-3-mofimr.html"><a href="5-3-mofimr.html"><i class="fa fa-check"></i><b>5.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="5.4" data-path="5-4-ols-assumptions-in-multiple-regression.html"><a href="5-4-ols-assumptions-in-multiple-regression.html"><i class="fa fa-check"></i><b>5.4</b> OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="5-4-ols-assumptions-in-multiple-regression.html"><a href="5-4-ols-assumptions-in-multiple-regression.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
<li class="chapter" data-level="" data-path="5-4-ols-assumptions-in-multiple-regression.html"><a href="5-4-ols-assumptions-in-multiple-regression.html#simulation-study-imperfect-multicollinearity"><i class="fa fa-check"></i>Simulation Study: Imperfect Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="5-5-the-distribution-of-the-ols-estimators-in-multiple-regression.html"><a href="5-5-the-distribution-of-the-ols-estimators-in-multiple-regression.html"><i class="fa fa-check"></i><b>5.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
<li class="chapter" data-level="5.6" data-path="5-6-exercises-6.html"><a href="5-6-exercises-6.html"><i class="fa fa-check"></i><b>5.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-htaciimr.html"><a href="6-htaciimr.html"><i class="fa fa-check"></i><b>6</b> Hypothesis Tests and Confidence Intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-hypothesis-tests-and-confidence-intervals-for-a-single-coefficient.html"><a href="6-1-hypothesis-tests-and-confidence-intervals-for-a-single-coefficient.html"><i class="fa fa-check"></i><b>6.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="6.2" data-path="6-2-an-application-to-test-scores-and-the-student-teacher-ratio.html"><a href="6-2-an-application-to-test-scores-and-the-student-teacher-ratio.html"><i class="fa fa-check"></i><b>6.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="6-2-an-application-to-test-scores-and-the-student-teacher-ratio.html"><a href="6-2-an-application-to-test-scores-and-the-student-teacher-ratio.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-3-joint-hypothesis-testing-using-the-f-statistic.html"><a href="6-3-joint-hypothesis-testing-using-the-f-statistic.html"><i class="fa fa-check"></i><b>6.3</b> Joint Hypothesis Testing Using the F-Statistic</a></li>
<li class="chapter" data-level="6.4" data-path="6-4-confidence-sets-for-multiple-coefficients.html"><a href="6-4-confidence-sets-for-multiple-coefficients.html"><i class="fa fa-check"></i><b>6.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="6.5" data-path="6-5-model-specification-for-multiple-regression.html"><a href="6-5-model-specification-for-multiple-regression.html"><i class="fa fa-check"></i><b>6.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="6-5-model-specification-for-multiple-regression.html"><a href="6-5-model-specification-for-multiple-regression.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6-6-analysis-of-the-test-score-data-set.html"><a href="6-6-analysis-of-the-test-score-data-set.html"><i class="fa fa-check"></i><b>6.6</b> Analysis of the Test Score Data Set</a></li>
<li class="chapter" data-level="6.7" data-path="6-7-exercises-7.html"><a href="6-7-exercises-7.html"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-nrf.html"><a href="7-nrf.html"><i class="fa fa-check"></i><b>7</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="7.1" data-path="7-1-a-general-strategy-for-modelling-nonlinear-regression-functions.html"><a href="7-1-a-general-strategy-for-modelling-nonlinear-regression-functions.html"><i class="fa fa-check"></i><b>7.1</b> A General Strategy for Modelling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="7.2" data-path="7-2-nfoasiv.html"><a href="7-2-nfoasiv.html"><i class="fa fa-check"></i><b>7.2</b> Nonlinear Functions of a Single Independent Variable</a><ul>
<li class="chapter" data-level="" data-path="7-2-nfoasiv.html"><a href="7-2-nfoasiv.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="7-2-nfoasiv.html"><a href="7-2-nfoasiv.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-3-interactions-between-independent-variables.html"><a href="7-3-interactions-between-independent-variables.html"><i class="fa fa-check"></i><b>7.3</b> Interactions Between Independent Variables</a></li>
<li class="chapter" data-level="7.4" data-path="7-4-nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html"><a href="7-4-nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html"><i class="fa fa-check"></i><b>7.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
<li class="chapter" data-level="7.5" data-path="7-5-exercises-8.html"><a href="7-5-exercises-8.html"><i class="fa fa-check"></i><b>7.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-asbomr.html"><a href="8-asbomr.html"><i class="fa fa-check"></i><b>8</b> Assessing Studies Based on Multiple Regression</a><ul>
<li class="chapter" data-level="8.1" data-path="8-1-internal-and-external-validity.html"><a href="8-1-internal-and-external-validity.html"><i class="fa fa-check"></i><b>8.1</b> Internal and External Validity</a></li>
<li class="chapter" data-level="8.2" data-path="8-2-ttivomra.html"><a href="8-2-ttivomra.html"><i class="fa fa-check"></i><b>8.2</b> Threats to Internal Validity of Multiple Regression Analysis</a></li>
<li class="chapter" data-level="8.3" data-path="8-3-internal-and-external-validity-when-the-regression-is-used-for-forecasting.html"><a href="8-3-internal-and-external-validity-when-the-regression-is-used-for-forecasting.html"><i class="fa fa-check"></i><b>8.3</b> Internal and External Validity when the Regression is Used for Forecasting</a></li>
<li class="chapter" data-level="8.4" data-path="8-4-etsacs.html"><a href="8-4-etsacs.html"><i class="fa fa-check"></i><b>8.4</b> Example: Test Scores and Class Size</a></li>
<li class="chapter" data-level="8.5" data-path="8-5-exercises-9.html"><a href="8-5-exercises-9.html"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-rwpd.html"><a href="9-rwpd.html"><i class="fa fa-check"></i><b>9</b> Regression with Panel Data</a><ul>
<li class="chapter" data-level="9.1" data-path="9-1-panel-data.html"><a href="9-1-panel-data.html"><i class="fa fa-check"></i><b>9.1</b> Panel Data</a></li>
<li class="chapter" data-level="9.2" data-path="9-2-PDWTTP.html"><a href="9-2-PDWTTP.html"><i class="fa fa-check"></i><b>9.2</b> Panel Data with Two Time Periods: “Before and After” Comparisons</a></li>
<li class="chapter" data-level="9.3" data-path="9-3-fixed-effects-regression.html"><a href="9-3-fixed-effects-regression.html"><i class="fa fa-check"></i><b>9.3</b> Fixed Effects Regression</a><ul>
<li class="chapter" data-level="" data-path="9-3-fixed-effects-regression.html"><a href="9-3-fixed-effects-regression.html#estimation-and-inference"><i class="fa fa-check"></i>Estimation and Inference</a></li>
<li class="chapter" data-level="" data-path="9-3-fixed-effects-regression.html"><a href="9-3-fixed-effects-regression.html#application-to-traffic-deaths"><i class="fa fa-check"></i>Application to Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9-4-regression-with-time-fixed-effects.html"><a href="9-4-regression-with-time-fixed-effects.html"><i class="fa fa-check"></i><b>9.4</b> Regression with Time Fixed Effects</a></li>
<li class="chapter" data-level="9.5" data-path="9-5-tferaaseffer.html"><a href="9-5-tferaaseffer.html"><i class="fa fa-check"></i><b>9.5</b> The Fixed Effects Regression Assumptions and Standard Errors for Fixed Effects Regression</a></li>
<li class="chapter" data-level="9.6" data-path="9-6-drunk-driving-laws-and-traffic-deaths.html"><a href="9-6-drunk-driving-laws-and-traffic-deaths.html"><i class="fa fa-check"></i><b>9.6</b> Drunk Driving Laws and Traffic Deaths</a></li>
<li class="chapter" data-level="9.7" data-path="9-7-exercises-10.html"><a href="9-7-exercises-10.html"><i class="fa fa-check"></i><b>9.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-rwabdv.html"><a href="10-rwabdv.html"><i class="fa fa-check"></i><b>10</b> Regression with a Binary Dependent Variable</a><ul>
<li class="chapter" data-level="10.1" data-path="10-1-binary-dependent-variables-and-the-linear-probability-model.html"><a href="10-1-binary-dependent-variables-and-the-linear-probability-model.html"><i class="fa fa-check"></i><b>10.1</b> Binary Dependent Variables and the Linear Probability Model</a></li>
<li class="chapter" data-level="10.2" data-path="10-2-palr.html"><a href="10-2-palr.html"><i class="fa fa-check"></i><b>10.2</b> Probit and Logit Regression</a><ul>
<li class="chapter" data-level="" data-path="10-2-palr.html"><a href="10-2-palr.html#probit-regression"><i class="fa fa-check"></i>Probit Regression</a></li>
<li class="chapter" data-level="" data-path="10-2-palr.html"><a href="10-2-palr.html#logit-regression"><i class="fa fa-check"></i>Logit Regression</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10-3-estimation-and-inference-in-the-logit-and-probit-models.html"><a href="10-3-estimation-and-inference-in-the-logit-and-probit-models.html"><i class="fa fa-check"></i><b>10.3</b> Estimation and Inference in the Logit and Probit Models</a></li>
<li class="chapter" data-level="10.4" data-path="10-4-application-to-the-boston-hmda-data.html"><a href="10-4-application-to-the-boston-hmda-data.html"><i class="fa fa-check"></i><b>10.4</b> Application to the Boston HMDA Data</a></li>
<li class="chapter" data-level="10.5" data-path="10-5-exercises-11.html"><a href="10-5-exercises-11.html"><i class="fa fa-check"></i><b>10.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-ivr.html"><a href="11-ivr.html"><i class="fa fa-check"></i><b>11</b> Instrumental Variables Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="11-1-TIVEWASRAASI.html"><a href="11-1-TIVEWASRAASI.html"><i class="fa fa-check"></i><b>11.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a></li>
<li class="chapter" data-level="11.2" data-path="11-2-TGIVRM.html"><a href="11-2-TGIVRM.html"><i class="fa fa-check"></i><b>11.2</b> The General IV Regression Model</a></li>
<li class="chapter" data-level="11.3" data-path="11-3-civ.html"><a href="11-3-civ.html"><i class="fa fa-check"></i><b>11.3</b> Checking Instrument Validity</a></li>
<li class="chapter" data-level="11.4" data-path="11-4-attdfc.html"><a href="11-4-attdfc.html"><i class="fa fa-check"></i><b>11.4</b> Application to the Demand for Cigarettes</a></li>
<li class="chapter" data-level="11.5" data-path="11-5-where-do-valid-instruments-come-from.html"><a href="11-5-where-do-valid-instruments-come-from.html"><i class="fa fa-check"></i><b>11.5</b> Where Do Valid Instruments Come From?</a></li>
<li class="chapter" data-level="11.6" data-path="11-6-exercises-12.html"><a href="11-6-exercises-12.html"><i class="fa fa-check"></i><b>11.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-eaqe.html"><a href="12-eaqe.html"><i class="fa fa-check"></i><b>12</b> Experiments and Quasi-Experiments</a><ul>
<li class="chapter" data-level="12.1" data-path="12-1-poceaie.html"><a href="12-1-poceaie.html"><i class="fa fa-check"></i><b>12.1</b> Potential Outcomes, Causal Effects and Idealized Experiments</a></li>
<li class="chapter" data-level="12.2" data-path="12-2-threats-to-validity-of-experiments.html"><a href="12-2-threats-to-validity-of-experiments.html"><i class="fa fa-check"></i><b>12.2</b> Threats to Validity of Experiments</a></li>
<li class="chapter" data-level="12.3" data-path="12-3-experimental-estimates-of-the-effect-of-class-size-reductions.html"><a href="12-3-experimental-estimates-of-the-effect-of-class-size-reductions.html"><i class="fa fa-check"></i><b>12.3</b> Experimental Estimates of the Effect of Class Size Reductions</a><ul>
<li class="chapter" data-level="" data-path="12-3-experimental-estimates-of-the-effect-of-class-size-reductions.html"><a href="12-3-experimental-estimates-of-the-effect-of-class-size-reductions.html#experimental-design-and-the-data-set"><i class="fa fa-check"></i>Experimental Design and the Data Set</a></li>
<li class="chapter" data-level="" data-path="12-3-experimental-estimates-of-the-effect-of-class-size-reductions.html"><a href="12-3-experimental-estimates-of-the-effect-of-class-size-reductions.html#analysis-of-the-star-data"><i class="fa fa-check"></i>Analysis of the STAR Data</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12-4-qe.html"><a href="12-4-qe.html"><i class="fa fa-check"></i><b>12.4</b> Quasi Experiments</a><ul>
<li class="chapter" data-level="" data-path="12-4-qe.html"><a href="12-4-qe.html#the-differences-in-differences-estimator"><i class="fa fa-check"></i>The Differences-in-Differences Estimator</a></li>
<li class="chapter" data-level="" data-path="12-4-qe.html"><a href="12-4-qe.html#regression-discontinuity-estimators"><i class="fa fa-check"></i>Regression Discontinuity Estimators</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="12-5-exercises-13.html"><a href="12-5-exercises-13.html"><i class="fa fa-check"></i><b>12.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-ittsraf.html"><a href="13-ittsraf.html"><i class="fa fa-check"></i><b>13</b> Introduction to Time Series Regression and Forecasting</a><ul>
<li class="chapter" data-level="13.1" data-path="13-1-using-regression-models-for-forecasting.html"><a href="13-1-using-regression-models-for-forecasting.html"><i class="fa fa-check"></i><b>13.1</b> Using Regression Models for Forecasting</a></li>
<li class="chapter" data-level="13.2" data-path="13-2-tsdasc.html"><a href="13-2-tsdasc.html"><i class="fa fa-check"></i><b>13.2</b> Time Series Data and Serial Correlation</a><ul>
<li class="chapter" data-level="" data-path="13-2-tsdasc.html"><a href="13-2-tsdasc.html#notation-lags-differences-logarithms-and-growth-rates"><i class="fa fa-check"></i>Notation, Lags, Differences, Logarithms and Growth Rates</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13-3-autoregressions.html"><a href="13-3-autoregressions.html"><i class="fa fa-check"></i><b>13.3</b> Autoregressions</a><ul>
<li><a href="13-3-autoregressions.html#autoregressive-models-of-order-p">Autoregressive Models of Order <span class="math inline">\(p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="13-4-cybtmpi.html"><a href="13-4-cybtmpi.html"><i class="fa fa-check"></i><b>13.4</b> Can You Beat the Market? (Part I)</a></li>
<li class="chapter" data-level="13.5" data-path="13-5-apatadlm.html"><a href="13-5-apatadlm.html"><i class="fa fa-check"></i><b>13.5</b> Additional Predictors and The ADL Model</a><ul>
<li class="chapter" data-level="" data-path="13-5-apatadlm.html"><a href="13-5-apatadlm.html#forecast-uncertainty-and-forecast-intervals"><i class="fa fa-check"></i>Forecast Uncertainty and Forecast Intervals</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13-6-llsuic.html"><a href="13-6-llsuic.html"><i class="fa fa-check"></i><b>13.6</b> Lag Length Selection Using Information Criteria</a></li>
<li class="chapter" data-level="13.7" data-path="13-7-nit.html"><a href="13-7-nit.html"><i class="fa fa-check"></i><b>13.7</b> Nonstationarity I: Trends</a></li>
<li class="chapter" data-level="13.8" data-path="13-8-niib.html"><a href="13-8-niib.html"><i class="fa fa-check"></i><b>13.8</b> Nonstationarity II: Breaks</a></li>
<li class="chapter" data-level="13.9" data-path="13-9-can-you-beat-the-market-part-ii.html"><a href="13-9-can-you-beat-the-market-part-ii.html"><i class="fa fa-check"></i><b>13.9</b> Can You Beat the Market? (Part II)</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-eodce.html"><a href="14-eodce.html"><i class="fa fa-check"></i><b>14</b> Estimation of Dynamic Causal Effects</a><ul>
<li class="chapter" data-level="14.1" data-path="14-1-the-orange-juice-data.html"><a href="14-1-the-orange-juice-data.html"><i class="fa fa-check"></i><b>14.1</b> The Orange Juice Data</a></li>
<li class="chapter" data-level="14.2" data-path="14-2-dynamic-causal-effects.html"><a href="14-2-dynamic-causal-effects.html"><i class="fa fa-check"></i><b>14.2</b> Dynamic Causal Effects</a></li>
<li class="chapter" data-level="14.3" data-path="14-3-dynamic-multipliers-and-cumulative-dynamic-multipliers.html"><a href="14-3-dynamic-multipliers-and-cumulative-dynamic-multipliers.html"><i class="fa fa-check"></i><b>14.3</b> Dynamic Multipliers and Cumulative Dynamic Multipliers</a></li>
<li class="chapter" data-level="14.4" data-path="14-4-hac-standard-errors.html"><a href="14-4-hac-standard-errors.html"><i class="fa fa-check"></i><b>14.4</b> HAC Standard Errors</a></li>
<li class="chapter" data-level="14.5" data-path="14-5-estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors.html"><a href="14-5-estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors.html"><i class="fa fa-check"></i><b>14.5</b> Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors</a></li>
<li class="chapter" data-level="14.6" data-path="14-6-orange-juice-prices-and-cold-weather.html"><a href="14-6-orange-juice-prices-and-cold-weather.html"><i class="fa fa-check"></i><b>14.6</b> Orange Juice Prices and Cold Weather</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="15-atitsr.html"><a href="15-atitsr.html"><i class="fa fa-check"></i><b>15</b> Additional Topics in Time Series Regression</a><ul>
<li class="chapter" data-level="15.1" data-path="15-1-vector-autoregressions.html"><a href="15-1-vector-autoregressions.html"><i class="fa fa-check"></i><b>15.1</b> Vector Autoregressions</a></li>
<li class="chapter" data-level="15.2" data-path="15-2-ooiatdfglsurt.html"><a href="15-2-ooiatdfglsurt.html"><i class="fa fa-check"></i><b>15.2</b> Orders of Integration and the DF-GLS Unit Root Test</a></li>
<li class="chapter" data-level="15.3" data-path="15-3-cointegration.html"><a href="15-3-cointegration.html"><i class="fa fa-check"></i><b>15.3</b> Cointegration</a></li>
<li class="chapter" data-level="15.4" data-path="15-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html"><a href="15-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html"><i class="fa fa-check"></i><b>15.4</b> Volatility Clustering and Autoregressive Conditional Heteroskedasticity</a><ul>
<li class="chapter" data-level="" data-path="15-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html"><a href="15-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html#arch-and-garch-models"><i class="fa fa-check"></i>ARCH and GARCH Models</a></li>
<li class="chapter" data-level="" data-path="15-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html"><a href="15-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html#application-to-stock-price-volatility"><i class="fa fa-check"></i>Application to Stock Price Volatility</a></li>
<li class="chapter" data-level="" data-path="7-4-nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html"><a href="7-4-nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html#summary"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Regression Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tsdotoe" class="section level2">
<h2><span class="header-section-number">3.5</span> The Sampling Distribution of the OLS Estimator</h2>
<p>Because <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are computed from a sample, the estimators themselves are random variables with a probability distribution — the so-called sampling distribution of the estimators — which describes the values they could take on over different samples. Although the sampling distribution of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> can be complicated when the sample size is small and generally changes with the number of observations, <span class="math inline">\(n\)</span>, it is possible, provided the assumptions discussed in the book are valid, to make certain statements about it that hold for all <span class="math inline">\(n\)</span>. In particular
<span class="math display">\[ E(\hat{\beta}_0) = \beta_0 \ \ \text{and} \ \  E(\hat{\beta}_1) = \beta_1,\]</span>
that is, <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are unbiased estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the true parameters. If the sample is sufficiently large, by the central limit theorem the <em>joint</em> sampling distribution of the estimators is well approximated by the bivariate normal distribution (<a href="#mjx-eqn-2.1">2.1</a>). This implies that the marginal distributions are also normal in large samples. Core facts on the large-sample distributions of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are presented in Key Concept 4.4.</p>
<div id="KC4.4" class="keyconcept">
<h3 class="right">
Key Concept 4.4
</h3>
<h3 class="left">
Large Sample Distribution of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>
</h3>
<p>
<p>If the least squares assumptions in Key Concept 4.3 hold, then in large samples <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> have a joint normal sampling distribution. The large sample normal distribution of <span class="math inline">\(\hat\beta_1\)</span> is <span class="math inline">\(\mathcal{N}(\beta_1, \sigma^2_{\hat\beta_1})\)</span>, where the variance of the distribution, <span class="math inline">\(\sigma^2_{\hat\beta_1}\)</span>, is</p>
<p><span class="math display" id="eq:olsvar1">\[\begin{align}
\sigma^2_{\hat\beta_1} = \frac{1}{n} \frac{Var \left[ \left(X_i - \mu_X \right) u_i  \right]}  {\left[  Var \left(X_i \right)  \right]^2}. \tag{3.1}
\end{align}\]</span></p>
<p>The large sample normal distribution of <span class="math inline">\(\hat\beta_0\)</span> is <span class="math inline">\(\mathcal{N}(\beta_0, \sigma^2_{\hat\beta_0})\)</span> with</p>
<p><span class="math display" id="eq:olsvar2">\[\begin{align}
\sigma^2_{\hat\beta_0} =  \frac{1}{n} \frac{Var \left( H_i u_i \right)}{ \left[  E \left(H_i^2  \right)  \right]^2 } \ , \ \text{where} \ \ H_i = 1 - \left[ \frac{\mu_X} {E \left( X_i^2\right)} \right] X_i. \tag{3.2}
\end{align}\]</span></p>
<p>The interactive simulation below continuously generates random samples <span class="math inline">\((X_i,Y_i)\)</span> of <span class="math inline">\(200\)</span> observations where <span class="math inline">\(E(Y\vert X) = 100 + 3X\)</span>, estimates a simple regression model, stores the estimate of the slope <span class="math inline">\(\beta_1\)</span> and visualizes the distribution of the <span class="math inline">\(\widehat{\beta}_1\)</span>s observed so far using a histogram. The idea here is that for a large number of <span class="math inline">\(\widehat{\beta}_1\)</span>s, the histogram gives a good approximation of the sampling distribution of the estimator. By decreasing the time between two sampling iterations, it becomes clear that the shape of the histogram approaches the characteristic bell shape of a normal distribution centered at the true slope of <span class="math inline">\(3\)</span>.</p>
<iframe height="470" width="700" frameborder="0" scrolling="no" src="SmallSampleDIstReg.html">
</iframe>
<p>(Double-click on the histogram to restart the simulation.)</p>
</p>
</div>
<div id="simulation-study-1" class="section level3 unnumbered">
<h3>Simulation Study 1</h3>
<p>Whether the statements of Key Concept 4.4 really hold can also be verified using <tt>R</tt>. For this we first we build our own population of <span class="math inline">\(100000\)</span> observations in total. To do this we need values for the independent variable <span class="math inline">\(X\)</span>, for the error term <span class="math inline">\(u\)</span>, and for the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. With these combined in a simple regression model, we compute the dependent variable <span class="math inline">\(Y\)</span>. <br> In our example we generate the numbers <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i = 1\)</span>, … ,<span class="math inline">\(100000\)</span> by drawing a random sample from a uniform distribution on the interval <span class="math inline">\([0,20]\)</span>. The realizations of the error terms <span class="math inline">\(u_i\)</span> are drawn from a standard normal distribution with parameters <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma^2 = 100\)</span> (note that <tt>rnorm()</tt> requires <span class="math inline">\(\sigma\)</span> as input for the argument <tt>sd</tt>, see <tt>?rnorm</tt>). Furthermore we chose <span class="math inline">\(\beta_0 = -2\)</span> and <span class="math inline">\(\beta_1 = 3.5\)</span> so the true model is</p>
<p><span class="math display">\[ Y_i = -2 + 3.5 \cdot X_i. \]</span></p>
<p>Finally, we store the results in a data.frame.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1"><span class="co"># simulate data</span></a>
<a class="sourceLine" id="cb96-2" data-line-number="2">N &lt;-<span class="st"> </span><span class="dv">100000</span></a>
<a class="sourceLine" id="cb96-3" data-line-number="3">X &lt;-<span class="st"> </span><span class="kw">runif</span>(N, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb96-4" data-line-number="4">u &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dt">sd =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb96-5" data-line-number="5"></a>
<a class="sourceLine" id="cb96-6" data-line-number="6"><span class="co"># population regression</span></a>
<a class="sourceLine" id="cb96-7" data-line-number="7">Y &lt;-<span class="st"> </span><span class="dv">-2</span> <span class="op">+</span><span class="st"> </span><span class="fl">3.5</span> <span class="op">*</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span>u</a>
<a class="sourceLine" id="cb96-8" data-line-number="8">population &lt;-<span class="st"> </span><span class="kw">data.frame</span>(X, Y)</a></code></pre></div>
<p>From now on we will consider the previously generated data as the true population (which of course would be <em>unknown</em> in a real world application, otherwise there would be no reason to draw a random sample in the first place). The knowledge about the true population and the true relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> can be used to verify the statements made in Key Concept 4.4.</p>
<p>First, let us calculate the true variances <span class="math inline">\(\sigma^2_{\hat{\beta}_0}\)</span> and <span class="math inline">\(\sigma^2_{\hat{\beta}_1}\)</span> for a randomly drawn sample of size <span class="math inline">\(n = 100\)</span>.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" data-line-number="1"><span class="co"># set sample size</span></a>
<a class="sourceLine" id="cb97-2" data-line-number="2">n &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb97-3" data-line-number="3"></a>
<a class="sourceLine" id="cb97-4" data-line-number="4"><span class="co"># compute the variance of beta_hat_0</span></a>
<a class="sourceLine" id="cb97-5" data-line-number="5">H_i &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(X) <span class="op">/</span><span class="st"> </span><span class="kw">mean</span>(X<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>X</a>
<a class="sourceLine" id="cb97-6" data-line-number="6">var_b0 &lt;-<span class="st"> </span><span class="kw">var</span>(H_i <span class="op">*</span><span class="st"> </span>u) <span class="op">/</span><span class="st"> </span>(n <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(H_i<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb97-7" data-line-number="7"></a>
<a class="sourceLine" id="cb97-8" data-line-number="8"><span class="co"># compute the variance of hat_beta_1</span></a>
<a class="sourceLine" id="cb97-9" data-line-number="9">var_b1 &lt;-<span class="st"> </span><span class="kw">var</span>( ( X <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(X) ) <span class="op">*</span><span class="st"> </span>u ) <span class="op">/</span><span class="st"> </span>(<span class="dv">100</span> <span class="op">*</span><span class="st"> </span><span class="kw">var</span>(X)<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" data-line-number="1"><span class="co"># print variances to the console</span></a>
<a class="sourceLine" id="cb98-2" data-line-number="2">var_b0</a>
<a class="sourceLine" id="cb98-3" data-line-number="3"><span class="co">#&gt; [1] 4.045066</span></a>
<a class="sourceLine" id="cb98-4" data-line-number="4">var_b1</a>
<a class="sourceLine" id="cb98-5" data-line-number="5"><span class="co">#&gt; [1] 0.03018694</span></a></code></pre></div>
<p>Now let us assume that we do not know the true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and that it is not possible to observe the whole population. However, we can observe a random sample of <span class="math inline">\(n\)</span> observations. Then, it would not be possible to compute the true parameters but we could obtain estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> from the sample data using OLS. However, we know that these estimates are outcomes of random variables themselves since the observations are randomly sampled from the population. Key Concept 4.4 describes their distributions for large <span class="math inline">\(n\)</span>. When drawing a single sample of size <span class="math inline">\(n\)</span> it is not possible to make any statement about these distributions. Things change if we repeat the sampling scheme many times and compute the estimates for each sample: using this procedure we simulate outcomes of the respective distributions.</p>
<p>To achieve this in R, we employ the following approach:</p>
<ul>
<li>We assign the number of repetitions, say <span class="math inline">\(10000\)</span>, to <tt>reps</tt> and then initialize a matrix <tt>fit</tt> were the estimates obtained in each sampling iteration shall be stored row-wise. Thus <tt>fit</tt> has to be a matrix of dimensions <tt>reps</tt><span class="math inline">\(\times2\)</span>.</li>
<li>In the next step we draw <tt>reps</tt> random samples of size <tt>n</tt> from the population and obtain the OLS estimates for each sample. The results are stored as row entries in the outcome matrix <tt>fit</tt>. This is done using a <tt>for()</tt> loop.</li>
<li>At last, we estimate variances of both estimators using the sampled outcomes and plot histograms of the latter. We also add a plot of the density functions belonging to the distributions that follow from Key Concept 4.4. The function <tt>bquote()</tt> is used to obtain math expressions in the titles and labels of both plots. See <tt>?bquote</tt>.</li>
</ul>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" data-line-number="1"><span class="co"># set repetitions and sample size</span></a>
<a class="sourceLine" id="cb99-2" data-line-number="2">n &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb99-3" data-line-number="3">reps &lt;-<span class="st"> </span><span class="dv">10000</span></a>
<a class="sourceLine" id="cb99-4" data-line-number="4"></a>
<a class="sourceLine" id="cb99-5" data-line-number="5"><span class="co"># initialize the matrix of outcomes</span></a>
<a class="sourceLine" id="cb99-6" data-line-number="6">fit &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> reps)</a>
<a class="sourceLine" id="cb99-7" data-line-number="7"></a>
<a class="sourceLine" id="cb99-8" data-line-number="8"><span class="co"># loop sampling and estimation of the coefficients</span></a>
<a class="sourceLine" id="cb99-9" data-line-number="9"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>reps){</a>
<a class="sourceLine" id="cb99-10" data-line-number="10">  </a>
<a class="sourceLine" id="cb99-11" data-line-number="11"> sample &lt;-<span class="st"> </span>population[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>N, n), ]</a>
<a class="sourceLine" id="cb99-12" data-line-number="12"> fit[i, ] &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> sample)<span class="op">$</span>coefficients</a>
<a class="sourceLine" id="cb99-13" data-line-number="13"> </a>
<a class="sourceLine" id="cb99-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb99-15" data-line-number="15"></a>
<a class="sourceLine" id="cb99-16" data-line-number="16"><span class="co"># compute variance estimates using outcomes</span></a>
<a class="sourceLine" id="cb99-17" data-line-number="17"><span class="kw">var</span>(fit[, <span class="dv">1</span>])</a>
<a class="sourceLine" id="cb99-18" data-line-number="18"><span class="co">#&gt; [1] 4.186832</span></a>
<a class="sourceLine" id="cb99-19" data-line-number="19"><span class="kw">var</span>(fit[, <span class="dv">2</span>])</a>
<a class="sourceLine" id="cb99-20" data-line-number="20"><span class="co">#&gt; [1] 0.03096199</span></a></code></pre></div>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" data-line-number="1"><span class="co"># divide plotting area as 1-by-2 array</span></a>
<a class="sourceLine" id="cb100-2" data-line-number="2"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb100-3" data-line-number="3"></a>
<a class="sourceLine" id="cb100-4" data-line-number="4"><span class="co"># plot histograms of beta_0 estimates</span></a>
<a class="sourceLine" id="cb100-5" data-line-number="5"><span class="kw">hist</span>(fit[, <span class="dv">1</span>],</a>
<a class="sourceLine" id="cb100-6" data-line-number="6">     <span class="dt">cex.main =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb100-7" data-line-number="7">     <span class="dt">main =</span> <span class="kw">bquote</span>(The <span class="op">~</span><span class="st"> </span>Distribution  <span class="op">~</span><span class="st"> </span>of <span class="op">~</span><span class="st"> </span><span class="dv">10000</span> <span class="op">~</span><span class="st"> </span>beta[<span class="dv">0</span>] <span class="op">~</span><span class="st"> </span>Estimates), </a>
<a class="sourceLine" id="cb100-8" data-line-number="8">     <span class="dt">xlab =</span> <span class="kw">bquote</span>(<span class="kw">hat</span>(beta)[<span class="dv">0</span>]), </a>
<a class="sourceLine" id="cb100-9" data-line-number="9">     <span class="dt">freq =</span> F)</a>
<a class="sourceLine" id="cb100-10" data-line-number="10"></a>
<a class="sourceLine" id="cb100-11" data-line-number="11"><span class="co"># add true distribution to plot</span></a>
<a class="sourceLine" id="cb100-12" data-line-number="12"><span class="kw">curve</span>(<span class="kw">dnorm</span>(x, </a>
<a class="sourceLine" id="cb100-13" data-line-number="13">            <span class="dv">-2</span>, </a>
<a class="sourceLine" id="cb100-14" data-line-number="14">            <span class="kw">sqrt</span>(var_b0)), </a>
<a class="sourceLine" id="cb100-15" data-line-number="15">      <span class="dt">add =</span> T, </a>
<a class="sourceLine" id="cb100-16" data-line-number="16">      <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)</a>
<a class="sourceLine" id="cb100-17" data-line-number="17"></a>
<a class="sourceLine" id="cb100-18" data-line-number="18"><span class="co"># plot histograms of beta_hat_1 </span></a>
<a class="sourceLine" id="cb100-19" data-line-number="19"><span class="kw">hist</span>(fit[, <span class="dv">2</span>],</a>
<a class="sourceLine" id="cb100-20" data-line-number="20">    <span class="dt">cex.main =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb100-21" data-line-number="21">     <span class="dt">main =</span> <span class="kw">bquote</span>(The <span class="op">~</span><span class="st"> </span>Distribution  <span class="op">~</span><span class="st"> </span>of <span class="op">~</span><span class="st"> </span><span class="dv">10000</span> <span class="op">~</span><span class="st"> </span>beta[<span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>Estimates), </a>
<a class="sourceLine" id="cb100-22" data-line-number="22">     <span class="dt">xlab =</span> <span class="kw">bquote</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>]), </a>
<a class="sourceLine" id="cb100-23" data-line-number="23">     <span class="dt">freq =</span> F)</a>
<a class="sourceLine" id="cb100-24" data-line-number="24"></a>
<a class="sourceLine" id="cb100-25" data-line-number="25"><span class="co"># add true distribution to plot</span></a>
<a class="sourceLine" id="cb100-26" data-line-number="26"><span class="kw">curve</span>(<span class="kw">dnorm</span>(x, </a>
<a class="sourceLine" id="cb100-27" data-line-number="27">            <span class="fl">3.5</span>, </a>
<a class="sourceLine" id="cb100-28" data-line-number="28">            <span class="kw">sqrt</span>(var_b1)), </a>
<a class="sourceLine" id="cb100-29" data-line-number="29">      <span class="dt">add =</span> T, </a>
<a class="sourceLine" id="cb100-30" data-line-number="30">      <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)</a></code></pre></div>
<p><img src="AdvRA_files/figure-html/unnamed-chunk-168-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Our variance estimates support the statements made in Key Concept 4.4, coming close to the theoretical values. The histograms suggest that the distributions of the estimators can be well approximated by the respective theoretical normal distributions stated in Key Concept 4.4.</p>
</div>
<div id="simulation-study-2" class="section level3 unnumbered">
<h3>Simulation Study 2</h3>
<p>A further result implied by Key Concept 4.4 is that both estimators are consistent, i.e., they converge in probability to the true parameters we are interested in. This is because they are asymptotically unbiased and their variances converge to <span class="math inline">\(0\)</span> as <span class="math inline">\(n\)</span> increases. We can check this by repeating the simulation above for a sequence of increasing sample sizes. This means we no longer assign the sample size but a <em>vector</em> of sample sizes: <tt>n &lt;- c(…)</tt>. <br>
Let us look at the distributions of <span class="math inline">\(\beta_1\)</span>. The idea here is to add an additional call of <tt>for()</tt> to the code. This is done in order to loop over the vector of sample sizes <tt>n</tt>. For each of the sample sizes we carry out the same simulation as before but plot a density estimate for the outcomes of each iteration over <tt>n</tt>. Notice that we have to change <tt>n</tt> to <tt>n[j]</tt> in the inner loop to ensure that the <tt>j</tt><span class="math inline">\(^{th}\)</span> element of <tt>n</tt> is used. In the simulation, we use sample sizes of <span class="math inline">\(100, 250, 1000\)</span> and <span class="math inline">\(3000\)</span>. Consequently we have a total of four distinct simulations using different sample sizes.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb101-1" data-line-number="1"><span class="co"># set seed for reproducibility</span></a>
<a class="sourceLine" id="cb101-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb101-3" data-line-number="3"></a>
<a class="sourceLine" id="cb101-4" data-line-number="4"><span class="co"># set repetitions and the vector of sample sizes</span></a>
<a class="sourceLine" id="cb101-5" data-line-number="5">reps &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb101-6" data-line-number="6">n &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span>, <span class="dv">250</span>, <span class="dv">1000</span>, <span class="dv">3000</span>)</a>
<a class="sourceLine" id="cb101-7" data-line-number="7"></a>
<a class="sourceLine" id="cb101-8" data-line-number="8"><span class="co"># initialize the matrix of outcomes</span></a>
<a class="sourceLine" id="cb101-9" data-line-number="9">fit &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> reps)</a>
<a class="sourceLine" id="cb101-10" data-line-number="10"></a>
<a class="sourceLine" id="cb101-11" data-line-number="11"><span class="co"># divide the plot panel in a 2-by-2 array</span></a>
<a class="sourceLine" id="cb101-12" data-line-number="12"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb101-13" data-line-number="13"></a>
<a class="sourceLine" id="cb101-14" data-line-number="14"><span class="co"># loop sampling and plotting</span></a>
<a class="sourceLine" id="cb101-15" data-line-number="15"></a>
<a class="sourceLine" id="cb101-16" data-line-number="16"><span class="co"># outer loop over n</span></a>
<a class="sourceLine" id="cb101-17" data-line-number="17"><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n)) {</a>
<a class="sourceLine" id="cb101-18" data-line-number="18">  </a>
<a class="sourceLine" id="cb101-19" data-line-number="19">  <span class="co"># inner loop: sampling and estimating of the coefficients</span></a>
<a class="sourceLine" id="cb101-20" data-line-number="20">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>reps){</a>
<a class="sourceLine" id="cb101-21" data-line-number="21">    </a>
<a class="sourceLine" id="cb101-22" data-line-number="22">    sample &lt;-<span class="st"> </span>population[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>N, n[j]), ]</a>
<a class="sourceLine" id="cb101-23" data-line-number="23">    fit[i, ] &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> sample)<span class="op">$</span>coefficients</a>
<a class="sourceLine" id="cb101-24" data-line-number="24">    </a>
<a class="sourceLine" id="cb101-25" data-line-number="25">  }</a>
<a class="sourceLine" id="cb101-26" data-line-number="26">  </a>
<a class="sourceLine" id="cb101-27" data-line-number="27">  <span class="co"># draw density estimates</span></a>
<a class="sourceLine" id="cb101-28" data-line-number="28">  <span class="kw">plot</span>(<span class="kw">density</span>(fit[ ,<span class="dv">2</span>]), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="fl">2.5</span>, <span class="fl">4.5</span>), </a>
<a class="sourceLine" id="cb101-29" data-line-number="29">       <span class="dt">col =</span> j, </a>
<a class="sourceLine" id="cb101-30" data-line-number="30">       <span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&quot;n=&quot;</span>, n[j]), </a>
<a class="sourceLine" id="cb101-31" data-line-number="31">       <span class="dt">xlab =</span> <span class="kw">bquote</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>]))</a>
<a class="sourceLine" id="cb101-32" data-line-number="32">  </a>
<a class="sourceLine" id="cb101-33" data-line-number="33">}</a></code></pre></div>
<p><img src="AdvRA_files/figure-html/unnamed-chunk-169-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>We find that, as <span class="math inline">\(n\)</span> increases, the distribution of <span class="math inline">\(\hat\beta_1\)</span> concentrates around its mean, i.e., its variance decreases. Put differently, the likelihood of observing estimates close to the true value of <span class="math inline">\(\beta_1 = 3.5\)</span> grows as we increase the sample size. The same behavior can be observed if we analyze the distribution of <span class="math inline">\(\hat\beta_0\)</span> instead.</p>
</div>
<div id="simulation-study-3" class="section level3 unnumbered">
<h3>Simulation Study 3</h3>
<p>Furthermore, (4.1) reveals that the variance of the OLS estimator for <span class="math inline">\(\beta_1\)</span> decreases as the variance of the <span class="math inline">\(X_i\)</span> increases. In other words, as we increase the amount of information provided by the regressor, that is, increasing <span class="math inline">\(Var(X)\)</span>, which is used to estimate <span class="math inline">\(\beta_1\)</span>, we become more confident that the estimate is close to the true value (i.e., <span class="math inline">\(Var(\hat\beta_1)\)</span> decreases).<br>
We can visualize this by reproducing Figure 4.6 from the book. To do this, we sample observations <span class="math inline">\((X_i,Y_i)\)</span>, <span class="math inline">\(i=1,\dots,100\)</span> from a bivariate normal distribution with</p>
<p><span class="math display">\[E(X)=E(Y)=5,\]</span>
<span class="math display">\[Var(X)=Var(Y)=5\]</span>
and
<span class="math display">\[Cov(X,Y)=4.\]</span></p>
<p>Formally, this is written down as</p>
<p><span class="math display">\[\begin{align}
  \begin{pmatrix}
    X \\
    Y \\
  \end{pmatrix}
  \overset{i.i.d.}{\sim} &amp; \ \mathcal{N} 
  \left[
    \begin{pmatrix}
      5 \\
      5 \\
    \end{pmatrix}, \ 
    \begin{pmatrix}
      5 &amp; 4 \\
      4 &amp; 5 \\
    \end{pmatrix}
  \right]. \tag{4.3}
\end{align}\]</span></p>
<p>To carry out the random sampling, we make use of the function <tt>mvrnorm()</tt> from the package <tt>MASS</tt> <span class="citation">(Ripley <a href="#ref-R-MASS">2019</a>)</span> which allows to draw random samples from multivariate normal distributions, see <code>?mvtnorm</code>. Next, we use <tt>subset()</tt> to split the sample into two subsets such that the first set, <tt>set1</tt>, consists of observations that fulfill the condition <span class="math inline">\(\lvert X - \overline{X} \rvert &gt; 1\)</span> and the second set, <tt>set2</tt>, includes the remainder of the sample. We then plot both sets and use different colors to distinguish the observations.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" data-line-number="1"><span class="co"># load the MASS package</span></a>
<a class="sourceLine" id="cb102-2" data-line-number="2"><span class="kw">library</span>(MASS)</a>
<a class="sourceLine" id="cb102-3" data-line-number="3"></a>
<a class="sourceLine" id="cb102-4" data-line-number="4"><span class="co"># set seed for reproducibility</span></a>
<a class="sourceLine" id="cb102-5" data-line-number="5"><span class="kw">set.seed</span>(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb102-6" data-line-number="6"></a>
<a class="sourceLine" id="cb102-7" data-line-number="7"><span class="co"># simulate bivarite normal data</span></a>
<a class="sourceLine" id="cb102-8" data-line-number="8">bvndata &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dv">100</span>, </a>
<a class="sourceLine" id="cb102-9" data-line-number="9">                <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">5</span>), </a>
<a class="sourceLine" id="cb102-10" data-line-number="10">                <span class="dt">Sigma =</span> <span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">4</span>), <span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">5</span>))) </a>
<a class="sourceLine" id="cb102-11" data-line-number="11"></a>
<a class="sourceLine" id="cb102-12" data-line-number="12"><span class="co"># assign column names / convert to data.frame</span></a>
<a class="sourceLine" id="cb102-13" data-line-number="13"><span class="kw">colnames</span>(bvndata) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X&quot;</span>, <span class="st">&quot;Y&quot;</span>)</a>
<a class="sourceLine" id="cb102-14" data-line-number="14">bvndata &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(bvndata)</a>
<a class="sourceLine" id="cb102-15" data-line-number="15"></a>
<a class="sourceLine" id="cb102-16" data-line-number="16"><span class="co"># subset the data</span></a>
<a class="sourceLine" id="cb102-17" data-line-number="17">set1 &lt;-<span class="st"> </span><span class="kw">subset</span>(bvndata, <span class="kw">abs</span>(<span class="kw">mean</span>(X) <span class="op">-</span><span class="st"> </span>X) <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb102-18" data-line-number="18">set2 &lt;-<span class="st"> </span><span class="kw">subset</span>(bvndata, <span class="kw">abs</span>(<span class="kw">mean</span>(X) <span class="op">-</span><span class="st"> </span>X) <span class="op">&lt;=</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb102-19" data-line-number="19"></a>
<a class="sourceLine" id="cb102-20" data-line-number="20"><span class="co"># plot both data sets</span></a>
<a class="sourceLine" id="cb102-21" data-line-number="21"><span class="kw">plot</span>(set1, </a>
<a class="sourceLine" id="cb102-22" data-line-number="22">     <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>, </a>
<a class="sourceLine" id="cb102-23" data-line-number="23">     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>, </a>
<a class="sourceLine" id="cb102-24" data-line-number="24">     <span class="dt">pch =</span> <span class="dv">19</span>)</a>
<a class="sourceLine" id="cb102-25" data-line-number="25"></a>
<a class="sourceLine" id="cb102-26" data-line-number="26"><span class="kw">points</span>(set2, </a>
<a class="sourceLine" id="cb102-27" data-line-number="27">       <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, </a>
<a class="sourceLine" id="cb102-28" data-line-number="28">       <span class="dt">pch =</span> <span class="dv">19</span>)</a></code></pre></div>
<p><img src="AdvRA_files/figure-html/unnamed-chunk-170-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>It is clear that observations that are close to the sample average of the <span class="math inline">\(X_i\)</span> have less variance than those that are farther away. Now, if we were to draw a line as accurately as possible through either of the two sets it is intuitive that choosing the observations indicated by the black dots, i.e., using the set of observations which has larger variance than the blue ones, would result in a more precise line. Now, let us use OLS to estimate slope and intercept for both sets of observations. We then plot the observations along with both regression lines.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" data-line-number="1"><span class="co"># estimate both regression lines</span></a>
<a class="sourceLine" id="cb103-2" data-line-number="2">lm.set1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> set1)</a>
<a class="sourceLine" id="cb103-3" data-line-number="3">lm.set2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> set2)</a>
<a class="sourceLine" id="cb103-4" data-line-number="4"></a>
<a class="sourceLine" id="cb103-5" data-line-number="5"><span class="co"># plot observations</span></a>
<a class="sourceLine" id="cb103-6" data-line-number="6"><span class="kw">plot</span>(set1, <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>)</a>
<a class="sourceLine" id="cb103-7" data-line-number="7"><span class="kw">points</span>(set2, <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>)</a>
<a class="sourceLine" id="cb103-8" data-line-number="8"></a>
<a class="sourceLine" id="cb103-9" data-line-number="9"><span class="co"># add both lines to the plot</span></a>
<a class="sourceLine" id="cb103-10" data-line-number="10"><span class="kw">abline</span>(lm.set1, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>)</a>
<a class="sourceLine" id="cb103-11" data-line-number="11"><span class="kw">abline</span>(lm.set2, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p><img src="AdvRA_files/figure-html/unnamed-chunk-171-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Evidently, the green regression line does far better in describing data sampled from the bivariate normal distribution stated in (<a href="#mjx-eqn-4.3">4.3</a>) than the red line. This is a nice example for demonstrating why we are interested in a high variance of the regressor <span class="math inline">\(X\)</span>: more variance in the <span class="math inline">\(X_i\)</span> means more information from which the precision of the estimation benefits.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-MASS">
<p>Ripley, Brian. 2019. <em>MASS: Support Functions and Datasets for Venables and Ripley’s MASS</em> (version 7.3-51.4). <a href="https://CRAN.R-project.org/package=MASS">https://CRAN.R-project.org/package=MASS</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-4-tlsa.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-6-exercises-4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/lidom/AdvRA/edit/master/04-ch4.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
