<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.3 Properties of the OLS estimator | Econometrics (M.Sc.)</title>
  <meta name="description" content="2.3 Properties of the OLS estimator | Econometrics (M.Sc.)" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2.3 Properties of the OLS estimator | Econometrics (M.Sc.)" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/mylogo.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.3 Properties of the OLS estimator | Econometrics (M.Sc.)" />
  
  
  <meta name="twitter:image" content="images/mylogo.png" />

<meta name="author" content="Prof. Dr. Dominik Liebl" />


<meta name="date" content="2020-10-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2-2-ordinary-least-squares-estimation.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="organization-of-the-course.html"><a href="organization-of-the-course.html"><i class="fa fa-check"></i>Organization of the Course</a></li>
<li class="chapter" data-level="" data-path="literature.html"><a href="literature.html"><i class="fa fa-check"></i>Literature</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-review-probability-and-statistics.html"><a href="1-review-probability-and-statistics.html"><i class="fa fa-check"></i><b>1</b> Review: Probability and Statistics</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-probability-theory.html"><a href="1-1-probability-theory.html"><i class="fa fa-check"></i><b>1.1</b> Probability Theory</a></li>
<li class="chapter" data-level="1.2" data-path="1-2-random-variables.html"><a href="1-2-random-variables.html"><i class="fa fa-check"></i><b>1.2</b> Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-review-simple-linear-regression.html"><a href="2-review-simple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Review: Simple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-the-simple-linear-regression-model.html"><a href="2-1-the-simple-linear-regression-model.html"><i class="fa fa-check"></i><b>2.1</b> The Simple Linear Regression Model</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-ordinary-least-squares-estimation.html"><a href="2-2-ordinary-least-squares-estimation.html"><i class="fa fa-check"></i><b>2.2</b> Ordinary Least Squares Estimation</a></li>
<li class="chapter" data-level="2.3" data-path="2-3-properties-of-the-ols-estimator.html"><a href="2-3-properties-of-the-ols-estimator.html"><i class="fa fa-check"></i><b>2.3</b> Properties of the OLS estimator</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics (M.Sc.)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="properties-of-the-ols-estimator" class="section level2">
<h2><span class="header-section-number">2.3</span> Properties of the OLS estimator</h2>
<p>Note that we did not need to make any assumptions about the error term, and in particular, its relationship to <span class="math inline">\(X\)</span> in order to derive the estimator. We also did not make any assumption about the variance of <span class="math inline">\(\epsilon\)</span> or about the relationship between the error terms <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(\epsilon_j\)</span> for <span class="math inline">\(i\neq j\)</span> in order to derive this estimator. So why is OLS a ``good’’ estimator?</p>
<div id="mean-and-bias-of-the-ols-estimator" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Mean and Bias of the OLS Estimator</h3>
<p>Because the estimator
<span class="math display">\[\begin{align}
\hat{\boldsymbol{\beta}}=
\begin{pmatrix}\hat\beta_0\\\hat\beta_1\end{pmatrix}=
\begin{pmatrix}
\bar Y - \hat\beta_1 \bar X\\
\dfrac{\sum_{i=1}^n (X_i- \bar X)Y_i}{\sum_{i=1}^n (X_i-\bar X)^2}
\end{pmatrix}
\label{eq:OLSslreg}
\end{align}\]</span>
is a function of the random variables in the random sample <span class="math inline">\(((Y_1,X_1),\dots,(Y_n,X_n))\)</span>, it is itself a random variable. It therefore has a probability distribution, and we can talk sensibly about its expectation <span class="math inline">\(E(\hat{\boldsymbol{\beta}})\)</span>.</p>
<p>Note that the estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> in  contains both the <span class="math inline">\(Y_i\)</span>’s and the <span class="math inline">\(X_i\)</span>’s. Therefore, under random designs, we need to be specific which mean (or variance) of the estimator we want to consider. For the random design, we consider <em>conditional</em> means (and later variances) <em>given</em> the design matrix <span class="math inline">\(X=\begin{pmatrix}1&amp;X_1\\\vdots&amp;\vdots\\1&amp;X_n\end{pmatrix}\)</span>. That is, under random designs, we consider <span class="math inline">\(E[\hat{\boldsymbol{\beta}}|X]\)</span> which leads to the definition of the conditional bias:
<span class="math display">\[\begin{align*}
\operatorname{Bias}(\hat{\boldsymbol{\beta}}|X)=E[\hat{\boldsymbol{\beta}}|X]-\boldsymbol{\beta},
\end{align*}\]</span>
For fixed designs, conditioning on <span class="math inline">\(X\)</span> is not false but superfluous since <span class="math inline">\(X\)</span> remains fixed in repeated samples and therefore <span class="math inline">\(\operatorname{Bias}(\hat{\boldsymbol{\beta}}|X)=\operatorname{Bias}(\hat{\boldsymbol{\beta}})=E[\hat{\boldsymbol{\beta}}]-\boldsymbol{\beta}\)</span>. Since random designs are the more relevant scenarios in economics we will use conditional means (and variances) which just simply under a fixed design.</p>
<!-- The **bias** of an estimator is defined as -->
<!-- \begin{description} -->
<!-- \item[Fixed design:\phantom{on}]\operatorname{Bias}(\hat{\boldsymbol{\beta}})=E[\hat{\boldsymbol{\beta}}]-\boldsymbol{\beta} -->
<!-- \item[Random design:]\operatorname{Bias}(\hat{\boldsymbol{\beta}}|X)=E[\hat{\boldsymbol{\beta}}|X]-\boldsymbol{\beta} -->
<!-- \end{description} -->
<p>The OLS estimator is called , if its conditional expectation is equal to the true parameter, i.e. if
<span class="math display">\[\begin{align*}
E[\hat{\boldsymbol{\beta}}|X]=\boldsymbol{\beta}\quad
\Leftrightarrow\quad\operatorname{Bias}(\boldsymbol{\beta}|X)=0
\end{align*}\]</span>
That is, the conditional mean of the random estimator equals the true parameter value which we want to estimate. Put another way, unbiasedness means that the conditional distribution of the estimator is centered on the true value of the parameter. Of course, the probability that a estimate is  equal to the truth in any particular sample is zero if the error terms <span class="math inline">\(\epsilon_i\)</span> (and therefore the <span class="math inline">\(Y_i\)</span>) are continuous random variables.</p>
<p>To show that <span class="math inline">\(\hat\beta_1\)</span> is conditionally unbiased, we want to show that its conditional expectation is equal to <span class="math inline">\(\beta_1\)</span>:
<span class="math display">\[\begin{align*}
E[\hat\beta_1|X]&amp;=E\left[\dfrac{\sum_{i=1}^n(X_i - \bar X)Y_i}{\sum_{i=1}^n(X_i - \bar X)^2}|X\right] 
               =E\left[\dfrac{\sum_{i=1}^n(X_i - \bar X)(\beta_0 + \beta_1 X_i + \epsilon_i)}{\sum_{i=1}^n(X_i - \bar X)^2}|X\right]
\end{align*}\]</span>
In case of a deterministic design, <span class="math inline">\(X\)</span> is fixed in repeated samples which makes the conditioning on <span class="math inline">\(X\)</span> superfluous since, for instance, <span class="math inline">\(\sum(X_i - \bar X)^2\)</span> is then a constant which allows us to bring it outside of the expectation; the same applies to all other terms that involve  <span class="math inline">\(X\)</span> variables. In order to do this under a random design, it is important to condition on the observed design matrix <span class="math inline">\(X\)</span>. That is, in case of a random design, <em>conditioning</em> <span class="math inline">\(X\)</span> also allows us to bring all terms containing  <span class="math inline">\(X\)</span> variables outside of the expectation. However, interpretations then must consider the conditioning on <span class="math inline">\(X\)</span>.</p>
So, we really only need to concern ourselves with the numerator, which we can write as:
<span class="math display">\[\begin{equation*}
\beta_0 \sum_{i=1}^n(X_i - \bar X)+ 
\beta_1 \sum_{i=1}^n(X_i - \bar X)X_i + 
E\left[\sum_{i=1}^n(X_i - \bar X)\epsilon_i\right]
\end{equation*}\]</span>
For the first two terms, note that
<span class="math display">\[\begin{align*}
\beta_0 \sum_{i=1}^n(X_i - \bar X)      &amp;=\beta_0 \left[\left(\sum_{i=1}^n X_i \right) - n \bar X\right] = 0 \\
\beta_1 \sum_{i=1}^n(X_i - \bar X)X_i &amp;=\beta_1 \sum_{i=1}^n(X_i - \bar X)^2.
\end{align*}\]</span>
So we are left with
<span class="math display">\[E[\hat\beta_1|X]= \beta_1 + \dfrac{1}{\sum_{i=1}^n(X_i-\bar X)^2}E\left[\sum_{i=1}^n(X_i - \bar X)\epsilon_i\right|X]\]</span>
Clearly, then, we want the last term here to be zero, so that <span class="math inline">\(\hat\beta_1\)</span> is conditionally unbiased. The last term is zero if the factor <span class="math inline">\(E\left[\sum(X_i - \bar X)\epsilon_i|X\right]=0\)</span>. For fixed/random designs we have the following expressions:

<p>That is, the OLS estimator, <span class="math inline">\(\hat\beta_1\)</span>, is (conditionally) unbiased if the orthogonality assumption <span class="math inline">\(E[\epsilon_i |X_i]=0\)</span> holds, which simplifies under the fixed design to <span class="math inline">\(E[\epsilon_i]=0\)</span>. We return, therefore, to the importance of the orthogonality assumption <span class="math inline">\(E[\epsilon | X_i]\)</span> since if this assumption does not hold, the estimator is biased and remains biased even asymptotically as <span class="math inline">\(n\to\infty\)</span>.</p>
<p>The unbiasedness of <span class="math inline">\(\hat\beta_1\)</span> implies that <span class="math inline">\(\hat\beta_0=\bar{Y}-\hat\beta_1\bar{X}\)</span> since
<span class="math display">\[\begin{align*}
E[\hat\beta_0|X]
&amp;=E\left[\bar Y - \hat\beta_1 \bar X|X\right]\\
&amp;=E\left[\dfrac{1}{n}\sum_{i=1}^n (\beta_0 + \beta_1 X_i + \epsilon_i) - \hat\beta_1 \bar X |X\right]\\
&amp;=\beta_0 + \beta_1\bar X + \dfrac{1}{n}\sum_{i=1}^n E[\epsilon_i|X_i] - E[\hat\beta_1|X] \bar X \\
&amp;=\beta_0
\end{align*}\]</span>
where the last line follows from the exogeneity (or orthogonality) assumption <span class="math inline">\(E[\epsilon_i|X]=0\)</span> and the unbiasedness of <span class="math inline">\(\hat\beta_1\)</span>.</p>
<p>Note that the exogeneity assumption, <span class="math inline">\(E[\epsilon_i|X]=0\)</span>, is the only distributional assumption on the error terms (and the <span class="math inline">\(X_i\)</span>’s) that is needed to show that the OLS estimator is unbiased. In fact, OLS remains unbiased if the error terms are heteroscedastic and/or autocorrelated. However, under the more restrictive assumption that the error terms are <em>homoscedastic</em> and <em>not autocorrelated</em>, one can show that the OLS estimator is the most efficient (lowest variance) estimator within the family of all unbiased estimators. This result is known as the Gauss-Markov Theorem and we will consider this theorem in the context of the multiple regression model in more detail.</p>
</div>
<div id="variance-of-the-ols-estimator" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Variance of the OLS Estimator</h3>
<p>Remember: An unbiased estimator can still be a very imprecise (inefficient) estimator if it has a large variance. In order to derive the variance of the OLS estimator one needs more than only the exogeneity assumption. One gets different variance expressions for different distributional assumptions on the error terms <span class="math inline">\(\epsilon_1,\dots,\epsilon_n\)</span>. As we consider this in more detail for the multiple regression model, we consider in the following the most simple case of i.i.d. error terms which implies homoscedastic and non-autocorrelated error terms. Under this simple scenario it can be shown that
<span class="math display">\[\begin{align}
\operatorname{Var}(\hat\beta_0|X)&amp;= \frac{1}{n} \sigma^2\cdot\frac{n^{-1}\sum_{i=1}^n X_i^2}{n^{-1}\sum_{i=1}^n\left(X_i-\bar{X} \right)^2}\\
\operatorname{Var}(\hat\beta_1|X)&amp;= \frac{1}{n} \sigma^2\cdot\frac{1}{n^{-1}\sum_{i=1}^n\left(X_i-\bar{X} \right)^2}, \label{eq:varbeta}
\end{align}\]</span>
where <span class="math inline">\(\operatorname{Var}(\epsilon_i)=\sigma^2\)</span>. That is
<span class="math display">\[\begin{align*}
\operatorname{Var}(\hat\beta_0|X)&amp;\to 0 \quad\text{as}\quad n\to\infty\\
\operatorname{Var}(\hat\beta_1|X)&amp;\to 0 \quad\text{as}\quad n\to\infty
\end{align*}\]</span>
which is good since this means that the estimators become more precise as the sample size <span class="math inline">\(n\)</span> (i.e. more information) increases.</p>
<p>Note that variance expressions in  are not really useful in practice since usually we do not know the variance of the error terms <span class="math inline">\(\sigma^2\)</span>. Under the i.i.d. assumption we can replace the unknown <span class="math inline">\(\sigma^2\)</span> by plugging-in reasonable estimates such as
<span class="math display">\[\begin{align*}
s_{ML}^2&amp;=\frac{1}{n}\sum_{i=1}^n e_i^2\qquad\text{or}\qquad s^2=\frac{1}{n-1}\sum_{i=1}^n e_i^2
\end{align*}\]</span>
where <span class="math inline">\(s^2\)</span> is the unbiased estimator and <span class="math inline">\(s_{ML}^2\)</span> the aximum likelihood estimator of <span class="math inline">\(\sigma^2\)</span>.
This leads to the practically relevant variance estimators
<span class="math display">\[\begin{align*}
\widehat{\operatorname{Var}}(\hat\beta_0|X)&amp;= \frac{1}{n} s^2\cdot\frac{n^{-1}\sum_{i=1}^n X_i^2}{n^{-1}\sum_{i=1}^n\left(X_i-\bar{X} \right)^2}\\
\widehat{\operatorname{Var}}(\hat\beta_1|X)&amp;= \frac{1}{n} s^2\cdot\frac{1}{n^{-1}\sum_{i=1}^n\left(X_i-\bar{X} \right)^2}, \label{eq:varbeta}
\end{align*}\]</span></p>
</div>
<div id="consistency-of-the-ols-estimator" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Consistency of the OLS Estimator</h3>
<p>Now, we have everything together in order to show that the OLS estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are <strong>consistent</strong> estimators of the unknown parameter values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Since the OLS estimator is unbiased, we do not need to bother with a possible bias-problem – provided the assumptions of the simple linear regression model are true. Moreover, since the variances of the estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> converge to zero as <span class="math inline">\(n\to\infty\)</span>, we have that the Mean Squared Error (MSE) converges to zero, i.e.
<span class="math display">\[\begin{align*}
\operatorname{MSE}(\hat\beta_j|X)&amp;=E\left[(\hat\beta_j-\beta_j|X)^2\right]\\
&amp;=\underbrace{\left(\operatorname{Bias}(\hat\beta_j|X)\right)^2}_{=0}+\operatorname{Var}(\beta_j|X)^2\to 0\;\;\text{as}\;\;n\to\infty\;\text{for}\;j=1,2.
\end{align*}\]</span>
This means that</p>

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="2-2-ordinary-least-squares-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["econometrics_MA_script.pdf"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
