<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Econometrics with R</title>
  <meta name="description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programming and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Econometrics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://www.econometrics-with-r.org/" />
  <meta property="og:image" content="https://www.econometrics-with-r.org/images/cover.png" />
  <meta property="og:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programming and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="github-repo" content="mca91/EconometricsWithR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Econometrics with R" />
  
  <meta name="twitter:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programming and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="twitter:image" content="https://www.econometrics-with-r.org/images/cover.png" />

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2018-11-18">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="13-3-experimental-estimates-of-the-effect-of-class-size-reductions.html">
<link rel="next" href="14-ittsraf.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<!-- font families -->

<link href="https://fonts.googleapis.com/css?family=PT+Sans|Pacifico|Source+Sans+Pro" rel="stylesheet">

<script src="js/hideOutput.js"></script>

<!-- Mathjax -->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/default.js"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110299877-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110299877-1');
</script>

<!-- open review block -->

<script async defer src="https://hypothes.is/embed.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><center><img src="images/logo.png" alt="logo" width="50%" height="50%"style="margin: 15px 0 0 0"></center></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-introduction.html"><a href="1-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-a-very-short-introduction-to-r-and-rstudio.html"><a href="1-1-a-very-short-introduction-to-r-and-rstudio.html"><i class="fa fa-check"></i><b>1.1</b> A Very Short Introduction to <tt>R</tt> and <em>RStudio</em></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-pt.html"><a href="2-pt.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-random-variables-and-probability-distributions.html"><a href="2-1-random-variables-and-probability-distributions.html"><i class="fa fa-check"></i><b>2.1</b> Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="2-1-random-variables-and-probability-distributions.html"><a href="2-1-random-variables-and-probability-distributions.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="2-1-random-variables-and-probability-distributions.html"><a href="2-1-random-variables-and-probability-distributions.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="2-1-random-variables-and-probability-distributions.html"><a href="2-1-random-variables-and-probability-distributions.html#expected-value-mean-and-variance"><i class="fa fa-check"></i>Expected Value, Mean and Variance</a></li>
<li class="chapter" data-level="" data-path="2-1-random-variables-and-probability-distributions.html"><a href="2-1-random-variables-and-probability-distributions.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a></li>
<li class="chapter" data-level="" data-path="2-1-random-variables-and-probability-distributions.html"><a href="2-1-random-variables-and-probability-distributions.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="2-1-random-variables-and-probability-distributions.html"><a href="2-1-random-variables-and-probability-distributions.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="" data-path="2-1-random-variables-and-probability-distributions.html"><a href="2-1-random-variables-and-probability-distributions.html#thetdist"><i class="fa fa-check"></i>The Student t Distribution</a></li>
<li class="chapter" data-level="" data-path="2-1-random-variables-and-probability-distributions.html"><a href="2-1-random-variables-and-probability-distributions.html#the-f-distribution"><i class="fa fa-check"></i>The F Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-RSATDOSA.html"><a href="2-2-RSATDOSA.html"><i class="fa fa-check"></i><b>2.2</b> Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="2-2-RSATDOSA.html"><a href="2-2-RSATDOSA.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="2-2-RSATDOSA.html"><a href="2-2-RSATDOSA.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-exercises.html"><a href="2-3-exercises.html"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-arosur.html"><a href="3-arosur.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-estimation-of-the-population-mean.html"><a href="3-1-estimation-of-the-population-mean.html"><i class="fa fa-check"></i><b>3.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-potsm.html"><a href="3-2-potsm.html"><i class="fa fa-check"></i><b>3.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="3.3" data-path="3-3-hypothesis-tests-concerning-the-population-mean.html"><a href="3-3-hypothesis-tests-concerning-the-population-mean.html"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests Concerning the Population Mean</a><ul>
<li class="chapter" data-level="" data-path="3-3-hypothesis-tests-concerning-the-population-mean.html"><a href="3-3-hypothesis-tests-concerning-the-population-mean.html#the-p-value"><i class="fa fa-check"></i>The p-Value</a></li>
<li class="chapter" data-level="" data-path="3-3-hypothesis-tests-concerning-the-population-mean.html"><a href="3-3-hypothesis-tests-concerning-the-population-mean.html#calculating-the-p-value-when-the-standard-deviation-is-known"><i class="fa fa-check"></i>Calculating the p-Value when the Standard Deviation is Known</a></li>
<li class="chapter" data-level="" data-path="3-3-hypothesis-tests-concerning-the-population-mean.html"><a href="3-3-hypothesis-tests-concerning-the-population-mean.html#SVSSDASE"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li class="chapter" data-level="" data-path="3-3-hypothesis-tests-concerning-the-population-mean.html"><a href="3-3-hypothesis-tests-concerning-the-population-mean.html#calculating-the-p-value-when-the-standard-deviation-is-unknown"><i class="fa fa-check"></i>Calculating the p-value When the Standard Deviation is Unknown</a></li>
<li class="chapter" data-level="" data-path="3-3-hypothesis-tests-concerning-the-population-mean.html"><a href="3-3-hypothesis-tests-concerning-the-population-mean.html#the-t-statistic"><i class="fa fa-check"></i>The t-statistic</a></li>
<li class="chapter" data-level="" data-path="3-3-hypothesis-tests-concerning-the-population-mean.html"><a href="3-3-hypothesis-tests-concerning-the-population-mean.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="3-3-hypothesis-tests-concerning-the-population-mean.html"><a href="3-3-hypothesis-tests-concerning-the-population-mean.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-4-confidence-intervals-for-the-population-mean.html"><a href="3-4-confidence-intervals-for-the-population-mean.html"><i class="fa fa-check"></i><b>3.4</b> Confidence Intervals for the Population Mean</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-cmfdp.html"><a href="3-5-cmfdp.html"><i class="fa fa-check"></i><b>3.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="3.6" data-path="3-6-aattggoe.html"><a href="3-6-aattggoe.html"><i class="fa fa-check"></i><b>3.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="3.7" data-path="3-7-scatterplots-sample-covariance-and-sample-correlation.html"><a href="3-7-scatterplots-sample-covariance-and-sample-correlation.html"><i class="fa fa-check"></i><b>3.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
<li class="chapter" data-level="3.8" data-path="3-8-exercises-1.html"><a href="3-8-exercises-1.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-lrwor.html"><a href="4-lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-simple-linear-regression.html"><a href="4-1-simple-linear-regression.html"><i class="fa fa-check"></i><b>4.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="4.2" data-path="4-2-estimating-the-coefficients-of-the-linear-regression-model.html"><a href="4-2-estimating-the-coefficients-of-the-linear-regression-model.html"><i class="fa fa-check"></i><b>4.2</b> Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="4-2-estimating-the-coefficients-of-the-linear-regression-model.html"><a href="4-2-estimating-the-coefficients-of-the-linear-regression-model.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-3-measures-of-fit.html"><a href="4-3-measures-of-fit.html"><i class="fa fa-check"></i><b>4.3</b> Measures of Fit</a><ul>
<li class="chapter" data-level="" data-path="4-3-measures-of-fit.html"><a href="4-3-measures-of-fit.html#the-coefficient-of-determination"><i class="fa fa-check"></i>The Coefficient of Determination</a></li>
<li class="chapter" data-level="" data-path="4-3-measures-of-fit.html"><a href="4-3-measures-of-fit.html#the-standard-error-of-the-regression"><i class="fa fa-check"></i>The Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="4-3-measures-of-fit.html"><a href="4-3-measures-of-fit.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-4-tlsa.html"><a href="4-4-tlsa.html"><i class="fa fa-check"></i><b>4.4</b> The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="4-4-tlsa.html"><a href="4-4-tlsa.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption 1: The Error Term has Conditional Mean of Zero</a></li>
<li class="chapter" data-level="" data-path="4-4-tlsa.html"><a href="4-4-tlsa.html#assumption-2-independently-and-identically-distributed-data"><i class="fa fa-check"></i>Assumption 2: Independently and Identically Distributed Data</a></li>
<li class="chapter" data-level="" data-path="4-4-tlsa.html"><a href="4-4-tlsa.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption 3: Large Outliers are Unlikely</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-5-tsdotoe.html"><a href="4-5-tsdotoe.html"><i class="fa fa-check"></i><b>4.5</b> The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="4-5-tsdotoe.html"><a href="4-5-tsdotoe.html#simulation-study-1"><i class="fa fa-check"></i>Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="4-5-tsdotoe.html"><a href="4-5-tsdotoe.html#simulation-study-2"><i class="fa fa-check"></i>Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="4-5-tsdotoe.html"><a href="4-5-tsdotoe.html#simulation-study-3"><i class="fa fa-check"></i>Simulation Study 3</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4-6-exercises-2.html"><a href="4-6-exercises-2.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-htaciitslrm.html"><a href="5-htaciitslrm.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-testing-two-sided-hypotheses-concerning-the-slope-coefficient.html"><a href="5-1-testing-two-sided-hypotheses-concerning-the-slope-coefficient.html"><i class="fa fa-check"></i><b>5.1</b> Testing Two-Sided Hypotheses Concerning the Slope Coefficient</a></li>
<li class="chapter" data-level="5.2" data-path="5-2-cifrc.html"><a href="5-2-cifrc.html"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="5-2-cifrc.html"><a href="5-2-cifrc.html#simulation-study-confidence-intervals"><i class="fa fa-check"></i>Simulation Study: Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-3-rwxiabv.html"><a href="5-3-rwxiabv.html"><i class="fa fa-check"></i><b>5.3</b> Regression when X is a Binary Variable</a></li>
<li class="chapter" data-level="5.4" data-path="5-4-hah.html"><a href="5-4-hah.html"><i class="fa fa-check"></i><b>5.4</b> Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="5-4-hah.html"><a href="5-4-hah.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="5-4-hah.html"><a href="5-4-hah.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="5-4-hah.html"><a href="5-4-hah.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="5-5-the-gauss-markov-theorem.html"><a href="5-5-the-gauss-markov-theorem.html"><i class="fa fa-check"></i><b>5.5</b> The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="5-5-the-gauss-markov-theorem.html"><a href="5-5-the-gauss-markov-theorem.html#simulation-study-blue-estimator"><i class="fa fa-check"></i>Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="5-6-using-the-t-statistic-in-regression-when-the-sample-size-is-small.html"><a href="5-6-using-the-t-statistic-in-regression-when-the-sample-size-is-small.html"><i class="fa fa-check"></i><b>5.6</b> Using the t-Statistic in Regression When the Sample Size Is Small</a></li>
<li class="chapter" data-level="5.7" data-path="5-7-exercises-3.html"><a href="5-7-exercises-3.html"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-rmwmr.html"><a href="6-rmwmr.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-omitted-variable-bias.html"><a href="6-1-omitted-variable-bias.html"><i class="fa fa-check"></i><b>6.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="6.2" data-path="6-2-tmrm.html"><a href="6-2-tmrm.html"><i class="fa fa-check"></i><b>6.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="6-3-mofimr.html"><a href="6-3-mofimr.html"><i class="fa fa-check"></i><b>6.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="6.4" data-path="6-4-ols-assumptions-in-multiple-regression.html"><a href="6-4-ols-assumptions-in-multiple-regression.html"><i class="fa fa-check"></i><b>6.4</b> OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="6-4-ols-assumptions-in-multiple-regression.html"><a href="6-4-ols-assumptions-in-multiple-regression.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
<li class="chapter" data-level="" data-path="6-4-ols-assumptions-in-multiple-regression.html"><a href="6-4-ols-assumptions-in-multiple-regression.html#simulation-study-imperfect-multicollinearity"><i class="fa fa-check"></i>Simulation Study: Imperfect Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-5-the-distribution-of-the-ols-estimators-in-multiple-regression.html"><a href="6-5-the-distribution-of-the-ols-estimators-in-multiple-regression.html"><i class="fa fa-check"></i><b>6.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
<li class="chapter" data-level="6.6" data-path="6-6-exercises-4.html"><a href="6-6-exercises-4.html"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-htaciimr.html"><a href="7-htaciimr.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence Intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="7-1-hypothesis-tests-and-confidence-intervals-for-a-single-coefficient.html"><a href="7-1-hypothesis-tests-and-confidence-intervals-for-a-single-coefficient.html"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="7-2-an-application-to-test-scores-and-the-student-teacher-ratio.html"><a href="7-2-an-application-to-test-scores-and-the-student-teacher-ratio.html"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="7-2-an-application-to-test-scores-and-the-student-teacher-ratio.html"><a href="7-2-an-application-to-test-scores-and-the-student-teacher-ratio.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-3-joint-hypothesis-testing-using-the-f-statistic.html"><a href="7-3-joint-hypothesis-testing-using-the-f-statistic.html"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the F-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="7-4-confidence-sets-for-multiple-coefficients.html"><a href="7-4-confidence-sets-for-multiple-coefficients.html"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="7-5-model-specification-for-multiple-regression.html"><a href="7-5-model-specification-for-multiple-regression.html"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="7-5-model-specification-for-multiple-regression.html"><a href="7-5-model-specification-for-multiple-regression.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7-6-analysis-of-the-test-score-data-set.html"><a href="7-6-analysis-of-the-test-score-data-set.html"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
<li class="chapter" data-level="7.7" data-path="7-7-exercises-5.html"><a href="7-7-exercises-5.html"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-nrf.html"><a href="8-nrf.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="8.1" data-path="8-1-a-general-strategy-for-modelling-nonlinear-regression-functions.html"><a href="8-1-a-general-strategy-for-modelling-nonlinear-regression-functions.html"><i class="fa fa-check"></i><b>8.1</b> A General Strategy for Modelling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="8.2" data-path="8-2-nfoasiv.html"><a href="8-2-nfoasiv.html"><i class="fa fa-check"></i><b>8.2</b> Nonlinear Functions of a Single Independent Variable</a><ul>
<li class="chapter" data-level="" data-path="8-2-nfoasiv.html"><a href="8-2-nfoasiv.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="8-2-nfoasiv.html"><a href="8-2-nfoasiv.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8-3-interactions-between-independent-variables.html"><a href="8-3-interactions-between-independent-variables.html"><i class="fa fa-check"></i><b>8.3</b> Interactions Between Independent Variables</a></li>
<li class="chapter" data-level="8.4" data-path="8-4-nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html"><a href="8-4-nonlinear-effects-on-test-scores-of-the-student-teacher-ratio.html"><i class="fa fa-check"></i><b>8.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
<li class="chapter" data-level="8.5" data-path="8-5-exercises-6.html"><a href="8-5-exercises-6.html"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-asbomr.html"><a href="9-asbomr.html"><i class="fa fa-check"></i><b>9</b> Assessing Studies Based on Multiple Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="9-1-internal-and-external-validity.html"><a href="9-1-internal-and-external-validity.html"><i class="fa fa-check"></i><b>9.1</b> Internal and External Validity</a></li>
<li class="chapter" data-level="9.2" data-path="9-2-ttivomra.html"><a href="9-2-ttivomra.html"><i class="fa fa-check"></i><b>9.2</b> Threats to Internal Validity of Multiple Regression Analysis</a></li>
<li class="chapter" data-level="9.3" data-path="9-3-internal-and-external-validity-when-the-regression-is-used-for-forecasting.html"><a href="9-3-internal-and-external-validity-when-the-regression-is-used-for-forecasting.html"><i class="fa fa-check"></i><b>9.3</b> Internal and External Validity when the Regression is Used for Forecasting</a></li>
<li class="chapter" data-level="9.4" data-path="9-4-etsacs.html"><a href="9-4-etsacs.html"><i class="fa fa-check"></i><b>9.4</b> Example: Test Scores and Class Size</a></li>
<li class="chapter" data-level="9.5" data-path="9-5-exercises-7.html"><a href="9-5-exercises-7.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-rwpd.html"><a href="10-rwpd.html"><i class="fa fa-check"></i><b>10</b> Regression with Panel Data</a><ul>
<li class="chapter" data-level="10.1" data-path="10-1-panel-data.html"><a href="10-1-panel-data.html"><i class="fa fa-check"></i><b>10.1</b> Panel Data</a></li>
<li class="chapter" data-level="10.2" data-path="10-2-PDWTTP.html"><a href="10-2-PDWTTP.html"><i class="fa fa-check"></i><b>10.2</b> Panel Data with Two Time Periods: “Before and After” Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="10-3-fixed-effects-regression.html"><a href="10-3-fixed-effects-regression.html"><i class="fa fa-check"></i><b>10.3</b> Fixed Effects Regression</a><ul>
<li class="chapter" data-level="" data-path="10-3-fixed-effects-regression.html"><a href="10-3-fixed-effects-regression.html#estimation-and-inference"><i class="fa fa-check"></i>Estimation and Inference</a></li>
<li class="chapter" data-level="" data-path="10-3-fixed-effects-regression.html"><a href="10-3-fixed-effects-regression.html#application-to-traffic-deaths"><i class="fa fa-check"></i>Application to Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="10-4-regression-with-time-fixed-effects.html"><a href="10-4-regression-with-time-fixed-effects.html"><i class="fa fa-check"></i><b>10.4</b> Regression with Time Fixed Effects</a></li>
<li class="chapter" data-level="10.5" data-path="10-5-tferaaseffer.html"><a href="10-5-tferaaseffer.html"><i class="fa fa-check"></i><b>10.5</b> The Fixed Effects Regression Assumptions and Standard Errors for Fixed Effects Regression</a></li>
<li class="chapter" data-level="10.6" data-path="10-6-drunk-driving-laws-and-traffic-deaths.html"><a href="10-6-drunk-driving-laws-and-traffic-deaths.html"><i class="fa fa-check"></i><b>10.6</b> Drunk Driving Laws and Traffic Deaths</a></li>
<li class="chapter" data-level="10.7" data-path="10-7-exercises-8.html"><a href="10-7-exercises-8.html"><i class="fa fa-check"></i><b>10.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-rwabdv.html"><a href="11-rwabdv.html"><i class="fa fa-check"></i><b>11</b> Regression with a Binary Dependent Variable</a><ul>
<li class="chapter" data-level="11.1" data-path="11-1-binary-dependent-variables-and-the-linear-probability-model.html"><a href="11-1-binary-dependent-variables-and-the-linear-probability-model.html"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables and the Linear Probability Model</a></li>
<li class="chapter" data-level="11.2" data-path="11-2-palr.html"><a href="11-2-palr.html"><i class="fa fa-check"></i><b>11.2</b> Probit and Logit Regression</a><ul>
<li class="chapter" data-level="" data-path="11-2-palr.html"><a href="11-2-palr.html#probit-regression"><i class="fa fa-check"></i>Probit Regression</a></li>
<li class="chapter" data-level="" data-path="11-2-palr.html"><a href="11-2-palr.html#logit-regression"><i class="fa fa-check"></i>Logit Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="11-3-estimation-and-inference-in-the-logit-and-probit-models.html"><a href="11-3-estimation-and-inference-in-the-logit-and-probit-models.html"><i class="fa fa-check"></i><b>11.3</b> Estimation and Inference in the Logit and Probit Models</a></li>
<li class="chapter" data-level="11.4" data-path="11-4-application-to-the-boston-hmda-data.html"><a href="11-4-application-to-the-boston-hmda-data.html"><i class="fa fa-check"></i><b>11.4</b> Application to the Boston HMDA Data</a></li>
<li class="chapter" data-level="11.5" data-path="11-5-exercises-9.html"><a href="11-5-exercises-9.html"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-ivr.html"><a href="12-ivr.html"><i class="fa fa-check"></i><b>12</b> Instrumental Variables Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="12-1-TIVEWASRAASI.html"><a href="12-1-TIVEWASRAASI.html"><i class="fa fa-check"></i><b>12.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a></li>
<li class="chapter" data-level="12.2" data-path="12-2-TGIVRM.html"><a href="12-2-TGIVRM.html"><i class="fa fa-check"></i><b>12.2</b> The General IV Regression Model</a></li>
<li class="chapter" data-level="12.3" data-path="12-3-civ.html"><a href="12-3-civ.html"><i class="fa fa-check"></i><b>12.3</b> Checking Instrument Validity</a></li>
<li class="chapter" data-level="12.4" data-path="12-4-attdfc.html"><a href="12-4-attdfc.html"><i class="fa fa-check"></i><b>12.4</b> Application to the Demand for Cigarettes</a></li>
<li class="chapter" data-level="12.5" data-path="12-5-where-do-valid-instruments-come-from.html"><a href="12-5-where-do-valid-instruments-come-from.html"><i class="fa fa-check"></i><b>12.5</b> Where Do Valid Instruments Come From?</a></li>
<li class="chapter" data-level="12.6" data-path="12-6-exercises-10.html"><a href="12-6-exercises-10.html"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-eaqe.html"><a href="13-eaqe.html"><i class="fa fa-check"></i><b>13</b> Experiments and Quasi-Experiments</a><ul>
<li class="chapter" data-level="13.1" data-path="13-1-potential-outcomes-causal-effects-and-idealized-experiments.html"><a href="13-1-potential-outcomes-causal-effects-and-idealized-experiments.html"><i class="fa fa-check"></i><b>13.1</b> Potential Outcomes, Causal Effects and Idealized Experiments</a></li>
<li class="chapter" data-level="13.2" data-path="13-2-threats-to-validity-of-experiments.html"><a href="13-2-threats-to-validity-of-experiments.html"><i class="fa fa-check"></i><b>13.2</b> Threats to Validity of Experiments</a></li>
<li class="chapter" data-level="13.3" data-path="13-3-experimental-estimates-of-the-effect-of-class-size-reductions.html"><a href="13-3-experimental-estimates-of-the-effect-of-class-size-reductions.html"><i class="fa fa-check"></i><b>13.3</b> Experimental Estimates of the Effect of Class Size Reductions</a><ul>
<li class="chapter" data-level="" data-path="13-3-experimental-estimates-of-the-effect-of-class-size-reductions.html"><a href="13-3-experimental-estimates-of-the-effect-of-class-size-reductions.html#experimental-design-and-the-data-set"><i class="fa fa-check"></i>Experimental Design and the Data Set</a></li>
<li class="chapter" data-level="" data-path="13-3-experimental-estimates-of-the-effect-of-class-size-reductions.html"><a href="13-3-experimental-estimates-of-the-effect-of-class-size-reductions.html#analysis-of-the-star-data"><i class="fa fa-check"></i>Analysis of the STAR Data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="13-4-quasi-experiments.html"><a href="13-4-quasi-experiments.html"><i class="fa fa-check"></i><b>13.4</b> Quasi Experiments</a><ul>
<li class="chapter" data-level="" data-path="13-4-quasi-experiments.html"><a href="13-4-quasi-experiments.html#the-differences-in-differences-estimator"><i class="fa fa-check"></i>The Differences-in-Differences Estimator</a></li>
<li class="chapter" data-level="" data-path="13-4-quasi-experiments.html"><a href="13-4-quasi-experiments.html#regression-discontinuity-estimators"><i class="fa fa-check"></i>Regression Discontinuity Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-ittsraf.html"><a href="14-ittsraf.html"><i class="fa fa-check"></i><b>14</b> Introduction to Time Series Regression and Forecasting</a><ul>
<li class="chapter" data-level="14.1" data-path="14-1-using-regression-models-for-forecasting.html"><a href="14-1-using-regression-models-for-forecasting.html"><i class="fa fa-check"></i><b>14.1</b> Using Regression Models for Forecasting</a></li>
<li class="chapter" data-level="14.2" data-path="14-2-tsdasc.html"><a href="14-2-tsdasc.html"><i class="fa fa-check"></i><b>14.2</b> Time Series Data and Serial Correlation</a><ul>
<li class="chapter" data-level="" data-path="14-2-tsdasc.html"><a href="14-2-tsdasc.html#notation-lags-differences-logarithms-and-growth-rates"><i class="fa fa-check"></i>Notation, Lags, Differences, Logarithms and Growth Rates</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="14-3-autoregressions.html"><a href="14-3-autoregressions.html"><i class="fa fa-check"></i><b>14.3</b> Autoregressions</a><ul>
<li><a href="14-3-autoregressions.html#autoregressive-models-of-order-p">Autoregressive Models of Order <span class="math inline">\(p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14-4-cybtmpi.html"><a href="14-4-cybtmpi.html"><i class="fa fa-check"></i><b>14.4</b> Can You Beat the Market? (Part I)</a></li>
<li class="chapter" data-level="14.5" data-path="14-5-apatadlm.html"><a href="14-5-apatadlm.html"><i class="fa fa-check"></i><b>14.5</b> Additional Predictors and The ADL Model</a><ul>
<li class="chapter" data-level="" data-path="14-5-apatadlm.html"><a href="14-5-apatadlm.html#forecast-uncertainty-and-forecast-intervals"><i class="fa fa-check"></i>Forecast Uncertainty and Forecast Intervals</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="14-6-llsuic.html"><a href="14-6-llsuic.html"><i class="fa fa-check"></i><b>14.6</b> Lag Length Selection Using Information Criteria</a></li>
<li class="chapter" data-level="14.7" data-path="14-7-nit.html"><a href="14-7-nit.html"><i class="fa fa-check"></i><b>14.7</b> Nonstationarity I: Trends</a></li>
<li class="chapter" data-level="14.8" data-path="14-8-niib.html"><a href="14-8-niib.html"><i class="fa fa-check"></i><b>14.8</b> Nonstationarity II: Breaks</a></li>
<li class="chapter" data-level="14.9" data-path="14-9-can-you-beat-the-market-part-ii.html"><a href="14-9-can-you-beat-the-market-part-ii.html"><i class="fa fa-check"></i><b>14.9</b> Can You Beat the Market? (Part II)</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="15-eodce.html"><a href="15-eodce.html"><i class="fa fa-check"></i><b>15</b> Estimation of Dynamic Causal Effects</a><ul>
<li class="chapter" data-level="15.1" data-path="15-1-the-orange-juice-data.html"><a href="15-1-the-orange-juice-data.html"><i class="fa fa-check"></i><b>15.1</b> The Orange Juice Data</a></li>
<li class="chapter" data-level="15.2" data-path="15-2-dynamic-causal-effects.html"><a href="15-2-dynamic-causal-effects.html"><i class="fa fa-check"></i><b>15.2</b> Dynamic Causal Effects</a></li>
<li class="chapter" data-level="15.3" data-path="15-3-dynamic-multipliers-and-cumulative-dynamic-multipliers.html"><a href="15-3-dynamic-multipliers-and-cumulative-dynamic-multipliers.html"><i class="fa fa-check"></i><b>15.3</b> Dynamic Multipliers and Cumulative Dynamic Multipliers</a></li>
<li class="chapter" data-level="15.4" data-path="15-4-hac-standard-errors.html"><a href="15-4-hac-standard-errors.html"><i class="fa fa-check"></i><b>15.4</b> HAC Standard Errors</a></li>
<li class="chapter" data-level="15.5" data-path="15-5-estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors.html"><a href="15-5-estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors.html"><i class="fa fa-check"></i><b>15.5</b> Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors</a></li>
<li class="chapter" data-level="15.6" data-path="15-6-orange-juice-prices-and-cold-weather.html"><a href="15-6-orange-juice-prices-and-cold-weather.html"><i class="fa fa-check"></i><b>15.6</b> Orange Juice Prices and Cold Weather</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="16-atitsr.html"><a href="16-atitsr.html"><i class="fa fa-check"></i><b>16</b> Additional Topics in Time Series Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="16-1-vector-autoregressions.html"><a href="16-1-vector-autoregressions.html"><i class="fa fa-check"></i><b>16.1</b> Vector Autoregressions</a></li>
<li class="chapter" data-level="16.2" data-path="16-2-ooiatdfglsurt.html"><a href="16-2-ooiatdfglsurt.html"><i class="fa fa-check"></i><b>16.2</b> Orders of Integration and the DF-GLS Unit Root Test</a></li>
<li class="chapter" data-level="16.3" data-path="16-3-cointegration.html"><a href="16-3-cointegration.html"><i class="fa fa-check"></i><b>16.3</b> Cointegration</a></li>
<li class="chapter" data-level="16.4" data-path="16-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html"><a href="16-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html"><i class="fa fa-check"></i><b>16.4</b> Volatility Clustering and Autoregressive Conditional Heteroskedasticity</a><ul>
<li class="chapter" data-level="" data-path="16-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html"><a href="16-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html#arch-and-garch-models"><i class="fa fa-check"></i>ARCH and GARCH Models</a></li>
<li class="chapter" data-level="" data-path="16-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html"><a href="16-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html#application-to-stock-price-volatility"><i class="fa fa-check"></i>Application to Stock Price Volatility</a></li>
<li class="chapter" data-level="" data-path="16-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html"><a href="16-4-volatility-clustering-and-autoregressive-conditional-heteroskedasticity.html#summary-8"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Econometrics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div class = rmdreview>
This book is in <b>Open Review</b>. We want your feedback to make the book better for you and other students. You may annotate some text by <span style="background-color: #3297FD; color: white">selecting it with the cursor</span> and then click the <i class="h-icon-annotate"></i> on the pop-up menu. You can also see the annotations of others: click the <i class="h-icon-chevron-left"></i> in the upper right hand corner of the page <i class="fa fa-arrow-circle-right  fa-rotate-315" aria-hidden="true"></i>
</div>
<div id="quasi-experiments" class="section level2">
<h2><span class="header-section-number">13.4</span> Quasi Experiments</h2>
<p>In quasi-experiments, “as if” randomness is exploited to use methods similar to those that have been discussed in the previous chapter. There are two types of quasi-experiments:<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<ol style="list-style-type: decimal">
<li><p>Random variations in individual circumstances allow to view the treatment “as if” it was randomly determined.</p></li>
<li><p>The treatment is only partially determined by “as if” random variation.</p></li>
</ol>
<p>The former allows to estimate the effect using either model <a href="13-1-potential-outcomes-causal-effects-and-idealized-experiments.html#eq:diffestwar">(13.2)</a>, i.e., the <em>difference estimator with additional regressors</em>, or, if there is doubt that the “as if” randomness does not entirely ensure that there are no systematic differences between control and treatment group, using the <em>differences-in-differences</em> (DID) estimator. In the latter case, an IV approach for estimation of a model like <a href="13-1-potential-outcomes-causal-effects-and-idealized-experiments.html#eq:diffestwar">(13.2)</a> which uses the source of “as if” randomness in treatment assignment as the instrument may be applied.</p>
<p>Some more advanced techniques that are helpful in settings where the treatment assignment is (partially) determined by a threshold in a so-called running variable are <em>sharp regression discontinuity design</em> (RDD) and <em>fuzzy regression discontinuity design</em> (FRDD).</p>
<p>We briefly review these techniques and, since the book does not provide any empirical examples in this section, we will use our own simulated data in a minimal example to discuss how DID, RDD and FRDD can be applied in <tt>R</tt>.</p>
<div id="the-differences-in-differences-estimator" class="section level3 unnumbered">
<h3>The Differences-in-Differences Estimator</h3>
<p>In quasi-experiments the source of “as if” randomness in treatment assignment can often not entirely prevent systematic differences between control and treatment groups. This problem was encountered by <span class="citation">Card &amp; Krueger (<a href="#ref-card1994">1994</a>)</span> who use geography as the “as if” random treatment assignment to study the effect on employment in fast-food restaurants caused by an increase in the state minimum wage in New Jersey in the year of 1992. Their idea was to use the fact that the increase in minimum wage applied to employees in New Jersey (treatment group) but not to those living in neighboring Pennsylvania (control group).</p>
<p>It is quite conceivable that such a wage hike is not correlated with other determinants of employment. However, there still might be some state-specific differences and thus differences between control and treatment group. This would render the <em>differences estimator</em> biased and inconsistent. <span class="citation">Card &amp; Krueger (<a href="#ref-card1994">1994</a>)</span> solved this by using a DID estimator: they collected data in February 1992 (before the treatment) and November 1992 (after the treatment) for the same restaurants and estimated the effect of the wage hike by analyzing differences in the differences in employment for New Jersey and Pennsylvania before and after the increase.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> The DID estimator is</p>
<span class="math display" id="eq:DID">\[\begin{align}
  \widehat{\beta}_1^{\text{diffs-in-diffs}} =&amp; \, (\overline{Y}^{\text{treatment,after}} - \overline{Y}^{\text{treatment,before}}) - (\overline{Y}^{\text{control,after}} - \overline{Y}^{\text{control,before}}) \\
  =&amp; \Delta \overline{Y}^{\text{treatment}} - \Delta \overline{Y}^{\text{control}} \tag{13.8}
\end{align}\]</span>
<p>with</p>
<ul>
<li><p><span class="math inline">\(\overline{Y}^{\text{treatment,before}}\)</span> - the sample average in the treatment group before the treatment</p></li>
<li><p><span class="math inline">\(\overline{Y}^{\text{treatment,after}}\)</span> - the sample average in the treatment group after the treatment</p></li>
<li><p><span class="math inline">\(\overline{Y}^{\text{treatment,before}}\)</span> - the sample average in the control group before the treatment</p></li>
<li><p><span class="math inline">\(\overline{Y}^{\text{treatment,after}}\)</span> - the sample average in the control group after the treatment.</p></li>
</ul>
<p>We now use <tt>R</tt> to reproduce Figure 13.1 of the book.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize plot and add control group</span>
<span class="kw">plot</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">6</span>, <span class="dv">8</span>), 
     <span class="dt">type =</span> <span class="st">&quot;p&quot;</span>,
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">12</span>),
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.3</span>, <span class="fl">1.3</span>),
     <span class="dt">main =</span> <span class="st">&quot;The Differences-in-Differences Estimator&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Period&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">xaxt =</span> <span class="st">&quot;n&quot;</span>,
     <span class="dt">yaxt =</span> <span class="st">&quot;n&quot;</span>)

<span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;before&quot;</span>, <span class="st">&quot;after&quot;</span>))
<span class="kw">axis</span>(<span class="dv">2</span>, <span class="dt">at =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">13</span>))

<span class="co"># add treatment group</span>
<span class="kw">points</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">11</span>), 
       <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>,
       <span class="dt">pch =</span> <span class="dv">20</span>)

<span class="co"># add line segments</span>
<span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">11</span>), <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)
<span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">6</span>, <span class="dv">8</span>), <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>)
<span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">9</span>), <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">9</span>, <span class="dv">11</span>), <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)

<span class="co"># add annotations</span>
<span class="kw">text</span>(<span class="dv">1</span>, <span class="dv">10</span>, <span class="kw">expression</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>]<span class="op">^</span>{DID}), <span class="dt">cex =</span> <span class="fl">0.8</span>, <span class="dt">pos =</span> <span class="dv">4</span>)
<span class="kw">text</span>(<span class="dv">0</span>, <span class="fl">5.5</span>, <span class="st">&quot;s. mean control&quot;</span>, <span class="dt">cex =</span> <span class="fl">0.8</span> , <span class="dt">pos =</span> <span class="dv">4</span>)
<span class="kw">text</span>(<span class="dv">0</span>, <span class="fl">6.8</span>, <span class="st">&quot;s. mean treatment&quot;</span>, <span class="dt">cex =</span> <span class="fl">0.8</span> , <span class="dt">pos =</span> <span class="dv">4</span>)
<span class="kw">text</span>(<span class="dv">1</span>, <span class="fl">7.9</span>, <span class="st">&quot;s. mean control&quot;</span>, <span class="dt">cex =</span> <span class="fl">0.8</span> , <span class="dt">pos =</span> <span class="dv">4</span>)
<span class="kw">text</span>(<span class="dv">1</span>, <span class="fl">11.1</span>, <span class="st">&quot;s. mean treatment&quot;</span>, <span class="dt">cex =</span> <span class="fl">0.8</span> , <span class="dt">pos =</span> <span class="dv">4</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-553-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
The DID estimator <a href="13-4-quasi-experiments.html#eq:DID">(13.8)</a> can also be written in regression notation: <span class="math inline">\(\widehat{\beta}_1^{\text{DID}}\)</span> is the OLS estimator of <span class="math inline">\(\beta_1\)</span> in
<span class="math display" id="eq:did">\[\begin{align}
  \Delta Y_i = \beta_0 + \beta_1 X_i + u_i, \tag{13.9}
\end{align}\]</span>
<p>where <span class="math inline">\(\Delta Y_i\)</span> denotes the difference in pre- and post-treatment outcomes of individual <span class="math inline">\(i\)</span> and <span class="math inline">\(X_i\)</span> is the treatment indicator.</p>
Adding additional regressors that measure pre-treatment characteristics to <a href="13-4-quasi-experiments.html#eq:did">(13.9)</a> we obtain
<span class="math display" id="eq:didwar">\[\begin{align}
  \Delta Y_i = \beta_0 + \beta_1 X_i + \beta_2 W_{1i} + \dots + \beta_{1+r} W_{ri} + u_i, \tag{13.10}
\end{align}\]</span>
<p>the <em>difference-in-differences estimator</em> with additional regressors. The additional regressors may lead to a more precise estimate of <span class="math inline">\(\beta_1\)</span>.</p>
<p>We keep things simple and focus on estimation of the treatment effect using DID in the simplest case, that is a control and a treatment group observed for two time periods — one before and one after the treatment. In particular, we will see that there are three different ways to proceed.</p>
<p>First, we simulate pre- and post-treatment data using <tt>R</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set sample size</span>
n &lt;-<span class="st"> </span><span class="dv">200</span>

<span class="co"># define treatment effect</span>
TEffect &lt;-<span class="st"> </span><span class="dv">4</span>

<span class="co"># generate treatment dummy</span>
TDummy &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">/</span><span class="dv">2</span>), <span class="kw">rep</span>(<span class="dv">1</span>, n<span class="op">/</span><span class="dv">2</span>))

<span class="co"># simulate pre- and post-treatment values of the dependent variable</span>
y_pre &lt;-<span class="st"> </span><span class="dv">7</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n)
y_pre[<span class="dv">1</span><span class="op">:</span>n<span class="op">/</span><span class="dv">2</span>] &lt;-<span class="st"> </span>y_pre[<span class="dv">1</span><span class="op">:</span>n<span class="op">/</span><span class="dv">2</span>] <span class="op">-</span><span class="st"> </span><span class="dv">1</span>
y_post &lt;-<span class="st"> </span><span class="dv">7</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>TEffect <span class="op">*</span><span class="st"> </span>TDummy <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n)
y_post[<span class="dv">1</span><span class="op">:</span>n<span class="op">/</span><span class="dv">2</span>] &lt;-<span class="st"> </span>y_post[<span class="dv">1</span><span class="op">:</span>n<span class="op">/</span><span class="dv">2</span>] <span class="op">-</span><span class="st"> </span><span class="dv">1</span> </code></pre></div>
<p>Next plot the data. The function <tt>jitter()</tt> is used to add some artificial dispersion in the horizontal component of the points so that there is less overplotting. The function <tt>alpha()</tt> from the package <tt>scales</tt> allows to adjust the opacity of colors used in plots.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(scales)

pre &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(y_pre[TDummy<span class="op">==</span><span class="dv">0</span>]))
post &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">length</span>(y_pre[TDummy<span class="op">==</span><span class="dv">0</span>]))

<span class="co"># plot control group in t=1</span>
<span class="kw">plot</span>(<span class="kw">jitter</span>(pre, <span class="fl">0.6</span>), 
     y_pre[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">0</span>], 
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">16</span>), 
     <span class="dt">col =</span> <span class="kw">alpha</span>(<span class="st">&quot;steelblue&quot;</span>, <span class="fl">0.3</span>),
     <span class="dt">pch =</span> <span class="dv">20</span>, 
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>),
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Period&quot;</span>,
     <span class="dt">xaxt =</span> <span class="st">&quot;n&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Artificial Data for DID Estimation&quot;</span>)

<span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;before&quot;</span>, <span class="st">&quot;after&quot;</span>))

<span class="co"># add treatment group in t=1</span>
<span class="kw">points</span>(<span class="kw">jitter</span>(pre, <span class="fl">0.6</span>), 
       y_pre[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">1</span>], 
       <span class="dt">col =</span> <span class="kw">alpha</span>(<span class="st">&quot;darkred&quot;</span>, <span class="fl">0.3</span>), 
       <span class="dt">pch =</span> <span class="dv">20</span>)

<span class="co"># add control group in t=2</span>
<span class="kw">points</span>(<span class="kw">jitter</span>(post, <span class="fl">0.6</span>),
       y_post[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">0</span>], 
       <span class="dt">col =</span> <span class="kw">alpha</span>(<span class="st">&quot;steelblue&quot;</span>, <span class="fl">0.5</span>),
       <span class="dt">pch =</span> <span class="dv">20</span>)

<span class="co"># add treatment group in t=2</span>
<span class="kw">points</span>(<span class="kw">jitter</span>(post, <span class="fl">0.6</span>), 
       y_post[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">1</span>], 
       <span class="dt">col =</span> <span class="kw">alpha</span>(<span class="st">&quot;darkred&quot;</span>, <span class="fl">0.5</span>),
       <span class="dt">pch =</span> <span class="dv">20</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-555-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>Observations from both the control and treatment group have a higher mean after the treatment but that the increase is stronger for the treatment group. Using DID we may estimate how much of that difference is due to the treatment.</p>
<p>It is straightforward to compute the DID estimate in the fashion of <a href="13-4-quasi-experiments.html#eq:DID">(13.8)</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the DID estimator for the treatment effect &#39;by hand&#39;</span>
<span class="kw">mean</span>(y_post[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">1</span>]) <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(y_pre[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">1</span>]) <span class="op">-</span><span class="st"> </span>
(<span class="kw">mean</span>(y_post[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">0</span>]) <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(y_pre[TDummy <span class="op">==</span><span class="st"> </span><span class="dv">0</span>]))</code></pre></div>
<pre><code>## [1] 3.960268</code></pre>
<p>Notice that the estimate is close to <span class="math inline">\(4\)</span>, the value chosen as the treatment effect <tt>TEffect</tt> above. Since <a href="13-4-quasi-experiments.html#eq:did">(13.9)</a> is a simple linear model, we may perform OLS estimation of this regression specification using <tt>lm()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the DID estimator using a linear model</span>
<span class="kw">lm</span>(<span class="kw">I</span>(y_post <span class="op">-</span><span class="st"> </span>y_pre) <span class="op">~</span><span class="st"> </span>TDummy)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = I(y_post - y_pre) ~ TDummy)
## 
## Coefficients:
## (Intercept)       TDummy  
##       2.104        3.960</code></pre>
We find that the estimates coincide. Furthermore, one can show that the DID estimate obtained by estimating specification <a href="13-4-quasi-experiments.html#eq:did">(13.9)</a> OLS is the same as the OLS estimate of <span class="math inline">\(\beta_{TE}\)</span> in
<span class="math display" id="eq:DIDint">\[\begin{align}
  Y_i =&amp; \beta_0 + \beta_1 D_i + \beta_2 Period_i + \beta_{TE} (Period_i \times D_i) \tag{13.11}
\end{align}\]</span>
<p>where <span class="math inline">\(D_i\)</span> is the binary treatment indicator, <span class="math inline">\(Period_i\)</span> is a binary indicator for the after-treatment period and the <span class="math inline">\(Period_i \times D_i\)</span> is the interaction of both.</p>
<p>As for <a href="13-4-quasi-experiments.html#eq:did">(13.9)</a>, estimation of <a href="13-4-quasi-experiments.html#eq:DIDint">(13.11)</a> using <tt>R</tt> is straightforward. See Chapter <a href="8-nrf.html#nrf">8</a> for a discussion of interaction terms.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># prepare data for DID regression using the interaction term </span>
d &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="st">&quot;Y&quot;</span> =<span class="st"> </span><span class="kw">c</span>(y_pre,y_post),
                <span class="st">&quot;Treatment&quot;</span> =<span class="st"> </span>TDummy, 
                <span class="st">&quot;Period&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;1&quot;</span>, n), <span class="kw">rep</span>(<span class="st">&quot;2&quot;</span>, n)))

<span class="co"># estimate the model</span>
<span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>Treatment <span class="op">*</span><span class="st"> </span>Period, <span class="dt">data =</span> d)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ Treatment * Period, data = d)
## 
## Coefficients:
##       (Intercept)          Treatment            Period2  
##             5.858              1.197              2.104  
## Treatment:Period2  
##             3.960</code></pre>
<p>As expected, the estimate of the coefficient on the interaction of the treatment dummy and the time dummy coincide with the estimates obtained using <a href="13-4-quasi-experiments.html#eq:DID">(13.8)</a> and OLS estimation of <a href="13-4-quasi-experiments.html#eq:did">(13.9)</a>.</p>
</div>
<div id="regression-discontinuity-estimators" class="section level3 unnumbered">
<h3>Regression Discontinuity Estimators</h3>
Consider the model
<span class="math display" id="eq:SRDDsetting">\[\begin{align}
  Y_i =&amp; \beta_0 + \beta_1 X_i + \beta_2 W_i + u_i \tag{13.12}
\end{align}\]</span>
and let
<span class="math display">\[\begin{align*}
X_i =&amp; 
  \begin{cases}
    1, &amp; W_i \geq c \\
    0, &amp; W_i &lt; c
  \end{cases}
\end{align*}\]</span>
<p>so that the receipt of treatment, <span class="math inline">\(X_i\)</span>, is determined by some threshold <span class="math inline">\(c\)</span> of a continuous variable <span class="math inline">\(W_i\)</span>, the so called running variable. The idea of <em>regression discontinuity design</em> is to use observations with a <span class="math inline">\(W_i\)</span> close to <span class="math inline">\(c\)</span> for estimation of <span class="math inline">\(\beta_1\)</span>. <span class="math inline">\(\beta_1\)</span> is the average treatment effect for individuals with <span class="math inline">\(W_i = c\)</span> which is assumed to be a good approximation to the treatment effect in the population. <a href="13-4-quasi-experiments.html#eq:SRDDsetting">(13.12)</a> is called a <em>sharp regression discontinuity design</em> because treatment assignment is deterministic and discontinuous at the cutoff: all observations with <span class="math inline">\(W_i &lt; c\)</span> do not receive treatment and all observations where <span class="math inline">\(W_i \geq c\)</span> are treated.</p>
<p>The subsequent code chunks show how to estimate a linear SRDD using <tt>R</tt> and how to produce plots in the way of Figure 13.2 of the book.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate some sample data</span>
W &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1000</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)
y &lt;-<span class="st"> </span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>W <span class="op">+</span><span class="st"> </span><span class="dv">10</span> <span class="op">*</span><span class="st"> </span>(W<span class="op">&gt;=</span><span class="dv">0</span>) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>)</code></pre></div>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the package &#39;rddtools&#39;</span>
<span class="kw">library</span>(rddtools)

<span class="co"># construct rdd_data </span>
data &lt;-<span class="st"> </span><span class="kw">rdd_data</span>(y, W, <span class="dt">cutpoint =</span> <span class="dv">0</span>)

<span class="co"># plot the sample data</span>
<span class="kw">plot</span>(data,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">cex =</span> <span class="fl">0.35</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;W&quot;</span>, 
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-560-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>The argument <tt>nbins</tt> sets the number of bins the running variable is divided into for aggregation. The dots represent bin averages of the outcome variable.</p>
<p>We may use the function <tt>rdd_reg_lm()</tt> to estimate the treatment effect using model <a href="13-4-quasi-experiments.html#eq:SRDDsetting">(13.12)</a> for the artificial data generated above. By choosing <tt>slope = “same”</tt> we restrict the slopes of the estimated regression function to be the same on both sides of the jump at the cutpoint <span class="math inline">\(W=0\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the sharp RDD model</span>
rdd_mod &lt;-<span class="st"> </span><span class="kw">rdd_reg_lm</span>(<span class="dt">rdd_object =</span> data, 
                      <span class="dt">slope =</span> <span class="st">&quot;same&quot;</span>)
<span class="kw">summary</span>(rdd_mod)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ ., data = dat_step1, weights = weights)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2361 -0.6779 -0.0039  0.7113  3.0096 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.93889    0.07082   41.50   &lt;2e-16 ***
## D           10.12692    0.12631   80.18   &lt;2e-16 ***
## x            1.88249    0.11074   17.00   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.019 on 997 degrees of freedom
## Multiple R-squared:  0.972,  Adjusted R-squared:  0.972 
## F-statistic: 1.732e+04 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The coefficient estimate of interest is labeled <tt>D</tt>. The estimate is very close to the treatment effect chosen in the DGP above.</p>
<p>It is easy to visualize the result: simply call <tt>plot()</tt> on the estimated model object.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the RDD model along with binned observations</span>
<span class="kw">plot</span>(rdd_mod,
     <span class="dt">cex =</span> <span class="fl">0.35</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;W&quot;</span>, 
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-562-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>As above, the dots represent averages of binned observations.</p>
<p>So far we assumed that crossing of the threshold determines receipt of treatment so that the jump of the population regression functions at the threshold can be regarded as the causal effect of the treatment.</p>
<p>When crossing the threshold <span class="math inline">\(c\)</span> is not the only cause for receipt of the treatment, treatment is not a deterministic function of <span class="math inline">\(W_i\)</span>. Instead, it is useful to think of <span class="math inline">\(c\)</span> as a threshold where the <em>probability</em> of receiving the treatment jumps.</p>
This jump may be due to unobservable variables that have impact on the probability of being treated. Thus, <span class="math inline">\(X_i\)</span> in <a href="13-4-quasi-experiments.html#eq:SRDDsetting">(13.12)</a> will be correlated with the error <span class="math inline">\(u_i\)</span> and it becomes more difficult to consistently estimate the treatment effect. In this setting, using a <em>fuzzy regression discontinuity design</em> which is based an IV approach may be a remedy: take the binary variable <span class="math inline">\(Z_i\)</span> as an indicator for crossing of the threshold,
<span class="math display">\[\begin{align*}
  Z_i = \begin{cases}
    1, &amp; W_i \geq c \\
    0, &amp; W_i &lt; c,
  \end{cases}
\end{align*}\]</span>
<p>and assume that <span class="math inline">\(Z_i\)</span> relates to <span class="math inline">\(Y_i\)</span> only through the treatment indicator <span class="math inline">\(X_i\)</span>. Then <span class="math inline">\(Z_i\)</span> and <span class="math inline">\(u_i\)</span> are uncorrelated but <span class="math inline">\(Z_i\)</span> influences receipt of treatment so it is correlated with <span class="math inline">\(X_i\)</span>. Thus, <span class="math inline">\(Z_i\)</span> is a valid instrument for <span class="math inline">\(X_i\)</span> and <a href="13-4-quasi-experiments.html#eq:SRDDsetting">(13.12)</a> can be estimated using TSLS.</p>
<p>The following code chunk generates sample data where observations with a value of the running variable <span class="math inline">\(W_i\)</span> below the cutoff <span class="math inline">\(c=0\)</span> do not receive treatment and observations with <span class="math inline">\(W_i \geq 0\)</span> do receive treatment with a probability of <span class="math inline">\(80\%\)</span> so that treatment status is only partially determined by the running variable and the cutoff. Treatment leads to an increase in <span class="math inline">\(Y\)</span> by <span class="math inline">\(2\)</span> units. Observations with <span class="math inline">\(W_i \geq 0\)</span> that do not receive treatment are called <em>no-shows</em>: think of an individual that was assigned to receive the treatment but somehow manages to avoid it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)

<span class="co"># generate sample data</span>
mu &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>)
sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">0.7</span>, <span class="fl">0.7</span>, <span class="dv">1</span>), <span class="dt">ncol =</span> <span class="dv">2</span>)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
d &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">mvrnorm</span>(<span class="dv">2000</span>, mu, sigma))
<span class="kw">colnames</span>(d) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;W&quot;</span>, <span class="st">&quot;Y&quot;</span>)

<span class="co"># introduce fuzziness</span>
d<span class="op">$</span>treatProb &lt;-<span class="st"> </span><span class="kw">ifelse</span>(d<span class="op">$</span>W <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.8</span>)

fuzz &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dt">X =</span> d<span class="op">$</span>treatProb, <span class="dt">FUN =</span> <span class="cf">function</span>(x) <span class="kw">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">prob =</span> x))

<span class="co"># treatment effect</span>
d<span class="op">$</span>Y &lt;-<span class="st"> </span>d<span class="op">$</span>Y <span class="op">+</span><span class="st"> </span>fuzz <span class="op">*</span><span class="st"> </span><span class="dv">2</span></code></pre></div>
<p><tt>sapply()</tt> applies the function provided to <tt>FUN</tt> to every element of the argument <tt>X</tt>. Here, <tt>d$treatProb</tt> is a vector and the result is a vector, too.</p>
<p>We plot all observations and use blue color to mark individuals that did not receive the treatment and use red color for those who received the treatment.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate a colored plot of treatment and control group</span>
<span class="kw">plot</span>(d<span class="op">$</span>W, d<span class="op">$</span>Y,
     <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;steelblue&quot;</span>, <span class="st">&quot;darkred&quot;</span>)[<span class="kw">factor</span>(fuzz)], 
     <span class="dt">pch=</span> <span class="dv">20</span>, 
     <span class="dt">cex =</span> <span class="fl">0.5</span>,
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>),
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="dv">5</span>),
     <span class="dt">xlab =</span> <span class="st">&quot;W&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)

<span class="co"># add a dashed vertical line at cutoff</span>
<span class="kw">abline</span>(<span class="dt">v =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-564-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>Obviously, receipt of treatment is no longer a deterministic function of the running variable <span class="math inline">\(W\)</span>. Some observations with <span class="math inline">\(W\geq0\)</span> <em>did not</em> receive the treatment. We may estimate a FRDD by additionally setting <tt>treatProb</tt> as the assignment variable <tt>z</tt> in <tt>rdd_data()</tt>. Then <tt>rdd_reg_lm()</tt> applies the following TSLS procedure: treatment is predicted using <span class="math inline">\(W_i\)</span> and the cutoff dummy <span class="math inline">\(Z_i\)</span>, the instrumental variable, in the first stage regression. The fitted values from the first stage regression are used to obtain a consistent estimate of the treatment effect using the second stage where the outcome <span class="math inline">\(Y\)</span> is regressed on the fitted values and the running variable <span class="math inline">\(W\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the Fuzzy RDD</span>
data &lt;-<span class="st"> </span><span class="kw">rdd_data</span>(d<span class="op">$</span>Y, d<span class="op">$</span>W, 
                 <span class="dt">cutpoint =</span> <span class="dv">0</span>, 
                 <span class="dt">z =</span> d<span class="op">$</span>treatProb)

frdd_mod &lt;-<span class="st"> </span><span class="kw">rdd_reg_lm</span>(<span class="dt">rdd_object =</span> data, 
                       <span class="dt">slope =</span> <span class="st">&quot;same&quot;</span>)
frdd_mod</code></pre></div>
<pre><code>## ### RDD regression: parametric ###
##  Polynomial order:  1 
##  Slopes:  same 
##  Number of obs: 2000 (left: 999, right: 1001)
## 
##  Coefficient:
##   Estimate Std. Error t value  Pr(&gt;|t|)    
## D 1.981297   0.084696  23.393 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The estimate is close to <span class="math inline">\(2\)</span>, the population treatment effect. We may call <tt>plot()</tt> on the model object to obtain a figure consisting of binned data and the estimated regression function.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot estimated FRDD function</span>
<span class="kw">plot</span>(frdd_mod, 
     <span class="dt">cex =</span> <span class="fl">0.5</span>, 
     <span class="dt">lwd =</span> <span class="fl">0.4</span>,
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>),
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="dv">5</span>),
     <span class="dt">xlab =</span> <span class="st">&quot;W&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-566-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p>What if we used a SRDD instead, thereby ignoring the fact that treatment is not perfectly determined by the cutoff in <span class="math inline">\(W\)</span>? We may get an impression of the consequences by estimating an SRDD using the previously simulated data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate SRDD</span>
data &lt;-<span class="st"> </span><span class="kw">rdd_data</span>(d<span class="op">$</span>Y, 
                 d<span class="op">$</span>W, 
                 <span class="dt">cutpoint =</span> <span class="dv">0</span>)

srdd_mod &lt;-<span class="st"> </span><span class="kw">rdd_reg_lm</span>(<span class="dt">rdd_object =</span> data, 
                       <span class="dt">slope =</span> <span class="st">&quot;same&quot;</span>)
srdd_mod</code></pre></div>
<pre><code>## ### RDD regression: parametric ###
##  Polynomial order:  1 
##  Slopes:  same 
##  Number of obs: 2000 (left: 999, right: 1001)
## 
##  Coefficient:
##   Estimate Std. Error t value  Pr(&gt;|t|)    
## D 1.585038   0.067756  23.393 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The estimate obtained using a SRDD is suggestive of a substantial downward bias. In fact, this procedure is inconsistent for the true causal effect so increasing the sample would not alleviate the bias.</p>
<p>The book continues with a discussion of potential problems with quasi-experiments. As for all empirical studies, these potential problems are related to internal and external validity. This part is followed by a technical discussion of treatment effect estimation when the causal effect of treatment is heterogeneous in the population. We encourage you to work on these sections on your own.</p>
<div id="summary-5" class="section level4 unnumbered">
<h4>Summary</h4>
<p>This chapter has introduced the concept of causal effects in randomized controlled experiments and quasi-experiments where variations in circumstances or accidents of nature are treated as sources of “as if” random assignment to treatment. We have also discussed methods that allow for consistent estimation of these effects in both settings. These included the <em>differences estimator</em>, the <em>differences-in-differences estimator</em> as well as <em>sharp</em> and <em>fuzzy regression discontinuity design</em> estimators. It was shown how to apply these estimation techniques in <tt>R</tt>.</p>
<p>In an empirical application we have shown how to replicate the results of the analysis of the STAR data presented in Chapter 13.3 of the book using <tt>R</tt>. This study uses a randomized controlled experiment to assess whether smaller classes improve students’ performance on standardized tests. Being related to a randomized controlled experiment, the data of this study is fundamentally different to those used in the cross-section studies in Chapters <a href="4-lrwor.html#lrwor">4</a> to <a href="8-nrf.html#nrf">8</a>. We therefore have motivated usage of a <em>differences estimator</em>.</p>
<p>Chapter <a href="12-4-attdfc.html#attdfc">12.4</a> demonstrated how estimates of treatment effects can be obtained when the design of the study is a quasi-experiment that allows for <em>differences-in-differences</em> or <em>regression discontinuity design</em> estimators. In particular, we have introduced functions of the package <tt>rddtools</tt> that are convenient for estimation as well as graphical analysis when estimating a regression discontinuity design.</p>

</div>
</div>
</div>
<!-- </div> -->
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-card1994">
<p>Card, D., &amp; Krueger, A. B. (1994). Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania. <em>The American Economic Review</em>, <em>84</em>(4), 772–793.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="9">
<li id="fn9"><p>See Chapter 13.4 of the book for some example studies that are based on quasi-experiments.<a href="13-4-quasi-experiments.html#fnref9">↩</a></p></li>
<li id="fn10"><p>Also see the box <em>What is the Effect on Employment of the Minimum Wage?</em> in Chapter 13.4 of the book.<a href="13-4-quasi-experiments.html#fnref10">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="13-3-experimental-estimates-of-the-effect-of-class-size-reductions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="14-ittsraf.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": true,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/mca91/EconometricsWithR/edit/master/13-ch13.Rmd",
"text": "Edit"
},
"download": ["ITER.pdf"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
