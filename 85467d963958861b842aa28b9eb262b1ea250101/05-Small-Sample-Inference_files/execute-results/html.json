{
  "hash": "57fe699306e12bd6c6cd6e2d5fa82efc",
  "result": {
    "markdown": "# Small Sample Inference {#sec-ssinf}\n\nThe content of this chapter is very much inspired by the book by @Hayashi2000. \n\n\nIt's is very hard to say when a sample size $n$ is small. Often people say something like $n<30$ means small samples and $n\\geq 30$ large samples, but this is, of course, only a very rough rule of thumb that may fail. The core issue with small sample sizes is that we cannot do inference using the central limit theorem. Thus we need rather strict assumptions on the distribution of the error term, in order to do **exact** inference in finite samples. \n\n**Exact inference:**  By \"exact inference\" we mean correct inference for each sample size $n$. That is, no asymptotic $(n\\to\\infty)$ arguments will be used.  \n\n**Assumptions:** Recall that we, in general, did not impose a complete distributional assumption on $\\varepsilon$ in Assumption 4 (@sec-MLR); the i.i.d. normal case in Assumption 4 was only one possible *option*.  However, to do exact inference, the normality Assumption on the error terms is not a mere option, but a *necessity*. So for this chapter we assume that Assumptions 1-3 from @sec-MLR hold and that additionally the following assumption holds: \n\n**Assumption 4$^\\boldsymbol{\\ast}$: Error distribution:** For small sample cases, we assume that the error terms are \n**i.i.d. normal**, i.e., $\\varepsilon_i\\overset{\\operatorname{i.i.d.}}{\\sim}\\mathcal{N}(0,\\sigma^2)$ for all $i=1,\\dots,n$ which leads to spherical errors. That is, \n\n$$\n\\varepsilon\\sim\\mathcal{N}\\left(0,\\sigma^2I_n\\right),\n$$\n\nwhere $\\varepsilon=(\\varepsilon_1,\\dots,\\varepsilon_n)'$.\n\n\\bigskip\n\n\n::: {#thm-normalbeta}\n\n# Normality of $\\hat\\beta$ \n\nUnder Assumptions 1-4$^\\ast$ we have that \n\n$$\n\\hat\\beta_n|X \\sim \\mathcal{N}\\left(\\beta,Var(\\hat\\beta_n|X)\\right),\n$${#eq-ssnorm}\n\nwhere $Var(\\hat\\beta_n|X)=\\sigma^2(X'X)^{-1}$.\n:::\n\nThis result follows from noting that $\\hat\\beta_n=(X'X)^{-1}X'Y=\\beta+(X'X)^{-1}X'\\varepsilon$ and because $(X'X)^{-1}X'\\varepsilon$ is just a linear combination of the normally distributed error terms $\\varepsilon$ which, therefore, is again normally distributed, conditionally on $X$. Note that the specific normal distribution depends on the observed realization of $X$. \n\n\n**Remark:** The subscript $n$ in $\\hat\\beta_n$ is here only to emphasize that the distribution of $\\hat\\beta_n$ depends on $n$; we will, however, often simply write $\\hat\\beta$.\n\n\n## Hypothesis Tests about Multiple Parameters (F-Tests) {#sec-testmultp}\n\nLet us consider the following system of $q$-many null hypotheses:\n\\begin{align*}\nH_0: \\underset{(q\\times K)}{R}\\underset{(K\\times 1)}{\\beta} - \\underset{(q\\times 1)}{r} = \\underset{(q\\times 1)}{0},\n\\end{align*}\nwhere the $(q \\times K)$ matrix $R$ and the $q$-vector $r=(r_{1},\\dots,r_{q})'$ are chosen by the statistician to specify her/his null hypothesis about the unknown true parameter vector $\\beta$. To make sure that there are no redundant equations, it is required that $\\operatorname{rank}(R)=q$.\n\nWe must also specify the alternative against which we are testing the null hypothesis, for instance\n\\begin{equation*}\nH_A: R\\beta -r \\neq 0\n\\end{equation*}\n\nThe above multiple parameter hypotheses cover also the special case of single parameter hypothesis; for instance, by setting $R=(0,1,0\\dots,0)$ and $r=0$ one get's\n\\begin{equation*}\n\\begin{array}{ll}\nH_0:  & \\beta_{k}=0 \\\\\nH_A:  & \\beta_{k} \\ne 0 \\\\\n\\end{array}\n\\end{equation*}\n\nUnder our assumptions (Assumptions 1 to 4$^\\ast$), we have that \n\n$$\n(R\\hat\\beta_n-r)|X\\sim\\mathcal{N}\\left(R\\beta -r,RVar(\\hat\\beta_n|X)R'\\right).\n$$ \n\nSo, the realizations of $(R\\hat\\beta_n -r)|X$ will scatter around the *unknown* $(R\\beta -r)$ in a non-systematical, Gaussian fashion. Therefore, if the null hypothesis is correct (i.e., $(R\\beta-r)=0$), the realizations of $(R\\hat\\beta_n-r)|X$ scatter around the $q$-vector $0$.  If, however, the alternative hypothesis is correct (i.e., $(R\\beta-r)=a\\neq 0$), the realizations of $R\\hat\\beta_n-r|X$ scatter around the $q$-vector $a\\neq 0$.  So, under the alternative hypothesis, there will be a systematic location-shift of the $q$-dimensional random variable $R\\hat\\beta_n|X$ away from $r$ which we try to detect using statistical hypothesis testing. \n\n\n### The Test Statistic and its Null Distribution\n\nThe fact that $(R\\hat\\beta_n-r)\\in\\mathbb{R}^q$ is a $q$-dimensional random variable makes it a little bothersome to use as a test-statistic.  Fortunately, we can turn $(R\\hat\\beta_n-r)$ into a scalar-valued test statistic using the following quadratic form:\n\n$$\nW=\\underbrace{(R\\hat\\beta_n -r)'}_{(1\\times q)}\\underbrace{[RVar(\\hat\\beta_n|X)R']^{-1}}_{(q\\times q)}\\underbrace{(R\\hat\\beta_n -r)}_{(q\\times 1)}\n$$\n\nNote that the test statistic $W$ is simply measuring the distance (it's a weighted L2-distance of two vectors) between $R\\hat\\beta_n$ and $r$. \n\nUnder the null hypothesis (i.e., if the null hypothesis is true), $W$ is just a sum of $q$-many independent squared standard normal random variables. Therefore, under the null hypothesis, $W$ is chi-square distributed with $q$ degrees of freedom (see @sec-chisqdist), \n\n$$\nW\\overset{H_0}{\\sim} \\chi^2_{(q)}.\n$$\n\nNote that the distribution of $W$ conditional on $X$ does not depend on $X$; i.e. $W|X\\overset{H_0}{\\sim}\\chi^2_{(q)},$ but $\\chi^2_{(q)}$ does here not depend on $X$, thus we can write $W\\overset{H_0}{\\sim} \\chi^2_{(q)}.$\n\n\nUsually, we do not know $Var(\\hat\\beta_n|X)$ , but need to estimate this quantity from the data. Unfortunately, in the small sample case, we can only deal with homoskedastic error terms. For truly exact finite sample inference, we need a variance estimator for which we can derive the exact small sample distribution. Therefore, we assume in Assumption 4$^*$ spherical errors (i.e., $Var(\\varepsilon|X)=I_n\\sigma^2$) which yields that $Var(\\hat\\beta_n|X)=\\sigma^2(X'X)^{-1}$, and where $\\sigma^2$ can be estimated by the unbiased ($UB$) variance estimator  \n\n$$\ns_{UB}^2=(n-K)^{-1}\\sum_{i=1}^n\\hat\\varepsilon_i^2.\n$$  \n\nFrom the normality assumption in Assumption 4$^*$, it follows then that \n\n$$\n\\frac{(n-K)}{\\sigma^{2}}s_{UB}^2\\sim\\chi^2_{(n-K)}.\n$${#eq-distsquared} \n\n\nThe $F$ test statistic \n\n$$\nF=(R\\hat\\beta_n -r)'[R(s_{UB}^2(X'X)^{-1})R']^{-1}(R\\hat\\beta_n -r)/q\n$$\n\ntakes into account the additional randomness due to the estimator $s_{UB}^2$, which leads to the following exact null distribution of the $F$ test\n\n$$\nF\\overset{H_0}{\\sim} F_{(q,n-K)},\n$${#eq-Ftest}\n\nwhere $F_{(q,n-K)}$ denotes the $F$-distribution with $q$ numerator and $n-K$ denominator degrees of freedom. As in the case of $W$, the distribution of $F$ conditional on $X$ does not depend on $X$; i.e. $F|X\\overset{H_0}{\\sim}F_{(q,n-K)},$ but $F_{(q,n-K)}$ does here not depend on $X$, thus we can write $F\\overset{H_0}{\\sim}F_{(q,n-K)}.$\n\nThe distributional statements in @eq-distsquared and @eq-Ftest are a little cumbersome to derive and we do not go into details here, but in case you're interested you can find some more details in Chapter 1 of @Hayashi2000. \n\nBy contrast to $W$, $F$ is now a practically useful test statistic, and we can use the observed value $F_{\\text{obs}}$ to measure the distance of our estimate $R\\hat\\beta_n$ from value $r$.  Observed values, $F_{\\text{obs}}$, that are \"unusually large\" under the null hypothesis, lead to a rejection of the null hypothesis. The null distribution $F_{(q,n-K)}$ of $F$ is used to judge what's \"unusually large\" under the null hypothesis. \n\n\n**The F distribution:** The F distribution is a ratio of two $\\chi^2$ distributions. It has two parameters: the numerator degrees of freedom, and the denominator degrees of freedom. Each combination of the parameters yields a different F distribution. See @sec-Fdist for more information on the $F$ distribution. \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-Small-Sample-Inference_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=1\\textwidth}\n:::\n:::\n\n\n\n## Tests about One Parameter (t-Tests) {#ch:testingsinglep}\n\nFor testing a hypothesis about only one parameter $\\beta_k$, with $k=1,\\dots,K$\n\\begin{equation*}\n\\begin{array}{ll}\nH_0: & \\beta_k=r\\\\\nH_A: & \\beta_k\\ne r\\\\\n\\end{array}\n\\end{equation*}\nthe $(q\\times K)=(1\\times K)$-matrix $R$ equals a row-vector of zeros but with a one as the $k$th element; e.g., for $k=2$ we have $R=(0,1,0,\\dots,0).$ Thus, the $F$ test statistic simplifies to\n\n$$\nF=\\frac{\\left(\\hat{\\beta}_k-r\\right)^2}{\\widehat{Var}(\\hat{\\beta}_k|X)},\n$$\n\nwhere $\\widehat{Var}(\\hat{\\beta}_k|X)=s^2_{UB}[(X'X)^{-1}]_{kk}.$ \n\nTaking square roots yields the $t$ test statistic \n\n$$\nt=\\frac{\\hat{\\beta}_k-r}{\\widehat{\\operatorname{SE}}(\\hat{\\beta}_k|X)},\n$$\n\nwhere $\\widehat{\\operatorname{SE}}(\\hat{\\beta}_k|X)=s_{UB}[(X'X)^{-1/2}]_{kk}.$\n\nUnder the null hypothesis, the $t$ test statistic is $t$-distributed with $n-K$ degrees of freedom\n\n$$\nt\\overset{H_0}{\\sim}t_{(n-K)}.\n$$\n\nThus the $t$-distribution with $n-K$ degrees of freedom is the appropriate distribution to judge whether or not an observed value $t_{obs}$ of the test statistic is unusually small or large under the null hypothesis. \n\n**Note:** All commonly used statistical software packages report $t$-tests testing the null hypothesis $H_0:\\beta_k=0$, i.e., with $r=0$.  This means to test the null hypothesis that $X_k$ has \"no (linear) effect\" on $Y$. \n\nThe following plot illustrates that as the degrees of freedom increase, the shape of the $t$ distribution comes closer to that of a standard normal bell curve. Already for $25$ degrees of freedom we find little difference to the standard normal density. In case of small degrees of freedom values, we find the distribution to have heavier tails than a standard normal.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-Small-Sample-Inference_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=1\\textwidth}\n:::\n:::\n\n\n## Testtheory\n\n### Significance Level $\\alpha$\n\nTo actually test the null hypothesis (e.g., $H_0$: $R\\beta-r=0$ or $H_0$: $\\beta_k=0$), we need to have a decision rule on when we will reject or not reject the null hypothesis. This amounts to deciding on a probability with which we are comfortable with committing a Type I error ($\\alpha$-error): rejecting the null hypothesis when it is in fact true. The probability of such a Type I error shall be bounded from above by a (small) significance level $\\alpha$, that is \n\n$$\nP(\\text{reject } H_0| H_0\\text{ is true})=P(\\text{Type I Error})=\\alpha\n$$\n\nFor a given significance level (e.g., $\\alpha=0.05$) and a given alternative hypothesis, we can divide the range of all possible values of the test statistic (i.e., $\\mathbb{R}$ since both $t\\in\\mathbb{R}$ and $F\\in\\mathbb{R}$) into a **rejection region** and a **non-rejection region** by using **critical values** derived from the distribution of the test statistic under the null hypothesis.  We can do this because the test statistics $t$ and $F$ have known distributions under the null hypothesis ($t\\overset{H_0}{\\sim}t_{n-K}$ and $F\\overset{H_0}{\\sim}F_{(q,n-K)}$). \n\nIndeed, under Assumption 4$^\\ast$, we know the *exact* null distributions for every sample size $n$. Having decided on the rejection and non-rejection regions, we only need to check whether the observed (obs) sample values $t_{obs}$ or $F_{obs}$ of the test statistics $t$ or $F$ are either in the rejection or in the non-rejection region to either rejection the null hypothesis or not to rejection the null hypothesis. \n\n\n**Non-conservative versus conservative tests:** Since the test statistics $F$ and $t$ are continuous random variables of which we know the *exact* distributions (under Assumptions 1-4$^\\ast$), we can find critical values such that \n\n$$\nP(\\text{Type I Error})=\\alpha\n$$\n\nWe call such tests \"non-conservative\" since the probability of a type I error equals the significance level $\\alpha$.  Test statistics with \n\n$$\nP(\\text{Type I Error})<\\alpha\n$$\n\nare called *conservative* test statistics; they lead to valid inferences, but will detect a correct violation of the null hypothesis less often than a non-conservative test. A test statistic with $P(\\text{Type I Error})>\\alpha$ leads to *invalid* inferences! \n\n\n### Critical Values \n\n#### The $F$-Test {-}\n\nThe critical value $c_{1-\\alpha}>0$ defines the \n\n- rejection region, $]c_{1-\\alpha},\\infty[$, and \n- non-rejection region, $]0,c_{1-\\alpha}]$ \n\nwhich together divide the range of test-statistic values (here $\\mathbb{R}^+$ since $F\\in\\mathbb{R}^+$) for a given significance level $\\alpha\\in(0,1)$, such that\n\n$$\nP(\\text{Type I Error})=P_{H_0}\\Big(F\\in]c_{1-\\alpha},\\infty[\\Big)=\\alpha,\n$$\n\nwhere $c_{1-\\alpha}$ is here the $(1-\\alpha)$ quantile of the $F$-distribution with $(q,n-K)$ degrees of freedom, and where $P_{H_0}$ means that we compute the probability under the assumption that $H_0$ is true.  \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-Small-Sample-Inference_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=1\\textwidth}\n:::\n:::\n\n\n**The rejection region:** The rejection region describes a range of values of the test statistic $F$ which we rarely see if the null hypothesis is true (only in at most $\\alpha \\cdot 100\\%$ cases). If the observed value of the test statistic, $F_{\\text{obs}}$, falls in this region, we will reject the null  hypothesis and accept Type I error rate of $\\alpha$. \n\n**The non-rejection region:** The non-rejection region describes a range of values of the test statistic $F$ which we expect to see (in $(1-\\alpha) \\cdot 100\\%$ cases) if the null hypothesis is true. If the observed value of the test statistic, $F_{\\text{obs}}$ falls in this region, we will not reject the null hypothesis. \n\n\n**Caution:** Not rejecting the null hypothesis does not mean that we can conclude that the null hypothesis is true. We only had no sufficiently strong evidence against the null hypothesis. A violation of the null hypothesis, for instance $R\\beta -r=a\\neq 0$, may simply be too small (too small $a$ value) to stand out from the estimation errors (measured by the standard error) in $\\hat\\beta_k.$\n\n\nFortunately, you do not need to read old-school distribution tables to find the critical value $c_{1-\\alpha}$, but can simply use `R`\n\n::: {.cell}\n\n```{.r .cell-code}\ndf1   <- 9    # numerator df\ndf2   <- 120  # denominator df\nalpha <- 0.05 # significance level\n## Critical value:\ncrit_value <- qf(p = 1-alpha, df1 = df1, df2 = df2)\ncrit_value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.958763\n```\n:::\n:::\n\n\\noindent Changing the significance level from $\\alpha=0.05$ to $\\alpha=0.01$ makes the critical value $c_{1-\\alpha}$ larger and, therefore, the  rejection region smaller (fewer Type I errors)\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.01\n## Critical value:\ncrit_value <- qf(p = 1-alpha, df1 = df1, df2 = df2)\ncrit_value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.558574\n```\n:::\n:::\n\n\n\n#### The $t$-Test {-}\n\nIn case of the $t$-test, we need to differentiate between two-sided and one-sided testing.\n\n##### Two-Sided $t$-Test {-}\n\nTwo-sided hypothesis:\n\\begin{equation*}\n\\begin{array}{ll}\nH_0: & \\beta_k=r \\\\\nH_A: & \\beta_k\\ne r\n\\end{array}\n\\end{equation*}\nIn case of a two-sided $t$-test, we reject the null hypothesis if the observed realization of the $t$-test, $t_{obs}$, is \"far away\" from zero either by being sufficiently smaller or greater than $r$. The corresponding two-sided critical values are denoted by $-c_{1-\\alpha/2}=c_{\\alpha/2}<0$ and $c_{1-\\alpha/2}>0$, where $c_{1-\\alpha/2}>0$ is the $(1-\\alpha/2)$ quantile of the $t$-distribution with $(n-K)$ degrees of freedom, and where $-c_{1-\\alpha/2}=c_{\\alpha/2}$ due to the symmetry of the $t$-distribution. These critical values defines the following rejection and the non-rejection regions \n\\begin{align*}\n\\text{rejection region:}&\\hspace{1cm}]-\\infty,c_{\\alpha/2}[\\;\\;\\cup\\;\\;]c_{1-\\alpha/2}, \\infty[\\\\\n\\text{non-rejection region:}&\\hspace{1cm}[c_{\\alpha/2},c_{1-\\alpha/2}].\n\\end{align*}\nFor this rejection region it holds true that\n\n$$\nP(\\text{Type I Error})=P_{H_0}\\Big(t\\in\\;]-\\infty,c_{\\alpha/2}[\\;\\;\\cup\\;\\;]c_{1-\\alpha/2}, \\infty[\\Big)=\\alpha.\n$$\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-Small-Sample-Inference_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=1\\textwidth}\n:::\n:::\n\n\n##### One-Sided $t$-Test {-}\n\nOne-sided hypothesis:\n\\begin{equation*}\n\\begin{array}{lll}\n&H_0: & \\beta_k =r\\\\\n&H_A: & \\beta_k >r\\\\\n(\\text{or}&H_A: & \\beta_k< r)\n\\end{array}\n\\end{equation*}\nIn case of a one-sided $t$-test, we will reject the null if $t_{obs}$ is sufficiently \"far away\" from zero in the relevant direction of $H_A$. The corresponding critical value is either $-c_{1-\\alpha}$ ($H_A:\\beta_k< r$) or $c_{1-\\alpha}$ ($H_A:\\beta_k> r$), where $c_{1-\\alpha}$ is the $(1-\\alpha)$ quantile of the $t$-distribution with $(n-K)$ degrees of freedom, and where $-c_{1-\\alpha}=c_{\\alpha}$ due to the symmetry of the $t$-distribution. The critical value $c_{1-\\alpha}$ defines the following rejection and the non-rejection regions:\n\n\n\nFor $H_0: \\beta_k=0$ \\quad versus\\quad $H_A: \\beta_k < 0$:\n\\begin{align*}\n\\text{rejection region:}   &\\hspace{2cm}]-\\infty,c_{\\alpha}[ \\\\\n\\text{non-rejection region:}&\\hspace{2cm}[c_{\\alpha},\\infty[\n\\end{align*}\nsuch that \n\n$$\nP(\\text{Type I Error})=P_{H_0}\\Big(t\\in\\;]-\\infty,c_{\\alpha}[\\Big)=\\alpha.\n$$\n\n\n\n\nFor $H_0: \\beta_k=0$\\quad versus\\quad$H_A: \\beta_k > 0$:\n\\begin{align*}\n\\text{rejection region:}&\\hspace{1cm}]c_{1-\\alpha}, \\infty[\\\\\n\\text{non-rejection region:}&\\hspace{1cm}]-\\infty,c_{1-\\alpha}]\n\\end{align*}\nsuch that \n\n$$\nP(\\text{Type I Error})=P_{H_0}\\Big(t\\in\\;]c_{1-\\alpha}, \\infty[\\Big)=\\alpha.\n$$\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-Small-Sample-Inference_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=1\\textwidth}\n:::\n:::\n\n\n\nFortunately, you do not need to read old-school distribution tables to find the critical values, but you can simply use `R`\n\n::: {.cell}\n\n```{.r .cell-code}\ndf    <- 16   # degrees of freedom \nalpha <- 0.05 # significance level\n## One-sided critical value (= (1-alpha) quantile):\nc_oneSided <- qt(p = 1-alpha, df = df)\nc_oneSided\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.745884\n```\n:::\n\n```{.r .cell-code}\n## Two-sided critical value (= (1-alpha/2) quantile):\nc_twoSided <- qt(p = 1-alpha/2, df = df)\n## lower critical value\n-c_twoSided\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -2.119905\n```\n:::\n\n```{.r .cell-code}\n## upper critical value\nc_twoSided\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.119905\n```\n:::\n:::\n\n\n\n### Type II Error and Power\n\nA Type II error is the mistake of not rejecting the null hypothesis when in fact it should have been rejected. The probability of making a Type II error equals one minus the probability of correctly rejecting the null hypothesis (\"Power\"). For instance, in the case of using the $t$-test to test the null hypothesis $H_0: \\beta_k=0$ versus the one-sided alternative hypothesis $H_A:\\beta_k>0$) we have that \n\\begin{align*}\nP(\\text{Type II Error})\n&=P_{H_A}\\Big(t\\;\\in\\;\\overbrace{]-\\infty,c_{1-\\alpha}]}^{\\text{non-rejection region}}\\Big)\\\\\n&=1-\\underbrace{P_{H_A}\\Big(t\\;\\in\\;\\overbrace{]c_{1-\\alpha},\\infty[}^{\\text{rejection region}}\\Big)}_{\\text{\"Power\"}},\n\\end{align*}\nwhere $P_{H_A}$ means that we compute the probability under the assumption that $H_A$ is true. \n\nThere is a trade off between the probability of making a Type I error and the probability of making a Type II error: a lower significance level $\\alpha$, decreases $P(\\text{Type I Error})$, but necessarily increases $P(\\text{Type II Error})$ and vice versa.  Ideally, we would have some sense of the costs of making each of these errors, and would choose our significance level to minimize these total costs. However, the costs are often difficult to know. Moreover, the probability of making a Type II error is usually impossible to compute, since we usually do not know the true distribution of $\\hat\\beta_k$ under the alternative. \n\n\nFor illustration purposes, consider the case of a $t$ test for a one-sided hypothesis\n\\begin{equation*}\n\\begin{array}{ll}\nH_0:  & \\beta_k=0 \\\\\nH_A:  & \\beta_k>0,\n\\end{array}\n\\end{equation*}\nwhere the true (usually unknown) parameter value is $\\beta_k=3$ and where the true (usually also unknown) standard error is $\\operatorname{SE}(\\hat\\beta_k|X)=\\sqrt{\\sigma^2[(X'X)^{-1}]_{kk}}=1.5$. Only with the knowledge about these usually unknown quantities, we can derive the distribution of the $t$-test statistic under the alternative hypothesis. The distribution of the $t$-test statistic becomes here a standard normal distribution, since we assume $\\operatorname{SE}(\\hat\\beta_k|X)=\\sqrt{\\sigma^2[(X'X)^{-1}]_{kk}}=1.5$ to be a known **deterministic** quantity. (This completely unrealistic assumption is only used for illustrative purposes; namely to explain the probability of making a Type II error and the power (i.e. $1-P(\\text{Type II Error})$).)  \n\nUnder this setup, the distribution under the null hypothesis (i.e., if $\\beta_k=0$ were true) is:\n\n$$\nt=\\frac{\\hat\\beta_k-0}{\\sqrt{\\sigma^2[(X'X)^{-1}]_{kk}}}\\overset{H_0}{\\sim}\\mathcal{N}(0,1)\n$$\n\nLikewise, the distribution under the alternative hypothesis (i.e., for the actual $\\beta_k=3$) is:\n\\begin{align*}\nt=\\frac{\\hat\\beta_k-0}{\\sqrt{\\sigma^2[(X'X)^{-1}]_{kk}}}\n&=\\frac{\\hat\\beta_k+3-3-0}{\\sqrt{\\sigma^2[(X'X)^{-1}]_{kk}}}\\\\[2ex]\n\\Rightarrow\\quad&\\underbrace{\\frac{\\hat\\beta_k-3}{\\sqrt{\\sigma^2[(X'X)^{-1}]_{kk}}}}_{\\sim \\mathcal{N}(0,1)}+\\underbrace{\\frac{3-0}{\\sqrt{\\sigma^2[(X'X)^{-1}]_{kk}}}}_{=\\Delta\\text{ (mean-shift)}}\\overset{H_A}{\\sim}\\mathcal{N}(\\Delta,1)\n\\end{align*}\n\nSo, the mean-shift $\\Delta$ depends on the value of $\\sqrt{\\sigma^2[(X'X)^{-1}]_{kk}}$ (here $1.5$) and the difference between the actual parameter value ($\\beta_k=3$) and the hypothetical parameter value under the null-hypothesis (here $0$). The following Graphic illustrates the probability of a type II error and the power for the case where $\\sqrt{\\sigma^2[(X'X)^{-1}]_{kk}}=1.5$ such that $\\Delta=3/1.5=2$. \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-Small-Sample-Inference_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=1\\textwidth}\n:::\n:::\n\n\n\n### $p$-Value \n\nThe $p$-value of a test statistic is the significance level we would obtain if we took the sample value of the observed test statistic, $F_{\\text{obs}}$ or $t_{\\text{obs}},$ as the border between the rejection and non-rejection regions. \n\n- **$F$-test:** $p=P_{H_0}(F\\geq F_{\\text{obs}})$\n- **$t$-test:** ($H_A:\\beta_k\\neq r$)] $p=2\\cdot\\min\\{P_{H_0}(t\\leq t_{\\text{obs}}),P_{H_0}(t\\geq t_{\\text{obs}})\\}$\n- **$t$-test:** ($H_A:\\beta_k> r$)] $p=P_{H_0}(t\\geq t_{\\text{obs}})$\n- **$t$-test:** ($H_A:\\beta_k< r$)] $p=P_{H_0}(t\\leq t_{\\text{obs}})$\n\nPut another way, the $p$-value is the greatest significance level for which we just fail to reject the null. Therefore, the $p$-value is sometimes also called the marginal significance value. \n\nIf the $p$-value is strictly smaller than the chosen significance level $\\alpha$, we reject the null hypothesis. \n\n## Confidence Intervals {#sec-CIsmallsample}\n\nWe define a two-sided $(1-\\alpha)\\cdot 100\\%$ percent confidence interval for the *deterministic* (unknown) true $\\beta_k$ as the **random interval** $\\operatorname{CI}_{1-\\alpha}$ for which \n\n$$\nP\\Big(\\beta_k\\in\\operatorname{CI}_{1-\\alpha}\\Big)\\geq 1-\\alpha.\n$$\n\nDerivation of the random interval $\\operatorname{CI}_{1-\\alpha}$: \n\nObserve that\n\n$$\n\\frac{\\hat\\beta_k-\\beta_k}{\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)}\\sim t_{(n-K)}\n$$\n\nTherefore,\n\\begin{align*}\nP\\left(-t_{1-\\alpha/2,n-K}\\leq\\frac{\\hat\\beta_k-\\beta_k}{\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)}\\leq t_{1-\\alpha/2,n-K}\\right)=1-\\alpha,\n\\end{align*}\nwhere $t_{1-\\alpha/2,n-K}$ denotes the $(1-\\alpha)$ quantile of the $t$-distribution with $n-K$ degrees of freedom. Next, we can do the following equivalent transformations\n\\begin{align*}\nP\\left(-t_{1-\\alpha/2,n-K}\\leq\\frac{\\hat\\beta_k-\\beta_k}{\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)}\\leq t_{1-\\alpha/2,n-K}\\right)&=1-\\alpha\\\\\n\\Leftrightarrow P\\left(\\hat\\beta_k-t_{1-\\alpha/2,n-K}\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)\\leq \\beta_k\\leq\\hat\\beta_k +t_{1-\\alpha/2,n-K}\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)\\right)&=1-\\alpha\\\\\n\\Leftrightarrow P\\left(\\beta_k\\in\\underbrace{\\left[\\hat\\beta_k-t_{1-\\alpha/2,n-K}\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X),\\;\\hat\\beta_k +t_{1-\\alpha/2,n-K}\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)\\right]}_{=:\\operatorname{CI}_{1-\\alpha}}\\right)&=1-\\alpha\n\\end{align*}\nThat is, the random interval \n\n$$\n\\operatorname{CI}_{1-\\alpha}=\\left[\\hat\\beta_k-t_{1-\\alpha/2,n-K}\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X),\\;\\hat\\beta_k +t_{1-\\alpha/2,n-K}\\widehat{\\operatorname{SE}}(\\hat\\beta_k|X)\\right]\n$$\n\nis our $(1-\\alpha)\\cdot 100\\%$ percent confidence interval for $\\beta_k$.\n\n**Interpretation:** The random interval  $\\operatorname{CI}_{1-\\alpha}$ for $\\beta_k$ contains the true parameter value $\\beta_k$ in $(1-\\alpha)\\cdot 100\\%$ cases of its realizations when considering infinitely many realizations.[^1]  Unfortunately, this interpretation is not a statement about a single $\\operatorname{CI}_{1-\\alpha}$ computed for given data. A given, realized $\\operatorname{CI}_{1-\\alpha}$ will either contain the true parameter $\\beta_k$ or not, and usually we do not know the answer. So, the problem with $\\operatorname{CI}$s is that they are quite hard to interpret. However, they are very well suited for a visual comparison of the estimation uncertainties in different parameter estimators, for instance, across $\\hat\\beta_k$, $k=1,\\dots,K$.   \n\n![Try again!](images/Meme_CI.jpg)\n\n[^1]: Take a look at this visualization: [https://rpsychologist.com/d3/ci/](https://rpsychologist.com/d3/ci/)\n\n## Practice: Small Sample Inference {#sec-PSSI}\n\nLet's apply the above exact inference methods using `R`. First, we program a function `myDataGenerator()` which allows us to generate data from the following model, i.e., from the following fully specified data generating process:\n\\begin{align*}\nY_i &=\\beta_1+\\beta_2X_{i2}+\\beta_3X_{i3}+\\varepsilon_i,\\qquad i=1,\\dots,n\\\\\n\\beta &=(\\beta_1,\\beta_2,\\beta_3)'=(2,3,4)'\\\\\nX_{i2}&\\sim U[2,10]\\\\\nX_{i3}&\\sim U[12,22]\\\\\n\\varepsilon_i&\\sim\\mathcal{N}(0,3^2),\n\\end{align*}\nwhere $(Y_i,X_i)$ is assumed i.i.d. across $i=1,\\dots,n$. Below, in the codes, I use $n=10$, but, of course, other sample sizes can be considered too. Under the assumptions of this chapter, we do exact inference that is specific to any given sample size $n$. \n\n\nThe below function `myDataGenerator()` allows to sample new realizations of $Y_1,\\dots,Y_n$ conditionally on a given data matrix $X$.  Moreover, you can provide your own values for the sample size $n$ and for the parameter vector $\\beta=(\\beta_1,\\beta_2,\\beta_3)'$. \n\n::: {.cell}\n\n```{.r .cell-code}\n## Function to generate artificial data\n## If X=NULL: new X variables are generated\n## If the user gives X variables, \n## the sampling of new Y variables is conditionally on \n## the given X variables.\nmyDataGenerator <- function(n, beta, X=NULL, sigma=3){\n  if(is.null(X)){\n    X   <- cbind(rep(1, n), \n                 runif(n, 2, 10), \n                 runif(n,12, 22))\n  }\n  eps  <- rnorm(n, sd=sigma)\n  Y    <- X %*% beta + eps\n  data <- data.frame(\"Y\"=Y, \n                     \"X_1\"=X[,1], \"X_2\"=X[,2], \"X_3\"=X[,3])\n  ##\n  return(data)\n}\n\n## Define a true beta vector\nbeta_true <- c(2,3,4)\n\n## Check:\n## Generate Y and X data \ntest_data     <- myDataGenerator(n = 10, beta=beta_true)\n## Generate new Y data conditionally on X\nX_cond <- cbind(test_data$X_1,\n                test_data$X_2,\n                test_data$X_3)\ntest_data_new <- myDataGenerator(n    = 10, \n                                 beta = beta_true, \n                                 X    = X_cond)\n## compare\nround(head(test_data,     3), 2) # New Y, new X\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Y X_1  X_2   X_3\n1 116.59   1 9.55 20.94\n2  97.57   1 6.69 18.94\n3  73.37   1 7.82 13.25\n```\n:::\n\n```{.r .cell-code}\nround(head(test_data_new, 3), 2) # New Y, conditionally on X\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Y X_1  X_2   X_3\n1 111.14   1 9.55 20.94\n2  98.08   1 6.69 18.94\n3  79.06   1 7.82 13.25\n```\n:::\n:::\n\n\n###  Normally Distributed $\\hat\\beta|X$\n\nThe above data generating process fulfills our regulatory assumptions Assumption 1-4$^*$. So, by theory, the estimators $\\hat\\beta_k|X$ should be normal distributed conditionally on $X$,\n\n$$\n\\hat\\beta_k|X\\sim\\mathcal{N}(\\beta_k,\\sigma^2[(X'X)^{-1}]_{kk}),\n$$ \n\nwhere $[(X'X)^{-1}]_{kk}$ denotes the element in the $k$th row and $k$th column of the $K\\times K$ matrix $(X'X)^{-1}$. Let's check the distribution by means of a Monte Carlo simulation for the case of $\\hat\\beta_2|X$ with a small sample size of $n=10$.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn         <- 10       # a small sample size\nbeta_true <- c(2,3,4) # true data vector\nsigma     <- 3        # true standard deviation of the error term (var=sigma^2)\n\n## Let's generate a data set from our data generating process\nmydata  <- myDataGenerator(n = n, beta=beta_true)\nX_cond  <- cbind(mydata$X_1, mydata$X_2, mydata$X_3)\n\n## True mean and variance of the true normal distribution \n## of beta_hat_2|X=X_cond:\n# true mean\nbeta_true_2     <- beta_true[2] \n# true variance\nvar_true_beta_2 <- sigma^2 * diag(solve(t(X_cond) %*% X_cond))[2]\n\n## Let's generate 5000 realizations from beta_hat_2 \n## conditionally on X=X_cond and check whether the empirical  \n## distribution of these 5000 realizations is close \n## to the true normal distribution of beta_hat_2:\nrep        <- 5000 # MC replications\nbeta_hat_2 <- rep(NA, times=rep)\n##\nfor(r in 1:rep){\n    MC_data <- myDataGenerator(n    = n, \n                               beta = beta_true, \n                               X    = X_cond)\n    lm_obj        <- lm(Y ~ X_2 + X_3, data = MC_data)\n    beta_hat_2[r] <- coef(lm_obj)[2]\n}\n\n## Compare\n## True beta_2 versus average of beta_hat_2 estimates\nc(beta_true_2, round(mean(beta_hat_2), 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.0000 3.0091\n```\n:::\n:::\n\nThe values are very close to each other, thus, on average the estimation results are basically equal to the true parameter value. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Compare\n## True variance of beta_hat_2 versus \n## empirical variance of beta_hat_2 estimates\nc(round(var_true_beta_2, 4), round(var(beta_hat_2), 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4160 0.4235\n```\n:::\n:::\n\nThe values are very close to each other, thus the theoretical variance describes very well the actual variance of $\\hat\\beta_2|X$. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## True normal distribution of beta_hat_2 versus \n## empirical density of beta_hat_2 estimates\nlibrary(\"scales\")\ncurve(expr = dnorm(x, mean = beta_true_2, \n                   sd=sqrt(var_true_beta_2)), \n      xlab=\"\",ylab=\"\", col=gray(.2), lwd=3, lty=1, \n      xlim=range(beta_hat_2), ylim=c(0,1.1))\nhist(beta_hat_2, freq=FALSE, col=alpha(\"blue\",.35), add=TRUE)\nlines(density(beta_hat_2, bw = bw.SJ(beta_hat_2)), \n      col=alpha(\"blue\",.5), lwd=3)\nlegend(\"topleft\", lty=c(1,NA,1), lwd=c(3,NA,3), pch=c(NA,15,NA), pt.cex=c(NA,2,NA),\n     col=c(gray(.2), alpha(\"blue\",.45), alpha(\"blue\",.5)), bty=\"n\", legend= \nc(expression(\n  \"Theoretical Gaussian Density of\"~hat(beta)[2]~'|'~X), \n  expression(\n  \"Histogram based on the 5000 MC realizations of\"~\n  hat(beta)[2]~'|'~X), \n  expression(\n  \"Nonparametric Density Estimation based on the 5000 MC realizations of\"~\n  hat(beta)[2]~'|'~X)))\n```\n\n::: {.cell-output-display}\n![](05-Small-Sample-Inference_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=1\\textwidth}\n:::\n:::\n\nGreat! The nonparametric density estimation (estimated via `density()`) and the histogram computed based on the 5000 simulated realizations of $\\hat\\beta_2|X$ are indicating that $\\hat\\beta_2|X$ is really normally distributed as described by our theoretical result in Equation @eq-ssnorm. \n\nHowever, what would happen if we would sample *unconditionally* on $X$? How does the distribution of $\\hat\\beta_2$ would then look like? \n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1110)\n## Let's generate 5000 realizations from beta_hat_2 \n## WITHOUT conditioning on X\nbeta_hat_2_uncond <- rep(NA, times=rep)\n##\nfor(r in 1:rep){\n    MC_data <- myDataGenerator(n    = n, \n                               beta = beta_true)\n    lm_obj               <- lm(Y ~ X_2 + X_3, data = MC_data)\n    beta_hat_2_uncond[r] <- coef(lm_obj)[2]\n}\n\n## Compare:\n## True beta_2 versus average of beta_hat_2 estimates\nc(beta_true_2, round(mean(beta_hat_2_uncond), 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.0000 3.0078\n```\n:::\n:::\n\nOK, at least on average the point point estimates are basically equal to the true parameter value. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Compare: \n## True variance of beta_hat_2 versus \n## empirical variance of beta_hat_2 estimates\nc(round(var_true_beta_2, 4), round(var(beta_hat_2_uncond), 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4160 0.2478\n```\n:::\n:::\n\nOuch! The theoretical variance of $\\hat\\beta_2|X$ is quite different from the actual variance of $\\hat\\beta_2$ (i.e. simulated without conditioning in $X$). That is, we cannot simply neglect that the variance of $\\hat\\beta_2$ depends on the observed realization of $X$ in small samples. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Plotting the theoretical distribution of beta_hat_2|X \n## versus the simulated distribution\ncurve(expr = dnorm(x, mean = beta_true_2, sd=sqrt(var_true_beta_2)), \n      xlab=\"\",ylab=\"\", col=gray(.2), lwd=3, lty=1, \n      xlim=range(beta_hat_2_uncond), ylim=c(0,1.5))\nhist(beta_hat_2_uncond, freq=FALSE, col=alpha(\"blue\",.35), add=TRUE)\nlines(density(beta_hat_2_uncond, bw=bw.SJ(beta_hat_2_uncond)), \n      col=alpha(\"blue\",.5), lwd=3)\nlegend(\"topleft\", lty=c(1,NA,1), lwd=c(3,NA,3), pch=c(NA,15,NA), pt.cex=c(NA,2,NA),\n     col=c(gray(.2), alpha(\"blue\",.45), alpha(\"blue\",.5)), bty=\"n\", legend= \nc(expression(\n  \"Theoretical Gaussian Density of\"~hat(beta)[2]~'|'~X), \nexpression(\n  \"Histogram based on the 5000 MC realizations of\"~\n  hat(beta)[2]), \nexpression(\"Nonparam. Density Estimation based on the 5000 MC realizations of\"~\n  hat(beta)[2])))\n```\n\n::: {.cell-output-display}\n![](05-Small-Sample-Inference_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=1\\textwidth}\n:::\n:::\n\nNot so good. The theoretical distribution of $\\hat\\beta_2|X$ has much fatter tails than the simulated distribution of $\\hat\\beta_2$  (i.e. simulated without conditioning in $X$). That is, we cannot simply neglect that the distribution of $\\hat\\beta_2$ depends on the observed realization of $X$ in small samples. In large sample, however, the effect of a given realization $X$ is (fortunately) negligible, as we will see in @sec-lsinf. \n\n\n### Testing Multiple Parameters \n\nIn the following, we do inference about multiple parameters. We test \n\\begin{align*}\nH_0:\\;&\\beta_2=3\\quad\\text{and}\\quad\\beta_3=4\\\\\n\\text{versus}\\quad H_A:\\;&\\beta_2\\neq 3\\quad\\text{and/or}\\quad\\beta_3\\neq 4.\n\\end{align*}\nOr equivalently\n\\begin{align*}\nH_0:\\;&R\\beta -r = 0 \\\\\nH_A:\\;&R\\beta -r \\neq 0,\n\\end{align*}\nwhere \n\n$$\nR=\\left(\n\\begin{matrix}\n0&1&0\\\\\n0&0&1\\\\\n\\end{matrix}\\right)\\quad\\text{ and }\\quad \nr=\\left(\\begin{matrix}3\\\\4\\\\\\end{matrix}\\right).\n$$\n\nThe following `R` code can be used to test this hypothesis:\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressMessages(library(\"car\")) # for linearHyothesis()\n# ?linearHypothesis\n\n## Estimate the linear regression model parameters\nlm_obj <- lm(Y ~ X_2 + X_3, data = mydata)\n\n## Option 1:\ncar::linearHypothesis(model = lm_obj, \n                      hypothesis.matrix = c(\"X_2=3\", \"X_3=4\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear hypothesis test\n\nHypothesis:\nX_2 = 3\nX_3 = 4\n\nModel 1: restricted model\nModel 2: Y ~ X_2 + X_3\n\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n1      9 87.285                              \n2      7 37.599  2    49.686 4.6252 0.05246 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nThe $p$-value is larger than the chosen significance level $\\alpha=0.05$ thus we cannot reject the null hypothesis. \n\nThe following codes gives an alternative, equivalent way to compute the test result:\n\n::: {.cell}\n\n```{.r .cell-code}\n## Option 2:\nR <- rbind(c(0,1,0),\n           c(0,0,1))\ncar::linearHypothesis(model = lm_obj, \n                      hypothesis.matrix = R, \n                      rhs = c(3,4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear hypothesis test\n\nHypothesis:\nX_2 = 3\nX_3 = 4\n\nModel 1: restricted model\nModel 2: Y ~ X_2 + X_3\n\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n1      9 87.285                              \n2      7 37.599  2    49.686 4.6252 0.05246 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nAgain, we cannot reject the null hypothesis at a significance level of, for instance, $\\alpha=0.05$ since we actually test the true null hypothesis. However, in repeated samples we should nevertheless observe $\\alpha\\cdot 100\\%$ type I errors (false rejections of $H_0$).  Let's check this using the following Monte Carlo simulation:\n\n::: {.cell}\n\n```{.r .cell-code}\n## Let's generate 5000 F-test decisions and check \n## whether the empirical rate of type I errors is \n## close to the theoretical significance level. \nrep             <- 5000 # MC replications\nF_test_pvalues  <- rep(NA, times=rep)\n##\nfor(r in 1:rep){\n  ## generate new MC_data conditionally on X_cond\n    MC_data <- myDataGenerator(n    = n, \n                               beta = beta_true, \n                               X    = X_cond)\n    lm_obj            <- lm(Y ~ X_2 + X_3, data = MC_data)\n    ## save the p-value\n    p <- linearHypothesis(lm_obj, \n                          c(\"X_2=3\", \"X_3=4\"))$`Pr(>F)`[2]\n    F_test_pvalues[r] <- p\n}\n##\nsignif_level <-  0.05\nrejections   <- F_test_pvalues[F_test_pvalues < signif_level]\nround(length(rejections)/rep, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.047\n```\n:::\n\n```{.r .cell-code}\n## \nsignif_level <-  0.01\nrejections   <- F_test_pvalues[F_test_pvalues < signif_level]\nround(length(rejections)/rep, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0092\n```\n:::\n:::\n\nNote that this is actually a very strong result. First, it means that we correctly control for the type I error rate since the type I error rate is not larger than the chosen significance level $\\alpha$. Second, it means that the test is not conservative (i.e. very efficient) since the empirical type I error rate is really close to the chosen significance level $\\alpha$. (In fact, if we would increase the number of Monte Carlo repetitions, the empirical type I error rate would converge to the selected significance level $\\alpha$ due to the law of large numbers.)\n\nNext, we check how well the $F$ test detects certain violations of the null hypothesis. We do this by using the same data generating process, but by testing the following incorrect null hypothesis:\n\\begin{align*}\nH_0:\\;&\\beta_2=4\\quad\\text{and}\\quad\\beta_3=4\\\\\nH_A:\\;&\\beta_2\\neq 4\\quad\\text{and/or}\\quad\\beta_3\\neq 4\n\\end{align*}\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(321)\nrep             <- 5000 # MC replications\nF_test_pvalues  <- rep(NA, times=rep)\n##\nfor(r in 1:rep){\n  ## generate new MC_data conditionally on X_cond\n    MC_data <- myDataGenerator(n    = n, \n                               beta = beta_true, \n                               X    = X_cond)\n    lm_obj            <- lm(Y ~ X_2 + X_3, data = MC_data)\n    ## save p-values of all rep-many tests\n    F_test_pvalues[r] <- linearHypothesis(lm_obj,\n                         c(\"X_2=4\",\"X_3=4\"))$`Pr(>F)`[2]\n}\n##\nsignif_level <-  0.05\nrejections   <- F_test_pvalues[F_test_pvalues < signif_level]\nlength(rejections)/rep\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3924\n```\n:::\n:::\n\nIndeed, we can now reject the (here false) null hypothesis in approximately $39\\%$ of all resamplings from the true data generating process. **Caution:** This also means that we are not able to \"see\" the violation of the null hypothesis in $100\\%-39\\%=62\\%$ of cases. Therefore, we can never use an insignificant test result (p-value$\\geq\\alpha$) as a justification to accept the null hypothesis. Obviously, there are Type II error events, but since we typically do not know the distribution of the test statistic under the alternative hypothesis, we cannot control the Type II error rate. We can only control the Type I error rate by using a small significance level $\\alpha$. \n\nMoreover, note that the $F$ test is not informative about which part of the null hypothesis ($\\beta_2=4$ and/or $\\beta_3=4$) is violated -- we only get the information that at least one of the multiple parameter hypotheses is violated:\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::linearHypothesis(lm_obj, c(\"X_2=4\", \"X_3=4\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear hypothesis test\n\nHypothesis:\nX_2 = 4\nX_3 = 4\n\nModel 1: restricted model\nModel 2: Y ~ X_2 + X_3\n\n  Res.Df     RSS Df Sum of Sq      F   Pr(>F)   \n1      9 141.245                                \n2      7  32.572  2    108.67 11.677 0.005889 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n### Dualty of Confidence Intervals and Hypothesis Tests\n\nConfidence intervals can be computed using `R` as following:\n\n::: {.cell}\n\n```{.r .cell-code}\nsignif_level <- 0.05\n## 95% CI for beta_2\nconfint(lm_obj, parm = \"X_2\", level = 1 - signif_level)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       2.5 %   97.5 %\nX_2 1.370315 3.563536\n```\n:::\n\n```{.r .cell-code}\n## 95% CI for beta_3 \nconfint(lm_obj, parm = \"X_3\", level = 1 - signif_level)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       2.5 %   97.5 %\nX_3 3.195389 4.695134\n```\n:::\n:::\n\nWe can use these two-sided confidence intervals to do hypothesis tests. For instance, when testing the null hypothesis\n\\begin{align*}\nH_0:&\\beta_3=4\\\\\n\\text{versus}\\quad H_A: &\\beta_3\\neq 4\n\\end{align*}\nwe can check whether the confidence interval $\\operatorname{CI}_{1-\\alpha}$ contains the hypothetical value $4$ or not. In case of $4\\in \\operatorname{CI}_{1-\\alpha}$, we cannot reject the null hypothesis. In case of $4\\not\\in \\operatorname{CI}_{1-\\alpha}$, we reject the null hypothesis. \n\nIf the Assumption 1-4$^\\ast$ hold true, then $\\operatorname{CI}_{1-\\alpha}$ is an exact confidence interval. That is, under the null hypothesis, it falsely rejects the null hypothesis in only $\\alpha\\cdot 100\\%$ of resamplings. Let's check this in the following `R` code:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Let's generate 1000 CIs \nset.seed(123)\nsignif_level <-  0.05\nrep          <- 5000 # MC replications\nconfint_m    <- matrix(NA, nrow=2, ncol=rep)\n##\nfor(r in 1:rep){\n  ## generate new MC_data conditionally on X_cond\n    MC_data <- myDataGenerator(n    = n, \n                               beta = beta_true, \n                               X    = X_cond)\n    lm_obj            <- lm(Y ~ X_2 + X_3, data = MC_data)\n    ## save the p-value\n    CI <- confint(lm_obj, parm=\"X_2\", level=1-signif_level)\n    confint_m[,r] <- CI\n}\n##\ninside_CI  <- confint_m[1,] <= beta_true_2 & \n                beta_true_2 <= confint_m[2,]\n\n## CI-lower, CI-upper, beta_true_2 inside?\nhead(cbind(t(confint_m), inside_CI))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        inside_CI\n[1,] 0.8555396 3.639738         1\n[2,] 0.9143542 3.270731         1\n[3,] 1.9336526 4.984167         1\n[4,] 1.9985874 3.812695         1\n[5,] 3.0108642 5.621791         0\n[6,] 2.0967675 4.716398         1\n```\n:::\n:::\n\n\nThe following code computes the relative frequency of confidence intervals **not containing** the true parameter value $(\\beta_2=3)$:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nround(length(inside_CI[inside_CI == FALSE])/rep, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0516\n```\n:::\n:::\n\nThat's good! The relative frequency is basically equal to the chosen $\\alpha=0.05$ value. \n\n\nNext, we visualize a sample of `nCIs <- 100` realizations of the random confidence interval: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnCIs <- 100\nplot(x=0,y=0,type=\"n\",xlim=c(0,nCIs),ylim=range(confint_m[,1:nCIs]),\n     ylab=\"\", xlab=\"Resamplings\", main=\"Confidence Intervals\")\nfor(r in 1:nCIs){\n  if(inside_CI[r]==TRUE){\n      lines(x=c(r,r), y=c(confint_m[1,r], confint_m[2,r]), \n            lwd=2, col=gray(.5,.5))\n  }else{\n      lines(x=c(r,r), y=c(confint_m[1,r], confint_m[2,r]), \n            lwd=2, col=\"darkred\")\n    }\n}\naxis(4, at=beta_true_2, labels = expression(beta[2]))\nabline(h=beta_true_2)\n```\n\n::: {.cell-output-display}\n![](05-Small-Sample-Inference_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=1\\textwidth}\n:::\n:::\n\n\nAs expected, only about $\\alpha\\cdot 100\\%=5\\%$ of all confidence intervals do not contain the true parameter value $\\beta_2=3$, but about $(1-\\alpha)\\cdot 100\\%=95\\%$ of all confidence intervals contain the true parameter value $\\beta_2=3$. \n\n# References {-} \n\n",
    "supporting": [
      "05-Small-Sample-Inference_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}