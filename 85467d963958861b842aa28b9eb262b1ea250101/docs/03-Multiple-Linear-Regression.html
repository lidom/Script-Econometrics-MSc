<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Econometrics M.Sc. - 3&nbsp; Multiple Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04-Monte-Carlo-Simulations.html" rel="next">
<link href="./02-Probability.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Econometrics M.Sc.</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Organization of the Course</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-Introduction-to-R.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to R</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-Probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-Multiple-Linear-Regression.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-Monte-Carlo-Simulations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Monte Carlo Simulations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-Small-Sample-Inference.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Small Sample Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-Asymptotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Large Sample Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-Instrumental-Variables.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Instrumental Variables Regression</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#assumptions" id="toc-assumptions" class="nav-link active" data-scroll-target="#assumptions"><span class="toc-section-number">3.1</span>  Assumptions</a>
  <ul class="collapse">
  <li><a href="#some-implications-of-the-exogeneity-assumption" id="toc-some-implications-of-the-exogeneity-assumption" class="nav-link" data-scroll-target="#some-implications-of-the-exogeneity-assumption"><span class="toc-section-number">3.1.1</span>  Some Implications of the Exogeneity Assumption</a></li>
  </ul></li>
  <li><a href="#deriving-the-expression-of-the-ols-estimator" id="toc-deriving-the-expression-of-the-ols-estimator" class="nav-link" data-scroll-target="#deriving-the-expression-of-the-ols-estimator"><span class="toc-section-number">3.2</span>  Deriving the Expression of the OLS Estimator</a></li>
  <li><a href="#some-quantities-of-interest" id="toc-some-quantities-of-interest" class="nav-link" data-scroll-target="#some-quantities-of-interest"><span class="toc-section-number">3.3</span>  Some Quantities of Interest</a></li>
  <li><a href="#method-of-moments-estimator" id="toc-method-of-moments-estimator" class="nav-link" data-scroll-target="#method-of-moments-estimator"><span class="toc-section-number">3.4</span>  Method of Moments Estimator</a></li>
  <li><a href="#practice" id="toc-practice" class="nav-link" data-scroll-target="#practice"><span class="toc-section-number">3.5</span>  Practice</a>
  <ul class="collapse">
  <li><a href="#factor-variables-and-the-dummy-variable-trap" id="toc-factor-variables-and-the-dummy-variable-trap" class="nav-link" data-scroll-target="#factor-variables-and-the-dummy-variable-trap"><span class="toc-section-number">3.5.1</span>  Factor Variables and the Dummy-Variable Trap</a></li>
  <li><a href="#dummy-variable-trap" id="toc-dummy-variable-trap" class="nav-link" data-scroll-target="#dummy-variable-trap">Dummy-Variable Trap</a></li>
  <li><a href="#detecting-heteroskedasticity" id="toc-detecting-heteroskedasticity" class="nav-link" data-scroll-target="#detecting-heteroskedasticity"><span class="toc-section-number">3.5.2</span>  Detecting Heteroskedasticity</a></li>
  <li><a href="#checking-non-linearity-of-the-regression-line" id="toc-checking-non-linearity-of-the-regression-line" class="nav-link" data-scroll-target="#checking-non-linearity-of-the-regression-line"><span class="toc-section-number">3.5.3</span>  Checking (Non-)Linearity of the Regression Line</a></li>
  <li><a href="#behavior-of-the-ols-estimates-for-resampled-data" id="toc-behavior-of-the-ols-estimates-for-resampled-data" class="nav-link" data-scroll-target="#behavior-of-the-ols-estimates-for-resampled-data"><span class="toc-section-number">3.5.4</span>  Behavior of the OLS Estimates for Resampled Data</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">3.6</span>  Exercises</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-MLR" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>In the following we focus on case of random designs <span class="math inline">\(X\)</span> (i.e.&nbsp;<span class="math inline">\(X\)</span> being a random variable), since, first, this is the more relevant case in econometrics and, second, it includes the case of fixed designs (i.e.&nbsp;<span class="math inline">\(X\)</span> being deterministic) as a special case (“degenerated random variable”). Caution: A random <span class="math inline">\(X\)</span> requires us to consider conditional means and variances “given <span class="math inline">\(X\)</span>.” That is, if we would be able to resample from the model, we do so by fixing (conditioning on) the in-principle random explanatory variable <span class="math inline">\(X\)</span>.</p>
<section id="assumptions" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="assumptions"><span class="header-section-number">3.1</span> Assumptions</h2>
<p>The multiple linear regression model is defined by the following assumptions which together describe the relevant theoretical aspects of the underlying data generating process:</p>
<p><strong>Assumption 1: Model and Sampling</strong></p>
<p><strong>Part (a): Linear Model</strong></p>
<p><span id="eq-LinMod"><span class="math display">\[
\begin{align}
  Y_i=\sum_{k=1}^K\beta_k X_{ik}+\varepsilon_i, \quad i=1,\dots,n.
\end{align}
\tag{3.1}\]</span></span> Usually, a constant (intercept) is included, in this case <span class="math inline">\(X_{i1}=1\)</span> for all <span class="math inline">\(i\)</span>. In the following we will always assume that <span class="math inline">\(X_{i1}=1\)</span> for all <span class="math inline">\(i\)</span>, unless otherwise stated.<br>
It is convenient to write <a href="#eq-LinMod">Equation&nbsp;<span>3.1</span></a> using matrix notation <span class="math display">\[\begin{eqnarray*}
  Y_i&amp;=&amp;\underset{(1\times K)}{X_i'}\underset{(K\times 1)}{\beta} +\varepsilon_i, \quad i=1,\dots,n,
\end{eqnarray*}\]</span> where <span class="math inline">\(X_i=(X_{i1},\dots,X_{iK})'\)</span> and <span class="math inline">\(\beta=(\beta_1,\dots,\beta_K)'\)</span>. Stacking all individual rows <span class="math inline">\(i\)</span> leads to <span class="math display">\[\begin{eqnarray*}\label{LM}
  \underset{(n\times 1)}{Y}&amp;=&amp;\underset{(n\times K)}{X}\underset{(K\times 1)}{\beta} + \underset{(n\times 1)}{\varepsilon},
\end{eqnarray*}\]</span> where <span class="math display">\[\begin{equation*}
Y=\left(\begin{matrix}Y_1\\ \vdots\\Y_n\end{matrix}\right),\quad X=\left(\begin{matrix}X_{11}&amp;\dots&amp;X_{1K}\\\vdots&amp;\ddots&amp;\vdots\\ X_{n1}&amp;\dots&amp;X_{nK}\\\end{matrix}\right),\quad\text{and}\quad \varepsilon=\left(\begin{matrix}\varepsilon_1\\ \vdots\\ \varepsilon_n\end{matrix}\right).
\end{equation*}\]</span></p>
<p><strong>Part (b): Random Sample</strong></p>
<p>Moreover, we assume that the observed (“obs”) data points <span class="math display">\[
((Y_{1,obs},X_{11,obs},\dots,X_{1K,obs}),(Y_{2,obs},X_{21,obs},\dots,X_{2K,obs}),\dots,(Y_{n,obs},X_{n1,obs},\dots,X_{nK,obs}))
\]</span> are realizations of a random sample <span class="math display">\[
((Y_{1},X_{11},\dots,X_{1K}),(Y_{2},X_{21},\dots,X_{2K}),\dots,(Y_{n},X_{n1},\dots,X_{nK})).
\]</span></p>
<p>That is, the <span class="math inline">\(i\)</span>th observed <span class="math inline">\(K+1\)</span> dimensional data point <span class="math inline">\((Y_{i,obs},X_{i1,obs},\dots,X_{iK,obs})\in\mathbb{R}^{K+1}\)</span> is a realization of a <span class="math inline">\(K+1\)</span> dimensional random variable <span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1},\)</span> where</p>
<ol type="1">
<li><span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\)</span> has the identical joint distribution as <span class="math inline">\((Y_{j},X_{j1},\dots,X_{jK})\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and all <span class="math inline">\(j=1,\dots,n\)</span>, and where</li>
<li><span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\)</span> is independent of <span class="math inline">\((Y_{j},X_{j1},\dots,X_{jK})\)</span> for all <span class="math inline">\(i\neq j=1,\dots,n.\)</span></li>
</ol>
<p>Note: Due to <a href="#eq-LinMod">Equation&nbsp;<span>3.1</span></a>, this i.i.d. assumption is equivalent to assuming that the multivariate random variables <span class="math inline">\((\varepsilon_i,X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1}\)</span> are i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span>.</p>
<p><strong>Remark:</strong> Usually, we do not use a different notation for observed realizations <span class="math inline">\((Y_{i,obs},X_{i1,obs},\dots,X_{iK,obs})\in\mathbb{R}^{K+1}\)</span> and for the corresponding random variable <span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1}\)</span> since often both interpretations (random variable and its realizations) can make sense in the same statement and then it depends on the considered context whether the random variables point if view or the realization point of view applies.</p>
<!--  That is, the multivariate distribution of $(\varepsilon_i,X_{i1},\dots,X_{iK})$ is assumed equal 
across $i=1,\dots,n$, but the multivariate random variables $(\varepsilon_i,X_{i1},\dots,X_{iK})$ 
and $(\varepsilon_j,X_{j1},\dots,X_{jK})$ are independent for all $i\neq j$. -->
<!-- **Note.:** Of course, in principle, Assumption 1 can be violated; however, in classic econometrics one usually does not bother with a possibly violated model assumption (Assumption 1). In modern econometrics, this is a big deal.  -->
<!-- The following assumptions are the so-called *classic assumptions* and we will be often concerned about violations of these assumptions.  -->
<p><strong>Assumption 2: Exogeneity</strong> <span class="math display">\[E(\varepsilon_i|X_i)=0,\quad i=1,\dots,n\]</span> This assumption demands that the mean of the random error term <span class="math inline">\(\varepsilon_i\)</span> is zero irrespective of the realizations of <span class="math inline">\(X_i\)</span>.</p>
<p>This exogeneity assumption is also called “orthogonality assumption” or “mean independence assumption.”</p>
<p><strong>Note:</strong> Together with the random sample assumption (Assumption 1, Part (b)) this assumption implies even <strong>strict</strong> exogeneity <span class="math inline">\(E(\varepsilon_i|X) = 0\)</span> since we have independence across <span class="math inline">\(i=1,\dots,n\)</span>. Under strict exogeneity, the mean of <span class="math inline">\(\varepsilon_i\)</span> is zero irrespective of the realizations of <span class="math inline">\(X_1,\dots,X_n\)</span>.</p>
<!-- For one example, it cannot be fulfilled when the regressors include lagged dependent variables. -->
<!-- Notice that in the presence of a constant explanatory variable, setting the expectation to zero is a normalization.  -->
<p><strong>Assumption 3: Rank Condition (no perfect multicollinearity)</strong></p>
<p><span class="math display">\[\operatorname{rank}(X)=K\quad\text{a.s.}\]</span> This assumption demands that the event of one explanatory variable being linearly dependent on the others occurs with a probability equal to zero. (This is the literal translation of the “almost surely (a.s.)” concept.) The assumption implies that <span class="math inline">\(n\geq K\)</span>.</p>
<p>This assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: dummy variable trap, multicollinearity, variance inflation). The violation of this assumption harms any economic interpretation as we cannot disentangle the explanatory variables’ individual effects on <span class="math inline">\(Y\)</span>. Therefore, this assumption is also often called an “identification assumption.”</p>
<p><strong>Assumption 4: Error distribution</strong></p>
<p>Depending on the context (i.e., parameter estimation vs.&nbsp;hypothesis testing and small <span class="math inline">\(n\)</span> vs.&nbsp;large <span class="math inline">\(n\)</span>) there are different more or less restrictive assumptions. Some of the most common ones are the following (roughly order from least to most restrictive):</p>
<ul>
<li><strong>Conditional distribution:</strong> <span class="math inline">\(\varepsilon_i|X_i \sim f_{\varepsilon|X}\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and for any distribution <span class="math inline">\(f_{\varepsilon|X}\)</span> such that <span class="math inline">\(\varepsilon_i|X_i\)</span> has two (or more) finite moments.</li>
<li><strong>Conditional normal distribution:</strong> <span class="math inline">\(\varepsilon_i|X_i \sim \mathcal{N}(0,\sigma^2(X_i))\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>.</li>
<li><strong>Independence between error and predictors:</strong> <span class="math inline">\(\varepsilon_i\sim f_\varepsilon\)</span> such that <span class="math inline">\(f_\varepsilon=f_{\varepsilon|X}\)</span> and such that <span class="math inline">\(f_\varepsilon\)</span> has two (or more) finite moments.<br>
</li>
<li><strong>Independence between error and predictors and normal:</strong> As above, but with <span class="math inline">\(f_\varepsilon=\mathcal{N}(0,\sigma^2)\)</span>.</li>
<li><strong>Spherical errors (“Gauss-Markov assumptions”):</strong> The conditional distributions of <span class="math inline">\(\varepsilon_i|X_i\)</span> may generally depend on <span class="math inline">\(X_i\)</span>, but such that <span class="math display">\[\begin{align*}
E(\varepsilon_i^2|X_i)         &amp;=\sigma^2&gt;0\quad\text{for all }i=1,\dots,n\\
E(\varepsilon_i\varepsilon_j|X)&amp;=0\quad\text{for all }i\neq j\quad\text{with}\quad i=1,\dots,n\quad\text{and}\quad j=1,\dots,n.
\end{align*}\]</span> Thus, here one assumes that, for a given realization of <span class="math inline">\(X_i\)</span>, the error process is uncorrelated (i.e.&nbsp;<span class="math inline">\(Cov(\varepsilon_i,\varepsilon_j|X)=E(\varepsilon_i\varepsilon_j|X)=0\)</span> for all <span class="math inline">\(i\neq j\)</span>) and homoskedastic (i.e.&nbsp;<span class="math inline">\(Var(\varepsilon_i|X)=\sigma^2\)</span> for all <span class="math inline">\(i\)</span>).</li>
</ul>
<!-- **Technical Note:** When we write that $Var(\varepsilon_i|X)=\sigma^2$ or $Var(\varepsilon_i|X_i)=\sigma^2_i,$ we implicitly assume that these second moments exists and that they are finite.  -->
<section id="homoskedastic-versus-heteroskedastic-error-terms" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="homoskedastic-versus-heteroskedastic-error-terms">Homoskedastic versus Heteroskedastic Error Terms</h4>
<p>The i.i.d. assumption is not as restrictive as it may seem on first sight. It allows for dependence between <span class="math inline">\(\varepsilon_i\)</span> and <span class="math inline">\((X_{i1},\dots,X_{iK})\in\mathbb{R}^K\)</span>. That is, the error term <span class="math inline">\(\varepsilon_i\)</span> can have a conditional distribution which depends on <span class="math inline">\((X_{i1},\dots,X_{iK})\)</span>; see <a href="02-Probability.html#sec-condDistr"><span>Section&nbsp;2.2.2.5</span></a>.</p>
<!-- However, we need to rule out one certain 
dependency between $\varepsilon_i$ and $X_i$; namely the conditional mean of $\varepsilon_i$ is not 
allowed to depend on $X_i$ (see Assumption 2: Exogeneity). -->
<!-- Example: $\varepsilon|X_i\sim U[-0.5|X_{i2}+X_{i3}|, 0.5|X_{i2}+X_{i3}|]$, 
with $X_{i2}\sim U[-4,4]$. This error term is mean independent of $X_i$ since 
$E(\varepsilon_i|X_i)=0$, but it has heteroskedastic conditional variance 
$Var(\varepsilon_i|X_i)=\frac{1}{12}X_i^2$. -->
<p>The exogeneity assumption (Assumption 2: Exogeneity) requires that the conditional mean of <span class="math inline">\(\varepsilon_i\)</span> is independent of <span class="math inline">\(X_i\)</span>. Besides this, dependencies between <span class="math inline">\(\varepsilon_i\)</span> and <span class="math inline">\(X_{i1},\dots,X_{iK}\)</span> are allowed. For instance, the variance of <span class="math inline">\(\varepsilon_i\)</span> can be a function of <span class="math inline">\(X_{i1},\dots,X_{iK}\)</span>. If this is the case, <span class="math inline">\(\varepsilon_i\)</span> is said to be “heteroskedastic.”</p>
<ul>
<li><strong>Heteroskedastic error terms:</strong> The conditional variances <span class="math inline">\(Var(\varepsilon_i|X_i=x_i)=\sigma^2(x_i)\)</span> are equal to a non-constant variance-function <span class="math inline">\(\sigma^2(x_i)&gt;0\)</span> which is a function of the realization <span class="math inline">\(X_i=x_i.\)</span></li>
</ul>
<p><strong>Example:</strong> <span class="math inline">\(\varepsilon_i|X_i\sim U[-0.5|X_{i2}|, 0.5|X_{i2}|],\)</span> with <span class="math inline">\(X_{i2}\sim U[-4,4]\)</span>. This error term is mean independent of <span class="math inline">\(X_i\)</span> since <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span>, but it has a heteroskedastic conditional variance since <span class="math inline">\(Var(\varepsilon_i|X_i)=\frac{1}{12}X_i^2\)</span> depends on <span class="math inline">\(X_i\)</span>.</p>
<p>Sometimes, we need to be more restrictive by assuming that also the variances of the error terms <span class="math inline">\(\varepsilon_i\)</span> are independent from <span class="math inline">\(X_i\)</span>. (Higher moments may still depend on <span class="math inline">\(X_i\)</span>.) This assumption leads to “homoskedastic” error terms.</p>
<ul>
<li><strong>Homoskedastic error terms:</strong> The conditional variances <span class="math inline">\(Var(\varepsilon_i|X_i=x_i)=\sigma^2\)</span> are equal to some constant <span class="math inline">\(\sigma^2&gt;0\)</span> for every possible realization <span class="math inline">\(X_i=x_i.\)</span></li>
</ul>
<!-- Sometimes, we need to be even more restrictive by assuming that the error terms $\varepsilon_i$  -->
<!-- are themselves **i.i.d.** across $i=1,\dots,n$. This is more restrictive than the assumption that the  -->
<!-- multivariate random variables $(\varepsilon_i,X_{i1},\dots,X_{iK})$ are i.i.d. across $i=1,\dots,n$  -->
<!-- since it implies that the whole distribution (not only the first two moments) of $\varepsilon_i$  -->
<!-- does not depend on $X_i$. This assumption also implies  **homoskedastic** error terms since when  -->
<!-- the whole distribution of $\varepsilon_i$ does not depend on $X_i\in\mathbb{R}^K$ also its variance  -->
<!-- doesn't depend on $X_i$.  -->
<p><strong>Example:</strong> For doing small sample inference (see <a href="05-Small-Sample-Inference.html"><span>Chapter&nbsp;5</span></a>), we need to assume that the error terms <span class="math inline">\(\varepsilon_i\)</span> are i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span> plus the normality assumption, i.e., <span class="math inline">\(\varepsilon_i\stackrel{\textrm{i.i.d.}}{\sim}{\mathcal N} (0, \sigma^2)\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> which leads to homoskedastic variances <span class="math inline">\(Var(\varepsilon_i|X_i)=\sigma^2\)</span> for every possible realization of <span class="math inline">\(X_i\)</span>.</p>
</section>
<section id="some-implications-of-the-exogeneity-assumption" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="some-implications-of-the-exogeneity-assumption"><span class="header-section-number">3.1.1</span> Some Implications of the Exogeneity Assumption</h3>
<div id="thm-a" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1 (Unconditional Mean) </strong></span>If <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>, then the also the unconditional mean of the error term is zero, i.e. <span class="math display">\[
E(\varepsilon_i)=0,\quad i=1,\dots,n.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Using the <em>Law of Total Expectations</em> (i.e., <span class="math inline">\(E[E(Z|X)]=E(Z)\)</span>) we can rewrite <span class="math inline">\(E(\varepsilon_i)\)</span> as <span class="math display">\[
E(\varepsilon_i)=E[E(\varepsilon_i|X_i)]
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span> But the exogeneity assumption yields <span class="math display">\[
E[E(\varepsilon_i|X_i)]=E[0]=0
\]</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> which completes the proof. <span class="math inline">\(\square\)</span></p>
</div>
<p>Generally, two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be <strong>orthogonal</strong> if their cross moment is zero, i.e.&nbsp;if <span class="math inline">\(E(XY)=0\)</span>. Exogeneity is sometimes also called “orthogonality,” due to the following result.</p>
<div id="thm-b" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.2 (Orthogonality) </strong></span>Under exogeneity, i.e.&nbsp;if <span class="math inline">\(E(\varepsilon_i|X_{i})=0\)</span>, the regressors and the error term are orthogonal to each other, i.e, <span class="math display">\[
E(X_{ik}\varepsilon_i)=0
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and <span class="math inline">\(k=1,\dots,K\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
E(X_{ik}\varepsilon_i)
&amp;=E(E(X_{ik}\varepsilon_i|X_{ik}))\quad{\small\text{(By the Law of Total Expectations)}}\\
&amp;=E(X_{ik}E(\varepsilon_i|X_{ik}))\quad{\small\text{(By the linearity of cond.~expectations)}}
\end{align*}\]</span> Now, to show that <span class="math inline">\(E(X_{ik}\varepsilon_i)=0\)</span>, we need to show that <span class="math inline">\(E(\varepsilon_i|X_{ik})=0,\)</span> which is done in the following:</p>
<p>Since <span class="math inline">\(X_{ik}\)</span> is an element of <span class="math inline">\(X_i,\)</span> a slightly more sophisticated use of the <em>Law of Total Expectations</em> (i.e., <span class="math inline">\(E(Y|X)=E(E(Y|X,Z)|X)\)</span>) implies that <span class="math display">\[
E(\varepsilon_i|X_{ik})=E(E(\varepsilon_i|X_i)|X_{ik}).
\]</span> So, the exogeneity assumption, <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> yields <span class="math display">\[
E(\varepsilon_i|X_{ik})=E(\underbrace{E(\varepsilon_i|X_i)}_{=0}|X_{ik})=E(0|X_{ik})=0.
\]</span> I.e., we have that <span class="math inline">\(E(\varepsilon_i|X_{ik})=0\)</span> which allows us to conclude that <span class="math display">\[
E(X_{ik}\varepsilon_i)=E(X_{ik}E(\varepsilon_i|X_{ik}))=E(X_{ik}0)=0
\]</span> which completes the proof. <span class="math inline">\(\square\)</span></p>
</div>
<p>Because the mean of the error term is zero (<span class="math inline">\(E(\varepsilon_i)=0\)</span> for all <span class="math inline">\(i\)</span> (see <a href="#thm-a">Theorem&nbsp;<span>3.1</span></a>), it follows that the orthogonality property, <span class="math inline">\(E(X_{ik}\varepsilon_i)=0,\)</span> is equivalent to a zero correlation property.</p>
<div id="thm-c" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.3 (No Correlation) </strong></span>If <span class="math inline">\(E(\varepsilon_i|X_{i})=0\)</span>, then <span class="math display">\[\begin{eqnarray*}
  Cov(\varepsilon_i,X_{ik})&amp;=&amp;0\quad\text{for all}\quad i=1,\dots,n\quad\text{and}\quad k=1,\dots,K.
\end{eqnarray*}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{eqnarray*}
  Cov(\varepsilon_i,X_{ik})&amp;=&amp;E(X_{ik}\varepsilon_i)-E(X_{ik})\,E(\varepsilon_i)\quad{\small\text{(Def. of Cov)}}\\
  &amp;=&amp;E(X_{ik}\varepsilon_i)\quad{\small\text{(By point (a): $E(\varepsilon_i)=0$)}}\\
  &amp;=&amp;0\quad{\small\text{(By orthogonality result in point (b))}}\quad\square
\end{eqnarray*}\]</span></p>
</div>
<!-- ## The Algebra of Least Squares -->
<!-- In this section, we take the point of view where $(Y^{obs},X^{obs})$ is an observed data-set, i.e., a given realization of the random sample $((Y_1,X_1),\dots,(Y_n,X_n))$. That is, we are not yet interested in the statistical properties of estimators, but are only in doing some linear algebra.  -->
<!-- ### Exogeneity and Causality -->
<!-- https://allthingsstatistics.com/miscellaneous/exogeneity-assumption/ -->
<!--  The exogeneity assumption (Assumption 2) means that the dependent variable $Y_i$ does not causally affect the independent variables $X_i$.

$$
Y_i = \beta_1 + \beta_2 X_i + \varepsilon_i
$$

* $Y_i$: Depression test score of person $i$
* $X_i$: Number of cigarettes smoked per day by person $i$
* $\beta$: Mean effect of smoking on depression

Can we identify $\beta$, i.e. the causal effect of smoking on depression? 

While it is reasonable that X causes Y 
$$
Y_i = \beta_1 + \beta_2 X_i + \varepsilon_i
$$
It's also reasonable that Y causes to some extend $X$
$$
X_i = \gamma_1 + \gamma_2 Y_i + u_i
$$

Thus 
$$
\begin{align*}
Y_i 
& = \beta_1 + \overbrace{\beta_2 X_i}^{=\gamma_1 + \gamma_2 Y_i + u_i} + \varepsilon_i\\
& = \beta_1 + \beta_2 X_i + \varepsilon_i\\
\end{align*}
$$




* It's very likely that smoking causes depression (X \to Y)
* However, *simultaneously* depression may cause increased smoking behavior (Y\to X)

The latter point would mean that large values of $Y_i$ 




The independent and dependent variables are correlated but the causal link is only in one direction – the independent variable affects the dependent variable not vice-versa. -->
</section>
</section>
<section id="deriving-the-expression-of-the-ols-estimator" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="deriving-the-expression-of-the-ols-estimator"><span class="header-section-number">3.2</span> Deriving the Expression of the OLS Estimator</h2>
<p>We derive the expression for the OLS estimator <span class="math inline">\(\hat\beta=(\hat\beta_1,\dots,\hat\beta_K)'\in\mathbb{R}^K\)</span> as the vector-valued minimizing argument of the sum of squared residuals, <span class="math inline">\(S_n(b)\)</span> with <span class="math inline">\(b\in\mathbb{R}^K\)</span>, for a given sample <span class="math inline">\(((Y_1,X_1),\dots,(Y_n,X_n))\)</span>. In matrix terms this is <span class="math display">\[\begin{align*}
S_n(b)&amp;=(Y-X b)^{\prime}(Y-X b)=Y^{\prime}Y-2 Y^{\prime} X b+b^{\prime} X^{\prime} X b.
\end{align*}\]</span> To find the minimizing argument <span class="math display">\[\hat\beta=\arg\min_{b\in\mathbb{R}^K}S_n(b)\]</span> we compute all partial derivatives <span class="math display">\[
\begin{aligned}
\underset{(K\times 1)}{\frac{\partial S(b)}{\partial b}} &amp;=-2\left(X^{\prime}Y -X^{\prime} Xb\right)
\end{aligned}
\]</span> and set them equal to zero which leads to <span class="math inline">\(K\)</span> linear equations (the “normal equations”) in <span class="math inline">\(K\)</span> unknowns. This system of equations defines the OLS estimates, <span class="math inline">\(\hat{\beta}\)</span>, for a given data-set: <span class="math display">\[
\begin{aligned}
-2\left(X^{\prime}Y -X^{\prime} X\hat{\beta}\right)=\underset{(K\times 1)}{0}.
\end{aligned}
\]</span> From our rank assumption (Assumption 3) it follows that <span class="math inline">\(X^{\prime}X\)</span> is an invertible matrix which allows us to solve the equation system by <span class="math display">\[
\begin{aligned}
\underset{(K\times 1)}{\hat{\beta}} &amp;=\left(X^{\prime} X\right)^{-1} X^{\prime} Y.
\end{aligned}
\]</span></p>
<p>The following codes computes the estimate <span class="math inline">\(\hat{\beta}\)</span> for a given realization <span class="math inline">\((Y,X)\)</span> of the random sample <span class="math inline">\((Y,X)\)</span> with <span class="math inline">\(X_i\in\mathbb{R}^3\)</span>.</p>
<div class="cell" data-layout-align="center" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-1_f7ee0e7f24558bf3fb1f3125126248b4">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Some given data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>X_2     <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1.9</span>,<span class="fl">0.8</span>,<span class="fl">1.1</span>,<span class="fl">0.1</span>,<span class="sc">-</span><span class="fl">0.1</span>,<span class="fl">4.4</span>,<span class="fl">4.6</span>,<span class="fl">1.6</span>,<span class="fl">5.5</span>,<span class="fl">3.4</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>X_3     <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">66</span>, <span class="dv">62</span>, <span class="dv">64</span>, <span class="dv">61</span>, <span class="dv">63</span>, <span class="dv">70</span>, <span class="dv">68</span>, <span class="dv">62</span>, <span class="dv">68</span>, <span class="dv">66</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>Y       <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.7</span>,<span class="sc">-</span><span class="fl">1.0</span>,<span class="sc">-</span><span class="fl">0.2</span>,<span class="sc">-</span><span class="fl">1.2</span>,<span class="sc">-</span><span class="fl">0.1</span>,<span class="fl">3.4</span>,<span class="fl">0.0</span>,<span class="fl">0.8</span>,<span class="fl">3.7</span>,<span class="fl">2.0</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>dataset <span class="ot">&lt;-</span>  <span class="fu">data.frame</span>(<span class="st">"X_2"</span> <span class="ot">=</span> X_2, <span class="st">"X_3"</span> <span class="ot">=</span> X_3, <span class="st">"Y"</span> <span class="ot">=</span> Y)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Compute the OLS estimation</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>lmobj   <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X_2 <span class="sc">+</span> X_3, <span class="at">data =</span> dataset)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot sample regression surface</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"scatterplot3d"</span>) <span class="co"># library for 3d plots</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plot3d  <span class="ot">&lt;-</span> <span class="fu">scatterplot3d</span>(<span class="at">x =</span> X_2, <span class="at">y =</span> X_3, <span class="at">z =</span> Y,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            <span class="at">angle =</span> <span class="dv">33</span>, <span class="at">scale.y =</span> <span class="fl">0.8</span>, <span class="at">pch =</span> <span class="dv">16</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>            <span class="at">color =</span><span class="st">"red"</span>, </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>            <span class="at">xlab =</span> <span class="fu">expression</span>(X[<span class="dv">2</span>]),</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>            <span class="at">ylab =</span> <span class="fu">expression</span>(X[<span class="dv">3</span>]),</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            <span class="at">main =</span><span class="st">"OLS Regression Surface"</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plot3d<span class="sc">$</span><span class="fu">plane3d</span>(lmobj, <span class="at">lty.box =</span> <span class="st">"solid"</span>, <span class="at">col=</span><span class="fu">gray</span>(.<span class="dv">5</span>), <span class="at">draw_polygon=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="some-quantities-of-interest" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="some-quantities-of-interest"><span class="header-section-number">3.3</span> Some Quantities of Interest</h2>
<p><strong>Predicted values and residuals.</strong></p>
<ul>
<li>The (OLS) <strong>predicted values</strong>: <span class="math inline">\(\hat{Y}_i=X_i'\hat\beta\)</span>.<br>
In matrix notation: <span class="math inline">\(\hat Y=X\underbrace{(X'X)^{-1}X'Y}_{\hat\beta}=P_X Y\)</span></li>
<li>The (OLS) <strong>residuals</strong>: <span class="math inline">\(\hat\varepsilon_i=Y_i-\hat{Y}_i\)</span>. In matrix notation: <span class="math inline">\(\hat\varepsilon=Y-\hat{Y}=\left(I_n-X(X'X)^{-1}X'\right)Y=M_X Y\)</span></li>
</ul>
<p><strong>Projection matrices.</strong></p>
<p>The matrix <span class="math display">\[
P_X=X(X'X)^{-1}X'
\]</span> is the <span class="math inline">\((n\times n)\)</span> <strong>projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the column space spanned by the column vectors of <span class="math inline">\(X\)</span> and <span class="math display">\[
M_X=I_n-X(X'X)^{-1}X'=I_n-P_X
\]</span> is the associated <span class="math inline">\((n\times n)\)</span> <strong>orthogonal projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the vector space that is orthogonal to that spanned by <span class="math inline">\(X\)</span>.</p>
<p>The projection matrices <span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> have some nice properties:</p>
<ol type="a">
<li><span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> are <strong>symmetric</strong>, i.e.&nbsp;<span class="math inline">\(P_X=P_X'\)</span> and <span class="math inline">\(M_X=M_X'\)</span>.</li>
<li><span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> are <strong>idempotent</strong>, i.e.&nbsp;<span class="math inline">\(P_XP_X=P_X\)</span> and <span class="math inline">\(M_X M_X=M_X\)</span>.</li>
<li>Moreover, we have that <span class="math inline">\(X'P_X=X'\)</span>, <span class="math inline">\(P_XX=X\)</span>, <span class="math inline">\(X'M_X=0\)</span>, <span class="math inline">\(M_XX=0\)</span>, and <span class="math inline">\(P_XM_X=0\)</span>.</li>
</ol>
<p>The properties (a)-(c) follow directly from the definitions of <span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> (check it out). Using these properties one can show that the residual vector <span class="math inline">\(\hat\varepsilon=(\hat\varepsilon_1,\dots,\hat\varepsilon_n)'\)</span> is orthogonal to each of the column vectors in <span class="math inline">\(X\)</span>, i.e <span class="math display">\[\begin{eqnarray}
X'\hat\varepsilon&amp;=&amp;X'M_XY\quad\text{\small(By Def.~of $M_X$)}\\
\Leftrightarrow X'\hat\varepsilon&amp;=&amp;\underset{(K\times n)}{0}\underset{(n\times 1)}{Y}\quad\text{\small(since $X'M_X=0$)}\\
\Leftrightarrow X'\hat\varepsilon&amp;=&amp;\underset{(K\times 1)}{0}.
\end{eqnarray}\]</span> <!-- \begin{align} --> <!-- X'\hat\varepsilon=\underset{(K\times 1)}{0}.\label{eq:orthXe} --> <!-- \end{align} --> Note that, in the case with intercept, the result <span class="math inline">\(X'\hat\varepsilon=0\)</span> implies that <span class="math inline">\(\sum_{i=1}^n\hat\varepsilon_i=0\)</span>. Moreover, the equation <span class="math inline">\(X'\hat\varepsilon=0\)</span> implies also that the residual vector <span class="math inline">\(\hat{\varepsilon}\)</span> is orthogonal to the predicted values vector, since <span class="math display">\[\begin{align*}
X'\hat\varepsilon&amp;=0\\
\Rightarrow\;\hat\beta'X'\hat\varepsilon&amp;=\hat\beta'0\\
\Leftrightarrow\;\hat Y'\hat\varepsilon&amp;=0.
\end{align*}\]</span></p>
<p>Another insight from equation <span class="math inline">\(X'\hat\varepsilon=0\)</span> is that the vector <span class="math inline">\(\hat\varepsilon\)</span> has to satisfy <span class="math inline">\(K\)</span> linear restrictions which means it looses <span class="math inline">\(K\)</span> degrees of freedom.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Consequently, the vector of residuals <span class="math inline">\(\hat\varepsilon\)</span> has only <span class="math inline">\(n-K\)</span> so-called <em>degrees of freedom</em>. This loss of <span class="math inline">\(K\)</span> degrees of freedom also appears in the definition of the <em>unbiased</em> variance estimator <span class="math display">\[\begin{align}
  s_{UB}^2&amp;=\frac{1}{n-K}\sum_{i=1}^n\hat\varepsilon_i^2\label{EqVarEstim}.
\end{align}\]</span></p>
<p><strong>Variance decomposition:</strong> A further useful result that can be shown using the properties of <span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> is that <span class="math inline">\(Y'Y=\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon\)</span>, i.e. <span class="math display">\[\begin{eqnarray*}
Y'Y&amp;=&amp;(\hat Y+\hat\varepsilon)'(\hat Y+\hat\varepsilon)\notag\\
  &amp;=&amp;(P_XY+M_XY)'(P_XY+M_XY)\notag\\
  &amp;=&amp;(Y'P_X'+Y'M_X')(P_XY+M_XY)\notag\\
  &amp;=&amp;Y'P_X'P_XY+Y'M_X'M_XY+0\notag\\
  &amp;=&amp;\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon
\end{eqnarray*}\]</span> The decomposition <span class="math display">\[
\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon
\]</span> is the basis for the well-known variance decomposition result for OLS regressions.</p>
<div id="thm-vardecomp" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.4 </strong></span>For the linear regression model with intercept (<a href="#eq-LinMod">Equation&nbsp;<span>3.1</span></a>), the total sample variance of the dependent variable <span class="math inline">\(Y_1,\dots,Y_n\)</span> can be decomposed as following: <span class="math display">\[\begin{eqnarray}
\underset{\text{total sample variance}}{\frac{1}{n}\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}&amp;=&amp;\underset{\text{explained sample variance}}{\frac{1}{n}\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}+\underset{\text{unexplained sample variance}}{\frac{1}{n}\sum_{i=1}^n\hat\varepsilon_i^2,}\label{VarDecomp}
\end{eqnarray}\]</span> where <span class="math inline">\(\bar{Y}=\frac{1}{n}\sum_{i=1}^nY_i\)</span> and <span class="math inline">\(\bar{\hat{Y}}=\frac{1}{n}\sum_{i=1}^n\hat{Y}_i\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From equation <span class="math inline">\(X'\hat\varepsilon=0\)</span> we have for regressions with intercept that <span class="math inline">\(\sum_{i=1}^n\hat\varepsilon_i=0\)</span>. Hence, from <span class="math inline">\(Y_i=\hat{Y}_i+\hat\varepsilon_i\)</span> it follows that <span class="math display">\[\begin{eqnarray*}
  \frac{1}{n}\sum_{i=1}^n Y_i&amp;=&amp;\frac{1}{n}\sum_{i=1}^n \hat{Y}_i+\frac{1}{n}\sum_{i=1}^n \hat\varepsilon_i\\
  \bar{Y}&amp;=&amp;\bar{\hat{Y}}+0
\end{eqnarray*}\]</span></p>
<p>Using the decomposition <span class="math inline">\(Y'Y=\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon\)</span>, we can now derive the result: <span class="math display">\[\begin{eqnarray*}
   Y'Y&amp;=&amp;\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon\\
   Y'Y-n\bar{Y}^2&amp;=&amp;\hat{Y}'\hat{Y}-n\bar{Y}^2+\hat\varepsilon'\hat\varepsilon\\
   Y'Y-n\bar{Y}^2&amp;=&amp;\hat{Y}'\hat{Y}-n\bar{\hat{Y}}^2+\hat\varepsilon'\hat\varepsilon\quad\text{(by $\bar{Y}=\bar{\hat{Y}}$)}\\
   \sum_{i=1}^nY_i^2-n\bar{Y}^2&amp;=&amp;\sum_{i=1}^n\hat{Y}_i^2-n\bar{\hat{Y}}^2+\sum_{i=1}^n\hat\varepsilon_i^2\\
   \sum_{i=1}^n(Y_i-\bar{Y})^2&amp;=&amp;\sum_{i=1}^n(\hat{Y}_i-\bar{\hat{Y}})^2+\sum_{i=1}^n\hat\varepsilon_i^2\quad\square\\
\end{eqnarray*}\]</span></p>
</div>
<section id="coefficients-of-determination-r2-and-overliner2" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="coefficients-of-determination-r2-and-overliner2">Coefficients of determination: <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\overline{R}^2\)</span></h4>
<p>The larger the proportion of the explained variance, the better is the fit of the model. This motivates the definition of the so-called <span class="math inline">\(R^2\)</span> coefficient of determination: <span class="math display">\[\begin{eqnarray*}
R^2=\frac{\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\;=\;1-\frac{\sum_{i=1}^n\hat{\varepsilon}_i^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}
\end{eqnarray*}\]</span> Obviously, we have that <span class="math inline">\(0\leq R^2\leq 1\)</span>. The closer <span class="math inline">\(R^2\)</span> lies to <span class="math inline">\(1\)</span>, the better is the fit of the model to the observed data. However, a high/low <span class="math inline">\(R^2\)</span> does not mean a validation/falsification of the estimated model. Any relation (i.e., model assumption) needs a plausible explanation from relevant economic theory. The most often criticized disadvantage of the <span class="math inline">\(R^2\)</span> is that additional regressors (relevant or not) will always increase the <span class="math inline">\(R^2\)</span>. Here is an example of the problem.</p>
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-2_3ff44c46b0649c92e7c42e596014387e">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">100</span>                  <span class="co"># Sample size</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)      <span class="co"># Relevant X variable</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>X_ir  <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">5</span>, <span class="dv">20</span>)      <span class="co"># Irrelevant X variable</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>error <span class="ot">&lt;-</span> <span class="fu">rt</span>(n, <span class="at">df =</span> <span class="dv">10</span>)<span class="sc">*</span><span class="dv">10</span>    <span class="co"># True error</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>Y     <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">5</span> <span class="sc">*</span> X <span class="sc">+</span> error    <span class="co"># Y variable</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>lm1   <span class="ot">&lt;-</span> <span class="fu">summary</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))     <span class="co"># Correct OLS regression </span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>lm2   <span class="ot">&lt;-</span> <span class="fu">summary</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X<span class="sc">+</span>X_ir))<span class="co"># OLS regression with X_ir </span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>lm1<span class="sc">$</span>r.squared <span class="sc">&lt;</span> lm2<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
<p>So, <span class="math inline">\(R^2\)</span> increases here even though <code>X_ir</code> is a completely irrelevant explanatory variable. Because of this, the <span class="math inline">\(R^2\)</span> cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called <strong>adjusted</strong> <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\overline{R}^2,\)</span> defined as <span class="math display">\[\begin{eqnarray*}
  \overline{R}^2&amp;=&amp;1-\frac{\frac{1}{n-K}\sum_{i=1}^n\hat{\varepsilon}^2_i}{\frac{1}{n-1}\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\leq R^2%\\
  %=\dots=
  %&amp;=&amp;1-\frac{n-1}{n-K}\left(1-R^2\right)\quad{\small\text{(since $1-R^2=(\sum_i\hat\varepsilon_i^2)/(\sum_i(Y_i-\bar{Y}))$)}}\\
  %&amp;=&amp;1-\frac{n-1}{n-K}+\frac{n-1}{n-K}R^2\quad+\frac{K-1}{n-K}R^2-\frac{K-1}{n-K}R^2\\
  %&amp;=&amp;1-\frac{n-1}{n-K}+R^2\quad+\frac{K-1}{n-K}R^2\\
  %&amp;=&amp;-\frac{K-1}{n-K}+R^2\quad+\frac{K-1}{n-K}R^2\\
  %&amp;=&amp;R^2-\underbrace{\frac{K-1}{n-K}\left(1-R^2\right)}_{\geq 0\;\text{and}\;\leq(K-1)/(n-K)}\;\leq\;R^2
\end{eqnarray*}\]</span> The adjustment is in terms of the degrees of freedom <span class="math inline">\(n-K\)</span>.</p>
</section>
</section>
<section id="method-of-moments-estimator" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="method-of-moments-estimator"><span class="header-section-number">3.4</span> Method of Moments Estimator</h2>
<p>Remember that the exogeneity assumption (Assumption 2), <span class="math inline">\(E(\varepsilon_i|X_i)=0,\)</span> implies that <span class="math inline">\(E(X_{ik}\varepsilon_i)=0\)</span> for all <span class="math inline">\(k=1,\dots,K\)</span>. Thus, the exogeneity assumption gives us a system of <span class="math inline">\(K\)</span> linear equations: <span class="math display">\[
\left.
  \begin{array}{c}
  E(\varepsilon_i)=0\\
  E(X_{i2}\varepsilon_i)=0\\
  \vdots\\
  E(X_{iK}\varepsilon_i)=0
  \end{array}
\right\}\Leftrightarrow \underset{(K\times 1)}{E(X_i\varepsilon_i)}=\underset{(K\times 1)}{0}
\]</span></p>
<p>The linear equation system <span class="math display">\[
\underset{(K\times 1)}{E(X_i\varepsilon_i)}=\underset{(K\times 1)}{0}
\]</span> allows us to identify the unknown parameter vector <span class="math inline">\(\beta\in\mathbb{R}^K\)</span> in terms of population moments: <span class="math display">\[
\begin{align*}
E(X_i(\overbrace{Y_i - X_i'\beta}^{=\varepsilon_i})) &amp;= 0\\
%\Leftrightarrow \hspace{1.5cm}
E(X_iY_i) - E(X_iX_i')\beta &amp; = 0\\
E(X_iX_i')\beta &amp; =  E(X_iY_i)  \\
\beta &amp; = \left(E(X_iX_i')\right)^{-1} E(X_iY_i)  \\
\end{align*}
\]</span> The fundamental idea behind “method of moments estimation” is to define an estimator by substituting population moments by sample moment analogues (sample means): <span class="math display">\[
\begin{align*}
\hat\beta_{mm}
&amp; = \left(\frac{1}{n}\sum_{i=1}^n X_iX_i'\right)^{-1} \frac{1}{n}\sum_{i=1}^n X_iY_i,
\end{align*}
\]</span> where <span class="math inline">\(\hat\beta_{mm}\)</span> can be simplified as following: <span class="math display">\[
\begin{align*}
\hat\beta_{mm}
&amp; = \left(\frac{1}{n}\sum_{i=1}^n X_iX_i'\right)^{-1} \frac{1}{n}\sum_{i=1}^n X_iY_i  \\
&amp; = \left(\sum_{i=1}^n X_iX_i'\right)^{-1} \sum_{i=1}^n X_iY_i\\
&amp; = \left(X'X\right)^{-1} X'Y.\\
\end{align*}
\]</span></p>
<p>Thus the method of moments estimator, <span class="math inline">\(\hat\beta_{mm},\)</span> coincides with the OLS estimator.</p>
</section>
<section id="practice" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="practice"><span class="header-section-number">3.5</span> Practice</h2>
<section id="factor-variables-and-the-dummy-variable-trap" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="factor-variables-and-the-dummy-variable-trap"><span class="header-section-number">3.5.1</span> Factor Variables and the Dummy-Variable Trap</h3>
<p>In the following, we consider simple linear regression model that aims to predict wages in the year 2008 using gender as the only predictor. We use data provided in the accompanying materials of Stock and Watson’s <em>Introduction to Econometrics</em> textbook <span class="citation" data-cites="stock2015">(<a href="#ref-stock2015" role="doc-biblioref">Stock and Watson 2015</a>)</span>. You can download the data stored as an xlsx-file <code>cps_ch3.xlsx</code> <a href="https://github.com/lidom/Script-Econometrics-MSc/blob/main/85467d963958861b842aa28b9eb262b1ea250101/data/cps92_12.xlsx">HERE</a>.</p>
<p>Let us first prepare the dataset:</p>
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-3_4ea0283b3452f294228f0e3bac8d99d7">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="do">## load the 'tidyverse' package</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(<span class="st">"tidyverse"</span>))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="do">## load the 'readxl' package</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"readxl"</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="do">## import the data into R</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>cps <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="at">path =</span> <span class="st">"data/cps_ch3.xlsx"</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># names(cps)</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># range(cps$year)</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># range(cps$a_sex) # 1 = male, 2 = female</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Data wrangling</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>cps_2008 <span class="ot">&lt;-</span> cps <span class="sc">%&gt;%</span> </span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">wage   =</span> ahe08,      <span class="co"># rename "ahe08" as "wage"    </span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">gender =</span> <span class="fu">fct_recode</span>( <span class="co"># rename factor "a_sex" as "gender"</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>      <span class="fu">as_factor</span>(a_sex),     </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>                <span class="st">"male"</span> <span class="ot">=</span> <span class="st">"1"</span>,  <span class="co"># rename factor level "1" to "male"</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>                <span class="st">"female"</span> <span class="ot">=</span> <span class="st">"2"</span> <span class="co"># rename factor level "2" to "female"</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>             ) </span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span>  </span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(year <span class="sc">==</span> <span class="dv">2008</span>) <span class="sc">%&gt;%</span>     <span class="co"># Only data from year 2008</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(wage, gender)         <span class="co"># Select only the variables "wage" and "gender"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first six lines of the dataset look as following:</p>
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-4_6b54b6a1151a2e32b231b73098faa00a">
<div class="cell-output-display">

<table class="table" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:right;"> wage </th>
   <th style="text-align:left;"> gender </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:right;"> 38.46154 </td>
   <td style="text-align:left;"> male </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 12.50000 </td>
   <td style="text-align:left;"> male </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 17.78846 </td>
   <td style="text-align:left;"> male </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 30.38461 </td>
   <td style="text-align:left;"> male </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 23.66864 </td>
   <td style="text-align:left;"> male </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 12.01923 </td>
   <td style="text-align:left;"> female </td>
  </tr>
</tbody>
</table>

</div>
</div>
<p>Computing the estimation results:</p>
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-5_877108424879d3eac01d8bf52ac2e9f8">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>lm_obj <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> gender, <span class="at">data =</span> cps_2008)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lm_obj)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-6_fd7c006ff664cff07755d1264b87297a">

</div>
<p>gives</p>
<ul>
<li><span class="math inline">\(\hat\beta_1=\)</span> 24.98</li>
<li><span class="math inline">\(\hat\beta_2=\)</span> -4.1</li>
</ul>
<p>To compute these estimation results, one needs to assign a numeric 0/1 coding to the factor levels <code>male</code> and <code>female</code> of the factor variable <code>gender</code>. To see the numeric values used by <code>R</code> one can take a look at <code>model.matrix(lm_obj)</code>:</p>
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-7_20a0ff58ad1a0c7076b5f41cae8fb62b">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(lm_obj) <span class="co"># this is the internally used X-matrix</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>X[<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  (Intercept) genderfemale
1           1            0
2           1            0
3           1            0
4           1            0
5           1            0
6           1            1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>cps_2008<span class="sc">$</span>gender[<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] male   male   male   male   male   female
Levels: male female</code></pre>
</div>
</div>
<p>Thus <code>R</code> internally codes <code>female</code> subjects by a 1 and <code>male</code> subjects by a 0, such that <span class="math display">\[
\hat\beta_1  + \hat\beta_2 X_{i,gender}=\left\{\begin{array}{ll}\hat\beta_1&amp;\text{ if }X_{i,gender}=\text{male}\\\hat\beta_1 + \hat\beta_2&amp;\text{ if }X_{i,gender}=\text{female}\end{array} \right.
\]</span></p>
<p>Interpretation:</p>
<ul>
<li>The average wage of male workers in 2008 was <span class="math inline">\(\hat{\beta}_1=\)</span> 24.98 (USD/Hour).</li>
<li>The average wage of female workers in 2008 was <span class="math inline">\(\hat{\beta}_1 + \hat{\beta}_2=\)</span> 20.87 (USD/Hour).</li>
<li>The difference in the earnings between male and female works in 2008 is <span class="math inline">\(\hat{\beta}_2=\)</span> -4.1 (USD/Hour).</li>
</ul>
</section>
<section id="dummy-variable-trap" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="dummy-variable-trap">Dummy-Variable Trap</h3>
<p>Above, we used <code>R</code>’s internal handling of factor variables which you should always do. If you construct a dummy variable for each of the levels of a factor yourself, however, you may blundered into the dummy variable trap:</p>
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-8_d7fec53c9f32daa1eee01b705c5cb6cf">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept variable</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>X_1      <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="at">times =</span> <span class="fu">nrow</span>(cps_2008))</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="do">## 1. Dummy variable for 'female'</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>X_female <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(cps_2008<span class="sc">$</span>gender <span class="sc">==</span> <span class="st">"female"</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 2. Dummy variable for 'male'</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>X_male   <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(cps_2008<span class="sc">$</span>gender <span class="sc">==</span> <span class="st">"male"</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Construct the model matrix 'X'</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>X        <span class="ot">&lt;-</span> <span class="fu">cbind</span>(X_1, X_female, X_male)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-9_37c0172edeef216da6968245ff6e96f8">

</div>
<p>Computing the estimation result for <span class="math inline">\(\beta\)</span> “by hand” yields</p>
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-10_c1bd6d064322736e695bd5701e6aac1c">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Dependent variable</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>Y        <span class="ot">&lt;-</span> cps_2008<span class="sc">$</span>wage</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Computing the estimator</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-11_a671a25c43fc8ac2b62cfb58e79c69e1">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>Error <span class="cf">in</span> <span class="fu">solve.default</span>(X <span class="sc">%*%</span> <span class="fu">t</span>(X)) <span class="sc">:</span> </span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  Lapack routine dgesv<span class="sc">:</span> system is exactly singular<span class="sc">:</span> U[<span class="dv">3</span>,<span class="dv">3</span>] <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>Calls<span class="sc">:</span> .main ... eval_with_user_handlers <span class="ot">-&gt;</span> eval <span class="ot">-&gt;</span> eval <span class="ot">-&gt;</span> solve <span class="ot">-&gt;</span> solve.default</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>Execution halted</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><font color="#FF0000">An error message!</font> 🤬 We blundered into the dummy variable trap!</p>
The estimation result is not computable since <span class="math inline">\((X'X)\)</span> is not invertible due to the perfect multicollinearity between the intercept and the two dummy variables <code>X_female</code> and <code>X_male</code>
<center>
<code>X_1</code> = <code>X_female</code> <span class="math inline">\(+\)</span> <code>X_male</code>
</center>
<p>which violates Assumption 3 (no perfect multicollinearity).</p>
<p><strong>Solution:</strong> Use one dummy-variable less than factor levels. I.e., in this example you can either drop <code>X_female</code> or <code>X_male</code>.</p>
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-12_56b7d6c82377088dc4b8ef36745c31b9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="do">## New model matrix after dropping X_male:</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>X        <span class="ot">&lt;-</span> <span class="fu">cbind</span>(X_1, X_female)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Computing the estimator</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> Y</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>beta_hat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              [,1]
X_1      24.978404
X_female -4.103626</code></pre>
</div>
</div>
<p>This give the same result as computed by <code>R</code>’s <code>lm()</code> function using the factor variable <code>gender</code>.</p>
</section>
<section id="detecting-heteroskedasticity" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="detecting-heteroskedasticity"><span class="header-section-number">3.5.2</span> Detecting Heteroskedasticity</h3>
<p>A very simple, but highly effective way to check for heteroskedastic error terms is to plot the residuals. If we have the correct model (hopefully we have), then (and only then) the residuals are good approximations to the realizations of the error terms <span class="math inline">\(\varepsilon_i\)</span>. Thus a plot of the residuals can be used to check for heteroskedasticity.</p>
<p>You can use <code>R</code>’s internal diagnostic plots that can be called using the <code>plot()</code> method for <code>lm</code>-objects:</p>
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-13_85d4be23a1293017c8123978fb01353b">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("devtools")</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># library(devtools)</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># install_git("https://github.com/ccolonescu/PoEdata")</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(PoEdata) <span class="co"># for the "food" dataset contained in this package</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"food"</span>)     <span class="co"># makes the dataset "food" usable</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>lm_object <span class="ot">&lt;-</span> <span class="fu">lm</span>(food_exp <span class="sc">~</span> income, <span class="at">data =</span> food)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Diagnostic scatter plot of residuals vs fitted values </span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lm_object, <span class="at">which =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="03-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Interpretation:</p>
<ul>
<li>The plot shows the residuals <span class="math inline">\(\hat{\varepsilon}_i\)</span> plotted against the fitted values <span class="math inline">\(\hat{Y}_i\)</span>.</li>
<li>The diagnostic plot indicates that the variance increases with income.</li>
</ul>
<p><strong>Note:</strong> Plotting against the fitted values is actually a quite smart idea since this also works for multiple predictors <span class="math inline">\(X_{i1},\dots,X_{iK}\)</span> with <span class="math inline">\(K\geq 3\)</span>.</p>
</section>
<section id="checking-non-linearity-of-the-regression-line" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="checking-non-linearity-of-the-regression-line"><span class="header-section-number">3.5.3</span> Checking (Non-)Linearity of the Regression Line</h3>
<p>To check whether there are non-linear relationships between the outcome and the predictors, on should take a look at the data, using a pairs-plot function that procures scatter plots for all pairs of variables in a dataset.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Base <code>R</code></a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Tidyverse <code>R</code></a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-14_cfbdf8bf337ed1eb1396e256aa795fc0">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>car_data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv"</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>my_car_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"MPG"</span>   <span class="ot">=</span> car_data<span class="sc">$</span>MPG.city,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"HP"</span>    <span class="ot">=</span> car_data<span class="sc">$</span>Horsepower</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(my_car_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="03-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-15_2eb3b03848a7029b32db62f5d6cdf2d1">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages("tidyverse")</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages("GGally")</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(<span class="st">"tidyverse"</span>))</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(<span class="st">"GGally"</span>)) <span class="co"># nice pairs plot </span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>car_data <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv"</span>,</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">show_col_types =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>my_car_df <span class="ot">&lt;-</span> car_data <span class="sc">%&gt;%</span> </span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a> dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>  <span class="st">"MPG"</span>   <span class="ot">=</span> MPG.city, </span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>  <span class="st">"HP"</span>    <span class="ot">=</span> Horsepower) <span class="sc">%&gt;%</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a> <span class="fu">select</span>(<span class="st">"MPG"</span>, <span class="st">"HP"</span>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(my_car_df) <span class="sc">+</span> <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="03-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
<p>The plots indicate a positive, but non-linear relationship between the outcome variable <code>MPG</code> (gasoline consumption in milles per gallon) and the predictor <code>HP</code> (power of the mashing in horsepower).</p>
<p>If we do not take this into account, we get bad model fits which can be seen in the scatter plot of <code>MPG</code> vs.&nbsp;<code>HP</code> and in the diagnostic plot of the residuals vs.&nbsp;the fitted values. (The latter works also for <span class="math inline">\(K\geq 3\)</span>, the former not.)</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Base <code>R</code></a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Tidyverse <code>R</code></a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="cell" data-fig.dim="[5,10]" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-16_4c32b32dc0ba6c59c2881737fd0829b5">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>lm_obj_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(MPG <span class="sc">~</span> HP, <span class="at">data =</span> my_car_df)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">y=</span>my_car_df<span class="sc">$</span>MPG, <span class="at">x=</span>my_car_df<span class="sc">$</span>HP, </span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Simple Linear Regression"</span>,</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Horse Power"</span>, </span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Miles per Gallon"</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lm_obj_1)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lm_obj_1, <span class="at">which=</span><span class="dv">1</span>, </span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">"Simple Linear Regression"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="03-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid" width="480"></p>
</div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-17_238cb196e7105f9bdec1a0ceb8857233">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages("tidyverse")</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages("ggfortify")</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(<span class="st">"tidyverse"</span>))</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"ggfortify"</span>) <span class="co"># tidy diagnostic plots</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>lm_obj_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(MPG <span class="sc">~</span> HP, <span class="at">data =</span> my_car_df)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot: Simple Linear Regression</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>my_car_df <span class="sc">%&gt;%</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>HP, <span class="at">y=</span>MPG)) <span class="sc">+</span> </span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">alpha=</span><span class="fl">0.8</span>, <span class="at">size=</span><span class="dv">3</span>)<span class="sc">+</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stat_smooth</span>(<span class="at">method=</span><span class="st">'lm'</span>, <span class="at">formula =</span> y<span class="sc">~</span>x) <span class="sc">+</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_classic</span>() <span class="sc">+</span> </span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Horse Power"</span>, <span class="at">y =</span> <span class="st">"Miles per Gallon"</span>, </span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>         <span class="at">title =</span> <span class="st">"Simple Linear Regression"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="03-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Diagnostic Plot: Simple Linear Regression </span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(lm_obj_1, <span class="at">which =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="03-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-17-2.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
<p>A polynomial regression model with polynomial degree 2 improves the model fit.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">Base <code>R</code></a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">Tidyverse <code>R</code></a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="cell" data-fig.dim="[5,10]" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-18_01ec51261172059eb2db4ffa87b76f10">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Adding new variable HP squared:</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>my_car_df<span class="sc">$</span>HP_sq <span class="ot">&lt;-</span> car_data<span class="sc">$</span>Horsepower<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>lm_obj_2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(MPG <span class="sc">~</span> HP <span class="sc">+</span> HP_sq, <span class="at">data =</span> my_car_df)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">y=</span>my_car_df<span class="sc">$</span>MPG, <span class="at">x=</span>my_car_df<span class="sc">$</span>HP,</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Polynomial Regression (Degree: 2)"</span>,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Horse Power"</span>, </span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Miles per Gallon"</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>X_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="fu">min</span>(my_car_df<span class="sc">$</span>HP), </span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>             <span class="at">to   =</span> <span class="fu">max</span>(my_car_df<span class="sc">$</span>HP), <span class="at">len =</span> <span class="dv">25</span>)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">y =</span> <span class="fu">predict</span>(lm_obj_2, </span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>                  <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="st">"HP"</span>    <span class="ot">=</span> X_seq, </span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>                                     <span class="st">"HP_sq"</span> <span class="ot">=</span> X_seq<span class="sc">^</span><span class="dv">2</span>)), </span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> X_seq)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lm_obj_2, <span class="at">which=</span><span class="dv">1</span>, </span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">"Polynomial Regression (Degree: 2)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="03-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid" width="480"></p>
</div>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-19_e85b6aad66392d5c29f371608d9656d6">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages("tidyverse")</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages("ggfortify")</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(<span class="st">"tidyverse"</span>))</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"ggfortify"</span>) <span class="co"># tidy diagnostic plots</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Adding new variable HP squared:</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>my_car_df <span class="ot">&lt;-</span> my_car_df <span class="sc">%&gt;%</span> </span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a> dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>  <span class="st">"HP_sq"</span> <span class="ot">=</span> HP<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Polynomial Regression</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>lm_obj_2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(MPG <span class="sc">~</span> HP <span class="sc">+</span> HP_sq, <span class="at">data =</span> my_car_df)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot: Polynomial Regression</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>my_car_df <span class="sc">%&gt;%</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>HP, <span class="at">y=</span>MPG)) <span class="sc">+</span> </span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">alpha=</span><span class="fl">0.8</span>, <span class="at">size=</span><span class="dv">3</span>)<span class="sc">+</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stat_smooth</span>(<span class="at">method=</span><span class="st">'lm'</span>, <span class="at">formula =</span> y<span class="sc">~</span><span class="fu">poly</span>(x, <span class="dv">2</span>, <span class="at">raw =</span> <span class="cn">TRUE</span>)) <span class="sc">+</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_classic</span>() <span class="sc">+</span> </span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Horse Power"</span>, <span class="at">y =</span> <span class="st">"Miles per Gallon"</span>, </span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>         <span class="at">title =</span> <span class="st">"Polynomial Regression"</span>, </span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>         <span class="at">subtitle =</span> <span class="st">"Polynomial Degree: 2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="03-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Diagnostic Plot: Polynomial Regression</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(lm_obj_2, <span class="at">which =</span> <span class="dv">1</span>)     </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="03-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-19-2.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="behavior-of-the-ols-estimates-for-resampled-data" class="level3" data-number="3.5.4">
<h3 data-number="3.5.4" class="anchored" data-anchor-id="behavior-of-the-ols-estimates-for-resampled-data"><span class="header-section-number">3.5.4</span> Behavior of the OLS Estimates for Resampled Data</h3>
<p>Usually, we only observe <em>one</em> estimate <span class="math inline">\(\hat{\beta}\)</span> of <span class="math inline">\(\beta\)</span> computed based on <em>one</em> given dataset (one realization of the random sample). However, in order to understand the statistical properties of the estimators <span class="math inline">\(\hat{\beta}\)</span> we need to view them as random variables which yield different realizations in repeated realizations of the random sample from Model (<a href="#eq-LinMod">Equation&nbsp;<span>3.1</span></a>). This allows us then to think about questions like:</p>
<ul>
<li>Is the estimator able to estimate the unknown parameter-value correctly on average?<br>
</li>
<li>Are the estimation results more precise if we have more data?</li>
</ul>
<p>A first idea about the statistical properties of the estimator <span class="math inline">\(\hat{\beta}\)</span> can be gained using Monte Carlo simulations as following.</p>
<div class="cell" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-20_91f2af9dd12074ccf206ca1bc4c8422d">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Sample sizes</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>n_small      <span class="ot">&lt;-</span>  <span class="dv">30</span> <span class="co"># smallish sample size</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>n_large      <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="co"># largish sample size</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="do">## True parameter values</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Monte-Carlo (MC) Simulation </span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 1. Generate data</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="do">## 2. Compute and store estimates</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Repeat steps 1. and 2. many times</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">3</span>)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of Monte Carlo repetitions</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="do">## How many samples to draw from the models</span></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>B          <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Containers to store the estimation results</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>beta0_estimates_n_small <span class="ot">&lt;-</span> <span class="fu">numeric</span>(B)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>beta1_estimates_n_small <span class="ot">&lt;-</span> <span class="fu">numeric</span>(B)</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>beta0_estimates_n_large <span class="ot">&lt;-</span> <span class="fu">numeric</span>(B)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>beta1_estimates_n_large <span class="ot">&lt;-</span> <span class="fu">numeric</span>(B)</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B){</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate artificial samples (n_small)</span></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>error_n_small     <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_small, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">5</span>)</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>X_n_small         <span class="ot">&lt;-</span> <span class="fu">runif</span>(n_small, <span class="at">min =</span> <span class="dv">1</span>, <span class="at">max =</span> <span class="dv">10</span>)</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>Y_n_small         <span class="ot">&lt;-</span> beta0 <span class="sc">+</span> beta1 <span class="sc">*</span> X_n_small <span class="sc">+</span> error_n_small</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>lm_obj            <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y_n_small <span class="sc">~</span> X_n_small) </span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a><span class="do">## Save estimation results </span></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>beta0_estimates_n_small[b] <span class="ot">&lt;-</span> lm_obj<span class="sc">$</span>coefficients[<span class="dv">1</span>]</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>beta1_estimates_n_small[b] <span class="ot">&lt;-</span> lm_obj<span class="sc">$</span>coefficients[<span class="dv">2</span>]</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate artificial samples (n_large)</span></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>error_n_large     <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_large, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">5</span>)</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>X_n_large         <span class="ot">&lt;-</span> <span class="fu">runif</span>(n_large, <span class="at">min =</span> <span class="dv">1</span>, <span class="at">max =</span> <span class="dv">10</span>)</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>Y_n_large         <span class="ot">&lt;-</span> beta0 <span class="sc">+</span> beta1 <span class="sc">*</span> X_n_large <span class="sc">+</span> error_n_large</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>lm_obj            <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y_n_large <span class="sc">~</span> X_n_large)</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a><span class="do">## Save estimation results </span></span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>beta0_estimates_n_large[b] <span class="ot">&lt;-</span> lm_obj<span class="sc">$</span>coefficients[<span class="dv">1</span>]</span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>beta1_estimates_n_large[b] <span class="ot">&lt;-</span> lm_obj<span class="sc">$</span>coefficients[<span class="dv">2</span>] </span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we have produced <code>B=1000</code> realizations of the estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> and saved these realizations in the vectors</p>
<ul>
<li><code>beta0_estimates_n_small</code></li>
<li><code>beta1_estimates_n_small</code></li>
<li><code>beta0_estimates_n_large</code></li>
<li><code>beta1_estimates_n_large</code></li>
</ul>
<p>These artificially generated realizations allow us to visualize the behavior of the OLS estimates for the repeatedly sampled data.</p>
<div class="cell" data-layout-align="center" data-hash="03-Multiple-Linear-Regression_cache/html/unnamed-chunk-21_5955009c913f6cef6cd58dd7bee99581">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03-Multiple-Linear-Regression_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>This are promising plots:</p>
<ul>
<li>The realizations of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are scattered around the true (unknown) parameter values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. This indicates unbiasdness of the estimators.</li>
<li>In the case of a larger sample size, the realizations of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> concentrate stronger around the true (unknown) parameter values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. This indicates consistency of the estimators.</li>
</ul>
<p>However, this was only a simulation for one specific data generating process. Such a Monte Carlo simulation does not allow us to generalize these properties. In the following chapters, we use theoretical arguments to investigate under which assumptions we can make <em>general</em> statements about the distributional properties of the estimator <span class="math inline">\(\hat\beta.\)</span></p>
</section>
</section>
<section id="exercises" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="exercises"><span class="header-section-number">3.6</span> Exercises</h2>
<ul>
<li><a href="Exercises/Ch3_Exercises.pdf">Exercises for Chapter 3</a></li>
</ul>
<!-- * [Exercises of Chapter 3 with Solutions](Exercises/Ch3_Exercises_with_Solutions.pdf) -->
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-stock2015" class="csl-entry" role="doc-biblioentry">
Stock, J. H., and M. W. Watson. 2015. <em><span>I</span>ntroduction to <span>E</span>conometrics</em>. <span>P</span>earson <span>E</span>ducation.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>The <span class="math inline">\(K\)</span> linear restrictions follow from the fact that <span class="math inline">\(X'\hat\varepsilon=0\)</span> are <span class="math inline">\(K\)</span> equations <span class="math inline">\(\sum_{i=1}^nX_{ik}\hat\varepsilon_i=0\)</span> for <span class="math inline">\(k=1,\dots,K\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02-Probability.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04-Monte-Carlo-Simulations.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Monte Carlo Simulations</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>