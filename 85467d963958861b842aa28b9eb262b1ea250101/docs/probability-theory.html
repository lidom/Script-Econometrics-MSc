<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.1 Probability Theory | Econometrics (M.Sc.)</title>
  <meta name="description" content="2.1 Probability Theory | Econometrics (M.Sc.)" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="2.1 Probability Theory | Econometrics (M.Sc.)" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/mylogo.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.1 Probability Theory | Econometrics (M.Sc.)" />
  
  
  <meta name="twitter:image" content="/images/mylogo.png" />

<meta name="author" content="Prof. Dr. Dominik Liebl" />


<meta name="date" content="2022-09-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ReviewStats.html"/>
<link rel="next" href="random-variables.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="index.html#preface" id="toc-preface">Preface</a>
<ul>
<li><a href="index.html#organization-of-the-course" id="toc-organization-of-the-course">Organization of the Course</a></li>
<li><a href="index.html#literature" id="toc-literature">Literature</a></li>
</ul></li>
<li><a href="introduction-to-r.html#introduction-to-r" id="toc-introduction-to-r"><span class="toc-section-number">1</span> Introduction to R</a>
<ul>
<li><a href="short-glossary.html#short-glossary" id="toc-short-glossary"><span class="toc-section-number">1.1</span> Short Glossary</a></li>
<li><a href="first-steps.html#first-steps" id="toc-first-steps"><span class="toc-section-number">1.2</span> First Steps</a></li>
<li><a href="further-data-objects.html#further-data-objects" id="toc-further-data-objects"><span class="toc-section-number">1.3</span> Further Data Objects</a></li>
<li><a href="simple-regression-analysis-using-r.html#simple-regression-analysis-using-r" id="toc-simple-regression-analysis-using-r"><span class="toc-section-number">1.4</span> Simple Regression Analysis using R</a></li>
<li><a href="programming-in-r.html#programming-in-r" id="toc-programming-in-r"><span class="toc-section-number">1.5</span> Programming in R</a></li>
<li><a href="r-packages.html#r-packages" id="toc-r-packages"><span class="toc-section-number">1.6</span> R-packages</a></li>
<li><a href="tidyverse.html#tidyverse" id="toc-tidyverse"><span class="toc-section-number">1.7</span> Tidyverse</a>
<ul>
<li><a href="tidyverse.html#tidyverse-plotting-basics" id="toc-tidyverse-plotting-basics"><span class="toc-section-number">1.7.1</span> Tidyverse: Plotting Basics</a></li>
<li><a href="tidyverse.html#tidyverse-data-wrangling-basics" id="toc-tidyverse-data-wrangling-basics"><span class="toc-section-number">1.7.2</span> Tidyverse: Data Wrangling Basics</a></li>
<li><a href="tidyverse.html#the-pipe-operator" id="toc-the-pipe-operator"><span class="toc-section-number">1.7.3</span> The pipe operator <code>%&gt;%</code></a></li>
<li><a href="tidyverse.html#the-group_by-function" id="toc-the-group_by-function"><span class="toc-section-number">1.7.4</span> The <code>group_by()</code> function</a></li>
</ul></li>
<li><a href="further-links.html#further-links" id="toc-further-links"><span class="toc-section-number">1.8</span> Further Links</a>
<ul>
<li><a href="further-links.html#further-r-intros" id="toc-further-r-intros"><span class="toc-section-number">1.8.1</span> Further R-Intros</a></li>
<li><a href="further-links.html#version-control-gitgithub" id="toc-version-control-gitgithub"><span class="toc-section-number">1.8.2</span> Version Control (Git/GitHub)</a></li>
<li><a href="further-links.html#r-ladies" id="toc-r-ladies"><span class="toc-section-number">1.8.3</span> R-Ladies</a></li>
</ul></li>
</ul></li>
<li><a href="ReviewStats.html#ReviewStats" id="toc-ReviewStats"><span class="toc-section-number">2</span> Review: Probability and Statistics</a>
<ul>
<li><a href="probability-theory.html#probability-theory" id="toc-probability-theory"><span class="toc-section-number">2.1</span> Probability Theory</a>
<ul>
<li><a href="probability-theory.html#sample-spaces-and-elementary-events" id="toc-sample-spaces-and-elementary-events"><span class="toc-section-number">2.1.1</span> Sample Spaces and (Elementary) Events</a></li>
<li><a href="probability-theory.html#probability" id="toc-probability"><span class="toc-section-number">2.1.2</span> Probability</a></li>
<li><a href="probability-theory.html#independent-events" id="toc-independent-events"><span class="toc-section-number">2.1.3</span> Independent Events</a></li>
<li><a href="probability-theory.html#conditional-probability" id="toc-conditional-probability"><span class="toc-section-number">2.1.4</span> Conditional Probability</a></li>
</ul></li>
<li><a href="random-variables.html#random-variables" id="toc-random-variables"><span class="toc-section-number">2.2</span> Random Variables</a>
<ul>
<li><a href="random-variables.html#univariate-distribution-and-probability-functions" id="toc-univariate-distribution-and-probability-functions"><span class="toc-section-number">2.2.1</span> Univariate Distribution and Probability Functions</a></li>
<li><a href="random-variables.html#multivariate-distribution-and-probability-functions" id="toc-multivariate-distribution-and-probability-functions"><span class="toc-section-number">2.2.2</span> Multivariate Distribution and Probability Functions</a></li>
<li><a href="random-variables.html#means-and-moments" id="toc-means-and-moments"><span class="toc-section-number">2.2.3</span> Means and Moments</a></li>
<li><a href="random-variables.html#unconditional-means" id="toc-unconditional-means"><span class="toc-section-number">2.2.4</span> Unconditional Means</a></li>
<li><a href="random-variables.html#conditional-means" id="toc-conditional-means"><span class="toc-section-number">2.2.5</span> Conditional Means</a></li>
<li><a href="random-variables.html#means-of-transformed-random-variables-and-moments" id="toc-means-of-transformed-random-variables-and-moments"><span class="toc-section-number">2.2.6</span> Means of Transformed Random Variables and Moments</a></li>
<li><a href="random-variables.html#independent-random-variables" id="toc-independent-random-variables"><span class="toc-section-number">2.2.7</span> Independent Random Variables</a></li>
<li><a href="random-variables.html#i.i.d.-samples" id="toc-i.i.d.-samples"><span class="toc-section-number">2.2.8</span> I.I.D. Samples</a></li>
<li><a href="random-variables.html#some-important-discrete-random-variables" id="toc-some-important-discrete-random-variables"><span class="toc-section-number">2.2.9</span> Some Important Discrete Random Variables</a></li>
<li><a href="random-variables.html#some-important-continuous-random-variables" id="toc-some-important-continuous-random-variables"><span class="toc-section-number">2.2.10</span> Some Important Continuous Random Variables</a></li>
</ul></li>
</ul></li>
<li><a href="ch:MLR.html#ch:MLR" id="toc-ch:MLR"><span class="toc-section-number">3</span> Multiple Linear Regression</a>
<ul>
<li><a href="assumptions.html#assumptions" id="toc-assumptions"><span class="toc-section-number">3.1</span> Assumptions</a>
<ul>
<li><a href="assumptions.html#some-implications-of-the-exogeneity-assumption" id="toc-some-implications-of-the-exogeneity-assumption"><span class="toc-section-number">3.1.1</span> Some Implications of the Exogeneity Assumption</a></li>
</ul></li>
<li><a href="deriving-the-expression-of-the-ols-estimator.html#deriving-the-expression-of-the-ols-estimator" id="toc-deriving-the-expression-of-the-ols-estimator"><span class="toc-section-number">3.2</span> Deriving the Expression of the OLS Estimator</a></li>
<li><a href="some-quantities-of-interest.html#some-quantities-of-interest" id="toc-some-quantities-of-interest"><span class="toc-section-number">3.3</span> Some Quantities of Interest</a></li>
<li><a href="method-of-moments-estimator.html#method-of-moments-estimator" id="toc-method-of-moments-estimator"><span class="toc-section-number">3.4</span> Method of Moments Estimator</a></li>
<li><a href="unbiasedness-of-hatbetax-and-hatbeta.html#unbiasedness-of-hatbetax-and-hatbeta" id="toc-unbiasedness-of-hatbetax-and-hatbeta"><span class="toc-section-number">3.5</span> Unbiasedness of <span class="math inline">\(\hat\beta|X\)</span> and <span class="math inline">\(\hat\beta\)</span></a></li>
<li><a href="ch:VarEstBeta.html#ch:VarEstBeta" id="toc-ch:VarEstBeta"><span class="toc-section-number">3.6</span> Variance and Standard Error of <span class="math inline">\(\hat\beta|X\)</span></a></li>
<li><a href="the-gauss-markov-theorem.html#the-gauss-markov-theorem" id="toc-the-gauss-markov-theorem"><span class="toc-section-number">3.7</span> The Gauss-Markov Theorem</a></li>
<li><a href="practice-real-data.html#practice-real-data" id="toc-practice-real-data"><span class="toc-section-number">3.8</span> Practice: Real Data</a>
<ul>
<li><a href="practice-real-data.html#dummy-variables-contrast-codings-and-interactions" id="toc-dummy-variables-contrast-codings-and-interactions">Dummy Variables, Contrast Codings, and Interactions</a></li>
<li><a href="practice-real-data.html#the-function" id="toc-the-function">The Function </a></li>
</ul></li>
<li><a href="practice-simulation.html#practice-simulation" id="toc-practice-simulation"><span class="toc-section-number">3.9</span> Practice: Simulation</a>
<ul>
<li><a href="practice-simulation.html#behavior-of-the-ols-estimates-for-resampled-data-conditionally-on-x_i" id="toc-behavior-of-the-ols-estimates-for-resampled-data-conditionally-on-x_i"><span class="toc-section-number">3.9.1</span> Behavior of the OLS Estimates for Resampled Data (conditionally on <span class="math inline">\(X_i\)</span>)</a></li>
</ul></li>
</ul></li>
<li><a href="ch:SSINF.html#ch:SSINF" id="toc-ch:SSINF"><span class="toc-section-number">4</span> Small Sample Inference</a>
<ul>
<li><a href="ch:testmultp.html#ch:testmultp" id="toc-ch:testmultp"><span class="toc-section-number">4.1</span> Hypothesis Tests about Multiple Parameters</a>
<ul>
<li><a href="ch:testmultp.html#the-test-statistic-and-its-null-distribution" id="toc-the-test-statistic-and-its-null-distribution"><span class="toc-section-number">4.1.1</span> The Test Statistic and its Null Distribution</a></li>
</ul></li>
<li><a href="ch:testingsinglep.html#ch:testingsinglep" id="toc-ch:testingsinglep"><span class="toc-section-number">4.2</span> Tests about One Parameter</a></li>
<li><a href="testtheory.html#testtheory" id="toc-testtheory"><span class="toc-section-number">4.3</span> Testtheory</a>
<ul>
<li><a href="testtheory.html#significance-level" id="toc-significance-level"><span class="toc-section-number">4.3.1</span> Significance Level</a></li>
<li><a href="testtheory.html#critical-value-for-the-f-test" id="toc-critical-value-for-the-f-test"><span class="toc-section-number">4.3.2</span> Critical Value for the <span class="math inline">\(F\)</span>-Test</a></li>
<li><a href="testtheory.html#critical-values-for-the-t-test" id="toc-critical-values-for-the-t-test"><span class="toc-section-number">4.3.3</span> Critical Value(s) for the <span class="math inline">\(t\)</span>-Test</a></li>
</ul></li>
<li><a href="type-ii-error-and-power.html#type-ii-error-and-power" id="toc-type-ii-error-and-power"><span class="toc-section-number">4.4</span> Type II Error and Power</a></li>
<li><a href="p-value.html#p-value" id="toc-p-value"><span class="toc-section-number">4.5</span> <span class="math inline">\(p\)</span>-Value</a></li>
<li><a href="CIsmallsample.html#CIsmallsample" id="toc-CIsmallsample"><span class="toc-section-number">4.6</span> Confidence Intervals</a></li>
<li><a href="PSSI.html#PSSI" id="toc-PSSI"><span class="toc-section-number">4.7</span> Practice: Small Sample Inference</a>
<ul>
<li><a href="PSSI.html#normally-distributed-hatbetax" id="toc-normally-distributed-hatbetax"><span class="toc-section-number">4.7.1</span> Normally Distributed <span class="math inline">\(\hat\beta|X\)</span></a></li>
<li><a href="PSSI.html#testing-multiple-parameters" id="toc-testing-multiple-parameters"><span class="toc-section-number">4.7.2</span> Testing Multiple Parameters</a></li>
<li><a href="PSSI.html#dualty-of-confidence-intervals-and-hypothesis-tests" id="toc-dualty-of-confidence-intervals-and-hypothesis-tests"><span class="toc-section-number">4.7.3</span> Dualty of Confidence Intervals and Hypothesis Tests</a></li>
</ul></li>
</ul></li>
<li><a href="ch:LSINF.html#ch:LSINF" id="toc-ch:LSINF"><span class="toc-section-number">5</span> Large Sample Inference</a>
<ul>
<li><a href="tools-for-asymptotic-statistics.html#tools-for-asymptotic-statistics" id="toc-tools-for-asymptotic-statistics"><span class="toc-section-number">5.1</span> Tools for Asymptotic Statistics</a>
<ul>
<li><a href="tools-for-asymptotic-statistics.html#modes-of-convergence" id="toc-modes-of-convergence"><span class="toc-section-number">5.1.1</span> Modes of Convergence</a></li>
<li><a href="tools-for-asymptotic-statistics.html#four-important-modes-of-convergence" id="toc-four-important-modes-of-convergence">Four Important Modes of Convergence</a></li>
<li><a href="tools-for-asymptotic-statistics.html#relations-among-modes-of-convergence" id="toc-relations-among-modes-of-convergence">Relations among Modes of Convergence</a></li>
<li><a href="tools-for-asymptotic-statistics.html#continuous-mapping-theorem-cmt" id="toc-continuous-mapping-theorem-cmt"><span class="toc-section-number">5.1.2</span> Continuous Mapping Theorem (CMT)</a></li>
<li><a href="tools-for-asymptotic-statistics.html#slutsky-theorem" id="toc-slutsky-theorem"><span class="toc-section-number">5.1.3</span> Slutsky Theorem</a></li>
<li><a href="tools-for-asymptotic-statistics.html#law-of-large-numbers-lln-and-central-limit-theorem-clt" id="toc-law-of-large-numbers-lln-and-central-limit-theorem-clt"><span class="toc-section-number">5.1.4</span> Law of Large Numbers (LLN) and Central Limit Theorem (CLT)</a></li>
<li><a href="tools-for-asymptotic-statistics.html#estimators-as-a-sequences-of-random-variables" id="toc-estimators-as-a-sequences-of-random-variables"><span class="toc-section-number">5.1.5</span> Estimators as a Sequences of Random Variables</a></li>
</ul></li>
<li><a href="asymptotics-under-the-classic-regression-model.html#asymptotics-under-the-classic-regression-model" id="toc-asymptotics-under-the-classic-regression-model"><span class="toc-section-number">5.2</span> Asymptotics under the Classic Regression Model</a>
<ul>
<li><a href="asymptotics-under-the-classic-regression-model.html#the-case-of-heteroscedasticity" id="toc-the-case-of-heteroscedasticity"><span class="toc-section-number">5.2.1</span> The Case of Heteroscedasticity</a></li>
<li><a href="asymptotics-under-the-classic-regression-model.html#hypothesis-testing-and-confidence-intervals" id="toc-hypothesis-testing-and-confidence-intervals"><span class="toc-section-number">5.2.2</span> Hypothesis Testing and Confidence Intervals</a></li>
</ul></li>
<li><a href="practice-large-sample-inference.html#practice-large-sample-inference" id="toc-practice-large-sample-inference"><span class="toc-section-number">5.3</span> Practice: Large Sample Inference</a>
<ul>
<li><a href="practice-large-sample-inference.html#normally-distributed-hatbeta-for-ntoinfty" id="toc-normally-distributed-hatbeta-for-ntoinfty"><span class="toc-section-number">5.3.1</span> Normally Distributed <span class="math inline">\(\hat\beta\)</span> for <span class="math inline">\(n\to\infty\)</span></a></li>
<li><a href="practice-large-sample-inference.html#testing-multiple-and-single-parameters" id="toc-testing-multiple-and-single-parameters"><span class="toc-section-number">5.3.2</span> Testing Multiple and Single Parameters</a></li>
</ul></li>
</ul></li>
<li><a href="ivr.html#ivr" id="toc-ivr"><span class="toc-section-number">6</span> Instrumental Variables Regression</a>
<ul>
<li><a href="TIVEWASRAASI.html#TIVEWASRAASI" id="toc-TIVEWASRAASI"><span class="toc-section-number">6.1</span> The IV Estimator with a Single Regressor and a Single Instrument</a>
<ul>
<li><a href="TIVEWASRAASI.html#the-two-stage-least-squares-estimator" id="toc-the-two-stage-least-squares-estimator"><span class="toc-section-number">6.1.1</span> The Two-Stage Least Squares Estimator</a></li>
<li><a href="TIVEWASRAASI.html#application-demand-for-cigarettes-12" id="toc-application-demand-for-cigarettes-12"><span class="toc-section-number">6.1.2</span> Application: Demand For Cigarettes (1/2)</a></li>
</ul></li>
<li><a href="TGIVRM.html#TGIVRM" id="toc-TGIVRM"><span class="toc-section-number">6.2</span> The General IV Regression Model</a>
<ul>
<li><a href="TGIVRM.html#application-demand-for-cigarettes-22" id="toc-application-demand-for-cigarettes-22"><span class="toc-section-number">6.2.1</span> Application: Demand for Cigarettes (2/2)</a></li>
</ul></li>
<li><a href="civ.html#civ" id="toc-civ"><span class="toc-section-number">6.3</span> Checking Instrument Validity</a>
<ul>
<li><a href="civ.html#instrument-relevance" id="toc-instrument-relevance"><span class="toc-section-number">6.3.1</span> Instrument Relevance</a></li>
<li><a href="civ.html#instrument-validity" id="toc-instrument-validity"><span class="toc-section-number">6.3.2</span> Instrument Validity</a></li>
</ul></li>
<li><a href="attdfc.html#attdfc" id="toc-attdfc"><span class="toc-section-number">6.4</span> Application to the Demand for Cigarettes</a></li>
</ul></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics (M.Sc.)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability-theory" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Probability Theory</h2>
<p>Probability is the mathematical language for quantifying uncertainty. We can apply probability theory to a diverse set of problems, from coin flipping to the analysis of econometric problems. The starting point is to specify the <strong>sample space</strong>, that is, the set of possible outcomes.</p>
<div id="sample-spaces-and-elementary-events" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Sample Spaces and (Elementary) Events</h3>
<p>The <strong>sample space</strong> <span class="math inline">\(\Omega,\)</span> is the set of possible outcomes of an experiment. Points <span class="math inline">\(\omega\)</span> in <span class="math inline">\(\Omega\)</span> are called <strong>sample outcomes</strong> or <strong>realizations</strong> or <strong>elementary events</strong>. <strong>Events</strong> are subsets of <span class="math inline">\(\Omega\)</span>.</p>
<p> If we toss a coin twice then <span class="math inline">\(\Omega=\{H H, H T, T H, T T\} .\)</span> The event that the first toss is heads is <span class="math inline">\(A=\{H H, H T\}\)</span>.</p>
<p> Let <span class="math inline">\(\omega\)</span> be the outcome of a measurement of some physical quantity, for example, temperature. Then <span class="math inline">\(\Omega=\mathbb{R}=(-\infty, \infty).\)</span> The event that the measurement is larger than 10 but less than or equal to 23 is <span class="math inline">\(A=(10,23]\)</span>.</p>
<p> If we toss a coin forever then the sample space is the infinite set <span class="math inline">\(\Omega=\left\{\omega=\left(\omega_{1}, \omega_{2}, \omega_{3}, \ldots,\right)|\omega_{i} \in\{H, T\}\right\}\)</span> Let <span class="math inline">\(A\)</span> be the event that the first head appears on the third toss. Then
<span class="math inline">\(A=\left\{\left(\omega_{1}, \omega_{2}, \omega_{3}, \ldots,\right)| \omega_{1}=T, \omega_{2}=T, \omega_{3}=H, \omega_{i} \in\{H, T\} \text { for } i&gt;3\right\}\)</span>.</p>
<p>Given an event <span class="math inline">\(A,\)</span> let <span class="math inline">\(A^{c}=\{\omega \in \Omega ; \omega \notin A\}\)</span> denote the <strong>complement</strong> of <span class="math inline">\(A\)</span>. Informally, <span class="math inline">\(A^{c}\)</span> can be read as The complement of <span class="math inline">\(\Omega\)</span> is the empty set <span class="math inline">\(\emptyset\)</span>. The <strong>union</strong> of events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is defined as
<span class="math display">\[
A\cup B=\{\omega \in \Omega|\omega\in A\text{ or }\omega \in B\text{ or }\omega\in\text{ both}\}
\]</span>
which can be thought of as If <span class="math inline">\(A_{1}, A_{2}, \ldots\)</span> is a sequence of sets then
<span class="math display">\[
\bigcup_{i=1}^{\infty} A_{i}=\left\{\omega \in \Omega: \omega \in A_{i} \text { for at least one i }\right\}.
\]</span>
The <strong>intersection</strong> of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is defined as
<span class="math display">\[
A \cap B=\{\omega \in \Omega ; \omega \in A\text{ and }\omega \in B\}\]</span>
which reads as Sometimes <span class="math inline">\(A \cap B\)</span> is also written shortly as <span class="math inline">\(AB\)</span>. If <span class="math inline">\(A_{1}, A_{2}, \ldots\)</span> is a sequence of sets then
<span class="math display">\[
\bigcap_{i=1}^{\infty} A_{i}=\left\{\omega \in \Omega: \omega \in A_{i} \text { for all i }\right\}.
\]</span>
If every element of <span class="math inline">\(A\)</span> is also contained in <span class="math inline">\(B\)</span> we write <span class="math inline">\(A \subset B\)</span> or, equivalently, <span class="math inline">\(B \supset A\)</span>. If <span class="math inline">\(A\)</span> is a finite set, let <span class="math inline">\(|A|\)</span> denote the number of elements in <span class="math inline">\(A .\)</span> We say that <span class="math inline">\(A_{1}, A_{2}, \ldots\)</span> are <strong>disjoint</strong> or <strong>mutually exclusive</strong> if <span class="math inline">\(A_{i} \cap A_{j}=\emptyset\)</span> whenever <span class="math inline">\(i \neq j\)</span>. For example, <span class="math inline">\(A_{1}=[0,1), A_{2}=[1,2), A_{3}=[2,3), \ldots\)</span> are disjoint. A <strong>partition</strong> of <span class="math inline">\(\Omega\)</span> is a sequence of disjoint sets <span class="math inline">\(A_{1}, A_{2}, \ldots\)</span> such that <span class="math inline">\(\bigcup_{i=1}^{\infty} A_{i}=\Omega\)</span>.</p>
<!-- \newpage -->

<!-- Given an event $A,$ define the indicator function of $A$ by -->
<!-- $$ -->
<!-- I_{A}(\omega)=I(\omega \in A)=\left\{\begin{array}{ll} -->
<!-- 1 & \text { if } \omega \in A \\ -->
<!-- 0 & \text { if } \omega \notin A -->
<!-- \end{array}\right. -->
<!-- $$ -->
<!-- A sequence of sets $A_{1}, A_{2}, \ldots$ is monotone increasing if $A_{1} \subset A_{2} \subset$ -->
<!-- $\cdots$ and we define $\lim _{n \rightarrow \infty} A_{n}=\bigcup_{i=1}^{\infty} A_{i} .$ A sequence of sets $A_{1}, A_{2}, \ldots$ is -->
<!-- monotone decreasing if $A_{1} \supset A_{2} \supset \cdots$ and then we define $\lim _{n \rightarrow \infty} A_{n}=$ -->
<!-- $\bigcap_{i=1}^{\infty} A_{i} .$ In either case, we will write $A_{n} \rightarrow A$ -->
<!-- Example 2.4 Let $\Omega=\mathbb{R}$ and let $A_{i}=[0,1 / i)$ for $i=1,2, \ldots$ Then $\bigcup_{i=1}^{\infty} A_{i}=$ [0,1) and $\bigcap_{i=1}^{\infty} A_{i}=\{0\} .$ If instead we define $A_{i}=(0,1 / i)$ then $\bigcup_{i=1}^{\infty} A_{i}=$ -->
<!-- (0,1) and $\bigcap_{i=1}^{\infty} A_{i}=\emptyset$ -->
</div>
<div id="probability" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Probability</h3>
We want to assign a real number <span class="math inline">\(P(A)\)</span> to every event <span class="math inline">\(A,\)</span> called the <strong>probability</strong> of <span class="math inline">\(A .\)</span> We also call <span class="math inline">\(P\)</span> a <strong>probability distribution</strong> or a <strong>probability measure</strong>. To qualify as a probability, <span class="math inline">\(P\)</span> has to satisfy three axioms. That is, a function <span class="math inline">\(P\)</span> that assigns a real number <span class="math inline">\(P(A)\in[0,1]\)</span> to each event <span class="math inline">\(A\)</span> is a or a if it satisfies the following three axioms:

<p> It is not always possible to assign a probability to every event <span class="math inline">\(A\)</span> if the sample space is large, such as, for instance, the whole real line, <span class="math inline">\(\Omega=\mathbb{R}\)</span>. In case of <span class="math inline">\(\Omega=\mathbb{R}\)</span> strange things can happen. There are pathological sets that simply break down the mathematics. An example of one of these pathological sets, also known as non-measurable sets because they literally can’t be measured (i.e. we cannot assign probabilities to them), are the Vitali sets. Therefore, in such cases like <span class="math inline">\(\Omega=\mathbb{R}\)</span>, we assign probabilities to a <em>limited</em> class of sets called a <strong><span class="math inline">\(\sigma\)</span>-field</strong> or <strong><span class="math inline">\(\sigma\)</span>-algebra</strong>. For <span class="math inline">\(\Omega=\mathbb{R}\)</span>, the canonical <strong><span class="math inline">\(\sigma\)</span>-algebra</strong> is the <strong>Borel <span class="math inline">\(\sigma\)</span>-algebra</strong>. The Borel <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\mathbb{R}\)</span> is generated by the collection of all open subsets of <span class="math inline">\(R\)</span>.</p>
One can derive many properties of <span class="math inline">\(P\)</span> from the axioms. Here are a few:
<p>A less obvious property is given in the following: For any events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> we have that,
<span class="math display">\[P(A \cup B)=P(A)+P(B)-P(A B).\]</span></p>
<p>Two consecutive coin tosses. Let <span class="math inline">\(H_{1}\)</span> be the event that heads occurs on toss 1 and let <span class="math inline">\(H_{2}\)</span> be the event that heads occurs on toss 2. If all outcomes are equally likely, that is, <span class="math inline">\(\mathrm{P}\left(\left\{H_{1}, H_{2}\right\}\right)=\mathrm{P}\left(\left\{H_{1}, T_{2}\right\}\right)=\mathrm{P}\left(\left\{T_{1}, H_{2}\right\}\right)=\mathrm{P}\left(\left\{T_{1}, T_{2}\right\}\right)=1 / 4\)</span>, then
<span class="math display">\[
\mathrm{P}\left(H_{1} \cup H_{2}\right)=\mathrm{P}\left(H_{1}\right)+\mathrm{P}\left(H_{2}\right)-\mathrm{P}\left(H_{1} H_{2}\right)=\frac{1}{2}+\frac{1}{2}-\frac{1}{4}=\frac{3}{4}.
\]</span></p>
<p>One can interpret <span class="math inline">\(P(A)\)</span> in terms of . That is, <span class="math inline">\(P(A)\)</span> is the (infinitely) long run proportion of times that <span class="math inline">\(A\)</span> is true in repetitions. For example, if we say that the probability of heads is <span class="math inline">\(1 / 2\)</span>, i.e <span class="math inline">\(P(H)=1/2\)</span> we mean that if we flip the coin many times then the proportion of times we get heads tends to <span class="math inline">\(1 / 2\)</span> as the number of tosses increases. An infinitely long, unpredictable sequence of tosses whose limiting proportion tends to a constant is an idealization, much like the idea of a straight line in geometry.
The following codes approximates the probability <span class="math inline">\(P(H)=1/2\)</span> using 5, 50 and 5,000 many (pseudo) random coin flips:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="probability-theory.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">869</span>)</span>
<span id="cb60-2"><a href="probability-theory.html#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 (fair) coin-flip:</span></span>
<span id="cb60-3"><a href="probability-theory.html#cb60-3" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="st">&quot;H&quot;</span>, <span class="st">&quot;T&quot;</span>), <span class="at">size =</span> <span class="dv">5</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb60-4"><a href="probability-theory.html#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Relative frequency of &quot;H&quot; in 5 coin-flips</span></span>
<span id="cb60-5"><a href="probability-theory.html#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(results[results<span class="sc">==</span><span class="st">&quot;H&quot;</span>])<span class="sc">/</span><span class="dv">5</span></span>
<span id="cb60-6"><a href="probability-theory.html#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.2</span></span>
<span id="cb60-7"><a href="probability-theory.html#cb60-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-8"><a href="probability-theory.html#cb60-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 10 (fair) coin-flips:</span></span>
<span id="cb60-9"><a href="probability-theory.html#cb60-9" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="st">&quot;H&quot;</span>, <span class="st">&quot;T&quot;</span>), <span class="at">size =</span> <span class="dv">50</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb60-10"><a href="probability-theory.html#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Relative frequency of &quot;H&quot; in 50 coin-flips</span></span>
<span id="cb60-11"><a href="probability-theory.html#cb60-11" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(results[results<span class="sc">==</span><span class="st">&quot;H&quot;</span>])<span class="sc">/</span><span class="dv">50</span></span>
<span id="cb60-12"><a href="probability-theory.html#cb60-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.52</span></span>
<span id="cb60-13"><a href="probability-theory.html#cb60-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-14"><a href="probability-theory.html#cb60-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 100000 (fair) coin-flips:</span></span>
<span id="cb60-15"><a href="probability-theory.html#cb60-15" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="st">&quot;H&quot;</span>, <span class="st">&quot;T&quot;</span>), <span class="at">size =</span> <span class="dv">5000</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb60-16"><a href="probability-theory.html#cb60-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Relative frequency of &quot;H&quot; in 5000 coin-flips</span></span>
<span id="cb60-17"><a href="probability-theory.html#cb60-17" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(results[results<span class="sc">==</span><span class="st">&quot;H&quot;</span>])<span class="sc">/</span><span class="dv">5000</span></span>
<span id="cb60-18"><a href="probability-theory.html#cb60-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.5024</span></span></code></pre></div>
</div>
<div id="independent-events" class="section level3" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Independent Events</h3>
<p>If we flip a fair coin twice, then the probability of two heads is <span class="math inline">\(\frac{1}{2} \times \frac{1}{2}\)</span>. We multiply the probabilities because we regard the two tosses as independent. Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are called if
<span class="math display">\[P(A B)=P(A) P(B).\]</span>
Or more generally, a whole set of events <span class="math inline">\(\{A_i|i\in I\}\)</span> is independent if
<span class="math display">\[
P\left(\bigcap_{i \in J} A_{i}\right)=\prod_{i \in J}P\left(A_{i}\right)
\]</span>
for every finite subset <span class="math inline">\(J\)</span> of <span class="math inline">\(I\)</span>, where <span class="math inline">\(I\)</span> denotes the not necessarily finite index set (e.g. <span class="math inline">\(I=\{1,2,\dots\}\)</span>).</p>
<p>Independence can arise in two distinct ways. Sometimes, we <strong>explicitly assume</strong> that two events are independent. For example, in tossing a coin twice, we usually assume the tosses are independent which reflects the fact that the coin has no memory of the first toss.
In other instances, we <strong>derive</strong> independence by verifying that the definition of independence <span class="math inline">\(P(A B)=P(A)P(B)\)</span> holds. For example, in tossing a fair die , let <span class="math inline">\(A=\{2,4,6\}\)</span> be the event of observing an even number and let <span class="math inline">\(B=\{1,2,3,4\}\)</span> be the event of observing no <span class="math inline">\(5\)</span> and no <span class="math inline">\(6\)</span>. Then,
<span class="math inline">\(A \cap B=\{2,4\}\)</span> is the event of observing either a <span class="math inline">\(2\)</span> or a <span class="math inline">\(4\)</span>. Are the events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> independent?<br />
<span class="math display">\[
P(A B)=\frac{2}{6}=P(A)P(B)=\frac{1}{2}\cdot \frac{2}{3}
\]</span>
and so <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent. In this case, we didn’t assume that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent it just turned out that they were.</p>
<p>Suppose that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are (i.e. <span class="math inline">\(AB=\emptyset\)</span>), each with positive probability (i.e. <span class="math inline">\(P(A)&gt;0\)</span> and <span class="math inline">\(P(B)&gt;0\)</span>). Can they be independent? No! This follows since
<span class="math display">\[
P(A B)=P(\emptyset)=0\neq P(A)P(B)&gt;0.
\]</span>
Except in this special case, there is no way to judge (in-)dependence by looking at the sets in a Venn diagram.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if <span class="math inline">\(P(A B)=P(A) P(B)\)</span>.</li>
<li>Independence is sometimes assumed and sometimes derived.</li>
<li>Disjoint events with strictly positive probabilities are not independent.</li>
</ol>
</div>
<div id="conditional-probability" class="section level3" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Conditional Probability</h3>
If <span class="math inline">\(P(B)&gt;0\)</span> then the of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> is
<span class="math display">\[
P(A \mid B)=\frac{P(A B)}{P(B)}.
\]</span>
Think of <span class="math inline">\(P(A \mid B)\)</span> as the fraction of times <span class="math inline">\(A\)</span> occurs among those in which <span class="math inline">\(B\)</span> occurs. Here are some facts about conditional probabilities:
<p>In general it is also the case that <span class="math inline">\(P(A \mid B)=P(B \mid A)\)</span>. People get this confused all the time. For example, the probability of spots given you have measles is 1 but the probability that you have measles given that you have spots is not <span class="math inline">\(1 .\)</span> In this case, the difference between <span class="math inline">\(P(A \mid B)\)</span> and <span class="math inline">\(P(B \mid A)\)</span> is obvious but there are cases where it is less obvious. This mistake is made often enough in legal cases that it is sometimes called the .</p>
A medical test for a disease <span class="math inline">\(D\)</span> has outcomes <span class="math inline">\(+\)</span> and <span class="math inline">\(-\)</span>. The probabilities are:
<span class="math display">\[
\begin{array}{c|cc|c}
&amp; D &amp; D^{c} \\
\hline
+ &amp; .0081 &amp; .0900 &amp;  .0981\\
- &amp; .0009 &amp; .9010 &amp;  .9019\\
\hline
  &amp; .0090 &amp; .9910 &amp;  1
\end{array}
\]</span>
From the definition of conditional probability, we have that:
<p>Apparently, the test is fairly accurate. Sick people yield a positive test result 90 percent of the time and healthy people yield a negative test result about 90 percent of the time. Suppose you go for a test and get a positive result. What is the probability you have the disease? Most people answer <span class="math inline">\(0.90=90\%\)</span>. The correct answer is <span class="math inline">\(P(D \mid+)=P(+\cap D) / P(+)=0.0081 /(0.0081+0.0900)=0.08\)</span>. The lesson here is that you need to compute the answer numerically. Don’t trust your intuition.</p>
<p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are then
<span class="math display">\[
P(A \mid B)=\frac{P(A B)}{P(B)}=\frac{P(A) P(B)}{P(B)}=P(A)
\]</span>
So another is that knowing <span class="math inline">\(B\)</span> doesn’t change the probability of <span class="math inline">\(A\)</span>.</p>
<p>From the definition of conditional probability we can write
<span class="math display">\[
P(A B)=P(A \mid B) P(B)\quad\text{and also}\quad P(A B)=P(B \mid A) P(A).
\]</span>
Often, these formulas give us a convenient way to compute <span class="math inline">\(P(A B)\)</span> when <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are not independent.</p>
<p>Note, sometimes <span class="math inline">\(P(A B)\)</span> is written as <span class="math inline">\(P(A,B)\)</span>.</p>
<p>Draw two cards from a deck, without replacement. Let <span class="math inline">\(A\)</span> be the event that the first draw is Ace of Clubs and let <span class="math inline">\(B\)</span> be the event that the second draw is Queen of Diamonds. Then <span class="math inline">\(P(A, B)=P(A) P(B \mid A)=(1 / 52) \times(1 / 51)\)</span></p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(P(B)&gt;0\)</span> then <span class="math inline">\(P(A \mid B)=P(A B)/P(B)\)</span></li>
<li><span class="math inline">\(P(\cdot \mid B)\)</span> satisfies the axioms of probability, for fixed <span class="math inline">\(B\)</span>. In general, <span class="math inline">\(P(A \mid \cdot)\)</span> does not satisfy the axioms of probability, for fixed <span class="math inline">\(A\)</span>.</li>
<li>In general, <span class="math inline">\(P(A \mid B) \neq P(B \mid A)\)</span>.</li>
<li><span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if and only if <span class="math inline">\(P(A \mid B)=P(A)\)</span>.</li>
</ol>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ReviewStats.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="random-variables.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Script_Econometrics_MSc.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed",
"download": "https://uni-bonn.sciebo.de/s/IZ2yeLeA4dRTFUY",
"search": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
