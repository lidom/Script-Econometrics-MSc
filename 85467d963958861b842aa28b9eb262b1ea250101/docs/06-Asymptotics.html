<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.36">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Econometrics M.Sc. - 6&nbsp; Large Sample Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./07-Instrumental-Variables.html" rel="next">
<link href="./05-Small-Sample-Inference.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Large Sample Inference</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Econometrics M.Sc.</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Organization of the Course</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-Introduction-to-R.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to R</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-Probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-Multiple-Linear-Regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-Monte-Carlo-Simulations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Monte Carlo Simulations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-Small-Sample-Inference.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Small Sample Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-Asymptotics.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Large Sample Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-Instrumental-Variables.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#tools-for-asymptotic-statistics" id="toc-tools-for-asymptotic-statistics" class="nav-link active" data-scroll-target="#tools-for-asymptotic-statistics"> <span class="header-section-number">6.1</span> Tools for Asymptotic Statistics</a>
  <ul class="collapse">
  <li><a href="#modes-of-convergence" id="toc-modes-of-convergence" class="nav-link" data-scroll-target="#modes-of-convergence"> <span class="header-section-number">6.1.1</span> Modes of Convergence</a></li>
  <li><a href="#four-important-modes-of-convergence" id="toc-four-important-modes-of-convergence" class="nav-link" data-scroll-target="#four-important-modes-of-convergence">Four Important Modes of Convergence</a></li>
  <li><a href="#relations-among-modes-of-convergence" id="toc-relations-among-modes-of-convergence" class="nav-link" data-scroll-target="#relations-among-modes-of-convergence">Relations among Modes of Convergence</a></li>
  <li><a href="#continuous-mapping-theorem-cmt" id="toc-continuous-mapping-theorem-cmt" class="nav-link" data-scroll-target="#continuous-mapping-theorem-cmt"> <span class="header-section-number">6.1.2</span> Continuous Mapping Theorem (CMT)</a></li>
  <li><a href="#slutskys-theorem" id="toc-slutskys-theorem" class="nav-link" data-scroll-target="#slutskys-theorem"> <span class="header-section-number">6.1.3</span> Slutsky’s Theorem</a></li>
  <li><a href="#law-of-large-numbers-and-central-limit-theorems" id="toc-law-of-large-numbers-and-central-limit-theorems" class="nav-link" data-scroll-target="#law-of-large-numbers-and-central-limit-theorems"> <span class="header-section-number">6.1.4</span> Law of Large Numbers and Central Limit Theorems</a></li>
  <li><a href="#estimators-sequences-of-random-variables" id="toc-estimators-sequences-of-random-variables" class="nav-link" data-scroll-target="#estimators-sequences-of-random-variables"> <span class="header-section-number">6.1.5</span> Estimators: Sequences of Random Variables</a></li>
  </ul></li>
  <li><a href="#asymptotics-under-the-classic-regression-model" id="toc-asymptotics-under-the-classic-regression-model" class="nav-link" data-scroll-target="#asymptotics-under-the-classic-regression-model"> <span class="header-section-number">6.2</span> Asymptotics under the Classic Regression Model</a>
  <ul class="collapse">
  <li><a href="#sec-CaseHetero" id="toc-sec-CaseHetero" class="nav-link" data-scroll-target="#sec-CaseHetero"> <span class="header-section-number">6.2.1</span> The Case of Heteroskedasticity</a></li>
  <li><a href="#robust-inference" id="toc-robust-inference" class="nav-link" data-scroll-target="#robust-inference"> <span class="header-section-number">6.2.2</span> Robust Inference</a></li>
  </ul></li>
  <li><a href="#sec-MCLS" id="toc-sec-MCLS" class="nav-link" data-scroll-target="#sec-MCLS"> <span class="header-section-number">6.3</span> Monte Carlo Simulations</a>
  <ul class="collapse">
  <li><a href="#check-distribution-of-hatbeta_n" id="toc-check-distribution-of-hatbeta_n" class="nav-link" data-scroll-target="#check-distribution-of-hatbeta_n"> <span class="header-section-number">6.3.1</span> Check: Distribution of <span class="math inline">\(\hat\beta_n\)</span></a></li>
  <li><a href="#check-testing-multiple-parameters" id="toc-check-testing-multiple-parameters" class="nav-link" data-scroll-target="#check-testing-multiple-parameters"> <span class="header-section-number">6.3.2</span> Check: Testing Multiple Parameters</a></li>
  <li><a href="#check-testing-single-parameters" id="toc-check-testing-single-parameters" class="nav-link" data-scroll-target="#check-testing-single-parameters"> <span class="header-section-number">6.3.3</span> Check: Testing Single Parameters</a></li>
  </ul></li>
  <li><a href="#sec-RDLSInf" id="toc-sec-RDLSInf" class="nav-link" data-scroll-target="#sec-RDLSInf"> <span class="header-section-number">6.4</span> Real Data Example</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-lsinf" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Large Sample Inference</span></span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>The content of this chapter is very much inspired by Chapter 2 of the textbook of <span class="citation" data-cites="Hayashi2000">Hayashi (<a href="#ref-Hayashi2000" role="doc-biblioref">2000</a>)</span>.</p>
<section id="tools-for-asymptotic-statistics" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="tools-for-asymptotic-statistics"><span class="header-section-number">6.1</span> Tools for Asymptotic Statistics</h2>
<p>Basically every modern econometric method is justified using the toolbox of <strong>asymptotic statistics</strong>. The following core concepts from asymptotic statistics will allow us to drop the restrictive normality assumption of <a href="05-Small-Sample-Inference.html"><span>Chapter&nbsp;5</span></a> and to introduce robust standard errors:</p>
<ul>
<li>Concepts on stochastic convergence</li>
<li>Continuous mapping theorem</li>
<li>Slutsky’s theorem</li>
<li>Law of large numbers</li>
<li>Central limit theorems</li>
<li>Cramér-Wold device</li>
</ul>
<section id="modes-of-convergence" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="modes-of-convergence"><span class="header-section-number">6.1.1</span> Modes of Convergence</h3>
<p>In the following we will discuss the four most important convergence concepts for sequences of random variables</p>
<p><span class="math display">\[
\{z_n\}:=(z_1,z_2,\dots,z_n).
\]</span></p>
<p>Non-random scalars (or vectors or matrices) will be denoted by Greek letters such as <span class="math inline">\(\alpha\)</span>.</p>
<!-- Sequences of random vectors (or matrices) will be denoted by $\{\mathbf{z}_n\}$.  -->
<!-- % Vector-Convergence if and only if element-wise converegence:  -->
<!-- %https://www.statlect.com/asymptotic-theory/mean-square-convergence#:~:text=The%20concept%20of%20mean%2Dsquare,difference%20is%20on%20average%20small. -->
</section>
<section id="four-important-modes-of-convergence" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="four-important-modes-of-convergence">Four Important Modes of Convergence</h3>
<p>Probably the most often used mode of convergence is <strong>convergence in probability</strong>. It states that a stochastic sequence <span class="math inline">\(\{z_n\}\)</span> concentrates around its limit <span class="math inline">\(\alpha\)</span> such that deviations <span class="math inline">\(|z_n-\epsilon|\)</span> larger than some small <span class="math inline">\(\epsilon &gt;0\)</span> occur eventually (as <span class="math inline">\(n\to\infty\)</span>) with probability zero. In order to show that some stochastic sequence converges in probability to its limit, one typically uses a <em>“weak law of large numbers”</em>.</p>
<div id="def-conv_prop" class="theorem definition">
<span class="theorem-title"><strong>Definition 6.1 (Convergence in Probability) </strong></span>A sequence of random scalars <span class="math inline">\(\{z_n\}\)</span> <strong>converges in probability</strong> to a constant (non-random) <span class="math inline">\(\alpha\)</span> if, for any (arbitrarily small) <span class="math inline">\(\varepsilon&gt;0\)</span>, <span class="math display">\[\begin{eqnarray*}
  \lim_{n\to\infty} P\left(|z_n-\alpha|&gt;\epsilon\right)=0.
\end{eqnarray*}\]</span> We write:
<center>
<span class="math inline">\(\operatorname{plim}_{n\to\infty}z_n=\alpha\quad\)</span> or shortly <span class="math inline">\(\quad z_n\to_{p}\alpha,\quad\)</span> as <span class="math inline">\(\quad n\to\infty.\)</span>
</center>
<p>Convergence in probability of a sequence of random vectors or matrices <span class="math inline">\(\{z_n\}\)</span> to a constant vector or matrix <span class="math inline">\(\alpha\)</span> requires <em>element-wise</em> convergence in probability.</p>
</div>
<p>A stricter mode of convergence is <strong>almost sure convergence</strong>. Almost sure convergence is (usually) rather hard to derive, since the probability is about an event concerning an infinite sequence. Fortunately, however, there are established <em>“strong laws of large numbers”</em> that we can use to argue that some stochastic sequence converges almost surely to its limit.</p>
<div id="def-conv_as" class="theorem definition">
<span class="theorem-title"><strong>Definition 6.2 (Almost Sure Convergence) </strong></span>A sequence of random scalars <span class="math inline">\(\{z_n\}\)</span> <strong>converges almost surely</strong> to a constant (non-random) <span class="math inline">\(\alpha\)</span> if <span class="math display">\[\begin{eqnarray*}
P\left(\lim_{n\to\infty}z_n=\alpha\right)=1.
\end{eqnarray*}\]</span> We write:
<center>
<span class="math inline">\(z_n\to_{as}\alpha,\quad\)</span> as <span class="math inline">\(\quad n\to\infty.\)</span>
</center>
<p>Almost sure convergence of a sequence of random vectors (or matrices) <span class="math inline">\(\{z_n\}\)</span> to a constant vector (or matrix) <span class="math inline">\(\alpha\)</span> requires <em>element-wise</em> almost sure convergence.</p>
</div>
<p><strong>Convergence in mean square</strong> is typically the most intuitive mode of convergence. If a stochastic sequence converges to a certain limit, then the mean of the stochastic sequence <span class="math inline">\(E(z_n)\)</span> must converge to this limit and the variance of the stochastic sequence must converge to zero. Convergence in mean square is typically easy to show. Moreover, every sequence that converges in mean square, also converges in probability.</p>
<div id="def-conv_ms" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.3 (Convergence in Mean Square) </strong></span>A sequence of random scalars <span class="math inline">\(\{z_n\}\)</span> <strong>converges in mean square</strong> (or <strong>in quadratic mean</strong>) to a constant (non-random) <span class="math inline">\(\alpha\)</span> if</p>
<p><span class="math display">\[
\begin{align*}
  \lim_{n\to\infty}E\left((z_n-\alpha)^2\right)&amp;=0.
\end{align*}
\]</span></p>
<!-- where


$$
\operatorname{MSE}(z_n,\alpha) = E\left((z_n-\alpha)^2\right)=\left(E(z_n)-\alpha\right)^2 + Var(z_n). 
$$ 


-->
We write:
<center>
<span class="math inline">\(z_n\to_{ms}\alpha,\quad\)</span> as <span class="math inline">\(\quad n\to\infty.\)</span>
</center>
<p>Mean square convergence of a sequence of random vectors (or matrices) <span class="math inline">\(\{z_n\}\)</span> to a deterministic vector (or matrix) <span class="math inline">\(\alpha\)</span> requires <em>element-wise</em> mean square convergence.</p>
</div>
<p>Note: If <span class="math inline">\(z_n\)</span> is an estimator (e.g., <span class="math inline">\(z_n=\hat\beta_{k,n}\)</span>), then <span class="math inline">\(E\left((z_n-\alpha)^2\right)\)</span> is called the <strong>mean squared error (MSE)</strong></p>
<p><span class="math display">\[
\begin{align*}
\operatorname{MSE}(z_n,\alpha)
&amp;=E\left((z_n-\alpha)^2\right)\\
&amp;=\left(E(z_n)-\alpha\right)^2 + Var(z_n)\\
&amp;=\left(\operatorname{Bias}(z_n,\alpha)\right)^2 + Var(z_n).
\end{align*}
\]</span></p>
<!-- **Convergence to a Random Variable:**The above presented definitions of convergence can be also applied to limits that are random variables. We say that a sequence of random vectors $\{\mathbf{z}_n\}$ converges to a random vector $\mathbf{z}$ and write $\mathbf{z}_n\to_{p}\mathbf{z}$ if the sequence $\{\mathbf{z}_n-\mathbf{z}\}$ converges to $\mathbf{0}$. Similarly, for $\mathbf{z}_n\to_{as}\mathbf{z}$ and $\mathbf{z}_n\to_{ms}\mathbf{z}$.  -->
<p><strong>Convergence in distribution</strong> is the weakest and at the same time most important mode of convergence.</p>
<div id="def-conv_distr" class="theorem definition">
<span class="theorem-title"><strong>Definition 6.4 (Convergence in Distribution) </strong></span>Let <span class="math inline">\(F_n\)</span> be the cumulative distribution function (cdf) of <span class="math inline">\(z_n\)</span> and <span class="math inline">\(F\)</span> the cdf of <span class="math inline">\(z\)</span>. A sequence of real valued random variables <span class="math inline">\(\{z_n\}\)</span> <strong>converges in distribution</strong> to a real valued random variable <span class="math inline">\(z\)</span> if <span class="math display">\[\begin{eqnarray*}
  \lim_{n\to\infty}F_n(t)=F(t)
\end{eqnarray*}\]</span> for all <span class="math inline">\(t\)</span> at which <span class="math inline">\(F(t)\)</span> is continuous <span class="math inline">\(t.\)</span> We write:
<center>
<span class="math inline">\(z_n\to_{d}z,\quad\)</span> as <span class="math inline">\(\quad n\to\infty,\)</span>
</center>
<p>and we call <span class="math inline">\(F\)</span> the <strong>asymptotic (or limit) distribution</strong> of <span class="math inline">\(z_n\)</span>.</p>
</div>
<p>Remarks on <a href="#def-conv_distr">Definition&nbsp;<span>6.4</span></a>:</p>
<ol type="1">
<li>Often you will see statements like <span class="math inline">\(z_n\to_{d} N(0,1)\)</span> or <span class="math inline">\(z_n\overset{a}{\sim}N(0,1)\)</span>, which should be read as <span class="math inline">\(\lim_{n\to\infty}F_n(t) = \Phi(t)\)</span> for all <span class="math inline">\(t\)</span>, where <span class="math inline">\(\Phi\)</span> denotes the distribution function of the standard normal distribution.</li>
<li>A stochastic sequence <span class="math inline">\(\{z_n\}\)</span> can also convergence in distribution to a <strong>deterministic scalar</strong> <span class="math inline">\(\alpha\)</span>. In this case <span class="math inline">\(\alpha\)</span> is treated as a degenerated random variable with cdf</li>
</ol>
<p><span class="math display">\[
F_\alpha(t)=\left\{\begin{matrix}0&amp;\text{if}\;\;t&lt;\alpha\\ 1&amp;\text{if}\;\;t\geq\alpha.\end{matrix}\right.
\]</span></p>
<p>By contrast to all other modes of convergence, <a href="#def-conv_distr">Definition&nbsp;<span>6.4</span></a> only addresses the univariate case. The reason for this is that <a href="#def-conv_distr">Definition&nbsp;<span>6.4</span></a> cannot simply applied element-wise since this would ignore all interdependencies (i.e.&nbsp;covariances, …) between the random variables in a radom vector. To handle multivariate convergence in distribution, we need the following theorem.</p>
<div id="thm-CramerWold" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.1 (Cramér-Wold) </strong></span>Let <span class="math inline">\(z_n\in\mathbb{R}^K\)</span> and <span class="math inline">\(z\in\mathbb{R}^K\)</span> be <span class="math inline">\(K\)</span>-dimensional random variables (<span class="math inline">\(K\geq 1\)</span>), then</p>
<p><span class="math display">\[
z_n\to_{d} z\text{\quad if and only if \quad}\lambda'z_n\to_{d}\lambda'z
\]</span></p>
<p>for any <span class="math inline">\(\lambda\in\mathbb{R}^K\)</span>.</p>
</div>
<p>A proof of <a href="#thm-CramerWold">Theorem&nbsp;<span>6.1</span></a> can be found, e.g., in <span class="citation" data-cites="Billingsley2008">Billingsley (<a href="#ref-Billingsley2008" role="doc-biblioref">2008</a>)</span> (p.&nbsp;383).</p>
<p>The Cramér-Wold theorem (<a href="#thm-CramerWold">Theorem&nbsp;<span>6.1</span></a>) is needed since element-wise convergence in distribution does generally not imply convergence of the <em>joint</em> distribution of <span class="math inline">\(z_n\)</span> to the <em>joint</em> distribution of <span class="math inline">\(z\)</span>; except, if all elements are independent from each other.</p>
<!-- **Remark:**Note that convergence in distribution, as the name suggests, only involves the distributions of the random variables. Thus, the random variables need not even be defined on the same probability space (that is, they need not be defined for the same random experiment), and indeed we don't even need the random variables at all. (But all this is just thought provoking \dots typically, we'll only consider cases with a common probability space.) -->
</section>
<section id="relations-among-modes-of-convergence" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="relations-among-modes-of-convergence">Relations among Modes of Convergence</h3>
<div id="lem-Relations" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 6.1 (Relationship among the four modes of convergence) </strong></span>The following relationships hold:</p>
<ol type="i">
<li>Mean square convergence implies convergence in probability:</li>
</ol>
<p><span class="math display">\[
z_n\to_{ms}\alpha\quad \Rightarrow\quad  z_n\to_{p}\alpha
\]</span></p>
<ol start="2" type="i">
<li>Almost sure convergence implies convergence in probability:</li>
</ol>
<p><span class="math display">\[
z_n\to_{as}\alpha\quad \Rightarrow\quad  z_n\to_{p}\alpha
\]</span></p>
<ol start="3" type="i">
<li>Convergence in distribution to a constant is equivalent to convergence in probability to the same constant:</li>
</ol>
<p><span class="math display">\[
z_n\to_{d}\alpha\quad \Leftrightarrow\quad z_n\to_{p}\alpha
\]</span></p>
</div>
<p>Proofs of the result in <a href="#lem-Relations">Lemma&nbsp;<span>6.1</span></a> can be found, e.g., here: <a href="https://www.statlect.com/asymptotic-theory/relations-among-modes-of-convergence">https://www.statlect.com/asymptotic-theory/relations-among-modes-of-convergence</a></p>
</section>
<section id="continuous-mapping-theorem-cmt" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="continuous-mapping-theorem-cmt"><span class="header-section-number">6.1.2</span> Continuous Mapping Theorem (CMT)</h3>
<div id="thm-Preserv" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.2 (Preservation of convergence for continuous transformations (or “continuous mapping theorem (CMT)”)) </strong></span>Suppose <span class="math inline">\(\{z_n\}\)</span> is a stochastic sequence of random scalars, vectors, or matrices and that <span class="math inline">\(f(\cdot)\)</span> is a <em>continuous</em> function that does not depended on <span class="math inline">\(n\)</span>. Then</p>
<ol type="i">
<li><span class="math inline">\(z_n\to_{p} \alpha\quad\Rightarrow\quad f(z_n)\to_{p} f(\alpha)\)</span></li>
<li><span class="math inline">\(z_n\to_{as} \alpha\quad\Rightarrow\quad f(z_n)\to_{as} f(\alpha)\)</span></li>
<li><span class="math inline">\(z_n\to_{d} \alpha\quad\Rightarrow\quad f(z_n)\to_{d} f(\alpha)\)</span></li>
</ol>
</div>
<p>Proofs of <a href="#thm-Preserv">Theorem&nbsp;<span>6.2</span></a> can be found, e.g., in <span class="citation" data-cites="Vaart2000">Van der Vaart (<a href="#ref-Vaart2000" role="doc-biblioref">2000</a>)</span> (see Theorem 2.3) or here: <a href="https://www.statlect.com/asymptotic-theory/continuous-mapping-theorem">https://www.statlect.com/asymptotic-theory/continuous-mapping-theorem</a></p>
<p><strong>Note:</strong> The CMT does <em>not</em> hold for m.s.-convergence except for the case where <span class="math inline">\(f(.)\)</span> is a linear function.<br>
<!-- %This is easily seen using the expression %$\E[(z_n-\alpha)^2]=Var(z_n)+(\E(z_n)-\alpha)^2$ and the  --> <!-- %application of Jensen's inequality (see below). --></p>
<p><strong>Examples:</strong> As a consequence of the CMT (<a href="#thm-Preserv">Theorem&nbsp;<span>6.2</span></a>) we have that the usual arithmetic operations preserve convergence in probability (and equivalently for almost sure convergence and convergence in distribution):</p>
<ul>
<li>If <span class="math inline">\(x_n\to_{p} \beta\)</span> and <span class="math inline">\(y_n\to_{p} \gamma\)</span> then <span class="math inline">\(x_n+y_n\to_{p} \beta+\gamma\)</span></li>
<li>If <span class="math inline">\(x_n\to_{p} \beta\)</span> and <span class="math inline">\(y_n\to_{p} \gamma\)</span> then <span class="math inline">\(x_n\cdot y_n\to_{p} \beta\cdot\gamma\)</span></li>
<li>If <span class="math inline">\(x_n\to_{p} \beta\)</span> and <span class="math inline">\(y_n\to_{p} \gamma\)</span> then <span class="math inline">\(x_n/y_n\to_{p} \beta/\gamma\)</span>, provided that <span class="math inline">\(\gamma\neq 0\)</span></li>
<li>If <span class="math inline">\(X_n'X_n\to_{p} \Sigma_{X'X}\)</span> then <span class="math inline">\((X_n'X_n)^{-1}\to_{p} \Sigma_{X'X}^{-1}\)</span>, provided <span class="math inline">\(\Sigma_{X'X}\)</span> is a nonsingular matrix.</li>
</ul>
<!-- The first statement above is immediately seen by setting $\mathbf{z}_n=\left(x_n, y_n\right)'$, $\boldsymbol\alpha=\left(\beta, \gamma\right)'$, and $\mathbf{a}(\boldsymbol\alpha)=(1,1)\boldsymbol\alpha$, similarly for all others.  -->
</section>
<section id="slutskys-theorem" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="slutskys-theorem"><span class="header-section-number">6.1.3</span> Slutsky’s Theorem</h3>
<p>The following results are concerned with combinations of convergence in probability and convergence in distribution. These are particularly important for the derivation of the asymptotic distribution of estimators.</p>
<div id="thm-Slutsky" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.3 (Slutsky’s Theorem) </strong></span>Let <span class="math inline">\(x_n\)</span> and <span class="math inline">\(y_n\)</span> denote sequences of random scalars or vectors and let <span class="math inline">\(A_n\)</span> denote a sequences of random matrices. Moreover, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(A\)</span> are deterministic limits of appropriate dimensions and <span class="math inline">\(x\)</span> is a random limit of appropriate dimension.</p>
<ol type="i">
<li>If <span class="math inline">\(x_n\to_{d} x\quad\)</span> and <span class="math inline">\(\quad y_n\to_{p} \alpha,\quad\)</span> then <span class="math inline">\(\quad x_n+y_n\to_{d} x+\alpha\)</span></li>
<li>If <span class="math inline">\(x_n\to_{d} x\quad\)</span> and <span class="math inline">\(\quad y_n\to_{p} 0,\quad\)</span> then <span class="math inline">\(\quad x_n'y_n\to_{p} 0\)</span></li>
<li>If <span class="math inline">\(x_n\to_{d} x\quad\)</span> and <span class="math inline">\(\quad A_n\to_{p} A,\quad\)</span> then <span class="math inline">\(\quad A_nx_n\to_{d} Ax\)</span>, where it is assumed that <span class="math inline">\(A_n\)</span> and <span class="math inline">\(x_n\)</span> are “conformable” (i.e., the matrix- and vector-dimensions fit to each other).</li>
</ol>
</div>
<p>Proofs of <a href="#thm-Slutsky">Theorem&nbsp;<span>6.3</span></a> can be found, e.g., in <span class="citation" data-cites="Vaart2000">Van der Vaart (<a href="#ref-Vaart2000" role="doc-biblioref">2000</a>)</span> (see Theorem 2.8) or here: <a href="https://www.statlect.com/asymptotic-theory/Slutsky-theorem">https://www.statlect.com/asymptotic-theory/Slutsky-theorem</a></p>
<p><strong>Remark:</strong> Sometimes, only parts i. and ii. of <a href="#thm-Slutsky">Theorem&nbsp;<span>6.3</span></a> are called “Slutsky’s theorem.”</p>
<p>Important special case of <a href="#thm-Slutsky">Theorem&nbsp;<span>6.3</span></a>:</p>
<p>If</p>
<p><span class="math display">\[
x_n\to_{d} N(0,\Sigma)\quad\text{and}\quad A_n\to_{p} A
\]</span></p>
<p>then</p>
<p><span class="math display">\[
A_nx_n\to_{d} N(0,A\Sigma A').
\]</span></p>
</section>
<section id="law-of-large-numbers-and-central-limit-theorems" class="level3" data-number="6.1.4">
<h3 data-number="6.1.4" class="anchored" data-anchor-id="law-of-large-numbers-and-central-limit-theorems"><span class="header-section-number">6.1.4</span> Law of Large Numbers and Central Limit Theorems</h3>
<p>So far, we discussed the definitions of the four most important convergence modes, their relations among each other, and basic theorems about functionals of stochastic sequences (CMT and Slutsky). Though, we still lack of tools that allow us to actually show that a stochastic sequence convergences (in some of the discussed modes) to some limit.</p>
<p>In the following we consider the stochastic sequences</p>
<p><span class="math display">\[
\bar{z}_1,\bar{z}_2,\dots,\bar{z}_n,\quad n \to\infty
\]</span></p>
<p>of sample means</p>
<p><span class="math display">\[
\bar{z}_n:=n^{-1}\sum_{i=1}^nz_i,
\]</span></p>
<p>where <span class="math inline">\(z_i\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>, are (scalar, vector, or matrix-valued) <em>random variables</em>.</p>
<p><strong>Remember:</strong> The sample mean <span class="math inline">\(\bar{z}_n\)</span> is an estimator of the deterministic population mean <span class="math inline">\(\mu\)</span>.</p>
<p>Weak Law of Large Numbers (WLLNs), Strong LLNs (SLLNs), and Central Limit Theorems (CLTs) tell us conditions under which arithmetic means <span class="math inline">\(\bar{z}_n=n^{-1}\sum_{i=1}^nz_i\)</span> converge in probability, almost surely, and in distribution, respectively:</p>
<ul>
<li>Weak LLN: <span class="math inline">\(\bar{z}_n \to_{p}\mu\)</span></li>
<li>Strong LLN: <span class="math inline">\(\bar{z}_n\to_{as}\mu\)</span></li>
<li>CLT: <span class="math inline">\(\sqrt{n}(\bar{z}_n-\mu)\to_{d}N(0,\sigma^2)\)</span></li>
</ul>
<p>In the following we introduce the most well-known versions of a WLLN, SLLN, and a CLT.</p>
<div id="thm-WLLN1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.4 (Weak LLN (by Chebychev)) </strong></span></p>
<p>If <span class="math inline">\(\lim_{n\to\infty} E(\bar{z}_n)=\mu\)</span> and <span class="math inline">\(\lim_{n\to\infty}Var(\bar{z}_n)=0,\)</span> then <span class="math inline">\(\bar{z}_n\to_{p}\mu.\)</span></p>
</div>
<p>A proof of <a href="#thm-WLLN1">Theorem&nbsp;<span>6.4</span></a> can be found, for instance, here: <a href="https://www.statlect.com/asymptotic-theory/law-of-large-numbers">https://www.statlect.com/asymptotic-theory/law-of-large-numbers</a></p>
<div id="thm-SLLN1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.5 (Strong LLN (by Kolmogorov)) </strong></span></p>
<p>If <span class="math inline">\(\{z_i\}\)</span> is an iid sequence with <span class="math inline">\(E(z_i)=\mu,\)</span> then <span class="math inline">\(\bar{z}_n\to_{as}\mu.\)</span></p>
</div>
<p>A proof of <a href="#thm-SLLN1">Theorem&nbsp;<span>6.5</span></a> can be found, e.g., in <em>Linear Statistical Inference and Its Applications</em>, Rao (1973), pp.&nbsp;112-114.</p>
<p><strong>Note:</strong> The WLLN and the SLLN for <strong>random vectors</strong> follow from applying the the theorem separately for each element of the random vectors.</p>
<div id="thm-CLT1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.6 (CLT (Lindeberg-Levy)) </strong></span>If <span class="math inline">\(\{z_i\}\)</span> is an iid sequence and <span class="math inline">\(E(z_i)=\mu\)</span> and <span class="math inline">\(Var(z_i)=\sigma^2\)</span> then</p>
<p><span class="math display">\[
\sqrt{n}(\bar{z}_n-\mu)\to_{d} N(0,\sigma^2)\quad\text{as}\quad n\to\infty
\]</span></p>
</div>
<p>A proof of <a href="#thm-CLT1">Theorem&nbsp;<span>6.6</span></a> can be found, e.g., in <span class="citation" data-cites="Vaart2000">Van der Vaart (<a href="#ref-Vaart2000" role="doc-biblioref">2000</a>)</span> (see Theorem 2.17).</p>
<p>Using the Cramér-Wold device (<a href="#thm-CramerWold">Theorem&nbsp;<span>6.1</span></a>), the Lindeberg-Levy CLT (<a href="#thm-CLT1">Theorem&nbsp;<span>6.6</span></a>) can also be applied to <span class="math inline">\(K\)</span>-dimensional random vectors: <br> To show that <span class="math inline">\(\sqrt{n}(\bar{z}_n-\mu)\)</span> converges to a multivariate (<span class="math inline">\(K&gt;1\)</span> dimensional) normal distribution as <span class="math inline">\(n\to\infty\)</span>, we need to check whether the <strong>uni</strong>variate stochastic sequence <span class="math inline">\(\{\lambda'z_i\}\)</span> is i.i.d. with <span class="math inline">\(E(\lambda'z_i)=\lambda'\mu\)</span> and <span class="math inline">\(Var(\lambda'z_i)=\lambda'\Sigma\lambda\)</span> for any <span class="math inline">\(\lambda\in\mathbb{R}^K\)</span>. This is the case if the multivariate stochastic sequence <span class="math inline">\(\{z_i\}\)</span> is an i.i.d. sequence with <span class="math inline">\(E(_i)=\mu\)</span> and <span class="math inline">\(Var(z_i)=\Sigma\)</span>.</p>
<p><strong>Note:</strong> The LLNs and the CLT are stated with respect to sequences of sample means <span class="math inline">\(\{\bar{z}_n\}\)</span>; i.e., the simplest estimators you probably can think of. We will see, however, that this is all we need in order to analyze also more complicated estimators such as the OLS estimator.</p>
</section>
<section id="estimators-sequences-of-random-variables" class="level3" data-number="6.1.5">
<h3 data-number="6.1.5" class="anchored" data-anchor-id="estimators-sequences-of-random-variables"><span class="header-section-number">6.1.5</span> Estimators: Sequences of Random Variables</h3>
<p>Our concepts above readily apply to univariate or multivariate (<span class="math inline">\(K\)</span>-dimensional) estimators <span class="math inline">\(\hat\theta_n\in\mathbb{R}^K\)</span> computed from random samples with sample size <span class="math inline">\(n\)</span>:<br> An increasing sample size <span class="math inline">\(n\to\infty\)</span> makes an estimator <span class="math inline">\(\hat\theta_n\)</span> nothing but a sequence of random variables converging (hopefully) to the correct limit <span class="math inline">\(\theta\)</span>.</p>
<p>If an estimator <span class="math inline">\(\hat\theta_n\)</span> converges in probability to its limit <span class="math inline">\(\theta\)</span>, we call the estimator <strong>weakly consistent</strong> or simply <strong>consistent</strong>. If it converges almost surely to <span class="math inline">\(\theta,\)</span> we call the estimator <strong>strongly consistent</strong>.</p>
<!-- **Note:**Under the asymptotic perspective ($n\to\infty$) it's not necessary anymore to condition on $X$ since as $n\to\infty$ the stochastic influence of $X$ on  vanishes  -->
<div id="def-WConsist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.5 ((Weak) Consistency) </strong></span>We say that an estimator <span class="math inline">\(\hat\theta_n\)</span> is <em>(weakly) consistent for <span class="math inline">\(\theta\)</span></em> if</p>
<p><span class="math display">\[
\hat\theta_n\to_{p}\theta,\quad\text{as}\quad n\to\infty.
\]</span></p>
</div>
<div id="def-SConsist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.6 (Strong Consistency) </strong></span>We say that an estimator <span class="math inline">\(\hat\theta_n\)</span> is <em>strongly consistent for <span class="math inline">\(\theta\)</span></em> if</p>
<p><span class="math display">\[
\hat\theta_n\to_{as}\theta,\quad\text{as}\quad n\to\infty.
\]</span></p>
</div>
<p>A necessary requirement for weak and strong consistency is that the estimator is asymptotically unbiased.</p>
<div id="def-ABias" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.7 (Asymptotic Bias) </strong></span>The asymptotic bias of an estimator <span class="math inline">\(\hat\theta_n\)</span> of some parameter <span class="math inline">\(\theta\)</span> is defined as</p>
<p><span class="math display">\[
\begin{align*}
\operatorname{ABias}(\hat\theta_n,\theta)
&amp;=\lim_{n\to\infty}\operatorname{Bias}(\hat\theta_n,\theta)\\
&amp;=\lim_{n\to\infty}E(\hat\theta_n)-\theta.
\end{align*}
\]</span></p>
<p>If <span class="math inline">\(\text{ABias}(\hat\theta_n,\theta)=0\)</span>, then <span class="math inline">\(\hat\theta_n\)</span> is called an <strong>asymptotically unbiased</strong>.</p>
</div>
<div id="def-ANorm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.8 (Asymptotic Normality) </strong></span>An estimator <span class="math inline">\(\hat\theta_n\)</span> is asymptotically normal distributed if</p>
<p><span class="math display">\[
\sqrt{n}(\hat\theta_n-\theta)\to_{d} \mathcal{N}(0,\Sigma)\quad\text{as}\quad n\to\infty,
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\Sigma=\lim_{n\to\infty}Var(\sqrt{n}(\hat\theta_n-\theta))=\lim_{n\to\infty}Var(\sqrt{n}\hat\theta_n)
\]</span></p>
<p>is called the asymptotic variance of <span class="math inline">\(\sqrt{n}(\hat\theta_n-\theta)\)</span>.</p>
</div>
<div id="def-RootnConsistent" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.9 (<span class="math inline">\(\sqrt{n}\)</span>-Consistent) </strong></span>Consistent estimators are called <span class="math inline">\(\sqrt{n}\)</span>-consistent if</p>
<p><span class="math display">\[
\sqrt{n}(\hat\theta_n-\theta)\to_{d} z \quad\text{as}\quad n\to\infty,
\]</span></p>
<p>where <span class="math inline">\(z\sim F.\)</span></p>
</div>
</section>
</section>
<section id="asymptotics-under-the-classic-regression-model" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="asymptotics-under-the-classic-regression-model"><span class="header-section-number">6.2</span> Asymptotics under the Classic Regression Model</h2>
Given the above introduced machinery on asymptotic concepts and results, we can now proof that the OLS estimators
<center>
<span class="math inline">\(\hat\beta_n\equiv\hat\beta\quad\)</span> and <span class="math inline">\(\quad s_{UB,n}\equiv s_{UB}^2\)</span>
</center>
<p>applied to the classic regression model (defined by Assumptions 1-4 in <a href="03-Multiple-Linear-Regression.html"><span>Chapter&nbsp;3</span></a> are both consistent, and that <span class="math inline">\(\hat\beta_n\)</span> is asymptotically normal distributed. That is, we can drop the unrealistic normality and spherical errors assumption (Assumption 4<span class="math inline">\(^\ast\)</span>) of <a href="05-Small-Sample-Inference.html"><span>Chapter&nbsp;5</span></a>, but still use our inference tools (<span class="math inline">\(t\)</span>-tests, <span class="math inline">\(F\)</span>-tests); as long as the sample size <span class="math inline">\(n\)</span> is “large.”</p>
<p>For the below results, we need to introduce an asymptotic version of the full rank assumption:</p>
<!-- However, before we can formally state the asymptotic properties, we, first, need to adjust our "data generating process"assumption (Assumption 1) such that we can apply Kolmogorov's strong LLN and Lindeberg-Levy's CLT. Second, we need to adjust the rank assumption (Assumption 3), such that the full column rank of $X$ is guaranteed for the limiting case as $n\to\infty$, too. Assumptions 2 and 4 from @sec-MLR are assumed to hold (and do not need to be changed).  -->
<!-- **Assumption 1$^\ast$: Data Generating Process (for Asymptotics)** Assumption 1 of @sec-MLR applies, but *additionally* we assume that $(\varepsilon_i, X_i)\in\mathbb{R}^{K+1}$ (or equivalently $(Y_i,X_i)\in\mathbb{R}^{K+1}$) is jointly i.i.d. for all $i=1,\dots,n$, with existing and finite second moments for $X_i$ and fourth moments for $\varepsilon_i$.


**Note 1:** The fourth moment of $\varepsilon_i$ is actually only needed for @thm-Consistency_s1; for the rest two moments are sufficient.


**Note 2:** The above adjustment of Assumption 1 is far less restrictive than assuming that the error-terms $\varepsilon_i$ are i.i.d. normally distributed and independent from $X_i$ (as it's necessary for small sample inference in @sec-ssinf).  -->
<p><strong>Assumption 3<span class="math inline">\(^\ast\)</span>: Rank Condition (for Asymptotics)</strong> The <span class="math inline">\((K\times K)\)</span> matrix</p>
<p><span class="math display">\[\Sigma_{X'X}:=E(S_{X'X})=E(n^{-1}X'X)=E(X_iX_i')\]</span></p>
<!-- $$ -->
<!-- n^{-1}X'X=S_{X'X}\to_{p}\Sigma_{X'X}\quad\text{as}\quad n\to\infty -->
<!-- $$ -->
<p>has full rank <span class="math inline">\(K\)</span>. I.e., <span class="math inline">\(\Sigma_{X'X}\)</span> is nonsingular and invertible. <!-- (Note, this assumption does not assume that $S_{X'X}\to_{p}\Sigma_{X'X}$, but if this convergence hold, it assumes that the limiting matrix is of full rank $K$.) --></p>
<!-- **Note:**The crucial part of Assumption 3$^\ast$ is that the limit matrix has full rank $K$. The convergence in probability statement ($S_{X'X}\to_{p}\Sigma_{X'X}$) follows already from Assumption 1$^\ast$.   -->
<!-- **Note:** Assumption 3$^\ast$ implies the existence and finiteness of the first two moments of $X_i$ (even without Assumption 1$^\ast$). -->
<!-- % Under the Assumptions 1$^\ast$, 2, 3$^\ast$, and 4, we can show the following results. -->
<p><!-- 1$^\ast$ and 3$^\ast$  --></p>
<div id="thm-Sxx1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.7 (Consistency of <span class="math inline">\(S_{X'X}^{-1}\)</span>) </strong></span>Under Assumptions 1, 2, 3<span class="math inline">\(^\ast\)</span>, and 4, and under the assumption that <span class="math inline">\(X_i\)</span> has finite second moments, we have that</p>
<p><span class="math display">\[
\left(\frac{1}{n}X'X\right)^{-1}=:S_{X'X}^{-1}\quad\to_{p}\quad\Sigma_{X'X}^{-1}\quad\text{as}\quad n\to\infty.
\]</span></p>
</div>
<p>The proof of <a href="#thm-Sxx1">Theorem&nbsp;<span>6.7</span></a> is done in the lecture.</p>
<!-- Under Assumption 1$^\ast$, 2 and 3$^\ast$ we have that -->
<div id="thm-bconsistent1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.8 (Consistency of <span class="math inline">\(\hat\beta\)</span>) </strong></span>Under Assumptions 1, 2, 3<span class="math inline">\(^\ast\)</span>, and 4 and under the assumption that <span class="math inline">\(X_i\)</span> and <span class="math inline">\(\varepsilon_i\)</span> have finite second moments, we have that</p>
<p><span class="math display">\[
\hat\beta_n\to_{p}\beta\quad\text{as}\quad n\to\infty.
\]</span></p>
</div>
<p>The proof of <a href="#thm-bconsistent1">Theorem&nbsp;<span>6.8</span></a> is done in the lecture.</p>
<p>Moreover, we can show that the appropriately scaled OLS estimator is asymptotically normal distributed. The following theorem is stated for the simpler homoskedastic case, the heteroskedastic case is presented in <a href="#sec-CaseHetero"><span>Section&nbsp;6.2.1</span></a>.</p>
<div id="thm-OLSnormality1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.9 (Sampling error limiting normality (homoskedastic case)) </strong></span>Under Assumption 1, 2, 3<span class="math inline">\(^\ast\)</span>, and 4, and under the assumption that <span class="math inline">\(X_i\)</span> and <span class="math inline">\(\varepsilon_i\)</span> have finite second moments, and under the simplifying assumption of spherical errors <span class="math inline">\((Var(\varepsilon_i|X)=\sigma^2I_n)\)</span>, we have that</p>
<p><span class="math display">\[\sqrt{n}(\hat\beta_n-\beta)\to_{d} \mathcal{N}\left(0,\sigma^2 \Sigma^{-1}_{X'X}\right)\quad\text{as}\quad n\to\infty\]</span></p>
</div>
<p>The proof of <a href="#thm-OLSnormality1">Theorem&nbsp;<span>6.9</span></a> is done in the lecture.</p>
<p>In principle, we can derive the usual test statistics from the latter result. Though, as long as we do not know (we usually don’t) <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\Sigma_{X'X}\)</span> we need to plug-in the (consistent!) estimators <span class="math inline">\(S_{X'X}^{-1}\)</span> and <span class="math inline">\(s_{UB}^2\)</span>, where the consistency of the former estimator is provided by <a href="#thm-Sxx1">Theorem&nbsp;<span>6.7</span></a> and the consistency of <span class="math inline">\(s_{UB}^2\)</span> is provided by the following result.</p>
<div id="thm-Consistency_s1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.10 (Consistency of <span class="math inline">\(s^2_{UB}\)</span>) </strong></span>Under the assumptions of <a href="#thm-OLSnormality1">Theorem&nbsp;<span>6.9</span></a>, but with the additional requirement that <span class="math inline">\(\varepsilon_i\)</span> has finite fourth moments, we have that</p>
<p><span class="math display">\[
s_{UB}^2\to_{p}\sigma^2\quad\text{as}\quad n\to\infty.
\]</span></p>
</div>
<p>The proof of <a href="#thm-Consistency_s1">Theorem&nbsp;<span>6.10</span></a> is skipped, but a detailed proof can be found here: <a href="https://www.statlect.com/fundamentals-of-statistics/OLS-estimator-properties">https://www.statlect.com/fundamentals-of-statistics/OLS-estimator-properties</a></p>
<section id="sec-CaseHetero" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="sec-CaseHetero"><span class="header-section-number">6.2.1</span> The Case of Heteroskedasticity</h3>
<p><a href="#thm-OLSnormality1">Theorem&nbsp;<span>6.9</span></a> can also be stated and proofed for conditionally heteroscedastic error terms. In this case one gets</p>
<p><span id="eq-OLSnormality1Rob"><span class="math display">\[
\sqrt{n}(\hat\beta_n-\beta)\to_{d} \mathcal{N}\left(0,\Sigma_{X'X}^{-1}E(\varepsilon^2_iX_iX_i')\Sigma_{X'X}^{-1}\right)
\tag{6.1}\]</span></span></p>
<p>as <span class="math inline">\(n\to\infty.\)</span></p>
<p>The asymptotic variance</p>
<p><span class="math display">\[
\lim_{n\to\infty}Var(\sqrt{n}(\hat\beta_n-\beta))=\Sigma_{X'X}^{-1}E(\varepsilon_i^2X_iX_i')\Sigma_{X'X}^{-1}
\]</span></p>
<p>is, of course, usually unknown and needs to be estimated from the data by some consistent estimator such that</p>
<p><span class="math display">\[
S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\to_{p} \Sigma_{X'X}^{-1}E(\varepsilon^2_iX_iX_i')\Sigma_{X'X}^{-1}
\]</span></p>
<p>as <span class="math inline">\(n\to\infty.\)</span></p>
<p>The <span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i')\)</span> is here a placeholder for one of the existing Heteroskedasticity Consistent (HC) estimators of <span class="math inline">\(E(\varepsilon^2X_iX_i')\)</span>:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 66%">
</colgroup>
<thead>
<tr class="header">
<th>HC-Type</th>
<th>Formular</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>HC0</td>
<td><span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\hat\varepsilon_i^2X_iX_i'\)</span></td>
</tr>
<tr class="even">
<td>HC1</td>
<td><span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\frac{n}{n-K}\hat\varepsilon_i^2X_iX_i'\)</span></td>
</tr>
<tr class="odd">
<td>HC2</td>
<td><span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{1-h_{i}}X_iX_i'\)</span></td>
</tr>
<tr class="even">
<td>HC3</td>
<td><span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{2}}X_iX_i'\)</span></td>
</tr>
<tr class="odd">
<td>HC4</td>
<td><span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i')=\frac{1}{n}\sum_{i=1}^n\frac{\hat{\varepsilon}_{i}^{2}}{\left(1-h_{i}\right)^{\delta_{i}}}X_iX_i'\)</span></td>
</tr>
</tbody>
</table>
<p>HC3 is the most often used HC-estimator.</p>
<p>The statistic <span class="math inline">\(h_i:=[P_X]_{ii}\)</span> is called the “leverage” of <span class="math inline">\(X_i\)</span>. One can show that <span class="math inline">\(1/n\leq h_i\leq 1\)</span> and that <span class="math inline">\(\bar{h}=n^{-1}\sum_{i=1}^nh_i=K/n\)</span>. Observations <span class="math inline">\(X_i\)</span> with leverage statistics <span class="math inline">\(h_i\)</span> that greatly exceed the average leverage value <span class="math inline">\(K/n\)</span> are referred to as “high leverage” observations. High leverage observations <span class="math inline">\(X_i\)</span> distort the estimation results, <span class="math inline">\(\hat\beta_n\)</span>, if the absolute value of the corresponding residual <span class="math inline">\(|\hat{\varepsilon}_i|\)</span> is unusually large (“influencial outliers”).</p>
<p>The estimator HC0 was suggested in the econometrics literature by <span class="citation" data-cites="White1980">White (<a href="#ref-White1980" role="doc-biblioref">1980</a>)</span> and is justified by asymptotic arguments. The estimators HC1, HC2 and HC3 were suggested by <span class="citation" data-cites="MacKinnon_White_1985">MacKinnon and White (<a href="#ref-MacKinnon_White_1985" role="doc-biblioref">1985</a>)</span> to improve the finite sample performance of HC0. Using an extensive Monte Carlo simulation study comparing HC0-HC3, <span class="citation" data-cites="Long_Ervin_2000">Long and Ervin (<a href="#ref-Long_Ervin_2000" role="doc-biblioref">2000</a>)</span> concludes that HC3 provides the best overall performance in finite samples. <span class="citation" data-cites="Cribari_2004">Cribari-Neto (<a href="#ref-Cribari_2004" role="doc-biblioref">2004</a>)</span> suggested the estimator HC4 to further improve the performance in finite sample behavior, especially in the presence of influential observations (large <span class="math inline">\(h_i\)</span> values).</p>
<p>General idea of the HC1-HC4 estimators is to increase the estimated variance in oder to account for the effects of “influencial outliers”. The residuals <span class="math inline">\(\hat\varepsilon_i\)</span> belonging to <span class="math inline">\(X_i\)</span> values that have a large leverate <span class="math inline">\(h_i\)</span> are multiplied by a factor <span class="math inline">\(\gg 1\)</span> leading to increased <span class="math inline">\(\widehat{E}(\varepsilon^2_iX_iX_i')\)</span> values. This strategy makes inference robust against issues due to heteroskedasticity. <!-- (Too small variance estimates lead to false inference, too large variance estimates lead to conservative inference.) --></p>
</section>
<section id="robust-inference" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="robust-inference"><span class="header-section-number">6.2.2</span> Robust Inference</h3>
<!-- From our asymptotic results under the classic regression model (Assumptions 1$^\ast$, 2, 3$^\ast$, and 4) we get the following results important for testing statistical hypothesis.  -->
<section id="robust-hypothesis-testing-multiple-parameters" class="level4" data-number="6.2.2.1">
<h4 data-number="6.2.2.1" class="anchored" data-anchor-id="robust-hypothesis-testing-multiple-parameters"><span class="header-section-number">6.2.2.1</span> Robust Hypothesis Testing: Multiple Parameters</h4>
<p>Let us reconsider the following system of <span class="math inline">\(q\)</span>-many null hypotheses: <span class="math display">\[\begin{align*}
H_0: \underset{(q\times K)}{R}\underset{(K\times 1)}{\beta} - \underset{(q\times 1)}{r} = \underset{(q\times 1)}{0},
\end{align*}\]</span> where the <span class="math inline">\((q \times K)\)</span> matrix <span class="math inline">\(R\)</span> and the <span class="math inline">\(q\)</span>-vector <span class="math inline">\(r=(r_{1},\dots,r_{q})'\)</span> are chosen by the statistician to specify her/his null hypothesis about the unknown true parameter vector <span class="math inline">\(\beta\)</span>. To make sure that there are no redundant equations, it is required that <span class="math inline">\(\operatorname{rank}(R)=q\)</span>.</p>
<p>By contrast to the multiple parameter tests for small samples (see <a href="05-Small-Sample-Inference.html#sec-testmultp"><span>Section&nbsp;5.1</span></a>), we can work here with a heterosedasticity robust test statistic which is applicable for heteroscedastic error terms:</p>
<p><span id="eq-Ftestasymp"><span class="math display">\[
W=n(R\hat\beta_n -r)'[R\,S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\,R']^{-1}(R\hat\beta_n-r)\overset{H_0}{\to}_d\chi^2(q)
\tag{6.2}\]</span></span></p>
<p>as <span class="math inline">\(n\to\infty\)</span>. The price to pay is that the distribution of the test statistic under the null hypothesis is only valid asymptotically for large <span class="math inline">\(n\)</span>. That is, the critical values taken from the asymptotic distribution will be useful only for “large”samples sizes. In case of homoscedastic error terms, one can substitute <span class="math inline">\(S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\)</span> by <span class="math inline">\(s_{UB}^2S_{X'X}^{-1}\)</span>.</p>
<p><strong>Finite-sample correction:</strong> In order to improve the finite-sample performance of this test, one usually uses the <span class="math inline">\(F_{q,n-K}\)</span> distribution with <span class="math inline">\(q\)</span> and <span class="math inline">\(n-K\)</span> degrees of freedoms instead of the <span class="math inline">\(\chi^2(q)\)</span> distribution. Asymptotically (<span class="math inline">\(n\to\infty\)</span>), <span class="math inline">\(F_{q,n-K}\)</span> is equivalent to <span class="math inline">\(\chi^2(q)\)</span>. However, for finite sample sizes <span class="math inline">\(n\)</span> (i.e., the practically relevant case) <span class="math inline">\(F_{q,n-K}\)</span> leads to larger critical values which helps to account for the estimation errors in <span class="math inline">\(S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\)</span> (or in <span class="math inline">\(s_{UB}^2S_{X'X}^{-1}\)</span>) which are otherwise neglected by the pure asymptotic perspective.</p>
</section>
<section id="robust-hypothesis-testing-single-parameters" class="level4" data-number="6.2.2.2">
<h4 data-number="6.2.2.2" class="anchored" data-anchor-id="robust-hypothesis-testing-single-parameters"><span class="header-section-number">6.2.2.2</span> Robust Hypothesis Testing: Single Parameters</h4>
<p>Let us reconsider the case of hypotheses about only one parameter <span class="math inline">\(\beta_k\)</span>, with <span class="math inline">\(k=1,\dots,K\)</span> <span class="math display">\[\begin{equation*}
\begin{array}{ll}
H_0: &amp; \beta_k=r\\
H_A: &amp; \beta_k\ne r\\
\end{array}
\end{equation*}\]</span> We can selecting the <span class="math inline">\(k\)</span>th diagonal element of the test-statistic in <a href="#eq-Ftestasymp">Equation&nbsp;<span>6.2</span></a> and taking the square root yields</p>
<p><span class="math display">\[
t=\frac{\sqrt{n}\left(\hat{\beta}_k-r\right)}{\sqrt{\left[S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}\right]_{kk}}}\overset{H_0}{\to}_d\mathcal{N}(0,1)
\]</span></p>
<p>as <span class="math inline">\(n\to\infty.\)</span> This <span class="math inline">\(t\)</span> test statistic allows for heteroscedastic error terms. In case of homoscedastic error terms, one can substitute <span class="math inline">\([S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{kk}\)</span> by <span class="math inline">\(s_{UB}^2[S_{X'X}^{-1}]_{kk}\)</span>.</p>
<p><strong>Finite-sample correction:</strong> In order to improve the finite-sample performance of this <span class="math inline">\(t\)</span> test, one usually uses the <span class="math inline">\(t_{(n-K)}\)</span> distribution with <span class="math inline">\(n-K\)</span> degrees of freedoms instead of the <span class="math inline">\(\mathcal{N}(0,1)\)</span> distribution. Asymptotically (<span class="math inline">\(n\to\infty\)</span>), <span class="math inline">\(t_{(n-K)}\)</span> is equivalent to <span class="math inline">\(\mathcal{N}(0,1)\)</span>. However, for finite sample sizes <span class="math inline">\(n\)</span> (i.e., the practically relevant case) <span class="math inline">\(t_{n-K}\)</span> leads to larger critical values which helps to account for the estimation errors in <span class="math inline">\([S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{kk}\)</span> (or in <span class="math inline">\(s_{UB}^2[S_{X'X}^{-1}]_{kk}\)</span>) which are otherwise neglected by the pure asymptotic perspective.</p>
</section>
<section id="robust-confidence-intervals" class="level4" data-number="6.2.2.3">
<h4 data-number="6.2.2.3" class="anchored" data-anchor-id="robust-confidence-intervals"><span class="header-section-number">6.2.2.3</span> Robust Confidence Intervals</h4>
<p>Following the derivations in Chapter <a href="05-Small-Sample-Inference.html#sec-CIsmallsample"><span>Section&nbsp;5.4</span></a>, but using the expression for the robust standard errors, we get the following heteroscedasticity robust (random) <span class="math inline">\((1-\alpha)\cdot 100\%\)</span> confidence interval</p>
<p><span class="math display">\[
\operatorname{CI}_{1-\alpha}=
\left[\hat\beta_k\pm t_{1-\alpha/2,n-K}\sqrt{n^{-1}[S_{X'X}^{-1}\widehat{E}(\varepsilon^2_iX_iX_i')S^{-1}_{X'X}]_{kk}}\right].
\]</span></p>
<p>Here, the coverage probability is an asymptotic coverage probability with <span class="math inline">\(P(\beta_k\in\operatorname{CI}_{1-\alpha})\to 1-\alpha\)</span>, as <span class="math inline">\(n\to\infty\)</span>.</p>
</section>
</section>
</section>
<section id="sec-MCLS" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="sec-MCLS"><span class="header-section-number">6.3</span> Monte Carlo Simulations</h2>
<p>Let’s apply the above asymptotic inference methods using <code>R</code>. As in Chapter <a href="05-Small-Sample-Inference.html#sec-PSSI"><span>Section&nbsp;5.5</span></a> we, first, program a function <code>myDataGenerator()</code> which allows us to generate data from the following model, i.e., from the following fully specified data generating process: <span class="math display">\[\begin{align*}
Y_i &amp;=\beta_1+\beta_2X_{i2}+\beta_3X_{i3}+\varepsilon_i,\qquad i=1,\dots,n\\
\beta &amp;=(\beta_1,\beta_2,\beta_3)'=(2,3,4)'\\
X_{i2}&amp;\sim U[-4,4]\\
X_{i3}&amp;\sim U[-5,5]\\
\varepsilon_i|X_i&amp;\sim U[-0.5 |X_{i2}|, 0.5 |X_{i2}|],
\end{align*}\]</span> where <span class="math inline">\((Y_i,X_i)\)</span> is assumed i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span> with <span class="math inline">\(X_{i2}\)</span> and <span class="math inline">\(X_{i3}\)</span> being independent of each other.</p>
<p>By contrast to Chapter <a href="05-Small-Sample-Inference.html#sec-PSSI"><span>Section&nbsp;5.5</span></a>, the error terms are <strong>heteroskedastic</strong></p>
<p><span class="math display">\[
Var(\varepsilon_i|X_i)=\frac{1}{12}X_{i2}^2
\]</span></p>
<p>and <strong>not Gaussian</strong>.</p>
<blockquote class="blockquote">
<p>As a side note: The unconditional variance follows by the law of total variance and is given by <span class="math display">\[\begin{align*}
Var(\varepsilon_i)
&amp;=E(Var(\varepsilon_i|X_i))+Var(E(\varepsilon_i|X_i))\\
&amp;=E\left(\frac{1}{12}X_{i2}^2\right)+0\\
&amp;=\frac{1}{12}\left(\frac{1}{12}(4-(-4))^2\right)=\frac{4}{9}.
\end{align*}\]</span></p>
</blockquote>
<p>Moreover, the large sample normality result in <a href="#eq-OLSnormality1Rob">Equation&nbsp;<span>6.1</span></a> does not need a conditioning on <span class="math inline">\(X\)</span>. Consequently, the option to condition on <span class="math inline">\(X\)</span> is removed in the following <code>R</code>-function <code>myDataGenerator()</code>.</p>
<div class="cell" data-hash="06-Asymptotics_cache/html/unnamed-chunk-1_e6c6a59031c65ce7ac53fcac0645cc81">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Function to generate artificial data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>myDataGenerator <span class="ot">&lt;-</span> <span class="cf">function</span>(n, beta){</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="do">##</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  X   <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, n), </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>), </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="do">##</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  eps  <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">abs</span>(X[,<span class="dv">2</span>]), <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">abs</span>(X[,<span class="dv">2</span>]))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  Y    <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> eps</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">"Y"</span><span class="ot">=</span>Y, <span class="st">"X_1"</span><span class="ot">=</span>X[,<span class="dv">1</span>], <span class="st">"X_2"</span><span class="ot">=</span>X[,<span class="dv">2</span>], <span class="st">"X_3"</span><span class="ot">=</span>X[,<span class="dv">3</span>])</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="do">##</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(data)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="check-distribution-of-hatbeta_n" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="check-distribution-of-hatbeta_n"><span class="header-section-number">6.3.1</span> Check: Distribution of <span class="math inline">\(\hat\beta_n\)</span></h3>
<p>The above data generating process fulfills our regulatory assumptions of this chapter. So, by theory, the estimators <span class="math inline">\(\hat\beta_k\)</span> should be normal distributed for sufficiently large sample sizes <span class="math inline">\(n\)</span>.</p>
<p><span class="math display">\[
\sqrt{n}\left(\hat\beta_{n,k}-\beta_k\right)\to_d\mathcal{N}\left(0,\left[\Sigma_{X'X}^{-1}E(\varepsilon^2_iX_iX_i')\Sigma_{X'X}^{-1}\right]_{kk}\right)
\]</span></p>
<p>or (slight above of notation):</p>
<p><span class="math display">\[
\hat\beta_{n,k}\to_d\mathcal{N}\left(\beta_k, \;n^{-1}\;\left[\Sigma_{X'X}^{-1}E(\varepsilon^2_iX_iX_i')\Sigma_{X'X}^{-1}\right]_{kk}\right)
\]</span></p>
<p>as <span class="math inline">\(n\to\infty.\)</span></p>
<p>Mathematically, the latter is a bit sloppy since the right hand side of <span class="math inline">\(\to_d\)</span> depends on <span class="math inline">\(n\)</span>, i.e., is not the stable limit object for <span class="math inline">\(n\to\infty\)</span>. However, this sloppiness is nevertheless instructive since it gives us the approximative distribution for given largish sample sizes like <span class="math inline">\(n=100\)</span>.</p>
<div class="cell" data-hash="06-Asymptotics_cache/html/unnamed-chunk-2_953265d857127e8ed3d5d2e8ee69acec">

</div>
<p>For our above specified data generating process, we can derive all (usually unknown) population quantities.</p>
<ul>
<li>From the assumed distributions of <span class="math inline">\(X_{i2}\)</span> and <span class="math inline">\(X_{i3}\)</span> we have that:</li>
</ul>
<p><span class="math display">\[
\Sigma_{X'X}=E(S_{X'X})=E(X_iX_i')
=\left(\begin{matrix}1&amp;0&amp;0\\0&amp;E(X_{i2}^2)&amp;0\\0&amp;0&amp;E(X_{i3}^2)\end{matrix}\right)
=\left(\begin{matrix}1&amp;0&amp;0\\0&amp;\frac{16}{3}&amp;0\\0&amp;0&amp;\frac{25}{3}\end{matrix}\right)
\]</span></p>
<p>The above result follows from observing that <span class="math inline">\(E(X^2)=Var(X)\)</span> if <span class="math inline">\(X\)</span> has mean zero, and that the variance of uniform <span class="math inline">\(U[a,b]\)</span> distributed random variables is given by <span class="math inline">\(\frac{1}{12}(b-a)^2\)</span>.</p>
<ul>
<li>Moreover, <span class="math inline">\(E(\varepsilon^2_iX_iX_i')=E(X_iX_i'E(\varepsilon^2_i|X_i))=E\left(X_iX_i'\left(\frac{1}{12}X_{i2}^2\right)\right)\)</span> such that <span class="math display">\[\begin{align*}
E(\varepsilon^2_iX_iX_i')
&amp;=\left(\begin{matrix}E\left(\frac{1}{12}X_{i2}^2\right)&amp;0&amp;0\\
                 0&amp;E\left(X_{i2}^2\cdot\frac{1}{12}X_{i2}^2\right)&amp;0\\0&amp;0&amp;E\left(X_{i3}^2\cdot\frac{1}{12}X_{i2}^2\right)
    \end{matrix}\right)\\
&amp;=\left(\begin{matrix}\frac{1}{12}E\left(X_{i2}^2\right)&amp;0&amp;0\\
     0&amp;\frac{1}{12}E\left(X_{i2}^4\right)&amp;0\\0&amp;0&amp;\frac{1}{12}E\left(X_{i2}^2\right)\,E\left(X_{i3}^2\right)
\end{matrix}\right)\\      
&amp;=\left(\begin{matrix}\frac{1}{12}\frac{16}{3}&amp;0&amp;0\\
                    0&amp;\frac{1}{12}\frac{256}{5}&amp;0\\
                    0&amp;0&amp;\frac{1}{12}\frac{16}{3}\frac{25}{3}\end{matrix}\right)\\
&amp;=\left(\begin{matrix}\frac{4}{9}&amp;0&amp;0\\0&amp;\frac{64}{15}&amp;0\\0&amp;0&amp;\frac{100}{27}\end{matrix}\right)
\end{align*}\]</span> The above result follow from observing that for <span class="math inline">\(X\sim U[a,b]\)</span> one has <span class="math inline">\(E(X^k)=\frac{b^{k+1}-a^{k+1}}{(k+1)(b-a)}\)</span>, <span class="math inline">\(k=1,2,\dots\)</span>; see, for instance, <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution">Wikipedia</a>.</li>
</ul>
<p>So, for instance, for <span class="math inline">\(\hat{\beta}_2\)</span> we have the following theoretical large sample distribution:</p>
<p><span class="math display">\[
\begin{align}
\hat\beta_{n,2}\to_d&amp;\mathcal{N}\left(\beta_2, \;\frac{1}{n}\;\left[\left(\begin{matrix}1&amp;0&amp;0\\0&amp;\frac{16}{3}&amp;0\\0&amp;0&amp;\frac{25}{3}\end{matrix}\right)^{-1}\left(\begin{matrix}\frac{4}{9}&amp;0&amp;0\\0&amp;\frac{64}{15}&amp;0\\0&amp;0&amp;\frac{100}{27}\end{matrix}\right)\left(\begin{matrix}1&amp;0&amp;0\\0&amp;\frac{16}{3}&amp;0\\0&amp;0&amp;\frac{25}{3}\end{matrix}\right)^{-1}\right]_{22}\right)\\
\hat\beta_{n,2}\to_d&amp;\mathcal{N}\left(\beta_2, \;\frac{1}{n}\;\left[
  \left(
  \begin{matrix}
  0.444 &amp; 0 &amp;0\\
  0 &amp; 0.15 &amp;0\\
  0&amp;0&amp;0.053
  \end{matrix}\right)\right]_{22}\right)\\
\hat\beta_{n,2}\to_d&amp;\mathcal{N}\left(\beta_2, \;\frac{1}{n}\;0.15\right)
\end{align}
\]</span></p>
<p>Let’s use a Monte Carlo simulation to check how well the theoretical large sample (<span class="math inline">\(n\to\infty\)</span>) distribution of <span class="math inline">\(\hat\beta_2\)</span> works as an approximative distribution for a practical largish sample size of <span class="math inline">\(n=100\)</span>.</p>
<div class="cell" data-hash="06-Asymptotics_cache/html/unnamed-chunk-3_508bb2551114ec05a08ae40b1e710634">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>n           <span class="ot">&lt;-</span> <span class="dv">100</span>      <span class="co"># a largish sample size</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>beta_true   <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>) <span class="co"># true data vector</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Mean and variance of the true asymptotic </span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="do">## normal distribution of beta_hat_2:</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># true mean</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>beta_true_2     <span class="ot">&lt;-</span> beta_true[<span class="dv">2</span>] </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># true variance</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>var_true_beta_2 <span class="ot">&lt;-</span> (<span class="fu">solve</span>(<span class="fu">diag</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">16</span><span class="sc">/</span><span class="dv">3</span>, <span class="dv">25</span><span class="sc">/</span><span class="dv">3</span>)))    <span class="sc">%*%</span> </span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>                          <span class="fu">diag</span>(<span class="fu">c</span>(<span class="dv">4</span><span class="sc">/</span><span class="dv">9</span>, <span class="dv">64</span><span class="sc">/</span><span class="dv">15</span>, <span class="dv">100</span><span class="sc">/</span><span class="dv">27</span>))<span class="sc">%*%</span> </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">solve</span>(<span class="fu">diag</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">16</span><span class="sc">/</span><span class="dv">3</span>, <span class="dv">25</span><span class="sc">/</span><span class="dv">3</span>))))[<span class="dv">2</span>,<span class="dv">2</span>]<span class="sc">/</span>n</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Let's generate 5000 realizations from beta_hat_2, and check </span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="do">## whether their distribution is close to the true normal </span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="do">## distribution.</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="do">## (We don't condition on X since the theoretical limit </span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="do">## distribution is unconditional on X)</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>rep        <span class="ot">&lt;-</span> <span class="dv">5000</span> <span class="co"># MC replications</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>beta_hat_2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="at">times=</span>rep)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>rep){</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    MC_data <span class="ot">&lt;-</span> <span class="fu">myDataGenerator</span>(<span class="at">n    =</span> n, </span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>                               <span class="at">beta =</span> beta_true)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    lm_obj        <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X_2 <span class="sc">+</span> X_3, <span class="at">data =</span> MC_data)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    beta_hat_2[r] <span class="ot">&lt;-</span> <span class="fu">coef</span>(lm_obj)[<span class="dv">2</span>]</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="do">## Compare:</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="do">## True beta_2 versus average of beta_hat_2 estimates</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(beta_true_2, <span class="fu">round</span>(<span class="fu">mean</span>(beta_hat_2), <span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.0000 2.9998</code></pre>
</div>
</div>
<p>Good! As expected, the average of the <code>5000</code> simulated realizations of <span class="math inline">\(\hat\beta_2\)</span> is basically equal to the theoretical true mean <span class="math inline">\(E(\hat\beta_2)=\beta_2=3\)</span> which indicates a bias of zero.</p>
<div class="cell" data-hash="06-Asymptotics_cache/html/unnamed-chunk-4_ae0c116d5f465a85afd08d659d7c9fad">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="do">## True variance of beta_hat_2 versus </span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="do">## empirical variance of beta_hat_2 estimates</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">round</span>(var_true_beta_2, <span class="dv">5</span>), <span class="fu">round</span>(<span class="fu">var</span>(beta_hat_2), <span class="dv">5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.00150 0.00147</code></pre>
</div>
</div>
<p>Great! The variance of the <code>5000</code> simulated realizations of <span class="math inline">\(\hat\beta_2\)</span> is basically equal to the theoretical true variance <span class="math inline">\(Var(\hat\beta_{n,2})=0.15/n=0.0015\)</span>.</p>
<div class="cell" data-layout-align="center" data-hash="06-Asymptotics_cache/html/unnamed-chunk-5_5c58151341b199c777a170dcc6c8c10b">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="do">## True normal distribution of beta_hat_2 versus </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="do">## empirical density of beta_hat_2 estimates</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"scales"</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="at">expr =</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> beta_true_2, </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                   <span class="at">sd=</span><span class="fu">sqrt</span>(var_true_beta_2)), </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab=</span><span class="st">""</span>,<span class="at">ylab=</span><span class="st">""</span>, <span class="at">col=</span><span class="fu">gray</span>(.<span class="dv">2</span>), <span class="at">lwd=</span><span class="dv">3</span>, <span class="at">lty=</span><span class="dv">1</span>, </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="at">xlim=</span><span class="fu">range</span>(beta_hat_2),<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">14.1</span>),<span class="at">main=</span><span class="fu">paste0</span>(<span class="st">"n="</span>,n))</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">density</span>(beta_hat_2, <span class="at">bw =</span> <span class="fu">bw.SJ</span>(beta_hat_2)), </span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="fu">alpha</span>(<span class="st">"blue"</span>,.<span class="dv">5</span>), <span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">lwd=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>), </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">col=</span><span class="fu">c</span>(<span class="fu">gray</span>(.<span class="dv">2</span>), <span class="fu">alpha</span>(<span class="st">"blue"</span>,.<span class="dv">5</span>)), <span class="at">bty=</span><span class="st">"n"</span>, <span class="at">legend=</span> </span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">expression</span>(</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Theoretical (Asymptotic) Gaussian Density of"</span><span class="sc">~</span><span class="fu">hat</span>(beta)[<span class="dv">2</span>]), </span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">expression</span>(</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Empirical Density Estimation based on MC realizations from"</span><span class="sc">~</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">hat</span>(beta)[<span class="dv">2</span>])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="06-Asymptotics_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Great! The nonparametric density estimation (estimated via <code>density()</code>) computed from the <code>5000</code> simulated realizations of <span class="math inline">\(\hat\beta_2\)</span> is indicating that <span class="math inline">\(\hat\beta_2\)</span> is really normally distributed as described by our theoretical results.</p>
<p><!-- in @thm-OLSnormality1 (homoscedastic case) and in Equation @eq-OLSnormality1Rob (heterosceda# car::linearHypothesis(model = lm_obj, 
#                       hypothesis.matrix = c("X_2=3", "X_3=4"), 
#                       vcov = vcovHC3_mat)
stic case).  --></p>
<p>However, is the asymptotic distribution of <span class="math inline">\(\hat\beta_2\)</span> also usable for (very) small samples like <span class="math inline">\(n=5\)</span>? Let’s check that:</p>
<div class="cell" data-hash="06-Asymptotics_cache/html/unnamed-chunk-6_6e694432b69e7837de48f6efb8f60a42">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>n           <span class="ot">&lt;-</span> <span class="dv">5</span>       <span class="co"># a small sample size</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>beta_true   <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>) <span class="co"># true data vector</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Mean and variance of the true asymptotic </span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="do">## normal distribution of beta_hat_2:</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># true mean</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>beta_true_2     <span class="ot">&lt;-</span> beta_true[<span class="dv">2</span>] </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># true variance</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>var_true_beta_2 <span class="ot">&lt;-</span> (<span class="fu">solve</span>(<span class="fu">diag</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">16</span><span class="sc">/</span><span class="dv">3</span>, <span class="dv">25</span><span class="sc">/</span><span class="dv">3</span>)))<span class="sc">%*%</span> </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>                          <span class="fu">diag</span>(<span class="fu">c</span>(<span class="dv">4</span><span class="sc">/</span><span class="dv">9</span>, <span class="dv">64</span><span class="sc">/</span><span class="dv">15</span>, <span class="dv">100</span><span class="sc">/</span><span class="dv">27</span>))<span class="sc">%*%</span> </span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">solve</span>(<span class="fu">diag</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">16</span><span class="sc">/</span><span class="dv">3</span>, <span class="dv">25</span><span class="sc">/</span><span class="dv">3</span>))))[<span class="dv">2</span>,<span class="dv">2</span>]<span class="sc">/</span>n</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Let's generate 5000 realizations from beta_hat_2, and check </span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="do">## whether their distribution is close to the true normal </span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="do">## distribution.</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="do">## (We don't condition on X since the theoretical limit </span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="do">## distribution is unconditional on X)</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>rep        <span class="ot">&lt;-</span> <span class="dv">5000</span> <span class="co"># MC replications</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>beta_hat_2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="at">times=</span>rep)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>rep){</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    MC_data <span class="ot">&lt;-</span> <span class="fu">myDataGenerator</span>(<span class="at">n    =</span> n, </span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>                               <span class="at">beta =</span> beta_true)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    lm_obj        <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X_2 <span class="sc">+</span> X_3, <span class="at">data =</span> MC_data)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    beta_hat_2[r] <span class="ot">&lt;-</span> <span class="fu">coef</span>(lm_obj)[<span class="dv">2</span>]</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="do">## Compare:</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="do">## True beta_2 versus average of beta_hat_2 estimates</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(beta_true_2, <span class="fu">round</span>(<span class="fu">mean</span>(beta_hat_2), <span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.0000 2.9963</code></pre>
</div>
</div>
<p>OK, at least on average the <code>5000</code> simulated realizations of <span class="math inline">\(\hat\beta_2\)</span> are basically equal to the true mean <span class="math inline">\(E(\hat\beta_2)=\beta_2=3\)</span>.</p>
<div class="cell" data-hash="06-Asymptotics_cache/html/unnamed-chunk-7_b9a22318c10a899ca92e35959b955a02">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="do">## True variance of beta_hat_2 versus </span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="do">## empirical variance of beta_hat_2 estimates</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">round</span>(var_true_beta_2, <span class="dv">4</span>), <span class="fu">round</span>(<span class="fu">var</span>(beta_hat_2), <span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0300 0.0562</code></pre>
</div>
</div>
<p>Ouch! The theoretical large sample variance <span class="math inline">\(Var(\hat\beta_2)=0.15/n=0.03\)</span> is about 50% smaller than the actual (small sample) variance of <span class="math inline">\(\hat\beta_2\)</span> approximated by the empirical variance of the <code>5000</code> simulated realizations of <span class="math inline">\(\hat\beta_2\)</span>. That is, <strong>we cannot simply use a large sample result in small samples</strong>.</p>
<p><code>{r, fig.align="center", echo=TRUE, fig.wi## Function to generate artificial data myDataGenerator &lt;- function(n, beta){   ##   X   &lt;- cbind(rep(1, n),                   runif(n, -4, 4),                   runif(n, -5, 5))   ##   eps  &lt;- runif(n, - 0.5 * abs(X[,2]), + 0.5 * abs(X[,2]))   Y    &lt;- X %*% beta + eps   data &lt;- data.frame("Y"=Y, "X_1"=X[,1], "X_2"=X[,2], "X_3"=X[,3])   ##   return(data) }dth = 8, fig.height = 5, out.width = "1\\textwidth"} ## True normal distribution of beta_hat_2 versus  ## empirical density of beta_hat_2 estimates library("scales") curve(expr = dnorm(x, mean = beta_true_2,                     sd=sqrt(var_true_beta_2)),        xlab="",ylab="", col=gray(.2), lwd=3, lty=1,  xlim=c(2,4), ylim=c(0,3),main=paste0("n=",n)) lines(density(beta_hat_2, bw = bw.SJ(beta_hat_2)),        col=alpha("blue",.5), lwd=3)## Function to generate artificial data myDataGenerator &lt;- function(n, beta){   ##   X   &lt;- cbind(rep(1, n),                   runif(n, -4, 4),                   runif(n, -5, 5))   ##   eps  &lt;- runif(n, - 0.5 * abs(X[,2]), + 0.5 * abs(X[,2]))   Y    &lt;- X %*% beta + eps   data &lt;- data.frame("Y"=Y, "X_1"=X[,1], "X_2"=X[,2], "X_3"=X[,3])   ##   return(data) }realizations from"~   hat(beta)[2])))</code> Not good. The actual distribution has substantially fatter tails. That is, if we would use the quantiles of the asymptotic distribution, we would falsely reject the null-hypothesis too often (probability of type I errors would be larger than the significance level).</p>
<!-- Fortunately, asymptotics are kicking in pretty fast here.  things become much more reliable already for $n\approx 15$.  -->
</section>
<section id="check-testing-multiple-parameters" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="check-testing-multiple-parameters"><span class="header-section-number">6.3.2</span> Check: Testing Multiple Parameters</h3>
<p>In the following, we do inference about multiple parameters. We test the (here correct) null hypothesis <span class="math display">\[\begin{align*}
H_0:\;&amp;\beta_2=3\quad\text{and}\quad\beta_3=4\\
\text{versus}\quad H_A:\;&amp;\beta_2\neq 3\quad\text{and/or}\quad\beta_3\neq 4.
\end{align*}\]</span> Or equivalently <span class="math display">\[\begin{align*}
H_0:\;&amp;R\beta -r = 0 \\
H_A:\;&amp;R\beta -r \neq 0,
\end{align*}\]</span> where</p>
<p><span class="math display">\[
R=\left(
\begin{matrix}
0&amp;1&amp;0\\
0&amp;0&amp;1\\
\end{matrix}\right)\quad\text{ and }\quad
r=\left(\begin{matrix}3\\5\\\end{matrix}\right).
\]</span></p>
<p>The following <code>R</code> code can be used to test this hypothesis. Note that we use HC3 robust variance estimation <code>sandwich::vcovHC(lm_obj, type="HC3")</code> to take into account that the error terms are heteroscedastic.</p>
<div class="cell" data-hash="06-Asymptotics_cache/html/unnamed-chunk-8_d48fa883e9d29732b9580de1e5358a94">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressMessages</span>(<span class="fu">library</span>(<span class="st">"car"</span>)) <span class="co"># for linearHyothesis()</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ?linearHypothesis</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"sandwich"</span>) <span class="co"># for vcovHC(), robust variance estimations</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1009</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate data</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>MC_data <span class="ot">&lt;-</span> <span class="fu">myDataGenerator</span>(<span class="at">n    =</span> <span class="dv">100</span>, </span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>                           <span class="at">beta =</span> beta_true)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Estimate the linear regression model parameters</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>lm_obj <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X_2 <span class="sc">+</span> X_3, <span class="at">data =</span> MC_data)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>vcovHC3_mat <span class="ot">&lt;-</span> sandwich<span class="sc">::</span><span class="fu">vcovHC</span>(lm_obj, <span class="at">type=</span><span class="st">"HC3"</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Option 1:</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co"># car::linearHypothesis(model = lm_obj, </span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co">#                       hypothesis.matrix = c("X_2=3", "X_3=4"), </span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co">#                       vcov = vcovHC3_mat)</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="do">## Option 2:</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>),</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>           <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">linearHypothesis</span>(<span class="at">model =</span> lm_obj, </span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>                      <span class="at">hypothesis.matrix =</span> R, </span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>                      <span class="at">rhs  =</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">4</span>),</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>                      <span class="at">vcov =</span> vcovHC3_mat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear hypothesis test

Hypothesis:
X_2 = 3
X_3 = 4

Model 1: restricted model
Model 2: Y ~ X_2 + X_3

Note: Coefficient covariance matrix supplied.

  Res.Df Df     F Pr(&gt;F)
1     99                
2     97  2 0.032 0.9685</code></pre>
</div>
</div>
<p>The large <span class="math inline">\(p\)</span>-value does not allow us to reject the null-hypothesis at any of the usual significance levels. This is good, since we test here a correct null-hypothesis and rejecting it would mean do do a false rejection (type I error, false positive).</p>
<p>On average, however, there will be some false rejections of the null-hypothesis, but this type I error rate needs to be samler or equal to <span class="math inline">\(\alpha\)</span>.</p>
<div class="cell" data-hash="06-Asymptotics_cache/html/unnamed-chunk-9_8da9c8fdcd8e70d23a0d96ab69948606">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1009</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>B        <span class="ot">&lt;-</span> <span class="dv">10000</span> </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>p_values <span class="ot">&lt;-</span> <span class="fu">numeric</span>(B)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B){ </span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  MC_data     <span class="ot">&lt;-</span> <span class="fu">myDataGenerator</span>(<span class="at">n    =</span> <span class="dv">100</span>, <span class="at">beta =</span> beta_true)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  lm_obj      <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X_2 <span class="sc">+</span> X_3, <span class="at">data =</span> MC_data)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  vcovHC3_mat <span class="ot">&lt;-</span> sandwich<span class="sc">::</span><span class="fu">vcovHC</span>(lm_obj, <span class="at">type=</span><span class="st">"HC3"</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  Ftest       <span class="ot">&lt;-</span> car<span class="sc">::</span><span class="fu">linearHypothesis</span>(<span class="at">model =</span> lm_obj, </span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>                        <span class="at">hypothesis.matrix =</span> <span class="fu">c</span>(<span class="st">"X_2=3"</span>, <span class="st">"X_3=4"</span>), </span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>                        <span class="at">vcov =</span> vcovHC3_mat)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  p_values[r] <span class="ot">&lt;-</span> Ftest<span class="sc">$</span><span class="st">'Pr(&gt;F)'</span>[<span class="dv">2</span>]                      </span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Nominal type I error rate </span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span> </span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Empirical type I error rate</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(p_values[p_values <span class="sc">&lt;</span> alpha])<span class="sc">/</span>B</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0595</code></pre>
</div>
</div>
<p>This is more or less ok, but you see that the upper bound is not so strict here as it was in the small sample inference case in <a href="05-Small-Sample-Inference.html#sec-PSSI"><span>Section&nbsp;5.5</span></a>.</p>
</section>
<section id="check-testing-single-parameters" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="check-testing-single-parameters"><span class="header-section-number">6.3.3</span> Check: Testing Single Parameters</h3>
<p>Next, we do inference about a single parameter. We test <span class="math display">\[\begin{align*}
H_0:&amp;\beta_3=5\\
\text{versus}\quad H_A:&amp;\beta_3\neq 5.
\end{align*}\]</span></p>
<div class="cell" data-hash="06-Asymptotics_cache/html/unnamed-chunk-10_dab5651cf1e02d068cb79cce4080f3b6">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load libraries</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressMessages</span>(<span class="fu">library</span>(<span class="st">"lmtest"</span>))  <span class="co"># for coeftest()</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate data</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>MC_data <span class="ot">&lt;-</span> <span class="fu">myDataGenerator</span>(<span class="at">n    =</span> n, </span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>                           <span class="at">beta =</span> beta_true)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Estimate the linear regression model parameters</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>lm_obj <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X_2 <span class="sc">+</span> X_3, <span class="at">data =</span> MC_data)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Robust t test</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Robust standard error for \hat{\beta}_3:</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>SE_rob <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">vcovHC</span>(lm_obj, <span class="at">type =</span> <span class="st">"HC3"</span>)[<span class="dv">3</span>,<span class="dv">3</span>])</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="do">## hypothetical (H0) value of \beta_3:</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>beta_3_H0 <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="do">## estimate for beta_3:</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>beta_3_hat <span class="ot">&lt;-</span> <span class="fu">coef</span>(lm_obj)[<span class="dv">3</span>]</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a><span class="do">## robust t-test statistic</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>t_test_stat <span class="ot">&lt;-</span> (beta_3_hat <span class="sc">-</span> beta_3_H0)<span class="sc">/</span>SE_rob</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="do">## p-value</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">coef</span>(lm_obj))</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>p_value <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">min</span>(   <span class="fu">pt</span>(<span class="at">q =</span> t_test_stat, <span class="at">df =</span> n <span class="sc">-</span> K), </span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>                   <span class="dv">1</span><span class="sc">-</span> <span class="fu">pt</span>(<span class="at">q =</span> t_test_stat, <span class="at">df =</span> n <span class="sc">-</span> K))</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>p_value</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2504485</code></pre>
</div>
</div>
<p>Agian, the large <span class="math inline">\(p\)</span>-value does not allow us to reject the (here true) null-hypothesis at any of the usual significance levels. Let us check the empirical type I error rate.</p>
<div class="cell" data-hash="06-Asymptotics_cache/html/unnamed-chunk-11_5c1840a3365da4171fcec50feffc109d">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1009</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>n        <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>B        <span class="ot">&lt;-</span> <span class="dv">10000</span> </span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>p_values <span class="ot">&lt;-</span> <span class="fu">numeric</span>(B)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B){ </span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>  MC_data     <span class="ot">&lt;-</span> <span class="fu">myDataGenerator</span>(<span class="at">n =</span> n, <span class="at">beta =</span> beta_true)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  lm_obj      <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X_2 <span class="sc">+</span> X_3, <span class="at">data =</span> MC_data)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>  SE_rob      <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">vcovHC</span>(lm_obj, <span class="at">type =</span> <span class="st">"HC3"</span>)[<span class="dv">3</span>,<span class="dv">3</span>])</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>  beta_3_hat  <span class="ot">&lt;-</span> <span class="fu">coef</span>(lm_obj)[<span class="dv">3</span>]</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>  <span class="do">## robust t-test statistic</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>  t_test_stat <span class="ot">&lt;-</span> (beta_3_hat <span class="sc">-</span> beta_3_H0)<span class="sc">/</span>SE_rob</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>  p_values[r] <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">min</span>(   <span class="fu">pt</span>(<span class="at">q =</span> t_test_stat, <span class="at">df =</span> n <span class="sc">-</span> <span class="fu">length</span>(beta_true)), </span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>                         <span class="dv">1</span><span class="sc">-</span> <span class="fu">pt</span>(<span class="at">q =</span> t_test_stat, <span class="at">df =</span> n <span class="sc">-</span> <span class="fu">length</span>(beta_true)))</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Nominal type I error rate </span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span> </span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Empirical type I error rate</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(p_values[p_values <span class="sc">&lt;</span> alpha])<span class="sc">/</span>B</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0496</code></pre>
</div>
</div>
</section>
</section>
<section id="sec-RDLSInf" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="sec-RDLSInf"><span class="header-section-number">6.4</span> Real Data Example</h2>
<p>In the following, we revisit the read data study from <a href="05-Small-Sample-Inference.html#sec-RDSSInf"><span>Section&nbsp;5.6</span></a>, but allow consider the case of heteroskedastic, non-Gaussian errors.</p>
<div class="cell" data-hash="06-Asymptotics_cache/html/unnamed-chunk-12_5e9b1898e83ae45d5d2e932eb085b8c2">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="do">## The AER package contains a lot of datasets </span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(AER))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Attach the DoctorVisits data to make it usable</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"DoctorVisits"</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>lm_obj <span class="ot">&lt;-</span> <span class="fu">lm</span>(visits <span class="sc">~</span> gender <span class="sc">+</span> age <span class="sc">+</span> income, <span class="at">data =</span> DoctorVisits)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The above <code>R</code> codes re-estimate the following regression model</p>
<p><span class="math display">\[
Y_i = \beta_1 + \beta_{gender} X_{gender,i}
              + \beta_{age} X_{age,i}
              + \beta_{income} X_{income,i} + \varepsilon_i,
\]</span></p>
<p>where <span class="math inline">\(i=1,\dots,n\)</span> and</p>
<ul>
<li><span class="math inline">\(X_{gender,i}=1\)</span> if the <span class="math inline">\(i\)</span>th subject is a woman and <span class="math inline">\(X_{gender,i}=0\)</span> if the <span class="math inline">\(i\)</span>th subject is a man</li>
<li><span class="math inline">\(X_{age,i}\)</span> is the age of subject <span class="math inline">\(i\)</span> measured in years divided by <span class="math inline">\(100\)</span></li>
<li><span class="math inline">\(X_{income,i}\)</span> is the annual income of subject <span class="math inline">\(i\)</span> in tens of thousands of dollars</li>
</ul>
<p>Now, to do heteroskedasticity consistent robust inference, one can use the <code>vcocHV()</code> frunction from the <code>R</code> package <code>sandwich</code> together with the <code>coeftest()</code> function from the <code>R</code> package <code>lmtest</code>.</p>
<div class="cell" data-hash="06-Asymptotics_cache/html/unnamed-chunk-13_bdb0e4e97063ed6260c1b10ad9f85789">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="do">## No robust standard errors:</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(lm_obj)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="do">## HC3 robust standard errors:</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="fu">coeftest</span>(lm_obj, <span class="at">vcov =</span> <span class="fu">vcovHC</span>(lm_obj, <span class="at">type =</span> <span class="st">"HC3"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can do this also using the <code>R</code> package <code>stargazer</code> which gives you puplication ready regression output tables:</p>
<div class="cell" data-hash="06-Asymptotics_cache/html/unnamed-chunk-14_224b054b6129abe9f017d32a75e9ca92">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Hint: use type = "latex" </span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="do">## to produce a latex table</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(<span class="st">"stargazer"</span>))</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust standard errors</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>cov_beta_HC3   <span class="ot">&lt;-</span> <span class="fu">vcovHC</span>(lm_obj, <span class="at">type =</span> <span class="st">"HC3"</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>robust_HC3_se  <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(cov_beta_HC3))</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Stargazer output (with and without RSE)</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="fu">stargazer</span>(lm_obj, lm_obj, </span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>          <span class="at">se   =</span> <span class="fu">list</span>(<span class="cn">NULL</span>, robust_HC3_se),</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>          <span class="at">type =</span> <span class="st">"html"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

<table style="text-align:center">
<tbody><tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td colspan="2">
<em>Dependent variable:</em>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td colspan="2">
visits
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(1)
</td>
<td>
(2)
</td>
</tr>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
genderfemale
</td>
<td>
0.062<sup>***</sup>
</td>
<td>
0.062<sup>**</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.023)
</td>
<td>
(0.025)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
age
</td>
<td>
0.402<sup>***</sup>
</td>
<td>
0.402<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.057)
</td>
<td>
(0.060)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
income
</td>
<td>
-0.082<sup>***</sup>
</td>
<td>
-0.082<sup>**</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.032)
</td>
<td>
(0.032)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Constant
</td>
<td>
0.154<sup>***</sup>
</td>
<td>
0.154<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.036)
</td>
<td>
(0.034)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
Observations
</td>
<td>
5,190
</td>
<td>
5,190
</td>
</tr>
<tr>
<td style="text-align:left">
R<sup>2</sup>
</td>
<td>
0.019
</td>
<td>
0.019
</td>
</tr>
<tr>
<td style="text-align:left">
Adjusted R<sup>2</sup>
</td>
<td>
0.018
</td>
<td>
0.018
</td>
</tr>
<tr>
<td style="text-align:left">
Residual Std. Error (df = 5186)
</td>
<td>
0.791
</td>
<td>
0.791
</td>
</tr>
<tr>
<td style="text-align:left">
F Statistic (df = 3; 5186)
</td>
<td>
33.218<sup>***</sup>
</td>
<td>
33.218<sup>***</sup>
</td>
</tr>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
<em>Note:</em>
</td>
<td colspan="2" style="text-align:right">
<sup><em></em></sup><em>p&lt;0.1; <sup><strong></strong></sup><strong>p&lt;0.05; <sup></sup></strong></em>p&lt;0.01
</td>
</tr>

</tbody></table>
</div>
<!-- 
### Variance Inflation

https://online.stat.psu.edu/stat462/node/180/
 -->
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Billingsley2008" class="csl-entry" role="doc-biblioentry">
Billingsley, Patrick. 2008. <em>Probability and Measure</em>. John Wiley &amp; Sons.
</div>
<div id="ref-Cribari_2004" class="csl-entry" role="doc-biblioentry">
Cribari-Neto, Francisco. 2004. <span>“Asymptotic Inference Under Heteroskedasticity of Unknown Form.”</span> <em>Computational Statistics &amp; Data Analysis</em> 45 (2): 215–33.
</div>
<div id="ref-Hayashi2000" class="csl-entry" role="doc-biblioentry">
Hayashi, Fumio. 2000. <em>Econometrics</em>. Princeton University Press.
</div>
<div id="ref-Long_Ervin_2000" class="csl-entry" role="doc-biblioentry">
Long, J Scott, and Laurie H Ervin. 2000. <span>“Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model.”</span> <em>The American Statistician</em> 54 (3): 217–24.
</div>
<div id="ref-MacKinnon_White_1985" class="csl-entry" role="doc-biblioentry">
MacKinnon, James G, and Halbert White. 1985. <span>“Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties.”</span> <em>Journal of Econometrics</em> 29 (3): 305–25.
</div>
<div id="ref-Vaart2000" class="csl-entry" role="doc-biblioentry">
Van der Vaart, Aad W. 2000. <em>Asymptotic Statistics</em>. Vol. 3. Cambridge university press.
</div>
<div id="ref-White1980" class="csl-entry" role="doc-biblioentry">
White, Halbert. 1980. <span>“A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.”</span> <em>Econometrica</em>, 817–38.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./05-Small-Sample-Inference.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Small Sample Inference</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./07-Instrumental-Variables.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>