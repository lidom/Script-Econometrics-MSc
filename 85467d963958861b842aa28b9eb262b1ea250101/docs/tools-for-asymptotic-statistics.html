<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.1 Tools for Asymptotic Statistics | Econometrics (M.Sc.)</title>
  <meta name="description" content="5.1 Tools for Asymptotic Statistics | Econometrics (M.Sc.)" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="5.1 Tools for Asymptotic Statistics | Econometrics (M.Sc.)" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/mylogo.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.1 Tools for Asymptotic Statistics | Econometrics (M.Sc.)" />
  
  
  <meta name="twitter:image" content="/images/mylogo.png" />

<meta name="author" content="Prof. Dr. Dominik Liebl" />


<meta name="date" content="2022-09-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch:LSINF.html"/>
<link rel="next" href="asymptotics-under-the-classic-regression-model.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="index.html#preface" id="toc-preface">Preface</a>
<ul>
<li><a href="index.html#organization-of-the-course" id="toc-organization-of-the-course">Organization of the Course</a></li>
<li><a href="index.html#literature" id="toc-literature">Literature</a></li>
</ul></li>
<li><a href="introduction-to-r.html#introduction-to-r" id="toc-introduction-to-r"><span class="toc-section-number">1</span> Introduction to R</a>
<ul>
<li><a href="short-glossary.html#short-glossary" id="toc-short-glossary"><span class="toc-section-number">1.1</span> Short Glossary</a></li>
<li><a href="first-steps.html#first-steps" id="toc-first-steps"><span class="toc-section-number">1.2</span> First Steps</a></li>
<li><a href="further-data-objects.html#further-data-objects" id="toc-further-data-objects"><span class="toc-section-number">1.3</span> Further Data Objects</a></li>
<li><a href="simple-regression-analysis-using-r.html#simple-regression-analysis-using-r" id="toc-simple-regression-analysis-using-r"><span class="toc-section-number">1.4</span> Simple Regression Analysis using R</a></li>
<li><a href="programming-in-r.html#programming-in-r" id="toc-programming-in-r"><span class="toc-section-number">1.5</span> Programming in R</a></li>
<li><a href="r-packages.html#r-packages" id="toc-r-packages"><span class="toc-section-number">1.6</span> R-packages</a></li>
<li><a href="tidyverse.html#tidyverse" id="toc-tidyverse"><span class="toc-section-number">1.7</span> Tidyverse</a>
<ul>
<li><a href="tidyverse.html#tidyverse-plotting-basics" id="toc-tidyverse-plotting-basics"><span class="toc-section-number">1.7.1</span> Tidyverse: Plotting Basics</a></li>
<li><a href="tidyverse.html#tidyverse-data-wrangling-basics" id="toc-tidyverse-data-wrangling-basics"><span class="toc-section-number">1.7.2</span> Tidyverse: Data Wrangling Basics</a></li>
<li><a href="tidyverse.html#the-pipe-operator" id="toc-the-pipe-operator"><span class="toc-section-number">1.7.3</span> The pipe operator <code>%&gt;%</code></a></li>
<li><a href="tidyverse.html#the-group_by-function" id="toc-the-group_by-function"><span class="toc-section-number">1.7.4</span> The <code>group_by()</code> function</a></li>
</ul></li>
<li><a href="further-links.html#further-links" id="toc-further-links"><span class="toc-section-number">1.8</span> Further Links</a>
<ul>
<li><a href="further-links.html#further-r-intros" id="toc-further-r-intros"><span class="toc-section-number">1.8.1</span> Further R-Intros</a></li>
<li><a href="further-links.html#version-control-gitgithub" id="toc-version-control-gitgithub"><span class="toc-section-number">1.8.2</span> Version Control (Git/GitHub)</a></li>
<li><a href="further-links.html#r-ladies" id="toc-r-ladies"><span class="toc-section-number">1.8.3</span> R-Ladies</a></li>
</ul></li>
</ul></li>
<li><a href="ReviewStats.html#ReviewStats" id="toc-ReviewStats"><span class="toc-section-number">2</span> Review: Probability and Statistics</a>
<ul>
<li><a href="probability-theory.html#probability-theory" id="toc-probability-theory"><span class="toc-section-number">2.1</span> Probability Theory</a>
<ul>
<li><a href="probability-theory.html#sample-spaces-and-elementary-events" id="toc-sample-spaces-and-elementary-events"><span class="toc-section-number">2.1.1</span> Sample Spaces and (Elementary) Events</a></li>
<li><a href="probability-theory.html#probability" id="toc-probability"><span class="toc-section-number">2.1.2</span> Probability</a></li>
<li><a href="probability-theory.html#independent-events" id="toc-independent-events"><span class="toc-section-number">2.1.3</span> Independent Events</a></li>
<li><a href="probability-theory.html#conditional-probability" id="toc-conditional-probability"><span class="toc-section-number">2.1.4</span> Conditional Probability</a></li>
</ul></li>
<li><a href="random-variables.html#random-variables" id="toc-random-variables"><span class="toc-section-number">2.2</span> Random Variables</a>
<ul>
<li><a href="random-variables.html#univariate-distribution-and-probability-functions" id="toc-univariate-distribution-and-probability-functions"><span class="toc-section-number">2.2.1</span> Univariate Distribution and Probability Functions</a></li>
<li><a href="random-variables.html#multivariate-distribution-and-probability-functions" id="toc-multivariate-distribution-and-probability-functions"><span class="toc-section-number">2.2.2</span> Multivariate Distribution and Probability Functions</a></li>
<li><a href="random-variables.html#means-and-moments" id="toc-means-and-moments"><span class="toc-section-number">2.2.3</span> Means and Moments</a></li>
<li><a href="random-variables.html#unconditional-means" id="toc-unconditional-means"><span class="toc-section-number">2.2.4</span> Unconditional Means</a></li>
<li><a href="random-variables.html#conditional-means" id="toc-conditional-means"><span class="toc-section-number">2.2.5</span> Conditional Means</a></li>
<li><a href="random-variables.html#means-of-transformed-random-variables-and-moments" id="toc-means-of-transformed-random-variables-and-moments"><span class="toc-section-number">2.2.6</span> Means of Transformed Random Variables and Moments</a></li>
<li><a href="random-variables.html#independent-random-variables" id="toc-independent-random-variables"><span class="toc-section-number">2.2.7</span> Independent Random Variables</a></li>
<li><a href="random-variables.html#i.i.d.-samples" id="toc-i.i.d.-samples"><span class="toc-section-number">2.2.8</span> I.I.D. Samples</a></li>
<li><a href="random-variables.html#some-important-discrete-random-variables" id="toc-some-important-discrete-random-variables"><span class="toc-section-number">2.2.9</span> Some Important Discrete Random Variables</a></li>
<li><a href="random-variables.html#some-important-continuous-random-variables" id="toc-some-important-continuous-random-variables"><span class="toc-section-number">2.2.10</span> Some Important Continuous Random Variables</a></li>
</ul></li>
</ul></li>
<li><a href="ch:MLR.html#ch:MLR" id="toc-ch:MLR"><span class="toc-section-number">3</span> Multiple Linear Regression</a>
<ul>
<li><a href="assumptions.html#assumptions" id="toc-assumptions"><span class="toc-section-number">3.1</span> Assumptions</a>
<ul>
<li><a href="assumptions.html#some-implications-of-the-exogeneity-assumption" id="toc-some-implications-of-the-exogeneity-assumption"><span class="toc-section-number">3.1.1</span> Some Implications of the Exogeneity Assumption</a></li>
</ul></li>
<li><a href="deriving-the-expression-of-the-ols-estimator.html#deriving-the-expression-of-the-ols-estimator" id="toc-deriving-the-expression-of-the-ols-estimator"><span class="toc-section-number">3.2</span> Deriving the Expression of the OLS Estimator</a></li>
<li><a href="some-quantities-of-interest.html#some-quantities-of-interest" id="toc-some-quantities-of-interest"><span class="toc-section-number">3.3</span> Some Quantities of Interest</a></li>
<li><a href="method-of-moments-estimator.html#method-of-moments-estimator" id="toc-method-of-moments-estimator"><span class="toc-section-number">3.4</span> Method of Moments Estimator</a></li>
<li><a href="unbiasedness-of-hatbetax-and-hatbeta.html#unbiasedness-of-hatbetax-and-hatbeta" id="toc-unbiasedness-of-hatbetax-and-hatbeta"><span class="toc-section-number">3.5</span> Unbiasedness of <span class="math inline">\(\hat\beta|X\)</span> and <span class="math inline">\(\hat\beta\)</span></a></li>
<li><a href="ch:VarEstBeta.html#ch:VarEstBeta" id="toc-ch:VarEstBeta"><span class="toc-section-number">3.6</span> Variance and Standard Error of <span class="math inline">\(\hat\beta|X\)</span></a></li>
<li><a href="the-gauss-markov-theorem.html#the-gauss-markov-theorem" id="toc-the-gauss-markov-theorem"><span class="toc-section-number">3.7</span> The Gauss-Markov Theorem</a></li>
<li><a href="practice-real-data.html#practice-real-data" id="toc-practice-real-data"><span class="toc-section-number">3.8</span> Practice: Real Data</a>
<ul>
<li><a href="practice-real-data.html#dummy-variables-contrast-codings-and-interactions" id="toc-dummy-variables-contrast-codings-and-interactions">Dummy Variables, Contrast Codings, and Interactions</a></li>
<li><a href="practice-real-data.html#the-function" id="toc-the-function">The Function </a></li>
</ul></li>
<li><a href="practice-simulation.html#practice-simulation" id="toc-practice-simulation"><span class="toc-section-number">3.9</span> Practice: Simulation</a>
<ul>
<li><a href="practice-simulation.html#behavior-of-the-ols-estimates-for-resampled-data-conditionally-on-x_i" id="toc-behavior-of-the-ols-estimates-for-resampled-data-conditionally-on-x_i"><span class="toc-section-number">3.9.1</span> Behavior of the OLS Estimates for Resampled Data (conditionally on <span class="math inline">\(X_i\)</span>)</a></li>
</ul></li>
</ul></li>
<li><a href="ch:SSINF.html#ch:SSINF" id="toc-ch:SSINF"><span class="toc-section-number">4</span> Small Sample Inference</a>
<ul>
<li><a href="ch:testmultp.html#ch:testmultp" id="toc-ch:testmultp"><span class="toc-section-number">4.1</span> Hypothesis Tests about Multiple Parameters</a>
<ul>
<li><a href="ch:testmultp.html#the-test-statistic-and-its-null-distribution" id="toc-the-test-statistic-and-its-null-distribution"><span class="toc-section-number">4.1.1</span> The Test Statistic and its Null Distribution</a></li>
</ul></li>
<li><a href="ch:testingsinglep.html#ch:testingsinglep" id="toc-ch:testingsinglep"><span class="toc-section-number">4.2</span> Tests about One Parameter</a></li>
<li><a href="testtheory.html#testtheory" id="toc-testtheory"><span class="toc-section-number">4.3</span> Testtheory</a>
<ul>
<li><a href="testtheory.html#significance-level" id="toc-significance-level"><span class="toc-section-number">4.3.1</span> Significance Level</a></li>
<li><a href="testtheory.html#critical-value-for-the-f-test" id="toc-critical-value-for-the-f-test"><span class="toc-section-number">4.3.2</span> Critical Value for the <span class="math inline">\(F\)</span>-Test</a></li>
<li><a href="testtheory.html#critical-values-for-the-t-test" id="toc-critical-values-for-the-t-test"><span class="toc-section-number">4.3.3</span> Critical Value(s) for the <span class="math inline">\(t\)</span>-Test</a></li>
</ul></li>
<li><a href="type-ii-error-and-power.html#type-ii-error-and-power" id="toc-type-ii-error-and-power"><span class="toc-section-number">4.4</span> Type II Error and Power</a></li>
<li><a href="p-value.html#p-value" id="toc-p-value"><span class="toc-section-number">4.5</span> <span class="math inline">\(p\)</span>-Value</a></li>
<li><a href="CIsmallsample.html#CIsmallsample" id="toc-CIsmallsample"><span class="toc-section-number">4.6</span> Confidence Intervals</a></li>
<li><a href="PSSI.html#PSSI" id="toc-PSSI"><span class="toc-section-number">4.7</span> Practice: Small Sample Inference</a>
<ul>
<li><a href="PSSI.html#normally-distributed-hatbetax" id="toc-normally-distributed-hatbetax"><span class="toc-section-number">4.7.1</span> Normally Distributed <span class="math inline">\(\hat\beta|X\)</span></a></li>
<li><a href="PSSI.html#testing-multiple-parameters" id="toc-testing-multiple-parameters"><span class="toc-section-number">4.7.2</span> Testing Multiple Parameters</a></li>
<li><a href="PSSI.html#dualty-of-confidence-intervals-and-hypothesis-tests" id="toc-dualty-of-confidence-intervals-and-hypothesis-tests"><span class="toc-section-number">4.7.3</span> Dualty of Confidence Intervals and Hypothesis Tests</a></li>
</ul></li>
</ul></li>
<li><a href="ch:LSINF.html#ch:LSINF" id="toc-ch:LSINF"><span class="toc-section-number">5</span> Large Sample Inference</a>
<ul>
<li><a href="tools-for-asymptotic-statistics.html#tools-for-asymptotic-statistics" id="toc-tools-for-asymptotic-statistics"><span class="toc-section-number">5.1</span> Tools for Asymptotic Statistics</a>
<ul>
<li><a href="tools-for-asymptotic-statistics.html#modes-of-convergence" id="toc-modes-of-convergence"><span class="toc-section-number">5.1.1</span> Modes of Convergence</a></li>
<li><a href="tools-for-asymptotic-statistics.html#four-important-modes-of-convergence" id="toc-four-important-modes-of-convergence">Four Important Modes of Convergence</a></li>
<li><a href="tools-for-asymptotic-statistics.html#relations-among-modes-of-convergence" id="toc-relations-among-modes-of-convergence">Relations among Modes of Convergence</a></li>
<li><a href="tools-for-asymptotic-statistics.html#continuous-mapping-theorem-cmt" id="toc-continuous-mapping-theorem-cmt"><span class="toc-section-number">5.1.2</span> Continuous Mapping Theorem (CMT)</a></li>
<li><a href="tools-for-asymptotic-statistics.html#slutsky-theorem" id="toc-slutsky-theorem"><span class="toc-section-number">5.1.3</span> Slutsky Theorem</a></li>
<li><a href="tools-for-asymptotic-statistics.html#law-of-large-numbers-lln-and-central-limit-theorem-clt" id="toc-law-of-large-numbers-lln-and-central-limit-theorem-clt"><span class="toc-section-number">5.1.4</span> Law of Large Numbers (LLN) and Central Limit Theorem (CLT)</a></li>
<li><a href="tools-for-asymptotic-statistics.html#estimators-as-a-sequences-of-random-variables" id="toc-estimators-as-a-sequences-of-random-variables"><span class="toc-section-number">5.1.5</span> Estimators as a Sequences of Random Variables</a></li>
</ul></li>
<li><a href="asymptotics-under-the-classic-regression-model.html#asymptotics-under-the-classic-regression-model" id="toc-asymptotics-under-the-classic-regression-model"><span class="toc-section-number">5.2</span> Asymptotics under the Classic Regression Model</a>
<ul>
<li><a href="asymptotics-under-the-classic-regression-model.html#the-case-of-heteroscedasticity" id="toc-the-case-of-heteroscedasticity"><span class="toc-section-number">5.2.1</span> The Case of Heteroscedasticity</a></li>
<li><a href="asymptotics-under-the-classic-regression-model.html#hypothesis-testing-and-confidence-intervals" id="toc-hypothesis-testing-and-confidence-intervals"><span class="toc-section-number">5.2.2</span> Hypothesis Testing and Confidence Intervals</a></li>
</ul></li>
<li><a href="practice-large-sample-inference.html#practice-large-sample-inference" id="toc-practice-large-sample-inference"><span class="toc-section-number">5.3</span> Practice: Large Sample Inference</a>
<ul>
<li><a href="practice-large-sample-inference.html#normally-distributed-hatbeta-for-ntoinfty" id="toc-normally-distributed-hatbeta-for-ntoinfty"><span class="toc-section-number">5.3.1</span> Normally Distributed <span class="math inline">\(\hat\beta\)</span> for <span class="math inline">\(n\to\infty\)</span></a></li>
<li><a href="practice-large-sample-inference.html#testing-multiple-and-single-parameters" id="toc-testing-multiple-and-single-parameters"><span class="toc-section-number">5.3.2</span> Testing Multiple and Single Parameters</a></li>
</ul></li>
</ul></li>
<li><a href="ivr.html#ivr" id="toc-ivr"><span class="toc-section-number">6</span> Instrumental Variables Regression</a>
<ul>
<li><a href="TIVEWASRAASI.html#TIVEWASRAASI" id="toc-TIVEWASRAASI"><span class="toc-section-number">6.1</span> The IV Estimator with a Single Regressor and a Single Instrument</a>
<ul>
<li><a href="TIVEWASRAASI.html#the-two-stage-least-squares-estimator" id="toc-the-two-stage-least-squares-estimator"><span class="toc-section-number">6.1.1</span> The Two-Stage Least Squares Estimator</a></li>
<li><a href="TIVEWASRAASI.html#application-demand-for-cigarettes-12" id="toc-application-demand-for-cigarettes-12"><span class="toc-section-number">6.1.2</span> Application: Demand For Cigarettes (1/2)</a></li>
</ul></li>
<li><a href="TGIVRM.html#TGIVRM" id="toc-TGIVRM"><span class="toc-section-number">6.2</span> The General IV Regression Model</a>
<ul>
<li><a href="TGIVRM.html#application-demand-for-cigarettes-22" id="toc-application-demand-for-cigarettes-22"><span class="toc-section-number">6.2.1</span> Application: Demand for Cigarettes (2/2)</a></li>
</ul></li>
<li><a href="civ.html#civ" id="toc-civ"><span class="toc-section-number">6.3</span> Checking Instrument Validity</a>
<ul>
<li><a href="civ.html#instrument-relevance" id="toc-instrument-relevance"><span class="toc-section-number">6.3.1</span> Instrument Relevance</a></li>
<li><a href="civ.html#instrument-validity" id="toc-instrument-validity"><span class="toc-section-number">6.3.2</span> Instrument Validity</a></li>
</ul></li>
<li><a href="attdfc.html#attdfc" id="toc-attdfc"><span class="toc-section-number">6.4</span> Application to the Demand for Cigarettes</a></li>
</ul></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics (M.Sc.)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tools-for-asymptotic-statistics" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Tools for Asymptotic Statistics</h2>
<div id="modes-of-convergence" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Modes of Convergence</h3>
<p>In the following we will discuss the four most important convergence concepts for sequences of random variables <span class="math inline">\((z_1,z_2,\dots,z_n)\)</span> shortly denoted by <span class="math inline">\(\{z_n\}\)</span>. Non-random scalars (or vectors or matrices) will be denoted by Greek letters such as <span class="math inline">\(\alpha\)</span>.</p>
<!-- Sequences of random vectors (or matrices) will be denoted by $\{\mathbf{z}_n\}$.  -->
<!-- % Vector-Convergence if and only if element-wise converegence:  -->
<!-- %https://www.statlect.com/asymptotic-theory/mean-square-convergence#:~:text=The%20concept%20of%20mean%2Dsquare,difference%20is%20on%20average%20small. -->
</div>
<div id="four-important-modes-of-convergence" class="section level3 unnumbered">
<h3>Four Important Modes of Convergence</h3>
<p>A sequence of random scalars <span class="math inline">\(\{z_n\}\)</span> to a constant (non-random) <span class="math inline">\(\alpha\)</span> if, for any (arbitrarily small) <span class="math inline">\(\eps&gt;0\)</span>,
<span class="math display">\[\begin{eqnarray*}
  \lim_{n\to\infty} P\left(|z_n-\alpha|&gt;\eps\right)=0.
\end{eqnarray*}\]</span>
We write: <span class="math inline">\(\plim_{n\to\infty}z_n=\alpha\)</span>, or <span class="math inline">\(z_n\toprob\alpha\)</span>. Convergence in probability of a sequence of random vectors (or matrices) <span class="math inline">\(\{z_n\}\)</span> to a constant vector (or matrix) <span class="math inline">\(\alpha\)</span> requires convergence in probability.</p>
<p>A sequence of random scalars <span class="math inline">\(\{z_n\}\)</span> to a constant (non-random) <span class="math inline">\(\alpha\)</span> if
<span class="math display">\[\begin{eqnarray*}
P\left(\lim_{n\to\infty}z_n=\alpha\right)=1.
\end{eqnarray*}\]</span>
We write: <span class="math inline">\(z_n\toas\alpha\)</span>. Almost sure convergence of a sequence of random vectors (or matrices) <span class="math inline">\(\{z_n\}\)</span> to a constant vector (or matrix) <span class="math inline">\(\alpha\)</span> requires almost sure convergence.</p>
<p>Almost sure convergence is (usually) rather hard to derive, since the probability is about an event concerning an infinite sequence. Fortunately, however, there are established strong laws of large numbers that we can use for showing almost sure convergence.</p>
<p>A sequence of random scalars <span class="math inline">\(\{z_n\}\)</span> (or ) to a constant (non-random) <span class="math inline">\(\alpha\)</span> if
<span class="math display">\[\begin{eqnarray*}
  \lim_{n\to\infty}E\left((z_n-\alpha)^2\right)=0
\end{eqnarray*}\]</span>
We write: <span class="math inline">\(z_n\toms\alpha\)</span>. If <span class="math inline">\(z_n\)</span> is an estimator (e.g., <span class="math inline">\(z_n=\hat\beta_{k,n}\)</span>) the expression <span class="math inline">\(E\left((z_n-\alpha)^2\right)\)</span> is termed the : <span class="math inline">\(\text{MSE}(z_n)=E\left((z_n-\alpha)^2\right)\)</span>. Mean square convergence of a sequence of random vectors (or matrices) <span class="math inline">\(\{z_n\}\)</span> to a deterministic vector (or matrix) <span class="math inline">\(\alpha\)</span> requires mean square convergence.</p>
<!-- \paragraph*{Convergence to a Random Variable:} The above presented definitions of convergence can be also applied to limits that are random variables. We say that a sequence of random vectors $\{\mathbf{z}_n\}$ converges to a random vector $\mathbf{z}$ and write $\mathbf{z}_n\toprob\mathbf{z}$ if the sequence $\{\mathbf{z}_n-\mathbf{z}\}$ converges to $\mathbf{0}$. Similarly, for $\mathbf{z}_n\toas\mathbf{z}$ and $\mathbf{z}_n\toms\mathbf{z}$.  -->
<p>Let <span class="math inline">\(F_n\)</span> be the cumulative distribution function (cdf) of <span class="math inline">\(z_n\)</span> and <span class="math inline">\(F\)</span> the cdf of <span class="math inline">\(z\)</span>. A sequence of random scalars <span class="math inline">\(\{z_n\}\)</span> to a random scalar <span class="math inline">\(z\)</span> if for all <span class="math inline">\(t\)</span> such that <span class="math inline">\(F(t)\)</span> is continuous at <span class="math inline">\(t\)</span>,
<span class="math display">\[\begin{eqnarray*}
  \lim_{n\to\infty}F_n(t)=F(t).
\end{eqnarray*}\]</span>
We write: <span class="math inline">\(z_n\todistr z\)</span> and call <span class="math inline">\(F\)</span> the (or ) of <span class="math inline">\(z_n\)</span>. Sometimes you will see statements like <span class="math inline">\(z_n\todistr N(0,1)\)</span> or <span class="math inline">\(z_n\overset{a}{\sim}N(0,1)\)</span>, which should be read as <span class="math inline">\(z_n\todistr z\)</span>, where <span class="math inline">\(z\sim N(0,1)\)</span>.</p>
<p>A stochastic sequence <span class="math inline">\(\{z_n\}\)</span> can also convergence in distribution to a <span class="math inline">\(\alpha\)</span>. In this case <span class="math inline">\(\alpha\)</span> is treated as a degenerated random variable with cdf
<span class="math display">\[
F_\alpha(t)=\left\{\begin{matrix}0&amp;\text{if}\;\;t&lt;\alpha\\ 1&amp;\text{if}\;\;t\geq\alpha\end{matrix}\right.
\]</span></p>
<p>Let <span class="math inline">\(z_n,z\in\mathbb{R}^K\)</span> be <span class="math inline">\(K\)</span>-dimensional random variables, then
<span class="math display">\[
z_n\todistr z\text{\quad if and only if \quad}\lambda&#39;z_n\todistr\lambda&#39;z
\]</span>
for any <span class="math inline">\(\lambda\in\mathbb{R}^K\)</span>. This statement is known as the . It is needed since element-wise convergence in distribution does generally not imply convergence of the distribution of <span class="math inline">\(z_n\)</span> to the distribution of <span class="math inline">\(z\)</span>; except, if all elements are independent from each other.</p>
<!-- \paragraph*{Remark:} Note that convergence in distribution, as the name suggests, only involves the distributions of the random variables. Thus, the random variables need not even be defined on the same probability space (that is, they need not be defined for the same random experiment), and indeed we don't even need the random variables at all. (But all this is just thought provoking \dots typically, we'll only consider cases with a common probability space.) -->
</div>
<div id="relations-among-modes-of-convergence" class="section level3 unnumbered">
<h3>Relations among Modes of Convergence</h3>
<p>Proofs can be found, e.g., here: </p>
</div>
<div id="continuous-mapping-theorem-cmt" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Continuous Mapping Theorem (CMT)</h3>
<p>Proof can be found, e.g., in , van der Vaart (1998), Theorem 2.3. Or here: </p>
<p>The CMT does hold for m.s.-convergence except for the case where <span class="math inline">\(a(.)\)</span> is linear.<br />
<!-- %This is easily seen using the expression %$\E[(z_n-\alpha)^2]=\V(z_n)+(\E(z_n)-\alpha)^2$ and the  -->
<!-- %application of Jensen's inequality (see below). --></p>
As a consequence of the CMT (Lemma <span class="math inline">\(\ref{Preserv}\)</span>) we have that the usual arithmetic operations preserve convergence in probability (equivalently for almost sure convergence and convergence in distribution):
<!-- The first statement above is immediately seen by setting $\mathbf{z}_n=\left(x_n, y_n\right)'$, $\boldsymbol\alpha=\left(\beta, \gamma\right)'$, and $\mathbf{a}(\boldsymbol\alpha)=(1,1)\boldsymbol\alpha$, similarly for all others.  -->
</div>
<div id="slutsky-theorem" class="section level3" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Slutsky Theorem</h3>
The following results are concerned with combinations of convergence in probability and convergence in distribution. These are particularly important for the derivation of the asymptotic distribution of estimators.<br />

<p>Proofs can be found, e.g., in , van der Vaart, Theorem 2.8. Or here: </p>
<p>Sometimes, only parts and of Lemma <span class="math inline">\(\ref{Slutsky}\)</span> are called </p>
</div>
<div id="law-of-large-numbers-lln-and-central-limit-theorem-clt" class="section level3" number="5.1.4">
<h3><span class="header-section-number">5.1.4</span> Law of Large Numbers (LLN) and Central Limit Theorem (CLT)</h3>
<p>So far, we discussed the definitions of the four most important convergence modes, their relations among each other, and basic theorems (CMT and Slutsky) about functionals of stochastic sequences. Though, we still lack of tools that allow us to actually show that a stochastic sequence convergences (in some of the discussed modes) to some limit.</p>
<p>In the following we consider the stochastic sequences <span class="math inline">\(\{\bar{z}_n\}\)</span> of sample means <span class="math inline">\(\bar{z}_n=n^{-1}\sum_{i=1}^nz_i\)</span>, where <span class="math inline">\(z_i\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>, are (scalar, vector, or matrix-valued) . Remember: the sample mean <span class="math inline">\(\bar{z}_n\)</span> is an estimator of the deterministic population mean <span class="math inline">\(\mu\)</span>.</p>
<p>Weak LLNs, strong LLNs, and CLTs tell us conditions under which arithmetic means <span class="math inline">\(\bar{z}_n=n^{-1}\sum_{i=1}^nz_i\)</span> converge:
<span class="math display">\[\begin{eqnarray*}
  \bar{z}_n&amp;\toprob&amp;\mu\quad\text{(weak LLN)}\\
  \bar{z}_n&amp;\toas&amp;\mu\quad\text{(strong LLN)}\\
  \sqrt{n}(\bar{z}_n-\mu)&amp;\todistr&amp;N(0,\sigma^2)\quad\text{(CLT)}
\end{eqnarray*}\]</span></p>
<p>In the following we introduce the most well-known versions of the weak, the strong LLN, and the CLT.</p>
<p>Proof can be found, for instance, here: </p>
<p>Proof can be found, e.g., in , Rao (1973), pp. 112-114.</p>
<p>The weak and the strong LLN for random vectors follow from requiring element-by-element convergence.</p>
<p>Proof can be found, e.g., in , van der Vaart (1998), Theorem 2.17.</p>
<p>The Lindeberg-Levy CLT for <span class="math inline">\(K\)</span>-dimensional random vectors follows from our above discussion on From this we know that if <span class="math inline">\(\bar{z}_n\in\mathbb{R}^K\)</span> and <span class="math inline">\(\mu\in\mathbb{R}^K\)</span>, then
<span class="math display">\[\sqrt{n}(\bar{z}_n-\mu)\todistr \mathcal{N}(0,\Sigma)\quad\Leftrightarrow\quad \sqrt{n}(\lambda&#39;\bar{z}_n-\lambda&#39;\mu)\todistr \mathcal{N}(0,\lambda&#39;\Sigma\lambda),\]</span>
for any <span class="math inline">\(\lambda\in\mathbb{R}^K\)</span>.</p>
<p>That is, to apply the Lindeberg-Levy CLT (Theorem <span class="math inline">\(\ref{CLT1}\)</span>) to multivariate (e.g., <span class="math inline">\(K\)</span>-dimensional) stochastic sequences, we need to check whether the univariate stochastic sequence <span class="math inline">\(\{\lambda&#39;z_i\}\)</span> is i.i.d. with <span class="math inline">\(E(\lambda&#39;z_i)=\lambda&#39;\mu\)</span> and <span class="math inline">\(\V(\lambda&#39;z_i)=\lambda&#39;\Sigma\lambda\)</span> for any <span class="math inline">\(\lambda\in\mathbb{R}^K\)</span>. This is the case if the multivariate (<span class="math inline">\(K\)</span>-dimensional) stochastic sequence <span class="math inline">\(\{z_i\}\)</span> is an i.i.d. sequence with <span class="math inline">\(E(z_i)=\mu\)</span> and <span class="math inline">\(\V(z_i)=\Sigma\)</span>.</p>
<p>The LLNs and the CLT are stated with respect to sequences of sample means <span class="math inline">\(\{\bar{z}_n\}\)</span>; i.e., the simplest estimators you probably can think of. We will see, however, that this is all we need in order to analyze also more complicated estimators such as the OLS estimator.</p>
</div>
<div id="estimators-as-a-sequences-of-random-variables" class="section level3" number="5.1.5">
<h3><span class="header-section-number">5.1.5</span> Estimators as a Sequences of Random Variables</h3>
<p>Our concepts above readily apply to general scalar-valued (univariate) or vector-valued (<span class="math inline">\(K\)</span>-dimensional) estimators, say <span class="math inline">\(\hat\theta_n\in\mathbb{R}^K\)</span>, that are computed from i.i.d. random samples.</p>
<!-- \paragraph*{Note.} Under the asymptotic perspective ($n\to\infty$) it's not necessary anymore to condition on $X$ since as $n\to\infty$ the stochastic influence of $X$ on  vanishes  -->
<p>We say that an estimator <span class="math inline">\(\hat\theta_n\)</span> is if
<span class="math display">\[\hat\theta_n\toprob\theta\quad\text{as}\quad n\to\infty\]</span></p>
<p>The of an estimator <span class="math inline">\(\hat\theta_n\)</span> of some true parameter <span class="math inline">\(\theta\)</span> is defined as:
<span class="math display">\[\text{ABias}(\hat\theta_n)=\lim_{n\to\infty}E(\hat\theta_n)-\theta\]</span>
If <span class="math inline">\(\text{ABias}(\hat\theta_n)=0\)</span>, then <span class="math inline">\(\hat\theta\)</span> is called an .</p>
<p>A consistent estimator <span class="math inline">\(\hat\theta_n\)</span> is if
<span class="math display">\[\sqrt{n}(\hat\theta_n-\theta)\todistr \mathcal{N}(0,\Sigma)\quad\text{as}\quad n\to\infty\]</span>
where <span class="math inline">\(\lim_{n\to\infty}\V(\sqrt{n}(\hat\theta_n-\theta))=\lim_{n\to\infty}\V(\sqrt{n}\hat\theta_n)=\Sigma\)</span> as called the asymptotic variance of <span class="math inline">\(\sqrt{n}(\hat\theta_n-\theta)\)</span>.</p>
<p>Consistent estimators <span class="math inline">\(\hat{\theta}_n\toprob\theta\)</span> are called if
<span class="math display">\[\sqrt{n}(\hat\theta_n-\theta)\todistr z \quad\text{as}\quad n\to\infty\]</span>
If additionally the random vector <span class="math inline">\(z\)</span> is normal distributed, then <span class="math inline">\(\hat\theta_n\)</span> is often called </p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch:LSINF.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="asymptotics-under-the-classic-regression-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Script_Econometrics_MSc.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed",
"download": "https://uni-bonn.sciebo.de/s/IZ2yeLeA4dRTFUY",
"search": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
