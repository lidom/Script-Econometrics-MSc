<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Econometrics M.Sc. - 4&nbsp; Multiple Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05-Small-Sample-Inference.html" rel="next">
<link href="./03-Monte-Carlo-Simulations.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Econometrics M.Sc.</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Organization of the Course</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-Introduction-to-R.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to R</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-Review-Prob_n_Stats.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Review: Probability</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-Monte-Carlo-Simulations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Monte Carlo Simulations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-MultipleReg.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-Small-Sample-Inference.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Small Sample Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-Asymptotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Large Sample Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-Instrumental-Variables.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Instrumental Variables Regression</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#assumptions" id="toc-assumptions" class="nav-link active" data-scroll-target="#assumptions"><span class="toc-section-number">4.1</span>  Assumptions</a>
  <ul class="collapse">
  <li><a href="#some-implications-of-the-exogeneity-assumption" id="toc-some-implications-of-the-exogeneity-assumption" class="nav-link" data-scroll-target="#some-implications-of-the-exogeneity-assumption"><span class="toc-section-number">4.1.1</span>  Some Implications of the Exogeneity Assumption</a></li>
  </ul></li>
  <li><a href="#deriving-the-expression-of-the-ols-estimator" id="toc-deriving-the-expression-of-the-ols-estimator" class="nav-link" data-scroll-target="#deriving-the-expression-of-the-ols-estimator"><span class="toc-section-number">4.2</span>  Deriving the Expression of the OLS Estimator</a></li>
  <li><a href="#some-quantities-of-interest" id="toc-some-quantities-of-interest" class="nav-link" data-scroll-target="#some-quantities-of-interest"><span class="toc-section-number">4.3</span>  Some Quantities of Interest</a></li>
  <li><a href="#method-of-moments-estimator" id="toc-method-of-moments-estimator" class="nav-link" data-scroll-target="#method-of-moments-estimator"><span class="toc-section-number">4.4</span>  Method of Moments Estimator</a></li>
  <li><a href="#practice-non-linearity-of-the-regression-line" id="toc-practice-non-linearity-of-the-regression-line" class="nav-link" data-scroll-target="#practice-non-linearity-of-the-regression-line"><span class="toc-section-number">4.5</span>  Practice: (Non-)Linearity of the Regression Line</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-MLR" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>In the following we focus on case of random designs <span class="math inline">\(X\)</span> (i.e.&nbsp;<span class="math inline">\(X\)</span> being a random variable), since, first, this is the more relevant case in econometrics and, second, it includes the case of fixed designs (i.e.&nbsp;<span class="math inline">\(X\)</span> being deterministic) as a special case (“degenerated random variable”). Caution: A random <span class="math inline">\(X\)</span> requires us to consider conditional means and variances “given <span class="math inline">\(X\)</span>.” That is, if we would be able to resample from the model, we do so by fixing (conditioning on) the in-principle random explanatory variable <span class="math inline">\(X\)</span>.</p>
<section id="assumptions" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="assumptions"><span class="header-section-number">4.1</span> Assumptions</h2>
<p>The multiple linear regression model is defined by the following assumptions:</p>
<p><strong>Assumption 1: Data Generating Process</strong></p>
<p><strong>Part (a): Linear Model</strong></p>
<p><span id="eq-LinMod"><span class="math display">\[
\begin{align}
  Y_i=\sum_{k=1}^K\beta_k X_{ik}+\varepsilon_i, \quad i=1,\dots,n.
\end{align}
\tag{4.1}\]</span></span> Usually, a constant (intercept) is included, in this case <span class="math inline">\(X_{i1}=1\)</span> for all <span class="math inline">\(i\)</span>. In the following we will always assume that <span class="math inline">\(X_{i1}=1\)</span> for all <span class="math inline">\(i\)</span>, unless otherwise stated.<br>
It is convenient to write <a href="#eq-LinMod">Equation&nbsp;<span>4.1</span></a> using matrix notation <span class="math display">\[\begin{eqnarray*}
  Y_i&amp;=&amp;\underset{(1\times K)}{X_i'}\underset{(K\times 1)}{\beta} +\varepsilon_i, \quad i=1,\dots,n,
\end{eqnarray*}\]</span> where <span class="math inline">\(X_i=(X_{i1},\dots,X_{iK})'\)</span> and <span class="math inline">\(\beta=(\beta_1,\dots,\beta_K)'\)</span>. Stacking all individual rows <span class="math inline">\(i\)</span> leads to <span class="math display">\[\begin{eqnarray*}\label{LM}
  \underset{(n\times 1)}{Y}&amp;=&amp;\underset{(n\times K)}{X}\underset{(K\times 1)}{\beta} + \underset{(n\times 1)}{\varepsilon},
\end{eqnarray*}\]</span> where <span class="math display">\[\begin{equation*}
Y=\left(\begin{matrix}Y_1\\ \vdots\\Y_n\end{matrix}\right),\quad X=\left(\begin{matrix}X_{11}&amp;\dots&amp;X_{1K}\\\vdots&amp;\ddots&amp;\vdots\\ X_{n1}&amp;\dots&amp;X_{nK}\\\end{matrix}\right),\quad\text{and}\quad \varepsilon=\left(\begin{matrix}\varepsilon_1\\ \vdots\\ \varepsilon_n\end{matrix}\right).
\end{equation*}\]</span></p>
<p><strong>Part (b): Random Sample</strong></p>
<p>Moreover, we assume that the observed (“obs”) data points <span class="math display">\[
((Y_{1,obs},X_{11,obs},\dots,X_{1K,obs}),(Y_{2,obs},X_{21,obs},\dots,X_{2K,obs}),\dots,(Y_{n,obs},X_{n1,obs},\dots,X_{nK,obs}))
\]</span> are realizations of a random sample <span class="math display">\[
((Y_{1},X_{11},\dots,X_{1K}),(Y_{2},X_{21},\dots,X_{2K}),\dots,(Y_{n},X_{n1},\dots,X_{nK})).
\]</span></p>
<p>That is, the <span class="math inline">\(i\)</span>th observed <span class="math inline">\(K+1\)</span> dimensional data point <span class="math inline">\((Y_{i,obs},X_{i1,obs},\dots,X_{iK,obs})\in\mathbb{R}^{K+1}\)</span> is a realization of a <span class="math inline">\(K+1\)</span> dimensional random variable <span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1},\)</span> where</p>
<ol type="1">
<li><span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\)</span> has the identical joint distribution as <span class="math inline">\((Y_{j},X_{j1},\dots,X_{jK})\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and all <span class="math inline">\(j=1,\dots,n\)</span>, and where</li>
<li><span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\)</span> is independent of <span class="math inline">\((Y_{j},X_{j1},\dots,X_{jK})\)</span> for all <span class="math inline">\(i\neq j=1,\dots,n.\)</span></li>
</ol>
<p>Note: Due to <a href="#eq-LinMod">Equation&nbsp;<span>4.1</span></a>, this i.i.d. assumption is equivalent to assuming that the multivariate random variables <span class="math inline">\((\varepsilon_i,X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1}\)</span> are i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span>.</p>
<p><strong>Remark:</strong> Usually, we do not use a different notation for observed realizations <span class="math inline">\((Y_{i,obs},X_{i1,obs},\dots,X_{iK,obs})\in\mathbb{R}^{K+1}\)</span> and for the corresponding random variable <span class="math inline">\((Y_{i},X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1}\)</span> since often both interpretations (random variable and its realizations) can make sense in the same statement and then it depends on the considered context whether the random variables point if view or the realization point of view applies.</p>
<!--  That is, the multivariate distribution of $(\varepsilon_i,X_{i1},\dots,X_{iK})$ is assumed equal 
across $i=1,\dots,n$, but the multivariate random variables $(\varepsilon_i,X_{i1},\dots,X_{iK})$ 
and $(\varepsilon_j,X_{j1},\dots,X_{jK})$ are independent for all $i\neq j$. -->
<!-- **Note.:** Of course, in principle, Assumption 1 can be violated; however, in classic econometrics one usually does not bother with a possibly violated model assumption (Assumption 1). In modern econometrics, this is a big deal.  -->
<!-- The following assumptions are the so-called *classic assumptions* and we will be often concerned about violations of these assumptions.  -->
<p><strong>Assumption 2: Exogeneity</strong> <span class="math display">\[E(\varepsilon_i|X_i)=0,\quad i=1,\dots,n\]</span> This assumption demands that the mean of the random error term <span class="math inline">\(\varepsilon_i\)</span> is zero irrespective of the realizations of <span class="math inline">\(X_i\)</span>.</p>
<p>This exogeneity assumption is also called “orthogonality assumption” or “mean independence assumption.”</p>
<p><strong>Note:</strong> Together with the random sample assumption (Assumption 1, Part (b)) this assumption implies even <strong>strict</strong> exogeneity <span class="math inline">\(E(\varepsilon_i|X) = 0\)</span> since we have independence across <span class="math inline">\(i=1,\dots,n\)</span>. Under strict exogeneity, the mean of <span class="math inline">\(\varepsilon_i\)</span> is zero irrespective of the realizations of <span class="math inline">\(X_1,\dots,X_n\)</span>.</p>
<!-- For one example, it cannot be fulfilled when the regressors include lagged dependent variables. -->
<!-- Notice that in the presence of a constant explanatory variable, setting the expectation to zero is a normalization.  -->
<p><strong>Assumption 3: Rank Condition (no perfect multicollinearity)</strong></p>
<p><span class="math display">\[\operatorname{rank}(X)=K\quad\text{a.s.}\]</span> This assumption demands that the event of one explanatory variable being linearly dependent on the others occurs with a probability equal to zero. (This is the literal translation of the “almost surely (a.s.)” concept.) The assumption implies that <span class="math inline">\(n\geq K\)</span>.</p>
<p>This assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: multicollinearity, dummy variable trap, variance inflation). The violation of this assumption harms any economic interpretation as we cannot disentangle the explanatory variables’ individual effects on <span class="math inline">\(Y\)</span>. Therefore, this assumption is also often called an “identification assumption.”</p>
<p><strong>Assumption 4: Error distribution</strong></p>
<p>Depending on the context (i.e., parameter estimation vs.&nbsp;hypothesis testing and small <span class="math inline">\(n\)</span> vs.&nbsp;large <span class="math inline">\(n\)</span>) there are different more or less restrictive assumptions. Some of the most common ones are the following (roughly order from least to most restrictive):</p>
<ul>
<li><strong>Conditional Distribution:</strong> <span class="math inline">\(\varepsilon_i|X_i \sim f_{\varepsilon|X}\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and for any distribution <span class="math inline">\(f_{\varepsilon|X}\)</span> such that <span class="math inline">\(\varepsilon_i|X_i\)</span> has two (or more) finite moments.</li>
<li><strong>Conditional Normal Distribution:</strong> <span class="math inline">\(\varepsilon_i|X_i \sim \mathcal{N}(0,\sigma^2(X_i))\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>.</li>
<li><strong>i.i.d.:</strong> <span class="math inline">\(\varepsilon_i\overset{\operatorname{i.i.d.}}{\sim}f_\varepsilon\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and for any distribution <span class="math inline">\(f_\varepsilon\)</span> such that <span class="math inline">\(\varepsilon_i\)</span> has two (or more) finite moments. Assuming that the error terms <span class="math inline">\(\varepsilon_i\)</span> are themselves i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span> implies that they do not depend on <span class="math inline">\(X_i\)</span>.</li>
<li><strong>i.i.d. Normal:</strong> As above, but with <span class="math inline">\(f=\mathcal{N}(0,1)\)</span>, i.e., <span class="math inline">\(\varepsilon_i\overset{\operatorname{i.i.d.}}{\sim}\mathcal{N}(0,\sigma^2)\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>.</li>
<li><strong>Spherical errors (“Gauss-Markov assumptions”):</strong> The conditional distributions of <span class="math inline">\(\varepsilon_i|X_i\)</span> may generally depend on <span class="math inline">\(X_i\)</span>, but without affecting the second moments such that <span class="math display">\[\begin{align*}
E(\varepsilon_i^2|X_i)         &amp;=\sigma^2&gt;0\quad\text{for all }i=1,\dots,n\\
E(\varepsilon_i\varepsilon_j|X)&amp;=0\quad\text{for all }i\neq j\quad\text{with}\quad i=1,\dots,n\quad\text{and}\quad j=1,\dots,n.
\end{align*}\]</span> Thus, here one assumes that, for a given realization of <span class="math inline">\(X_i\)</span>, the error process is uncorrelated (i.e.&nbsp;<span class="math inline">\(Cov(\varepsilon_i,\varepsilon_j|X)=E(\varepsilon_i\varepsilon_j|X)=0\)</span> for all <span class="math inline">\(i\neq j\)</span>) and homoskedastic (i.e.&nbsp;<span class="math inline">\(Var(\varepsilon_i|X)=\sigma^2\)</span> for all <span class="math inline">\(i\)</span>).</li>
</ul>
<!-- **Technical Note:** When we write that $Var(\varepsilon_i|X)=\sigma^2$ or $Var(\varepsilon_i|X_i)=\sigma^2_i,$ we implicitly assume that these second moments exists and that they are finite.  -->
<section id="homoskedastic-versus-heteroskedastic-error-terms" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="homoskedastic-versus-heteroskedastic-error-terms">Homoskedastic versus Heteroskedastic Error Terms</h4>
<p>The i.i.d. assumption is not as restrictive as it may seem on first sight. It allows for dependence between <span class="math inline">\(\varepsilon_i\)</span> and <span class="math inline">\((X_{i1},\dots,X_{iK})\in\mathbb{R}^K\)</span>. That is, the error term <span class="math inline">\(\varepsilon_i\)</span> can have a conditional distribution which depends on <span class="math inline">\((X_{i1},\dots,X_{iK})\)</span>; see <a href="02-Review-Prob_n_Stats.html#sec-condDistr"><span>Section&nbsp;2.2.2.5</span></a>.</p>
<!-- However, we need to rule out one certain 
dependency between $\varepsilon_i$ and $X_i$; namely the conditional mean of $\varepsilon_i$ is not 
allowed to depend on $X_i$ (see Assumption 2: Exogeneity). -->
<!-- Example: $\varepsilon|X_i\sim U[-0.5|X_{i2}+X_{i3}|, 0.5|X_{i2}+X_{i3}|]$, 
with $X_{i2}\sim U[-4,4]$. This error term is mean independent of $X_i$ since 
$E(\varepsilon_i|X_i)=0$, but it has heteroskedastic conditional variance 
$Var(\varepsilon_i|X_i)=\frac{1}{12}X_i^2$. -->
<p>The exogeneity assumption (Assumption 2: Exogeneity) requires that the conditional mean of <span class="math inline">\(\varepsilon_i\)</span> is independent of <span class="math inline">\(X_i\)</span>. Besides this, dependencies between <span class="math inline">\(\varepsilon_i\)</span> and <span class="math inline">\(X_{i1},\dots,X_{iK}\)</span> are allowed. For instance, the variance of <span class="math inline">\(\varepsilon_i\)</span> can be a function of <span class="math inline">\(X_{i1},\dots,X_{iK}\)</span>. If this is the case, <span class="math inline">\(\varepsilon_i\)</span> is said to be “heteroskedastic.”</p>
<ul>
<li><strong>Heteroskedastic error terms:</strong> The conditional variances <span class="math inline">\(Var(\varepsilon_i|X_i=x_i)=\sigma^2(x_i)\)</span> are equal to a non-constant variance-function <span class="math inline">\(\sigma^2(x_i)&gt;0\)</span> which is a function of the realization <span class="math inline">\(X_i=x_i.\)</span></li>
</ul>
<p><strong>Example:</strong> <span class="math inline">\(\varepsilon_i|X_i\sim U[-0.5|X_{i2}|, 0.5|X_{i2}|],\)</span> with <span class="math inline">\(X_{i2}\sim U[-4,4]\)</span>. This error term is mean independent of <span class="math inline">\(X_i\)</span> since <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span>, but it has a heteroskedastic conditional variance since <span class="math inline">\(Var(\varepsilon_i|X_i)=\frac{1}{12}X_i^2\)</span> depends on <span class="math inline">\(X_i\)</span>.</p>
<p>Sometimes, we need to be more restrictive by assuming that also the variances of the error terms <span class="math inline">\(\varepsilon_i\)</span> are independent from <span class="math inline">\(X_i\)</span>. (Higher moments may still depend on <span class="math inline">\(X_i\)</span>.) This assumption leads to “homoskedastic” error terms.</p>
<ul>
<li><strong>Homoskedastic error terms:</strong> The conditional variances <span class="math inline">\(Var(\varepsilon_i|X_i=x_i)=\sigma^2\)</span> are equal to some constant <span class="math inline">\(\sigma^2&gt;0\)</span> for every possible realization <span class="math inline">\(X_i=x_i.\)</span></li>
</ul>
<!-- Sometimes, we need to be even more restrictive by assuming that the error terms $\varepsilon_i$  -->
<!-- are themselves **i.i.d.** across $i=1,\dots,n$. This is more restrictive than the assumption that the  -->
<!-- multivariate random variables $(\varepsilon_i,X_{i1},\dots,X_{iK})$ are i.i.d. across $i=1,\dots,n$  -->
<!-- since it implies that the whole distribution (not only the first two moments) of $\varepsilon_i$  -->
<!-- does not depend on $X_i$. This assumption also implies  **homoskedastic** error terms since when  -->
<!-- the whole distribution of $\varepsilon_i$ does not depend on $X_i\in\mathbb{R}^K$ also its variance  -->
<!-- doesn't depend on $X_i$.  -->
<p><strong>Example:</strong> For doing small sample inference (see <a href="05-Small-Sample-Inference.html"><span>Chapter&nbsp;5</span></a>), we need to assume that the error terms <span class="math inline">\(\varepsilon_i\)</span> are i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span> plus the normality assumption, i.e., <span class="math inline">\(\varepsilon_i\stackrel{\textrm{i.i.d.}}{\sim}{\mathcal N} (0, \sigma^2)\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> which leads to homoskedastic variances <span class="math inline">\(Var(\varepsilon_i|X_i)=\sigma^2\)</span> for every possible realization of <span class="math inline">\(X_i\)</span>.</p>
</section>
<section id="some-implications-of-the-exogeneity-assumption" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="some-implications-of-the-exogeneity-assumption"><span class="header-section-number">4.1.1</span> Some Implications of the Exogeneity Assumption</h3>
<div id="thm-a" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.1 (Unconditional Mean) </strong></span>If <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>, then the also the unconditional mean of the error term is zero, i.e. <span class="math display">\[
E(\varepsilon_i)=0,\quad i=1,\dots,n.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Using the <em>Law of Total Expectations</em> (i.e., <span class="math inline">\(E[E(Z|X)]=E(Z)\)</span>) we can rewrite <span class="math inline">\(E(\varepsilon_i)\)</span> as <span class="math display">\[
E(\varepsilon_i)=E[E(\varepsilon_i|X_i)]
\]</span> for all <span class="math inline">\(i=1,\dots,n.\)</span> But the exogeneity assumption yields <span class="math display">\[
E[E(\varepsilon_i|X_i)]=E[0]=0
\]</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> which completes the proof. <span class="math inline">\(\square\)</span></p>
</div>
<p>Generally, two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be <strong>orthogonal</strong> if their cross moment is zero, i.e.&nbsp;if <span class="math inline">\(E(XY)=0\)</span>. Exogeneity is sometimes also called “orthogonality,” due to the following result.</p>
<div id="thm-b" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.2 (Orthogonality) </strong></span>Under exogeneity, i.e.&nbsp;if <span class="math inline">\(E(\varepsilon_i|X_{i})=0\)</span>, the regressors and the error term are orthogonal to each other, i.e, <span class="math display">\[
E(X_{ik}\varepsilon_i)=0
\]</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and <span class="math inline">\(k=1,\dots,K\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
E(X_{ik}\varepsilon_i)
&amp;=E(E(X_{ik}\varepsilon_i|X_{ik}))\quad{\small\text{(By the Law of Total Expectations)}}\\
&amp;=E(X_{ik}E(\varepsilon_i|X_{ik}))\quad{\small\text{(By the linearity of cond.~expectations)}}
\end{align*}\]</span> Now, to show that <span class="math inline">\(E(X_{ik}\varepsilon_i)=0\)</span>, we need to show that <span class="math inline">\(E(\varepsilon_i|X_{ik})=0,\)</span> which is done in the following:</p>
<p>Since <span class="math inline">\(X_{ik}\)</span> is an element of <span class="math inline">\(X_i,\)</span> a slightly more sophisticated use of the <em>Law of Total Expectations</em> (i.e., <span class="math inline">\(E(Y|X)=E(E(Y|X,Z)|X)\)</span>) implies that <span class="math display">\[
E(\varepsilon_i|X_{ik})=E(E(\varepsilon_i|X_i)|X_{ik}).
\]</span> So, the exogeneity assumption, <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> yields <span class="math display">\[
E(\varepsilon_i|X_{ik})=E(\underbrace{E(\varepsilon_i|X_i)}_{=0}|X_{ik})=E(0|X_{ik})=0.
\]</span> I.e., we have that <span class="math inline">\(E(\varepsilon_i|X_{ik})=0\)</span> which allows us to conclude that <span class="math display">\[
E(X_{ik}\varepsilon_i)=E(X_{ik}E(\varepsilon_i|X_{ik}))=E(X_{ik}0)=0
\]</span> which completes the proof. <span class="math inline">\(\square\)</span></p>
</div>
<p>Because the mean of the error term is zero (<span class="math inline">\(E(\varepsilon_i)=0\)</span> for all <span class="math inline">\(i\)</span> (see <a href="#thm-a">Theorem&nbsp;<span>4.1</span></a>), it follows that the orthogonality property, <span class="math inline">\(E(X_{ik}\varepsilon_i)=0,\)</span> is equivalent to a zero correlation property.</p>
<div id="thm-c" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.3 (No Correlation) </strong></span>If <span class="math inline">\(E(\varepsilon_i|X_{i})=0\)</span>, then <span class="math display">\[\begin{eqnarray*}
  Cov(\varepsilon_i,X_{ik})&amp;=&amp;0\quad\text{for all}\quad i=1,\dots,n\quad\text{and}\quad k=1,\dots,K.
\end{eqnarray*}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{eqnarray*}
  Cov(\varepsilon_i,X_{ik})&amp;=&amp;E(X_{ik}\varepsilon_i)-E(X_{ik})\,E(\varepsilon_i)\quad{\small\text{(Def. of Cov)}}\\
  &amp;=&amp;E(X_{ik}\varepsilon_i)\quad{\small\text{(By point (a): $E(\varepsilon_i)=0$)}}\\
  &amp;=&amp;0\quad{\small\text{(By orthogonality result in point (b))}}\quad\square
\end{eqnarray*}\]</span></p>
</div>
<!-- ## The Algebra of Least Squares -->
<!-- In this section, we take the point of view where $(Y^{obs},X^{obs})$ is an observed data-set, i.e., a given realization of the random sample $((Y_1,X_1),\dots,(Y_n,X_n))$. That is, we are not yet interested in the statistical properties of estimators, but are only in doing some linear algebra.  -->
</section>
</section>
<section id="deriving-the-expression-of-the-ols-estimator" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="deriving-the-expression-of-the-ols-estimator"><span class="header-section-number">4.2</span> Deriving the Expression of the OLS Estimator</h2>
<p>We derive the expression for the OLS estimator <span class="math inline">\(\hat\beta=(\hat\beta_1,\dots,\hat\beta_K)'\in\mathbb{R}^K\)</span> as the vector-valued minimizing argument of the sum of squared residuals, <span class="math inline">\(S_n(b)\)</span> with <span class="math inline">\(b\in\mathbb{R}^K\)</span>, for a given sample <span class="math inline">\(((Y_1,X_1),\dots,(Y_n,X_n))\)</span>. In matrix terms this is <span class="math display">\[\begin{align*}
S_n(b)&amp;=(Y-X b)^{\prime}(Y-X b)=Y^{\prime}Y-2 Y^{\prime} X b+b^{\prime} X^{\prime} X b.
\end{align*}\]</span> To find the minimizing argument <span class="math display">\[\hat\beta=\arg\min_{b\in\mathbb{R}^K}S_n(b)\]</span> we compute all partial derivatives <span class="math display">\[
\begin{aligned}
\underset{(K\times 1)}{\frac{\partial S(b)}{\partial b}} &amp;=-2\left(X^{\prime}Y -X^{\prime} Xb\right).
\end{aligned}
\]</span> and set them equal to zero which leads to <span class="math inline">\(K\)</span> linear equations (the “normal equations”) in <span class="math inline">\(K\)</span> unknowns. This system of equations defines the OLS estimates, <span class="math inline">\(\hat{\beta}\)</span>, for a given data-set: <span class="math display">\[
\begin{aligned}
-2\left(X^{\prime}Y -X^{\prime} X\hat{\beta}\right)=\underset{(K\times 1)}{0}.
\end{aligned}
\]</span> From our rank assumption (Assumption 3) it follows that <span class="math inline">\(X^{\prime}X\)</span> is an invertible matrix which allows us to solve the equation system by <span class="math display">\[
\begin{aligned}
\underset{(K\times 1)}{\hat{\beta}} &amp;=\left(X^{\prime} X\right)^{-1} X^{\prime} Y
\end{aligned}
\]</span></p>
<p>The following codes computes the estimate <span class="math inline">\(\hat{\beta}\)</span> for a given realization <span class="math inline">\((Y,X)\)</span> of the random sample <span class="math inline">\((Y,X)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Some given data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>X_1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1.9</span>,<span class="fl">0.8</span>,<span class="fl">1.1</span>,<span class="fl">0.1</span>,<span class="sc">-</span><span class="fl">0.1</span>,<span class="fl">4.4</span>,<span class="fl">4.6</span>,<span class="fl">1.6</span>,<span class="fl">5.5</span>,<span class="fl">3.4</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>X_2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">66</span>, <span class="dv">62</span>, <span class="dv">64</span>, <span class="dv">61</span>, <span class="dv">63</span>, <span class="dv">70</span>, <span class="dv">68</span>, <span class="dv">62</span>, <span class="dv">68</span>, <span class="dv">66</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>Y   <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.7</span>,<span class="sc">-</span><span class="fl">1.0</span>,<span class="sc">-</span><span class="fl">0.2</span>,<span class="sc">-</span><span class="fl">1.2</span>,<span class="sc">-</span><span class="fl">0.1</span>,<span class="fl">3.4</span>,<span class="fl">0.0</span>,<span class="fl">0.8</span>,<span class="fl">3.7</span>,<span class="fl">2.0</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>dataset <span class="ot">&lt;-</span>  <span class="fu">cbind.data.frame</span>(X_1,X_2,Y)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Compute the OLS estimation</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>my.lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X_1 <span class="sc">+</span> X_2, <span class="at">data =</span> dataset)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot sample regression surface</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"scatterplot3d"</span>) <span class="co"># library for 3d plots</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plot3d <span class="ot">&lt;-</span> <span class="fu">scatterplot3d</span>(<span class="at">x =</span> X_1, <span class="at">y =</span> X_2, <span class="at">z =</span> Y,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>          <span class="at">angle=</span><span class="dv">33</span>, <span class="at">scale.y=</span><span class="fl">0.8</span>, <span class="at">pch=</span><span class="dv">16</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>          <span class="at">color =</span><span class="st">"red"</span>, <span class="at">main =</span><span class="st">"OLS Regression Surface"</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plot3d<span class="sc">$</span><span class="fu">plane3d</span>(my.lm, <span class="at">lty.box =</span> <span class="st">"solid"</span>, <span class="at">col=</span><span class="fu">gray</span>(.<span class="dv">5</span>), </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>          <span class="at">draw_polygon=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="04-MultipleReg_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="some-quantities-of-interest" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="some-quantities-of-interest"><span class="header-section-number">4.3</span> Some Quantities of Interest</h2>
<p><strong>Predicted values and residuals.</strong></p>
<ul>
<li>The (OLS) <strong>predicted values</strong>: <span class="math inline">\(\hat{Y}_i=X_i'\hat\beta\)</span>.<br>
In matrix notation: <span class="math inline">\(\hat Y=X\underbrace{(X'X)^{-1}X'Y}_{\hat\beta}=P_X Y\)</span></li>
<li>The (OLS) <strong>residuals</strong>: <span class="math inline">\(\hat\varepsilon_i=Y_i-\hat{Y}_i\)</span>. In matrix notation: <span class="math inline">\(\hat\varepsilon=Y-\hat{Y}=\left(I_n-X(X'X)^{-1}X'\right)Y=M_X Y\)</span></li>
</ul>
<p><strong>Projection matrices.</strong></p>
<p>The matrix <span class="math display">\[
P_X=X(X'X)^{-1}X'
\]</span> is the <span class="math inline">\((n\times n)\)</span> <strong>projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the column space spanned by the column vectors of <span class="math inline">\(X\)</span> and <span class="math display">\[
M_X=I_n-X(X'X)^{-1}X'=I_n-P_X
\]</span> is the associated <span class="math inline">\((n\times n)\)</span> <strong>orthogonal projection matrix</strong> that projects any vector from <span class="math inline">\(\mathbb{R}^n\)</span> into the vector space that is orthogonal to that spanned by <span class="math inline">\(X\)</span>.</p>
<p>The projection matrices <span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> have some nice properties:</p>
<ol type="a">
<li><span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> are <strong>symmetric</strong>, i.e.&nbsp;<span class="math inline">\(P_X=P_X'\)</span> and <span class="math inline">\(M_X=M_X'\)</span>.</li>
<li><span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> are <strong>idempotent</strong>, i.e.&nbsp;<span class="math inline">\(P_XP_X=P_X\)</span> and <span class="math inline">\(M_X M_X=M_X\)</span>.</li>
<li>Moreover, we have that <span class="math inline">\(X'P_X=X'\)</span>, <span class="math inline">\(P_XX=X\)</span>, <span class="math inline">\(X'M_X=0\)</span>, <span class="math inline">\(M_XX=0\)</span>, and <span class="math inline">\(P_XM_X=0\)</span>.</li>
</ol>
<p>All of these properties follow directly from the definitions of <span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> (check it out). Using these properties one can show that the residual vector <span class="math inline">\(\hat\varepsilon=(\hat\varepsilon_1,\dots,\hat\varepsilon_n)'\)</span> is orthogonal to each of the column vectors in <span class="math inline">\(X\)</span>, i.e <span class="math display">\[\begin{eqnarray}
X'\hat\varepsilon&amp;=&amp;X'M_XY\quad\text{\small(By Def.~of $M_X$)}\\
\Leftrightarrow X'\hat\varepsilon&amp;=&amp;\underset{(K\times n)}{0}\underset{(n\times 1)}{Y}\quad\text{\small(since $X'M_X=0$)}\\
\Leftrightarrow X'\hat\varepsilon&amp;=&amp;\underset{(K\times 1)}{0}
\end{eqnarray}\]</span> <!-- \begin{align} --> <!-- X'\hat\varepsilon=\underset{(K\times 1)}{0}.\label{eq:orthXe} --> <!-- \end{align} --> Note that, in the case with intercept, the result <span class="math inline">\(X'\hat\varepsilon=0\)</span> implies that <span class="math inline">\(\sum_{i=1}^n\hat\varepsilon_i=0\)</span>. Moreover, the equation <span class="math inline">\(X'\hat\varepsilon=0\)</span> implies also that the residual vector <span class="math inline">\(\hat{\varepsilon}\)</span> is orthogonal to the predicted values vector, since <span class="math display">\[\begin{align*}
X'\hat\varepsilon&amp;=0\\
\Rightarrow\;\hat\beta'X'\hat\varepsilon&amp;=\hat\beta'0\\
\Leftrightarrow\;\hat Y'\hat\varepsilon&amp;=0.
\end{align*}\]</span></p>
<p>Another insight from equation <span class="math inline">\(X'\hat\varepsilon=0\)</span> is that the vector <span class="math inline">\(\hat\varepsilon\)</span> has to satisfy <span class="math inline">\(K\)</span> linear restrictions which means it looses <span class="math inline">\(K\)</span> degrees of freedom.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Consequently, the vector of residuals <span class="math inline">\(\hat\varepsilon\)</span> has only <span class="math inline">\(n-K\)</span> so-called <em>degrees of freedom</em>. This loss of <span class="math inline">\(K\)</span> degrees of freedom also appears in the definition of the <em>unbiased</em> variance estimator <span class="math display">\[\begin{align}
  s_{UB}^2&amp;=\frac{1}{n-K}\sum_{i=1}^n\hat\varepsilon_i^2\label{EqVarEstim}.
\end{align}\]</span></p>
<p><strong>Variance decomposition:</strong> A further useful result that can be shown using the properties of <span class="math inline">\(P_X\)</span> and <span class="math inline">\(M_X\)</span> is that <span class="math inline">\(Y'Y=\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon\)</span>, i.e. <span class="math display">\[\begin{eqnarray*}
Y'Y&amp;=&amp;(\hat Y+\hat\varepsilon)'(\hat Y+\hat\varepsilon)\notag\\
  &amp;=&amp;(P_XY+M_XY)'(P_XY+M_XY)\notag\\
  &amp;=&amp;(Y'P_X'+Y'M_X')(P_XY+M_XY)\notag\\
  &amp;=&amp;Y'P_X'P_XY+Y'M_X'M_XY+0\notag\\
  &amp;=&amp;\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon
\end{eqnarray*}\]</span> The decomposition <span class="math display">\[
\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon
\]</span> is the basis for the well-known variance decomposition result for OLS regressions.</p>
<div id="thm-vardecomp" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.4 </strong></span>For the linear OLS regression model <a href="#eq-LinMod">Equation&nbsp;<span>4.1</span></a> with intercept, the total sample variance of the dependent variable <span class="math inline">\(Y_1,\dots,Y_n\)</span> can be decomposed as following: <span class="math display">\[\begin{eqnarray}
\underset{\text{total sample variance}}{\frac{1}{n}\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}&amp;=&amp;\underset{\text{explained sample variance}}{\frac{1}{n}\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}+\underset{\text{unexplained sample variance}}{\frac{1}{n}\sum_{i=1}^n\hat\varepsilon_i^2,}\label{VarDecomp}
\end{eqnarray}\]</span> where <span class="math inline">\(\bar{Y}=\frac{1}{n}\sum_{i=1}^nY_i\)</span> and <span class="math inline">\(\bar{\hat{Y}}=\frac{1}{n}\sum_{i=1}^n\hat{Y}_i\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From equation <span class="math inline">\(X'\hat\varepsilon=0\)</span> we have for regressions with intercept that <span class="math inline">\(\sum_{i=1}^n\hat\varepsilon_i=0\)</span>. Hence, from <span class="math inline">\(Y_i=\hat{Y}_i+\hat\varepsilon_i\)</span> it follows that <span class="math display">\[\begin{eqnarray*}
  \frac{1}{n}\sum_{i=1}^n Y_i&amp;=&amp;\frac{1}{n}\sum_{i=1}^n \hat{Y}_i+\frac{1}{n}\sum_{i=1}^n \hat\varepsilon_i\\
  \bar{Y}&amp;=&amp;\bar{\hat{Y}}+0
\end{eqnarray*}\]</span></p>
<p>Using the decomposition <span class="math inline">\(Y'Y=\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon\)</span>, we can now derive the result: <span class="math display">\[\begin{eqnarray*}
   Y'Y&amp;=&amp;\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon\\
   Y'Y-n\bar{Y}^2&amp;=&amp;\hat{Y}'\hat{Y}-n\bar{Y}^2+\hat\varepsilon'\hat\varepsilon\\
   Y'Y-n\bar{Y}^2&amp;=&amp;\hat{Y}'\hat{Y}-n\bar{\hat{Y}}^2+\hat\varepsilon'\hat\varepsilon\quad\text{(by $\bar{Y}=\bar{\hat{Y}}$)}\\
   \sum_{i=1}^nY_i^2-n\bar{Y}^2&amp;=&amp;\sum_{i=1}^n\hat{Y}_i^2-n\bar{\hat{Y}}^2+\sum_{i=1}^n\hat\varepsilon_i^2\\
   \sum_{i=1}^n(Y_i-\bar{Y})^2&amp;=&amp;\sum_{i=1}^n(\hat{Y}_i-\bar{\hat{Y}})^2+\sum_{i=1}^n\hat\varepsilon_i^2\quad\square\\
\end{eqnarray*}\]</span></p>
</div>
<section id="coefficients-of-determination-r2-and-overliner2" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="coefficients-of-determination-r2-and-overliner2">Coefficients of determination: <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\overline{R}^2\)</span></h4>
<p>The larger the proportion of the explained variance, the better is the fit of the model. This motivates the definition of the so-called <span class="math inline">\(R^2\)</span> coefficient of determination: <span class="math display">\[\begin{eqnarray*}
R^2=\frac{\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\;=\;1-\frac{\sum_{i=1}^n\hat{\varepsilon}_i^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}
\end{eqnarray*}\]</span> Obviously, we have that <span class="math inline">\(0\leq R^2\leq 1\)</span>. The closer <span class="math inline">\(R^2\)</span> lies to <span class="math inline">\(1\)</span>, the better is the fit of the model to the observed data. However, a high/low <span class="math inline">\(R^2\)</span> does not mean a validation/falsification of the estimated model. Any relation (i.e., model assumption) needs a plausible explanation from relevant economic theory. The most often criticized disadvantage of the <span class="math inline">\(R^2\)</span> is that additional regressors (relevant or not) will always increase the <span class="math inline">\(R^2\)</span>. Here is an example of the problem.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">100</span>               <span class="co"># Sample size</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)   <span class="co"># Relevant X variable</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>X_ir  <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">5</span>, <span class="dv">20</span>)   <span class="co"># Irrelevant X variable</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>error <span class="ot">&lt;-</span> <span class="fu">rt</span>(n, <span class="at">df =</span> <span class="dv">10</span>)<span class="sc">*</span><span class="dv">10</span>  <span class="co"># True error</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>Y     <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">5</span> <span class="sc">*</span> X <span class="sc">+</span> error    <span class="co"># Y variable</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>lm1   <span class="ot">&lt;-</span> <span class="fu">summary</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))     <span class="co"># Correct OLS regression </span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>lm2   <span class="ot">&lt;-</span> <span class="fu">summary</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X<span class="sc">+</span>X_ir))<span class="co"># OLS regression with X_ir </span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>lm1<span class="sc">$</span>r.squared <span class="sc">&lt;</span> lm2<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
<p>So, <span class="math inline">\(R^2\)</span> increases here even though <code>X_ir</code> is a completely irrelevant explanatory variable. Because of this, the <span class="math inline">\(R^2\)</span> cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called <strong>adjusted</strong> <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\overline{R}^2,\)</span> defined as <span class="math display">\[\begin{eqnarray*}
  \overline{R}^2&amp;=&amp;1-\frac{\frac{1}{n-K}\sum_{i=1}^n\hat{\varepsilon}^2_i}{\frac{1}{n-1}\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\leq R^2%\\
  %=\dots=
  %&amp;=&amp;1-\frac{n-1}{n-K}\left(1-R^2\right)\quad{\small\text{(since $1-R^2=(\sum_i\hat\varepsilon_i^2)/(\sum_i(Y_i-\bar{Y}))$)}}\\
  %&amp;=&amp;1-\frac{n-1}{n-K}+\frac{n-1}{n-K}R^2\quad+\frac{K-1}{n-K}R^2-\frac{K-1}{n-K}R^2\\
  %&amp;=&amp;1-\frac{n-1}{n-K}+R^2\quad+\frac{K-1}{n-K}R^2\\
  %&amp;=&amp;-\frac{K-1}{n-K}+R^2\quad+\frac{K-1}{n-K}R^2\\
  %&amp;=&amp;R^2-\underbrace{\frac{K-1}{n-K}\left(1-R^2\right)}_{\geq 0\;\text{and}\;\leq(K-1)/(n-K)}\;\leq\;R^2
\end{eqnarray*}\]</span> The adjustment is in terms of the degrees of freedom <span class="math inline">\(n-K\)</span>.</p>
</section>
</section>
<section id="method-of-moments-estimator" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="method-of-moments-estimator"><span class="header-section-number">4.4</span> Method of Moments Estimator</h2>
<p>The methods of moments estimator exploits the exogeneity assumption that <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> (Assumption 2). Remember that <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> implies that <span class="math inline">\(E(X_{ik}\varepsilon_i)=0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span> and all <span class="math inline">\(k=1,\dots,K\)</span>. The fundamental idea behind “method of moments estimation” is to use the sample analogues of the population moment restrictions <span class="math inline">\(E(X_{ik}\varepsilon_i)=0\)</span>, <span class="math inline">\(k=1,\dots,K,\)</span> for deriving the estimator: <span class="math display">\[
\begin{array}{c||c}
\text{$K$ population moment restrictions\quad}&amp;\text{$K$ sample moment restrictions}\\[2ex]
\left.\begin{array}{c}
E(\varepsilon_i)=0\\
E(X_{i2}\varepsilon_i)=0\\
\vdots\\
E(X_{iK}\varepsilon_i)=0
\end{array}
\right\}\Leftrightarrow E(X_i\varepsilon_i)=\underset{(K\times 1)}{0} &amp;
\left.\begin{array}{c}
\displaystyle
\frac{1}{n}\sum_{i=1}^n\hat\varepsilon_i=0\\
\displaystyle
\frac{1}{n}\sum_{i=1}^nX_{i2}\hat\varepsilon_i=0\\
\vdots\\
\displaystyle
\frac{1}{n}\sum_{i=1}^nX_{iK}\hat\varepsilon_i=0\\
\end{array}
\right\}\Leftrightarrow \displaystyle\frac{1}{n}\sum_{i=1}^nX_i\hat\varepsilon_i=\underset{(K\times 1)}{0}
\end{array}
\]</span></p>
</section>
<section id="practice-non-linearity-of-the-regression-line" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="practice-non-linearity-of-the-regression-line"><span class="header-section-number">4.5</span> Practice: (Non-)Linearity of the Regression Line</h2>
<p>Diagnostic plots</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Base <code>R</code></a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Tidyverse <code>R</code></a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>car_data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>my_car_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"MPG"</span>   <span class="ot">=</span> car_data<span class="sc">$</span>MPG.city,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"HP"</span>    <span class="ot">=</span> car_data<span class="sc">$</span>Horsepower,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"HP_sq"</span> <span class="ot">=</span> car_data<span class="sc">$</span>Horsepower<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>lm_obj_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(MPG <span class="sc">~</span> HP, <span class="at">data =</span> my_car_df)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>lm_obj_2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(MPG <span class="sc">~</span> HP <span class="sc">+</span> HP_sq, <span class="at">data =</span> my_car_df)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">y=</span>my_car_df<span class="sc">$</span>MPG, <span class="at">x=</span>my_car_df<span class="sc">$</span>HP, </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Simple Linear Regression"</span>,</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Horse Power"</span>, </span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Miles per Gallon"</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lm_obj_1)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lm_obj_1, <span class="at">which=</span><span class="dv">1</span>, </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">"Simple Linear Regression"</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">y=</span>my_car_df<span class="sc">$</span>MPG, <span class="at">x=</span>my_car_df<span class="sc">$</span>HP,</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Polynomial Regression (Degree: 2)"</span>,</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Horse Power"</span>, </span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Miles per Gallon"</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>X_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="fu">min</span>(my_car_df<span class="sc">$</span>HP), </span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>             <span class="at">to   =</span> <span class="fu">max</span>(my_car_df<span class="sc">$</span>HP), <span class="at">len =</span> <span class="dv">25</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">y =</span> <span class="fu">predict</span>(lm_obj_2, </span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>                  <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="st">"HP"</span>    <span class="ot">=</span> X_seq, </span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>                                     <span class="st">"HP_sq"</span> <span class="ot">=</span> X_seq<span class="sc">^</span><span class="dv">2</span>)), </span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> X_seq)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lm_obj_2, <span class="at">which=</span><span class="dv">1</span>, </span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">"Polynomial Regression (Degree: 2)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="04-MultipleReg_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages("tidyverse")</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages("ggfortify")</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages("ggfortify")</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>(<span class="fu">library</span>(<span class="st">"tidyverse"</span>))</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"ggfortify"</span>) <span class="co"># tidy diagnostic plots</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>car_data <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">"https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv"</span>,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">show_col_types =</span> <span class="cn">FALSE</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>my_car_df <span class="ot">&lt;-</span> car_data <span class="sc">|&gt;</span> </span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a> dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>  <span class="st">"MPG"</span>   <span class="ot">=</span> MPG.city, </span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>  <span class="st">"HP"</span>    <span class="ot">=</span> Horsepower, </span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>  <span class="st">"HP_sq"</span> <span class="ot">=</span> Horsepower<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Simple Linear Regression</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>lm_obj_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(MPG <span class="sc">~</span> HP, <span class="at">data =</span> my_car_df)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot: Simple Linear Regression</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>my_car_df <span class="sc">|&gt;</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>HP, <span class="at">y=</span>MPG)) <span class="sc">+</span> </span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">alpha=</span><span class="fl">0.8</span>, <span class="at">size=</span><span class="dv">3</span>)<span class="sc">+</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stat_smooth</span>(<span class="at">method=</span><span class="st">'lm'</span>, <span class="at">formula =</span> y<span class="sc">~</span>x) <span class="sc">+</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_classic</span>() <span class="sc">+</span> </span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Horse Power"</span>, <span class="at">y =</span> <span class="st">"Miles per Gallon"</span>, </span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>         <span class="at">title =</span> <span class="st">"Simple Linear Regression"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="04-MultipleReg_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Diagnostic Plot: Simple Linear Regression </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(lm_obj_1, <span class="at">which =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="04-MultipleReg_files/figure-html/unnamed-chunk-4-2.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Polynomial Regression</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>lm_obj_2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(MPG <span class="sc">~</span> HP <span class="sc">+</span> HP_sq, <span class="at">data =</span> my_car_df)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot: Polynomial Regression</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>my_car_df <span class="sc">|&gt;</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>HP, <span class="at">y=</span>MPG)) <span class="sc">+</span> </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">alpha=</span><span class="fl">0.8</span>, <span class="at">size=</span><span class="dv">3</span>)<span class="sc">+</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stat_smooth</span>(<span class="at">method=</span><span class="st">'lm'</span>, <span class="at">formula =</span> y<span class="sc">~</span><span class="fu">poly</span>(x, <span class="dv">2</span>, <span class="at">raw =</span> <span class="cn">TRUE</span>)) <span class="sc">+</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_classic</span>() <span class="sc">+</span> </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Horse Power"</span>, <span class="at">y =</span> <span class="st">"Miles per Gallon"</span>, </span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>         <span class="at">title =</span> <span class="st">"Polynomial Regression"</span>, </span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>         <span class="at">subtitle =</span> <span class="st">"Polynomial Degree: 2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="04-MultipleReg_files/figure-html/unnamed-chunk-4-3.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Diagnostic Plot: Polynomial Regression</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(lm_obj_2, <span class="at">which =</span> <span class="dv">1</span>)     </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="04-MultipleReg_files/figure-html/unnamed-chunk-4-4.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
<!-- 


::: {.cell}

```{.r .cell-code}
## install.packages("AER")
suppressPackageStartupMessages(library("AER"))
## attach data to R session
data("HousePrices")
## names(HousePrices) ## gives variables names

Y     <- HousePrices$price
Y_log <- log(HousePrices$price)

par(mfrow=c(1,2))
plot(density(Y), main="", xlab="House Prices")
curve(dnorm(x, mean=mean(Y), sd(Y)), add=TRUE, col="blue", lwd=1.3)
legend("topleft", 
       legend=c("Nonparametric Density Estimate", 
                "Normal Density Estimate"), col=c("black", "blue"), lwd=1.3)
plot(density(Y_log), main="", xlab="Log of House Prices")
curve(dnorm(x, mean=mean(Y_log), sd(Y_log)), add=TRUE, col="blue", lwd=1.3)
```

::: {.cell-output-display}
![](04-MultipleReg_files/figure-html/unnamed-chunk-5-1.png){width=672}
:::

```{.r .cell-code}
## adding the house prices in logs to the dataset:
HousePrices$price_log <- Y_log

pairs(HousePrices)
```

::: {.cell-output-display}
![](04-MultipleReg_files/figure-html/unnamed-chunk-5-2.png){width=672}
:::
:::


The following code reads on the `HousePrices` dataset of the `R`-package `AER` and transforms the dependent variable $Y$ to a log-scale, $\log(Y)$, to make it close to normal distributed. 


Next we estimate the linear regression model

$$
\log(Y)_i = \beta_1 + \beta_{lotsize} X_{lotsize,i} + \beta_{bathrooms} + X_{bathrooms,i} + \varepsilon_i
$$




lm_obj <- lm(price_log ~ lotsize + bathrooms, data = HousePrices)

par(mfrow=c(2,2))
plot(lm_obj)
``` -->


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>The <span class="math inline">\(K\)</span> linear restrictions follow from the fact that <span class="math inline">\(X'\hat\varepsilon=0\)</span> are <span class="math inline">\(K\)</span> equations <span class="math inline">\(\sum_{i=1}^nX_{ik}\hat\varepsilon_i=0\)</span> for <span class="math inline">\(k=1,\dots,K\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03-Monte-Carlo-Simulations.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Monte Carlo Simulations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05-Small-Sample-Inference.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Small Sample Inference</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>