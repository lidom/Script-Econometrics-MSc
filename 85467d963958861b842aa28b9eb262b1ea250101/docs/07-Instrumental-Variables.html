<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Econometrics M.Sc. - 7&nbsp; Instrumental Variables</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./06-Asymptotics.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Econometrics M.Sc.</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Organization of the Course</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-Introduction-to-R.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to R</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-Probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-Multiple-Linear-Regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-Monte-Carlo-Simulations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Monte Carlo Simulations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-Small-Sample-Inference.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Small Sample Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-Asymptotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Large Sample Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-Instrumental-Variables.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">7.1</span>  Introduction</a></li>
  <li><a href="#examples-for-structural-equationsmodels" id="toc-examples-for-structural-equationsmodels" class="nav-link" data-scroll-target="#examples-for-structural-equationsmodels"><span class="toc-section-number">7.2</span>  Examples for Structural Equations/Models</a></li>
  <li><a href="#endogenous-regressors" id="toc-endogenous-regressors" class="nav-link" data-scroll-target="#endogenous-regressors"><span class="toc-section-number">7.3</span>  Endogenous Regressors</a></li>
  <li><a href="#instruments" id="toc-instruments" class="nav-link" data-scroll-target="#instruments"><span class="toc-section-number">7.4</span>  Instruments</a></li>
  <li><a href="#reduced-form" id="toc-reduced-form" class="nav-link" data-scroll-target="#reduced-form"><span class="toc-section-number">7.5</span>  Reduced Form</a></li>
  <li><a href="#two-stage-least-squares" id="toc-two-stage-least-squares" class="nav-link" data-scroll-target="#two-stage-least-squares"><span class="toc-section-number">7.6</span>  Two-Stage Least Squares</a></li>
  <li><a href="#monte-carlo-simulation" id="toc-monte-carlo-simulation" class="nav-link" data-scroll-target="#monte-carlo-simulation"><span class="toc-section-number">7.7</span>  Monte-Carlo Simulation</a></li>
  <li><a href="#real-data-example-college-proximity" id="toc-real-data-example-college-proximity" class="nav-link" data-scroll-target="#real-data-example-college-proximity"><span class="toc-section-number">7.8</span>  Real-Data Example: College Proximity</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="toc-section-number">7.9</span>  References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>This chapter builds upon Chapter 12 of <span class="citation" data-cites="Hansen2022">Hansen (<a href="#ref-Hansen2022" role="doc-biblioref">2022</a>)</span>.</p>
<section id="introduction" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">7.1</span> Introduction</h2>
<p>The concepts of <strong>endogeneity</strong> and <strong>instrumental variable</strong> are fundamental to econometrics, and mark a substantial departure from other branches of statistics.</p>
<p>The ideas of endogeneity arise naturally in economics from the structural model approach leading to models of simultaneous equations such as, for instance, the classic supply/demand model of price determination.</p>
<!-- ### Overview {-} -->
<p>We say that there is <strong>endogeneity</strong> in the linear model <span id="eq-IVLinMod"><span class="math display">\[
\begin{align}
  Y_i=X_i'\beta+\varepsilon_i
\end{align}
\tag{7.1}\]</span></span></p>
<ol type="1">
<li>if <span class="math inline">\(\beta\)</span> is the parameter of interest and</li>
<li>if <span class="math inline">\(E(\varepsilon_i|X_i)\neq 0\)</span></li>
</ol>
<p>and thus <span id="eq-endogen"><span class="math display">\[
E(X_i\varepsilon_i)\neq 0.
\tag{7.2}\]</span></span></p>
<p>When <a href="#eq-endogen">Equation&nbsp;<span>7.2</span></a> holds, <span class="math inline">\(X_i\)</span> is <strong>endogenous</strong> for <span class="math inline">\(\beta.\)</span></p>
<p>This situation constitutes a core problem in econometrics which is not so much in the focus of the statistics literature.</p>
<ul>
<li><a href="#eq-IVLinMod">Equation&nbsp;<span>7.1</span></a> is called a <strong>structural equation</strong></li>
<li><span class="math inline">\(\beta\)</span> in <a href="#eq-IVLinMod">Equation&nbsp;<span>7.1</span></a> is called a <strong>structural parameter</strong></li>
</ul>
<p>It is important to distinguish <a href="#eq-IVLinMod">Equation&nbsp;<span>7.1</span></a> from the regression or <strong>projection models</strong> we considered so far. It may be the case that a structural model coincides with a regression/projector model, but this is not necessarily the case.</p>
<p>Endogeneity <strong>cannot</strong> happen if the model coefficient is defined by a linear projection. We can <em>define</em> the linear (population) <strong>projection coefficient</strong> <span class="math display">\[
\beta^*=E(X_iX_i')^{-1}E(X_iY_i)
\]</span> and the corresponding linear (population) projection equation <span class="math display">\[
Y_i = X_i'\beta^* + \varepsilon_i^*,
\]</span> where then, by construction (projection properties), <span class="math inline">\(\varepsilon_i^*\)</span> is a <strong>projection error</strong>, i.e. <span class="math display">\[
E(X_i\varepsilon_i^*)=0.
\]</span></p>
<p><strong>Caution:</strong> Here we simply <em>define</em> <span class="math inline">\(\beta^*\)</span> as <span class="math inline">\(\beta^*=E(X_iX_i')^{-1}E(X_iY)\)</span> which results in the population version of the projection coefficient for the projection of <span class="math inline">\(Y\)</span> into the space spanned by <span class="math inline">\(X_i.\)</span> We did not derive the expression for <span class="math inline">\(\beta^*\)</span> using the exogeneity assumption as we did in <a href="03-Multiple-Linear-Regression.html#sec-MMEstimator"><span>Section&nbsp;3.4</span></a>; indeed, the exogeneity assumption may be violated here.</p>
<p>The (population) projection coefficient <span class="math inline">\(\beta^*\)</span> and the structural parameter <span class="math inline">\(\beta\)</span> coincide <span class="math inline">\((\beta^*=\beta)\)</span> under exogeneity.</p>
<p>However, under endogeneity (<a href="#eq-endogen">Equation&nbsp;<span>7.2</span></a>) the projection coefficient <span class="math inline">\(\beta^*\)</span> does not equal the structural parameter <span class="math inline">\((\beta^*\neq \beta):\)</span> <span class="math display">\[
\begin{align*}
\beta^* &amp; =E(X_iX_i')^{-1}E(X_iY_i)\\
\beta^* &amp; =E(X_iX_i')^{-1}E(X_i (X_i'\beta + \varepsilon_i) )\\
%\beta^* &amp; =E(X_iX_i')^{-1}E(X_i X_i') \beta +
%            E(X_iX_i')^{-1}E(X_i\varepsilon_i)\\
\beta^* &amp; = \beta + E(X_iX_i')^{-1}\underbrace{E(X_i \varepsilon_i)}_{\neq 0}\neq \beta\\
\end{align*}
\]</span></p>
<p>Thus under endogeneity we cannot simply use the projection coefficient to derive an estimator since <strong>the projection coefficient does not identify the structural parameter of interest</strong>.</p>
<p>That is, endogeneity implies that the least squares estimator is inconsistent for the structural parameter. Indeed, under i.i.d. sampling, least squares is consistent for the projection coefficient <span class="math display">\[
\hat\beta\to_pE(X_iX_i')^{-1}E(X_iY) = \beta^* \neq \beta\quad n\to\infty.
\]</span></p>
<p>But since the structural parameter <span class="math inline">\(\beta\)</span> is here the parameter of interest, endogeneity requires the development of alternative estimation methods.</p>
</section>
<section id="examples-for-structural-equationsmodels" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="examples-for-structural-equationsmodels"><span class="header-section-number">7.2</span> Examples for Structural Equations/Models</h2>
<p>The structural model approach in econometrics goes back to the seminal paper <em>The probability approach in econometrics</em> by <span class="citation" data-cites="Haavelmo1944">Haavelmo (<a href="#ref-Haavelmo1944" role="doc-biblioref">1944</a>)</span>.</p>
<p>Under the structural approach one, first, specifies a probabilistic economic model (taking into account economic theory), and then performs a quantitative analysis under the assumption that the economic model is correctly specified. This approach is reflected by Assumption 1 in <a href="03-Multiple-Linear-Regression.html"><span>Chapter&nbsp;3</span></a>. Researchers often describe this as “taking their model seriously”. A criticism of the structural approach is that it is misleading to treat an economic model as correctly specified. Rather, it is more accurate to view a model as a useful abstraction or approximation.</p>
<div id="exm-MeasurementError" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.1 (Measurement error in the regressor) </strong></span><br></p>
<p>Suppose that:</p>
<ul>
<li><span class="math inline">\((Y_i,Z_i)\)</span> is a multivariate random variable i.i.d. across <span class="math inline">\(i=1,\dots,n\)</span></li>
<li><span class="math inline">\(E(Y_i|Z_i)=Z_i'\beta\)</span></li>
<li><span class="math inline">\(\beta\)</span> is the structural parameter</li>
<li><span class="math inline">\(Y_i = Z_i'\beta + \varepsilon_i\)</span> is the structural equation for which our model assumptions of <a href="03-Multiple-Linear-Regression.html#sec-LinModAssumptions"><span>Section&nbsp;3.1</span></a> can be justified.</li>
</ul>
<p>Unfortunately, <span class="math inline">\(Z_i\)</span> is not observed (latent), instead we observe a noisy version of <span class="math inline">\(Z_i\)</span> <span class="math display">\[
X_i=Z_i + u_i,
\]</span> where <span class="math inline">\(u_i\)</span> is a <span class="math inline">\((K\times 1)\)</span> dimensional <strong>classic measurement error</strong>, i.e.</p>
<ul>
<li><span class="math inline">\(u_i\)</span> is independent of all other stochastic quantities in the model</li>
<li><span class="math inline">\(E(u_i)=0\)</span></li>
</ul>
<p>such that <span class="math inline">\(X_i\)</span> is a noisy, but unbiased measure of <span class="math inline">\(Z_i.\)</span></p>
<p>It’s easy to express <span class="math inline">\(Y_i\)</span> as a function of the observable <span class="math inline">\(X_i\)</span> <span class="math display">\[
\begin{align*}
Y_i
&amp; = Z_i'\beta + \varepsilon_i \\
&amp; = (X_i - u_i)'\beta + \varepsilon_i \\
&amp; = X_i'\beta  + v_i,\\
\end{align*}
\]</span> where <span class="math inline">\(v_i=\varepsilon_i - u_i'\beta\)</span>.</p>
<p>That is, the relationship of <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_i\)</span> is described by a linear equation <span class="math display">\[
Y_i = X_i'\beta  + v_i,
\]</span> with a stochastic error term <span class="math inline">\(v_i\)</span>, but this error term is (generally) not a projection error since <span id="eq-measurementBias2"><span class="math display">\[
\begin{align}
E(X_iv_i)
&amp; = E((Z_i + u_i)\,(\varepsilon_i - u_i'\beta)) \\
&amp; = \underbrace{E(Z_i \varepsilon_i)}_{=0} - \underbrace{E(Z_iu_i'\beta)}_{=0}  + \underbrace{E(u_i \varepsilon_i)}_{=0} - E(u_iu_i')\beta \\
&amp; = - E(u_iu_i')\beta \\
&amp;\neq 0
\end{align}
\tag{7.3}\]</span></span> if <span class="math inline">\(\beta\neq 0\)</span> and <span class="math inline">\(E(u_iu_i')\neq 0.\)</span></p>
<p>Generally</p>
<ul>
<li><span class="math inline">\(\beta\)</span> will not equal zero and</li>
<li>the <span class="math inline">\((K\times K)\)</span> variance-covariance matrix <span class="math inline">\(E(u_iu_i')\)</span> will also not equal a zero matrix since this would mean that the measurement errors have variance zero.</li>
</ul>
<p>We can calculate the (population) projection coefficient <span class="math inline">\(\beta^*\)</span> (which can be consistently estimated): <span class="math display">\[
\begin{align*}
\beta^*
&amp; = E(X_iX_i')^{-1} E(X_i Y_i)\\
&amp; = E(X_iX_i')^{-1} E(X_i (X_i'\beta + v_i))\\
&amp; = \beta + E(X_iX_i')^{-1} \underbrace{E(X_i v_i)}_{\neq 0}
\end{align*}
\]</span> In the special case where <span class="math inline">\(K=1\)</span>, we have <span id="eq-measurementBias"><span class="math display">\[
\begin{align}
\beta^*_1
&amp; = \beta_1 + \frac{E(X_i v_i)}{E(X_i^2)}\\
&amp; = \beta_1 - \frac{E(u_i^2)\beta_1}{E(X_i^2)}\\
&amp; = \beta_1\left(1 - \frac{ E(u_i^2)}{E(X_i^2)}\right) &lt; \beta_1,
\end{align}
\tag{7.4}\]</span></span> where we used that, by <a href="#eq-measurementBias2">Equation&nbsp;<span>7.3</span></a>, <span class="math inline">\(E(X_iv_i)=-E(u_i^2)\beta_1\)</span>, and where the inequality follows from observing that <span class="math inline">\(E(u_i^2)/E(X_i^2)&lt;1\)</span> since <span class="math display">\[
\begin{align*}
E(X_i^2)
&amp;= E((Z_i + u_i)^2)\\
&amp;= E(Z_i^2) + 2E(Z_iu_i) + E(u_i^2) \\
&amp;= E(Z_i^2) + 0 + E(u_i^2)\\  
&amp;&gt; E(u_i^2)\\  
\Leftrightarrow\quad 0\leq \frac{E(u_i^2)}{E(X_i^2)}&amp;&lt;1.
\end{align*}
\]</span> <a href="#eq-measurementBias">Equation&nbsp;<span>7.4</span></a> shows that projection coefficient <span class="math inline">\(\beta_1^*\)</span> shrinks the structural parameter <span class="math inline">\(\beta_1\)</span> towards zero. This is called <strong>measurement error bias</strong> or <strong>attenuation bias</strong>.</p>
</div>
<div id="exm-SupplyAndDemand" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.2 (Supply and Demand) </strong></span><br></p>
<p>The variables <span class="math inline">\(Q_i\in\mathbb{R}\)</span> and <span class="math inline">\(P_i\in\mathbb{R}\)</span> (Quantity and Price) are determined jointly by the following simultaneous equation system consisting of the demand equation <span class="math display">\[
Q_i = -\beta_1 P_i + \varepsilon_{1i}
\]</span> and of the supply equation <span class="math display">\[
Q_i =  \beta_2 P_i + \varepsilon_{2i},
\]</span> where we assume that <span class="math inline">\(Q_i\)</span> and <span class="math inline">\(P_i\)</span> are centered such that we can drop the intercepts.</p>
<p>Assume that the <span class="math inline">\((2\times 1)\)</span> dimensional joint random error <span class="math inline">\(\varepsilon_i=(\varepsilon_{1i},\varepsilon_{2i})'\)</span> satisfies <span class="math inline">\(E(\varepsilon_i)=0\)</span> and <span class="math inline">\(E(\varepsilon_i\varepsilon_i')=I_2\)</span> (the latter for simplicity).</p>
<blockquote class="blockquote">
<p>What happens if we regress <span class="math inline">\(Q_i\)</span> on <span class="math inline">\(P_i\)</span>? <span class="math display">\[
Q_i=\beta^* P_i+\varepsilon^*_i
\]</span></p>
</blockquote>
<p>To answer this question, we need to solve <span class="math inline">\(Q_i\)</span> and <span class="math inline">\(P_i\)</span> in terms of the errors <span class="math inline">\(\varepsilon_{1i}\)</span> and <span class="math inline">\(\varepsilon_{2i}\)</span>. <span class="math display">\[
\left(\begin{matrix}
1 &amp; \beta_1\\
1 &amp; -\beta_2\\
\end{matrix}
\right)
\left(\begin{matrix}
Q_i\\
P_i\\
\end{matrix}
\right) =
\left(\begin{matrix}
\varepsilon_{1i}\\
\varepsilon_{2i}\\
\end{matrix}
\right)
\]</span> Rewriting yields (assuming invertibility) <span class="math display">\[
\begin{align}
\left(\begin{matrix}
Q_i\\
P_i\\
\end{matrix}
\right)
&amp;=
\left(\begin{matrix}
1 &amp; \beta_1\\
1 &amp; -\beta_2\\
\end{matrix}
\right)^{-1}
\left(\begin{matrix}
\varepsilon_{1i}\\
\varepsilon_{2i}\\
\end{matrix}
\right)\\[2ex]
&amp;=
\frac{1}{-\beta_2 - \beta_1}
\left(\begin{matrix}
-\beta_2 &amp;   -\beta_1\\
- 1    &amp;           1\\
\end{matrix}
\right)
\left(\begin{matrix}
\varepsilon_{1i}\\
\varepsilon_{2i}\\
\end{matrix}
\right)\\[2ex]
&amp;=
\left(\begin{matrix}
\beta_2 &amp;   \beta_1\\
1    &amp;     -1\\
\end{matrix}\right)
\left(\begin{matrix}
\varepsilon_{1i}\\
\varepsilon_{2i}\\
\end{matrix}\right)
\frac{1}{\beta_1 +\beta_2}\\[2ex]
&amp;=
\left(
\begin{matrix}
(\beta_2 \varepsilon_{1i} + \beta_1  \varepsilon_{2i})/(\beta_1 + \beta_2)\\
(\varepsilon_{1i} - \varepsilon_{2i})/(\beta_1 + \beta_2)
\end{matrix}  
\right)
\end{align}
\]</span> Thus, under the simplification that <span class="math inline">\(E(\varepsilon_i\varepsilon_i')=I_2\)</span> <span class="math display">\[
\begin{align*}
E(P_iQ_i)
&amp;=\frac{E\left((\varepsilon_{1i} - \varepsilon_{2i})(\beta_2 \varepsilon_{1i} + \beta_1  \varepsilon_{2i})\right)}{(\beta_1 + \beta_2)^2}\\
&amp;=\frac{\beta_2 E\left(\varepsilon_{1i}^2\right) - \beta_1 E\left(\varepsilon_{2i}^2\right)}{(\beta_1 + \beta_2)^2}=\frac{\beta_2 - \beta_1}{(\beta_1 + \beta_2)^2}
\end{align*}
\]</span> <span class="math display">\[
\begin{align*}
E(P_i^2)
&amp;=\frac{E\left((\varepsilon_{1i} - \varepsilon_{2i})^2\right)}{(\beta_1 + \beta_2)^2}\\
&amp;=\frac{E\left(\varepsilon_{1i}^2\right) + E\left(\varepsilon_{2i}^2\right)}{(\beta_1 + \beta_2)^2} = \frac{2}{(\beta_1 + \beta_2)^2}
\end{align*}
\]</span></p>
<p>The (population) projection of <span class="math inline">\(Q_i\)</span> on <span class="math inline">\(P_i\)</span> yield <span class="math display">\[
Q_i=\beta^* P_i+\varepsilon^*_i
\]</span> with <span class="math inline">\(E(P_i\varepsilon^*_i)=0\)</span> and <span class="math display">\[
\beta^* = \frac{E(P_iQ_i)}{E(P_i^2)} = \frac{\beta_2 - \beta_1}{2}
\]</span> The projection coefficient equals the average of the demand slope <span class="math inline">\(\beta_1\)</span> and the supply slope <span class="math inline">\(\beta_2.\)</span> The OLS estimator satisfies <span class="math inline">\(\hat\beta\to_p\beta^*\)</span> and thus the limit does not equal one of the structural parameters <span class="math inline">\(\beta_1\)</span> or <span class="math inline">\(\beta_2.\)</span> This is called <strong>simultaneous equation bias</strong>.</p>
<p>Generally, when both the dependent variable and a regressor are <strong>simultaneously determined</strong> then the regressor should be treated as endogenous.</p>
</div>
</section>
<section id="endogenous-regressors" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="endogenous-regressors"><span class="header-section-number">7.3</span> Endogenous Regressors</h2>
<!-- Technically, we say that a regressor $X_i$ is **exogenous** for $\beta$ if $E(X_i\varepsilon_i)=0.$ In general the distinction in an economic model is that a regressor $X_i$ is endogenous if it is jointly determined with $Y_i,$ while a regressor $X_i$ is exogenous if it is determined separately from $Y_i.$  -->
<p>Usually, only a subset of the regressors need to be treated as <strong>endogenous</strong>. In the following, we partition the vector of regressors <span class="math display">\[
\underset{(K\times 1)}{X_i}=
\left(
\begin{matrix}
\underset{(K_1\times 1)}{X_{i1}}\\
\underset{(K_2\times 1)}{Y_{i2}}
\end{matrix}
\right)
\]</span> with <span class="math inline">\(K=K_1 + K_2\)</span>.</p>
<ul>
<li><span class="math inline">\(X_{i1}\)</span>: <span class="math inline">\((K_1\times 1)\)</span> vector of <strong>exogenous</strong> regressors, i.e.&nbsp;<span class="math inline">\(E(X_{i1}\varepsilon_i)=0\)</span><br>
</li>
<li><span class="math inline">\(Y_{i2}\)</span>: <span class="math inline">\((K_2\times 1)\)</span> vector of <strong>endogenous</strong> regressors, i.e.&nbsp;<span class="math inline">\(E(Y_{i2}\varepsilon_i)\neq 0\)</span></li>
</ul>
<p>The structural equation is then <span id="eq-StructuralEqEndogen1"><span class="math display">\[
Y_{i1}=X_{i1}'\beta_1 + Y_{i2}'\beta_2 + \varepsilon_i
\tag{7.5}\]</span></span></p>
<ul>
<li><span class="math inline">\(Y_{i1}=Y_i\)</span>: <strong>Endogenous</strong> dependent variable, i.e.&nbsp;<span class="math inline">\(E(Y_{i1}\varepsilon_i)\neq 0\)</span></li>
</ul>
<p>This notation clarifies which variables are exogenous and which endogenous.</p>
</section>
<section id="instruments" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="instruments"><span class="header-section-number">7.4</span> Instruments</h2>
<p>To consistently estimate the structural parameter <span class="math inline">\(\beta\)</span> we need additional information. One type of information commonly used in econometrics are called <strong>instruments</strong>.</p>
<div id="def-InstrumentalVariable" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.1 (Instrumental Variable) </strong></span>The <span class="math inline">\(\ell\times 1\)</span> random vector <span class="math inline">\(Z_i\)</span> is called an <strong>instrumental variable</strong> for <a href="#eq-StructuralEqEndogen1">Equation&nbsp;<span>7.5</span></a> if <span id="eq-IV1"><span class="math display">\[
E(Z_i\varepsilon_i)=0
\tag{7.6}\]</span></span> <span id="eq-IV2"><span class="math display">\[
\operatorname{rank}\Big(E(Z_iZ_i')\Big)=\ell  
\tag{7.7}\]</span></span> <span id="eq-IV3"><span class="math display">\[
\operatorname{rank}\Big(\underbrace{E(Z_iX_i')}_{(\ell\times K)}\Big)=K,
\tag{7.8}\]</span></span> where <span class="math inline">\(X_i'=(X_{i1}',Y_{i2}').\)</span></p>
</div>
<p><strong>Explanation of <a href="#def-InstrumentalVariable">Definition&nbsp;<span>7.1</span></a>:</strong></p>
<ul>
<li><a href="#eq-IV1">Equation&nbsp;<span>7.6</span></a> states that the instruments are uncorrelated with the error term (<strong>exogeneity condition</strong>)</li>
<li><a href="#eq-IV2">Equation&nbsp;<span>7.7</span></a> excludes linearly redundant instruments</li>
<li><a href="#eq-IV3">Equation&nbsp;<span>7.8</span></a> is called the <strong>relevance condition</strong> and is essential for the identification of the model, as we discuss later.
<ul>
<li>A necessary condition for <a href="#eq-IV3">Equation&nbsp;<span>7.8</span></a> is that <span class="math inline">\(\ell\geq K.\)</span></li>
</ul></li>
</ul>
<p>Note that the exogenous regressors <span class="math inline">\(X_{i1}\)</span> fulfill <a href="#eq-IV1">Equation&nbsp;<span>7.6</span></a> and thus should be included as (additional) instrumental variables, i.e.&nbsp; <span class="math display">\[
\underset{(\ell\times 1)}{Z_i}=
\underset{((K_1+\ell_2)\times 1)}{\left(\begin{matrix}Z_{i1}\\Z_{i2}\end{matrix}\right)} :=
\underset{((K_1+\ell_2)\times 1)}{\left(\begin{matrix}X_{i1}\\Z_{i2}\end{matrix}\right)}
\]</span> with <span class="math inline">\(\ell=K_1+\ell_2.\)</span></p>
<p>With this notation we can also write the structural <a href="#eq-StructuralEqEndogen1">Equation&nbsp;<span>7.5</span></a> as <span id="eq-StructuralEqEndogen2"><span class="math display">\[
Y_{i1} = Z_{i1}'\beta_1 + Y_{i2}'\beta_2 + \varepsilon_i
\tag{7.9}\]</span></span></p>
<ul>
<li><span class="math inline">\(Z_{i1}\)</span>: (<span class="math inline">\(K_1\times 1\)</span>) vector of <strong>included exogenous variables</strong> <!-- $(Z_{i1}=X_{i1})$ -->
<ul>
<li>uncorrelated with <span class="math inline">\(\varepsilon_i\)</span> and thus potentially usable</li>
<li><strong>included</strong> since they have potentially non-zero coefficients in <a href="#eq-StructuralEqEndogen2">Equation&nbsp;<span>7.9</span></a></li>
</ul></li>
<li><span class="math inline">\(Z_{i2}\)</span>: (<span class="math inline">\(\ell_2\times 1\)</span>) vector of <strong>excluded exogenous variables</strong>
<ul>
<li>uncorrelated with <span class="math inline">\(\varepsilon_i\)</span> and thus potentially usable</li>
<li><strong>excluded</strong> since they have <strong>zero coefficients</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> in <a href="#eq-StructuralEqEndogen2">Equation&nbsp;<span>7.9</span></a></li>
</ul></li>
</ul>
<!-- **Terminology:** -->
<!-- * $Z_{i1}$: $(K_1\times 1)$ vector of **exogenous** regressors $(Z_{i1}=X_{i1})$, i.e. $E(Z_{i1}\varepsilon_i)=0$   -->
<!-- * $Y_{i1}\in\mathbb{R}$:        *Endogenous dependent variable*
* $Z_{i1}\in\mathbb{R}^{K_1}$:  *Exogenous regressors* ($Z_{i1}=X_{i1}$)
* $Y_{i2}\in\mathbb{R}^{K_2}$:  *Endogenous regressor*
* $Z_{i2}\in\mathbb{R}^{\ell}$: *Instrumental variables* -->
<p>We say that the model is</p>
<ul>
<li><strong>just-identified</strong> if <span class="math inline">\(\ell = K\)</span></li>
<li><strong>over-identified</strong> if <span class="math inline">\(\ell &gt; K\)</span></li>
</ul>
<blockquote class="blockquote">
<p>What variables can be used as Instrumental Variables (IVs)?</p>
</blockquote>
<ol type="1">
<li>Instrumental variables must be uncorrelated with the error term (<a href="#eq-IV1">Equation&nbsp;<span>7.6</span></a>).</li>
<li>Instrumental variables must be correlated with the endogenous variables <span class="math inline">\(Y_{i2}\)</span> also after controlling for the other already included exogenous variables <span class="math inline">\(Z_{i1}\)</span> (<a href="#eq-IV3">Equation&nbsp;<span>7.8</span></a>).<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> (I.e. the IVs must contain <em>additional information</em>, and thus there must not be a perfect multicollinearity between the IVs <span class="math inline">\(Z_{i2}\)</span> and the already included exogenous regressors <span class="math inline">\(Z_{i1}\)</span>.)</li>
</ol>
<!-- From the rank condition  it follows that the  (Otherwise, the instrumental variables would not add any new information since they could be expressed as linear combinations of the already included exogenous variables $Z_{i1}.$) -->
<p>These two requirement mean that the IVs, <span class="math inline">\(Z_{i2},\)</span> …</p>
<ul>
<li>… are determined outside the systems for the endogenous variables <span class="math inline">\(Y_{i1}\)</span> and <span class="math inline">\(Y_{i2}.\)</span></li>
<li>… causally determine the endogenous variables <span class="math inline">\(Y_{i2}.\)</span></li>
<li>… not causally determine the dependent variable <span class="math inline">\(Y_{i1}\)</span> (except indirectly through their correlation with the exogenous variables <span class="math inline">\(Z_{i1}\)</span>)</li>
</ul>
<!-- **Measurement error in the regressor (@exm-MeasurementError continued)** 

When $X_{i}=Z_i+u_i$ is a mis-measured version of the latent $Z_i,$ a common choice for an instrument $Z_{i2}$ is an alternative (additional) measurement 
$$
Z_{i2} =  Z_i + \tilde{u}_{i}
$$
of $Z_i.$ This additional measurement $Z_{i2}$ is an IV if

* $\tilde{u}_{i}$ is independent of $X_i.$ Since then $E(Z_{i2})$
* 

satisfy the property of an instrumental variable the measurement error in $Z_{i2}$ must be independent of that in $X_i.$ -->
</section>
<section id="reduced-form" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="reduced-form"><span class="header-section-number">7.5</span> Reduced Form</h2>
<p>The <strong>reduced form</strong> models are <strong>projection models</strong> giving us the relationships</p>
<ol type="1">
<li>between the <span class="math inline">\((K_2\times 1)\)</span> endogenous regressors <span class="math inline">\(Y_{i2}\)</span> and the <span class="math inline">\((\ell\times 1)\)</span> instruments <span class="math inline">\(Z_{i}.\)</span></li>
<li>between the <span class="math inline">\((1\times 1)\)</span> endogenous regressors <span class="math inline">\(Y_{i1}\)</span> and the <span class="math inline">\((\ell\times 1)\)</span> instruments <span class="math inline">\(Z_{i}.\)</span></li>
</ol>
<p>The linear reduced form model for <span class="math inline">\(Y_{i2}\)</span> is given by the population projection model that regresses <span class="math inline">\(Y_{i2}\)</span> on the exogenous instruments <span class="math inline">\(Z_{i}:\)</span> <span id="eq-reducedFormModelY2"><span class="math display">\[
\begin{align}
Y_{i2}
&amp;= \Gamma'Z_i + u_{i2}\\
&amp;= \Gamma_{12}'Z_{i1} + \Gamma_{22}'Z_{i2} + u_{i2},
\end{align}
\tag{7.10}\]</span></span> which implies that <span class="math inline">\(E(Z_iu_{i2}')=0.\)</span></p>
<p>The <span class="math inline">\((\ell\times K_2)\)</span> dimensional coefficient matrix <span class="math inline">\(\Gamma\)</span> is defined by the projection coefficient <span id="eq-Gamma"><span class="math display">\[
\underset{(\ell\times K_2)}{\Gamma}=\left(\begin{matrix}\underset{(K_1\times K_2)}{\Gamma_{12}}\\ \underset{(\ell_2\times K_2)}{\Gamma_{22}}\end{matrix}\right) = E(Z_iZ_i')^{-1}E(Z_iY_{i2}')
\tag{7.11}\]</span></span></p>
<blockquote class="blockquote">
<p>Note: <a href="#eq-Gamma">Equation&nbsp;<span>7.11</span></a> defines a <em>matrix-valued</em> regression/projection parameter and can be thought as lining up single ordinary vector-valued regression parameters <span class="math inline">\([\Gamma]_{\cdot,l} = E(Z_iZ_i')^{-1}E(Z_iY_{i2,l}')\)</span>, <span class="math inline">\(l=1,\dots,\ell\)</span>, where <span class="math inline">\([\Gamma]_{\cdot,l}\)</span> is the <span class="math inline">\(l\)</span>th column of <span class="math inline">\(\Gamma\)</span> and <span class="math inline">\(Y_{i2,l}\)</span> the <span class="math inline">\(l\)</span>th element of <span class="math inline">\(Y_{i2}\)</span>. More details can be found, for instance, in Chapter 11 of <span class="citation" data-cites="Hansen2022">Hansen (<a href="#ref-Hansen2022" role="doc-biblioref">2022</a>)</span>.</p>
</blockquote>
<p>The projection coefficient <span class="math inline">\(\Gamma\)</span> (<a href="#eq-Gamma">Equation&nbsp;<span>7.11</span></a>) is well defined and unique if <a href="#eq-IV2">Equation&nbsp;<span>7.7</span></a> holds since then <span class="math inline">\(E(Z_iZ_i')\)</span> is invertible.</p>
<p>The linear reduced form projection model for <span class="math inline">\(Y_{i1}\)</span> is derived from substituting the reduced form projection model for <span class="math inline">\(Y_{i2}\)</span> (<a href="#eq-reducedFormModelY2">Equation&nbsp;<span>7.10</span></a>) into the structural equation for <span class="math inline">\(Y_{i1}\)</span> (<a href="#eq-StructuralEqEndogen2">Equation&nbsp;<span>7.9</span></a>): <span id="eq-reducedFormModelY1"><span class="math display">\[
\begin{align}
Y_{i1}
&amp;= Z_{i1}'\beta_1 + Y_{i2}'\beta_2 + \varepsilon_i\\
&amp;= Z_{i1}'\beta_1 + (\Gamma_{12}'Z_{i1} + \Gamma_{22}'Z_{i2} + u_{i2})'\beta_2 + \varepsilon_i\\
&amp;= Z_{i1}'\beta_1 + Z_{i1}'\Gamma_{12}\beta_2 + Z_{i2}'\Gamma_{22}\beta_2  + u_{i2}'\beta_2 + \varepsilon_i\\
&amp;= Z_{i1}' (\beta_1 + \Gamma_{12}\beta_2) + Z_{i2}' (\Gamma_{22}\beta_2)  + (u_{i2}'\beta_2 + \varepsilon_i)\\
&amp;= Z_{i1}' \lambda_1 + Z_{i2}' \lambda_2  + u_{i1}\\
&amp;= Z_{i}' \lambda + u_{i1},
\end{align}
\tag{7.12}\]</span></span> where <span class="math inline">\(u_{i1} = u_{i2}'\beta_2 + \varepsilon_i.\)</span> Since <a href="#eq-reducedFormModelY1">Equation&nbsp;<span>7.12</span></a> is a projection model, we have that <span class="math inline">\(E(Z_iu_{i1})=0\)</span> such that the <span class="math inline">\((\ell\times 1)\)</span> dimensional coefficient <span class="math inline">\(\lambda\)</span> is defined by the projection coefficient <span id="eq-lambda"><span class="math display">\[
\underset{(\ell\times 1)}{\lambda}=\left(\begin{matrix}\underset{(K_1\times 1)}{\lambda_{1}}\\ \underset{(\ell_2\times 1)}{\lambda_{2}}\end{matrix}\right) = E(Z_iZ_i')^{-1}E(Z_iY_{i})
\tag{7.13}\]</span></span></p>
<p>Note that the thus estimable projection parameter <span class="math inline">\(\lambda\)</span> of the reduced form projection model for <span class="math inline">\(Y_{i1}\)</span> (<a href="#eq-reducedFormModelY1">Equation&nbsp;<span>7.12</span></a>) can written as a linear function <span class="math inline">\(\bar{\Gamma}\beta\)</span> of the (not directly estimable) structural parameter <span class="math inline">\(\beta\)</span>: <span class="math display">\[
\begin{align*}
\underset{(\ell\times 1)}{
\lambda} =
\left(
\begin{matrix}
\lambda_1\\
\lambda_2
\end{matrix}
\right)
&amp;=
\left(
\begin{matrix}
\beta_1 + \Gamma_{12}\beta_2 \\
\Gamma_{22}\beta_2
\end{matrix}
\right) \\
&amp;=
\underset{(\ell\times (K_1+K_2))}{
\left[
\begin{matrix}
I_{K_1} &amp;  \Gamma_{12}\\
0 &amp;  \Gamma_{22}
\end{matrix}
\right]}
\underset{((K_1+K_2)\times 1)}{
\left(
\begin{matrix}
\beta_1\\
\beta_2
\end{matrix}
\right)} = \bar{\Gamma} \beta.
\end{align*}
\]</span> Luckily, the unknown components <span class="math inline">\(\Gamma_{12}\)</span> and <span class="math inline">\(\Gamma_{12}\)</span> of <span class="math inline">\(\bar{\Gamma}\)</span> are together the estimable projection parameter <span class="math inline">\(\Gamma\)</span> (<a href="#eq-Gamma">Equation&nbsp;<span>7.11</span></a>). Moreover, we can generalize <a href="#eq-Gamma">Equation&nbsp;<span>7.11</span></a> for <span class="math inline">\(\Gamma\)</span> to an expression for <span class="math inline">\(\bar{\Gamma}\)</span>: <!-- $$
\underset{(\ell\times K_2)}{\Gamma}=\left(\begin{matrix}\underset{(K_1\times K_2)}{\Gamma_{12}}\\ \underset{(\ell_2\times K_2)}{\Gamma_{22}}\end{matrix}\right) & = E(Z_iZ_i')^{-1}E(Z_iY_{i2}')
$$ --> <span id="eq-GammaBar"><span class="math display">\[
\underbrace{\left[\begin{matrix}I_{K_1}&amp;\underset{(K_1\times K_2)}{\Gamma_{12}}\\ 0&amp; \underset{(\ell_2\times K_2)}{\Gamma_{22}}\end{matrix}\right]}_{=\underset{(\ell\times K )}{\bar{\Gamma}}} = E(Z_iZ_i')^{-1}E(Z_i\;\overbrace{\underset{(1\times (K_1+K_2))}{(X_{i1}',Y_{i2}')}}^{=X_i'}\;)
\tag{7.14}\]</span></span></p>
<p>The least squares estimators of the regression/projection parameters</p>
<ul>
<li><span class="math inline">\(\Gamma\)</span> (<a href="#eq-Gamma">Equation&nbsp;<span>7.11</span></a>)</li>
<li><span class="math inline">\(\bar{\Gamma}\)</span> (<a href="#eq-GammaBar">Equation&nbsp;<span>7.14</span></a>)</li>
<li><span class="math inline">\(\lambda\)</span> (<a href="#eq-lambda">Equation&nbsp;<span>7.13</span></a>)</li>
</ul>
<p>are given by the following moment estimators: <span id="eq-GammaHat"><span class="math display">\[
\begin{align*}
\hat{\Gamma}
&amp;= \left(\sum_{i=1}^nZ_iZ_i'\right)^{-1}\left(\sum_{i=1}^nZ_iY_{i2}'\right)\\
&amp;= \left(Z'Z\right)^{-1}(Z'Y_{2})
\end{align*}
\tag{7.15}\]</span></span> <span id="eq-GammaBarHat"><span class="math display">\[
\begin{align*}
\widehat{\bar{\Gamma}}
&amp;= \left(\sum_{i=1}^nZ_iZ_i'\right)^{-1}\left(\sum_{i=1}^nZ_i X_i'\right)\\
&amp;= \left(Z'Z\right)^{-1}(Z'X)
\end{align*}
\tag{7.16}\]</span></span> <span id="eq-lambdaHat"><span class="math display">\[
\begin{align*}
\hat{\lambda}
&amp;= \left(\sum_{i=1}^nZ_iZ_i'\right)^{-1}\left(\sum_{i=1}^nZ_iY_{i1}\right)\\
&amp;= \left(Z'Z\right)^{-1}(Z'Y_1)
\end{align*}
\tag{7.17}\]</span></span> where</p>
<ul>
<li><span class="math inline">\(Z\)</span> is a <span class="math inline">\((n\times \ell)\)</span> matrix with <span class="math inline">\(Z_i'=(Z_{i1}',Z_{i2}')\)</span> in the <span class="math inline">\(i\)</span>th row</li>
<li><span class="math inline">\(X\)</span> is a <span class="math inline">\((n\times K)\)</span> matrix with <span class="math inline">\(X_i'=(X_{i1}',Y_{i2}')\)</span> in the <span class="math inline">\(i\)</span>th row</li>
<li><span class="math inline">\(Y_1\)</span> is a <span class="math inline">\((n\times 1)\)</span> vector with <span class="math inline">\(Y_{i1}\)</span> in the <span class="math inline">\(i\)</span>th element</li>
<li><span class="math inline">\(Y_2\)</span> is a <span class="math inline">\((n\times K_2)\)</span> matrix with <span class="math inline">\(Y_{i2}'\)</span> in the <span class="math inline">\(i\)</span>th row</li>
</ul>
<p>with <span class="math inline">\(i=1,\dots,n\)</span></p>
</section>
<section id="two-stage-least-squares" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="two-stage-least-squares"><span class="header-section-number">7.6</span> Two-Stage Least Squares</h2>
<p>From the reduced form projection equation <a href="#eq-reducedFormModelY1">Equation&nbsp;<span>7.12</span></a> with <span class="math inline">\(\lambda = \bar{\Gamma}\beta\)</span>, we have <span class="math display">\[
\begin{align*}
Y_i
&amp; = Z_i' \lambda + u_{i1}  \\
&amp; = Z_i' \bar{\Gamma}\beta + u_{i1}  \\
E(Z_iu_{i1}) &amp; = 0.   
\end{align*}
\]</span> The parameter <span class="math inline">\(\bar{\Gamma}\)</span> is unknown, but consistently estimable.</p>
<p>Let us, for a moment, assume we would know <span class="math inline">\(\bar{\Gamma}\)</span> and let<br>
<span class="math display">\[
W_{i}=\bar{\Gamma}'Z_i.
\]</span> <span class="math inline">\(W_{i}\)</span> is thus a specific linear combination of the <span class="math inline">\(\ell\)</span> instruments <span class="math inline">\(Z_i\)</span> which allows us to identify the structural parameter <span class="math inline">\(\beta\)</span> in a regression/projection model <span class="math display">\[
\begin{align*}
Y_i &amp; = W_i'\beta + u_{i1}  \\
E(W_iu_{i1}) &amp; = 0.
\end{align*}
\]</span></p>
<p>Thus, if we would know <span class="math inline">\(\bar{\Gamma}\)</span>, we would estimate <span class="math inline">\(\beta\)</span> by <span class="math display">\[
\begin{align*}
\hat{\beta}
&amp; = \left(\sum_{i=1}^nW_iW_i'\right)^{-1}\left(\sum_{i=1}^nW_iY_{i1}\right)\\
&amp; = \left(W'W\right)^{-1}\left(WY_{1}\right)\\
&amp; = \left(\bar{\Gamma}'Z'Z\bar{\Gamma}\right)^{-1}\left(\bar{\Gamma}'Z'Y_{1}\right)\\
\end{align*}
\]</span> To make <span class="math inline">\(\hat{\beta}\)</span> a feasible estimator, we simply replace the unknown <span class="math inline">\(\bar{\Gamma}\)</span> by its estimator <span class="math inline">\(\widehat{\bar{\Gamma}} = \left(\sum_{i=1}^nZ_iZ_i'\right)^{-1}\left(\sum_{i=1}^nZ_iX_{i}'\right)\)</span> (<a href="#eq-GammaBarHat">Equation&nbsp;<span>7.16</span></a>) which yields the <strong>Two State Lease Squares (2SLS) estimator</strong>: <span class="math display">\[
\begin{align*}
\hat{\beta}_{2SLS}
&amp; = \left(\widehat{\bar{\Gamma}}'Z'Z\widehat{\bar{\Gamma}}\right)^{-1}\left(\widehat{\bar{\Gamma}}'Z'Y_{1}\right)\\
&amp; = \left(X'Z\left(Z'Z\right)^{-1}Z'Z\left(Z'Z\right)^{-1}Z'X\right)^{-1}\left(X'Z\left(Z'Z\right)^{-1}Z'Y_{1}\right)\\
&amp; = \left(X'Z\left(Z'Z\right)^{-1}Z'X\right)^{-1}\left(X'Z\left(Z'Z\right)^{-1}Z'Y_{1}\right)\\
&amp; = \left(X'P_ZX\right)^{-1}\left(X'P_ZY_{1}\right),\\
\end{align*}
\]</span> where <span class="math inline">\(P_Z=Z\left(Z'Z\right)^{-1}Z'\)</span> is the projection matrix that projects into the vector spaced spanned by the columns of <span class="math inline">\(Z.\)</span></p>
<p>The projection matrix <span class="math inline">\(P_Z\)</span> motivates the name <strong>“Two Stage Least Squares”</strong> of the estimator <span class="math inline">\(\hat{\beta}_{2SLS}\)</span>. Computing <span class="math inline">\(\hat{\beta}_{2SLS}\)</span> is equivalent to conduct the following two stage procedure:</p>
<ol type="1">
<li>Compute the fitted values when regressing <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span>: <span class="math inline">\(\hat{X}=P_ZX\)</span></li>
<li>Regress <span class="math inline">\(Y_1\)</span> on <span class="math inline">\(\hat{X}:\;\)</span> <span class="math inline">\(\hat{\beta}_{2SLS}=\left(\hat{X}'\hat{X}\right)^{-1}\hat{X}'Y_1\)</span></li>
</ol>
<p>This two stage approach is indeed equivalent to the above direct approach: <span class="math display">\[
\begin{align*}
\left(\hat{X}'\hat{X}\right)^{-1}\hat{X}'Y_1
&amp; = \left((P_ZX)'P_ZX\right)^{-1}\left((P_ZX)'Y_{1}\right)\\
&amp; = \left(X'P_Z'P_ZX\right)^{-1}\left(X'P_Z'Y_{1}\right)\\
&amp; = \left(X'P_ZP_ZX\right)^{-1}\left(X'P_ZY_{1}\right)\\
&amp; = \left(X'P_ZX\right)^{-1}\left(X'P_ZY_{1}\right) = \hat{\beta}_{2SLS}
\end{align*}
\]</span></p>
</section>
<section id="monte-carlo-simulation" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="monte-carlo-simulation"><span class="header-section-number">7.7</span> Monte-Carlo Simulation</h2>
<div class="cell" data-hash="07-Instrumental-Variables_cache/html/unnamed-chunk-1_9fe1621876ce95764f995bd9d10ff516">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>n         <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>B         <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>beta_hat  <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">ncol =</span> B)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B){</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  eps          <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  X_1          <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">m =</span> <span class="dv">3</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  X_2          <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">m =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="fl">0.5</span>) <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> eps</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  Y            <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X_1, X_2) <span class="sc">%*%</span> beta_true <span class="sc">+</span> eps</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  beta_hat[,b] <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> X_1 <span class="sc">+</span> X_2))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>mean_beta_hat_2 <span class="ot">&lt;-</span> <span class="fu">mean</span>(beta_hat[<span class="dv">2</span>,])</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>mean_beta_hat_3 <span class="ot">&lt;-</span> <span class="fu">mean</span>(beta_hat[<span class="dv">3</span>,])</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="do">## MC-Approximations for the Bias:</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>mean_beta_hat_2  <span class="sc">-</span> beta_true[<span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -5.51182e-05</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>mean_beta_hat_3  <span class="sc">-</span> beta_true[<span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4707233</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">rowMeans</span>(beta_hat) <span class="sc">-</span> beta_true, <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.470  0.000  0.471</code></pre>
</div>
</div>
</section>
<section id="real-data-example-college-proximity" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="real-data-example-college-proximity"><span class="header-section-number">7.8</span> Real-Data Example: College Proximity</h2>
<p>The data can be downloaded from the home page of Bruce Hansen <a href="https://www.ssc.wisc.edu/~bhansen/econometrics/">DATA</a></p>
<div class="cell" data-hash="07-Instrumental-Variables_cache/html/unnamed-chunk-2_25e35c06d0632692569bbd2ef3cc7b25">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"tidyverse"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
✔ tibble  3.1.7      ✔ dplyr   1.0.10
✔ tidyr   1.2.1      ✔ stringr 1.4.0 
✔ readr   2.1.2      ✔ forcats 0.5.1 
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
✖ dplyr::recode() masks car::recode()
✖ purrr::some()   masks car::some()</code></pre>
</div>
</div>
</section>
<section id="references" class="level2" data-number="7.9">
<h2 data-number="7.9" class="anchored" data-anchor-id="references"><span class="header-section-number">7.9</span> References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Haavelmo1944" class="csl-entry" role="doc-biblioentry">
Haavelmo, Trygve. 1944. <span>“The Probability Approach in Econometrics.”</span> <em>Econometrica</em> 12: iii-vi+1-115.
</div>
<div id="ref-Hansen2022" class="csl-entry" role="doc-biblioentry">
Hansen, Bruce. 2022. <em><span>E</span>conometrics</em>. Princeton University Press.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>The instrumental variables <span class="math inline">\(Z_{i2}\)</span> correlate with <span class="math inline">\(Y_i\)</span> only indirectly, because they correlate with <span class="math inline">\(X_{i1}\)</span> (by <a href="#eq-IV3">Equation&nbsp;<span>7.8</span></a>). Therefore, since <span class="math inline">\(X_{i1}\)</span> are included in the equation for <span class="math inline">\(Y_i\)</span>, there are no variations left in <span class="math inline">\(Y_i\)</span> that correlate with <span class="math inline">\(Z_{i2}.\)</span> Thus the instrumental variables <span class="math inline">\(Z_{i2}\)</span> have zero coefficients in the equation for <span class="math inline">\(Y_i.\)</span><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Thus if you regress each of the instruments in <span class="math inline">\(Z_{i2}\)</span> on the exogenous variables <span class="math inline">\(Z_{i1}\)</span>, then the residuals of these regressions should still correlate with the endogenous variables in <span class="math inline">\(Y_{i2}.\)</span><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./06-Asymptotics.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Large Sample Inference</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>