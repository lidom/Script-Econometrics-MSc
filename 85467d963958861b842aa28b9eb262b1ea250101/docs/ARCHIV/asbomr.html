<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Econometrics with R</title>
  <meta name="description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Econometrics with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Econometrics with R" />
  
  <meta name="twitter:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2018-10-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="nrf.html">
<link rel="next" href="rwpd.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<!-- font families -->

<link href="https://fonts.googleapis.com/css?family=PT+Sans|Pacifico|Source+Sans+Pro" rel="stylesheet">

<!-- function to adjust height of iframes automatically depending on content loaded -->

<script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';
  }
</script>

<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/default.js"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110299877-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110299877-1');
</script>

<!-- open review block -->

<script async defer src="https://hypothes.is/embed.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://www.oek.wiwi.uni-due.de/en/">Chair of Econometrics at UDE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#a-very-short-introduction-to-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> A Very Short Introduction to <tt>R</tt> and <em>RStudio</em></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="pt.html"><a href="pt.html#random-variables-and-probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#expected-value-mean-and-variance"><i class="fa fa-check"></i>Expected Value, Mean and Variance</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#thetdist"><i class="fa fa-check"></i>The Student t Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-f-distribution"><i class="fa fa-check"></i>The F Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pt.html"><a href="pt.html#RSATDOSA"><i class="fa fa-check"></i><b>2.2</b> Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="pt.html"><a href="pt.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arosur.html"><a href="arosur.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="3.1" data-path="arosur.html"><a href="arosur.html#estimation-of-the-population-mean"><i class="fa fa-check"></i><b>3.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="3.2" data-path="arosur.html"><a href="arosur.html#potsm"><i class="fa fa-check"></i><b>3.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="3.3" data-path="arosur.html"><a href="arosur.html#hypothesis-tests-concerning-the-population-mean"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests Concerning the Population Mean</a><ul>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-p-value"><i class="fa fa-check"></i>The p-Value</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-known"><i class="fa fa-check"></i>Calculating the p-Value when the Standard Deviation is Known</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#SVSSDASE"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-unknown"><i class="fa fa-check"></i>Calculating the p-value When the Standard Deviation is Unknown</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-t-statistic"><i class="fa fa-check"></i>The t-statistic</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="arosur.html"><a href="arosur.html#confidence-intervals-for-the-population-mean"><i class="fa fa-check"></i><b>3.4</b> Confidence Intervals for the Population Mean</a></li>
<li class="chapter" data-level="3.5" data-path="arosur.html"><a href="arosur.html#cmfdp"><i class="fa fa-check"></i><b>3.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="3.6" data-path="arosur.html"><a href="arosur.html#aattggoe"><i class="fa fa-check"></i><b>3.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="3.7" data-path="arosur.html"><a href="arosur.html#scatterplots-sample-covariance-and-sample-correlation"><i class="fa fa-check"></i><b>3.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
<li class="chapter" data-level="3.8" data-path="arosur.html"><a href="arosur.html#exercises-1"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="4.1" data-path="lrwor.html"><a href="lrwor.html#simple-linear-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="4.2" data-path="lrwor.html"><a href="lrwor.html#estimating-the-coefficients-of-the-linear-regression-model"><i class="fa fa-check"></i><b>4.2</b> Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lrwor.html"><a href="lrwor.html#measures-of-fit"><i class="fa fa-check"></i><b>4.3</b> Measures of Fit</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-coefficient-of-determination"><i class="fa fa-check"></i>The Coefficient of Determination</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-standard-error-of-the-regression"><i class="fa fa-check"></i>The Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lrwor.html"><a href="lrwor.html#tlsa"><i class="fa fa-check"></i><b>4.4</b> The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption 1: The Error Term has Conditional Mean of Zero</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-2-independently-and-identically-distributed-data"><i class="fa fa-check"></i>Assumption 2: Independently and Identically Distributed Data</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption 3: Large Outliers are Unlikely</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="lrwor.html"><a href="lrwor.html#tsdotoe"><i class="fa fa-check"></i><b>4.5</b> The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-1"><i class="fa fa-check"></i>Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-2"><i class="fa fa-check"></i>Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-3"><i class="fa fa-check"></i>Simulation Study 3</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="lrwor.html"><a href="lrwor.html#exercises-2"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="htaciitslrm.html"><a href="htaciitslrm.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li class="chapter" data-level="5.1" data-path="htaciitslrm.html"><a href="htaciitslrm.html#testing-two-sided-hypotheses-concerning-the-slope-coefficient"><i class="fa fa-check"></i><b>5.1</b> Testing Two-Sided Hypotheses Concerning the Slope Coefficient</a></li>
<li class="chapter" data-level="5.2" data-path="htaciitslrm.html"><a href="htaciitslrm.html#cifrc"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-confidence-intervals"><i class="fa fa-check"></i>Simulation Study: Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="htaciitslrm.html"><a href="htaciitslrm.html#rwxiabv"><i class="fa fa-check"></i><b>5.3</b> Regression when X is a Binary Variable</a></li>
<li class="chapter" data-level="5.4" data-path="htaciitslrm.html"><a href="htaciitslrm.html#hah"><i class="fa fa-check"></i><b>5.4</b> Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="htaciitslrm.html"><a href="htaciitslrm.html#the-gauss-markov-theorem"><i class="fa fa-check"></i><b>5.5</b> The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-blue-estimator"><i class="fa fa-check"></i>Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="htaciitslrm.html"><a href="htaciitslrm.html#using-the-t-statistic-in-regression-when-the-sample-size-is-small"><i class="fa fa-check"></i><b>5.6</b> Using the t-Statistic in Regression When the Sample Size Is Small</a></li>
<li class="chapter" data-level="5.7" data-path="htaciitslrm.html"><a href="htaciitslrm.html#exercises-3"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rmwmr.html"><a href="rmwmr.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="6.1" data-path="rmwmr.html"><a href="rmwmr.html#omitted-variable-bias"><i class="fa fa-check"></i><b>6.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="6.2" data-path="rmwmr.html"><a href="rmwmr.html#tmrm"><i class="fa fa-check"></i><b>6.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="rmwmr.html"><a href="rmwmr.html#mofimr"><i class="fa fa-check"></i><b>6.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="6.4" data-path="rmwmr.html"><a href="rmwmr.html#ols-assumptions-in-multiple-regression"><i class="fa fa-check"></i><b>6.4</b> OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#simulation-study-imperfect-multicollinearity"><i class="fa fa-check"></i>Simulation Study: Imperfect Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="rmwmr.html"><a href="rmwmr.html#the-distribution-of-the-ols-estimators-in-multiple-regression"><i class="fa fa-check"></i><b>6.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
<li class="chapter" data-level="6.6" data-path="rmwmr.html"><a href="rmwmr.html#exercises-4"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="htaciimr.html"><a href="htaciimr.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence Intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="htaciimr.html"><a href="htaciimr.html#hypothesis-tests-and-confidence-intervals-for-a-single-coefficient"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="htaciimr.html"><a href="htaciimr.html#an-application-to-test-scores-and-the-student-teacher-ratio"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="htaciimr.html"><a href="htaciimr.html#joint-hypothesis-testing-using-the-f-statistic"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the F-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="htaciimr.html"><a href="htaciimr.html#confidence-sets-for-multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-for-multiple-regression"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="htaciimr.html"><a href="htaciimr.html#analysis-of-the-test-score-data-set"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
<li class="chapter" data-level="7.7" data-path="htaciimr.html"><a href="htaciimr.html#exercises-5"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nrf.html"><a href="nrf.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="8.1" data-path="nrf.html"><a href="nrf.html#a-general-strategy-for-modelling-nonlinear-regression-functions"><i class="fa fa-check"></i><b>8.1</b> A General Strategy for Modelling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="8.2" data-path="nrf.html"><a href="nrf.html#nfoasiv"><i class="fa fa-check"></i><b>8.2</b> Nonlinear Functions of a Single Independent Variable</a><ul>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="nrf.html"><a href="nrf.html#interactions-between-independent-variables"><i class="fa fa-check"></i><b>8.3</b> Interactions Between Independent Variables</a></li>
<li class="chapter" data-level="8.4" data-path="nrf.html"><a href="nrf.html#nonlinear-effects-on-test-scores-of-the-student-teacher-ratio"><i class="fa fa-check"></i><b>8.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
<li class="chapter" data-level="8.5" data-path="nrf.html"><a href="nrf.html#exercises-6"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="asbomr.html"><a href="asbomr.html"><i class="fa fa-check"></i><b>9</b> Assessing Studies Based on Multiple Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity"><i class="fa fa-check"></i><b>9.1</b> Internal and External Validity</a></li>
<li class="chapter" data-level="9.2" data-path="asbomr.html"><a href="asbomr.html#ttivomra"><i class="fa fa-check"></i><b>9.2</b> Threats to Internal Validity of Multiple Regression Analysis</a></li>
<li class="chapter" data-level="9.3" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity-when-the-regression-is-used-for-forecasting"><i class="fa fa-check"></i><b>9.3</b> Internal and External Validity when the Regression is Used for Forecasting</a></li>
<li class="chapter" data-level="9.4" data-path="asbomr.html"><a href="asbomr.html#etsacs"><i class="fa fa-check"></i><b>9.4</b> Example: Test Scores and Class Size</a></li>
<li class="chapter" data-level="9.5" data-path="asbomr.html"><a href="asbomr.html#exercises-7"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="rwpd.html"><a href="rwpd.html"><i class="fa fa-check"></i><b>10</b> Regression with Panel Data</a><ul>
<li class="chapter" data-level="10.1" data-path="rwpd.html"><a href="rwpd.html#panel-data"><i class="fa fa-check"></i><b>10.1</b> Panel Data</a></li>
<li class="chapter" data-level="10.2" data-path="rwpd.html"><a href="rwpd.html#PDWTTP"><i class="fa fa-check"></i><b>10.2</b> Panel Data with Two Time Periods: “Before and After” Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="rwpd.html"><a href="rwpd.html#fixed-effects-regression"><i class="fa fa-check"></i><b>10.3</b> Fixed Effects Regression</a><ul>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#estimation-and-inference"><i class="fa fa-check"></i>Estimation and Inference</a></li>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#application-to-traffic-deaths"><i class="fa fa-check"></i>Application to Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="rwpd.html"><a href="rwpd.html#regression-with-time-fixed-effects"><i class="fa fa-check"></i><b>10.4</b> Regression with Time Fixed Effects</a></li>
<li class="chapter" data-level="10.5" data-path="rwpd.html"><a href="rwpd.html#tferaaseffer"><i class="fa fa-check"></i><b>10.5</b> The Fixed Effects Regression Assumptions and Standard Errors for Fixed Effects Regression</a></li>
<li class="chapter" data-level="10.6" data-path="rwpd.html"><a href="rwpd.html#drunk-driving-laws-and-traffic-deaths"><i class="fa fa-check"></i><b>10.6</b> Drunk Driving Laws and Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="rwabdv.html"><a href="rwabdv.html"><i class="fa fa-check"></i><b>11</b> Regression with a Binary Dependent Variable</a><ul>
<li class="chapter" data-level="11.1" data-path="rwabdv.html"><a href="rwabdv.html#binary-dependent-variables-and-the-linear-probability-model"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables and the Linear Probability Model</a></li>
<li class="chapter" data-level="11.2" data-path="rwabdv.html"><a href="rwabdv.html#palr"><i class="fa fa-check"></i><b>11.2</b> Probit and Logit Regression</a><ul>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#probit-regression"><i class="fa fa-check"></i>Probit Regression</a></li>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#logit-regression"><i class="fa fa-check"></i>Logit Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="rwabdv.html"><a href="rwabdv.html#estimation-and-inference-in-the-logit-and-probit-models"><i class="fa fa-check"></i><b>11.3</b> Estimation and Inference in the Logit and Probit Models</a></li>
<li class="chapter" data-level="11.4" data-path="rwabdv.html"><a href="rwabdv.html#application-to-the-boston-hmda-data"><i class="fa fa-check"></i><b>11.4</b> Application to the Boston HMDA Data</a></li>
<li class="chapter" data-level="11.5" data-path="rwabdv.html"><a href="rwabdv.html#exercises-8"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ivr.html"><a href="ivr.html"><i class="fa fa-check"></i><b>12</b> Instrumental Variables Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="ivr.html"><a href="ivr.html#TIVEWASRAASI"><i class="fa fa-check"></i><b>12.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a></li>
<li class="chapter" data-level="12.2" data-path="ivr.html"><a href="ivr.html#TGIVRM"><i class="fa fa-check"></i><b>12.2</b> The General IV Regression Model</a></li>
<li class="chapter" data-level="12.3" data-path="ivr.html"><a href="ivr.html#civ"><i class="fa fa-check"></i><b>12.3</b> Checking Instrument Validity</a></li>
<li class="chapter" data-level="12.4" data-path="ivr.html"><a href="ivr.html#attdfc"><i class="fa fa-check"></i><b>12.4</b> Application to the Demand for Cigarettes</a></li>
<li class="chapter" data-level="12.5" data-path="ivr.html"><a href="ivr.html#where-do-valid-instruments-come-from"><i class="fa fa-check"></i><b>12.5</b> Where Do Valid Instruments Come From?</a></li>
<li class="chapter" data-level="12.6" data-path="ivr.html"><a href="ivr.html#exercises-9"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="eaqe.html"><a href="eaqe.html"><i class="fa fa-check"></i><b>13</b> Experiments and Quasi-Experiments</a><ul>
<li class="chapter" data-level="13.1" data-path="eaqe.html"><a href="eaqe.html#potential-outcomes-causal-effects-and-idealized-experiments"><i class="fa fa-check"></i><b>13.1</b> Potential Outcomes, Causal Effects and Idealized Experiments</a></li>
<li class="chapter" data-level="13.2" data-path="eaqe.html"><a href="eaqe.html#threats-to-validity-of-experiments"><i class="fa fa-check"></i><b>13.2</b> Threats to Validity of Experiments</a></li>
<li class="chapter" data-level="13.3" data-path="eaqe.html"><a href="eaqe.html#experimental-estimates-of-the-effect-of-class-size-reductions"><i class="fa fa-check"></i><b>13.3</b> Experimental Estimates of the Effect of Class Size Reductions</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#experimental-design-and-the-data-set"><i class="fa fa-check"></i>Experimental Design and the Data Set</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#analysis-of-the-star-data"><i class="fa fa-check"></i>Analysis of the STAR Data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="eaqe.html"><a href="eaqe.html#quasi-experiments"><i class="fa fa-check"></i><b>13.4</b> Quasi Experiments</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#the-differences-in-differences-estimator"><i class="fa fa-check"></i>The Differences-in-Differences Estimator</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#regression-discontinuity-estimators"><i class="fa fa-check"></i>Regression Discontinuity Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ittsraf.html"><a href="ittsraf.html"><i class="fa fa-check"></i><b>14</b> Introduction to Time Series Regression and Forecasting</a><ul>
<li class="chapter" data-level="14.1" data-path="ittsraf.html"><a href="ittsraf.html#using-regression-models-for-forecasting"><i class="fa fa-check"></i><b>14.1</b> Using Regression Models for Forecasting</a></li>
<li class="chapter" data-level="14.2" data-path="ittsraf.html"><a href="ittsraf.html#tsdasc"><i class="fa fa-check"></i><b>14.2</b> Time Series Data and Serial Correlation</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#notation-lags-differences-logarithms-and-growth-rates"><i class="fa fa-check"></i>Notation, Lags, Differences, Logarithms and Growth Rates</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ittsraf.html"><a href="ittsraf.html#autoregressions"><i class="fa fa-check"></i><b>14.3</b> Autoregressions</a><ul>
<li><a href="ittsraf.html#autoregressive-models-of-order-p">Autoregressive Models of Order <span class="math inline">\(p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ittsraf.html"><a href="ittsraf.html#cybtmpi"><i class="fa fa-check"></i><b>14.4</b> Can You Beat the Market? (Part I)</a></li>
<li class="chapter" data-level="14.5" data-path="ittsraf.html"><a href="ittsraf.html#apatadlm"><i class="fa fa-check"></i><b>14.5</b> Additional Predictors and The ADL Model</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#forecast-uncertainty-and-forecast-intervals"><i class="fa fa-check"></i>Forecast Uncertainty and Forecast Intervals</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="ittsraf.html"><a href="ittsraf.html#llsuic"><i class="fa fa-check"></i><b>14.6</b> Lag Length Selection Using Information Criteria</a></li>
<li class="chapter" data-level="14.7" data-path="ittsraf.html"><a href="ittsraf.html#nit"><i class="fa fa-check"></i><b>14.7</b> Nonstationarity I: Trends</a></li>
<li class="chapter" data-level="14.8" data-path="ittsraf.html"><a href="ittsraf.html#niib"><i class="fa fa-check"></i><b>14.8</b> Nonstationarity II: Breaks</a></li>
<li class="chapter" data-level="14.9" data-path="ittsraf.html"><a href="ittsraf.html#can-you-beat-the-market-part-ii"><i class="fa fa-check"></i><b>14.9</b> Can You Beat the Market? (Part II)</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="eodce.html"><a href="eodce.html"><i class="fa fa-check"></i><b>15</b> Estimation of Dynamic Causal Effects</a><ul>
<li class="chapter" data-level="15.1" data-path="eodce.html"><a href="eodce.html#the-orange-juice-data"><i class="fa fa-check"></i><b>15.1</b> The Orange Juice Data</a></li>
<li class="chapter" data-level="15.2" data-path="eodce.html"><a href="eodce.html#dynamic-causal-effects"><i class="fa fa-check"></i><b>15.2</b> Dynamic Causal Effects</a></li>
<li class="chapter" data-level="15.3" data-path="eodce.html"><a href="eodce.html#dynamic-multipliers-and-cumulative-dynamic-multipliers"><i class="fa fa-check"></i><b>15.3</b> Dynamic Multipliers and Cumulative Dynamic Multipliers</a></li>
<li class="chapter" data-level="15.4" data-path="eodce.html"><a href="eodce.html#hac-standard-errors"><i class="fa fa-check"></i><b>15.4</b> HAC Standard Errors</a></li>
<li class="chapter" data-level="15.5" data-path="eodce.html"><a href="eodce.html#estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors"><i class="fa fa-check"></i><b>15.5</b> Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors</a></li>
<li class="chapter" data-level="15.6" data-path="eodce.html"><a href="eodce.html#orange-juice-prices-and-cold-weather"><i class="fa fa-check"></i><b>15.6</b> Orange Juice Prices and Cold Weather</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="atitsr.html"><a href="atitsr.html"><i class="fa fa-check"></i><b>16</b> Additional Topics in Time Series Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="atitsr.html"><a href="atitsr.html#vector-autoregressions"><i class="fa fa-check"></i><b>16.1</b> Vector Autoregressions</a></li>
<li class="chapter" data-level="16.2" data-path="atitsr.html"><a href="atitsr.html#ooiatdfglsurt"><i class="fa fa-check"></i><b>16.2</b> Orders of Integration and the DF-GLS Unit Root Test</a></li>
<li class="chapter" data-level="16.3" data-path="atitsr.html"><a href="atitsr.html#cointegration"><i class="fa fa-check"></i><b>16.3</b> Cointegration</a></li>
<li class="chapter" data-level="16.4" data-path="atitsr.html"><a href="atitsr.html#volatility-clustering-and-autoregressive-conditional-heteroskedasticity"><i class="fa fa-check"></i><b>16.4</b> Volatility Clustering and Autoregressive Conditional Heteroskedasticity</a><ul>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#arch-and-garch-models"><i class="fa fa-check"></i>ARCH and GARCH Models</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#application-to-stock-price-volatility"><i class="fa fa-check"></i>Application to Stock Price Volatility</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#summary-8"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Econometrics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div class = rmdreview>
This book is in <b>Open Review</b>. We want your feedback to make the book better for you and other students. You may annotate some text by <span style="background-color: #3297FD; color: white">selecting it with the cursor</span> and then click the <i class="h-icon-annotate"></i> on the pop-up menu. You can also see the annotations of others: click the <i class="h-icon-chevron-left"></i> in the upper right hand corner of the page <i class="fa fa-arrow-circle-right  fa-rotate-315" aria-hidden="true"></i>
</div>
<div id="asbomr" class="section level1">
<h1><span class="header-section-number">9</span> Assessing Studies Based on Multiple Regression</h1>
<p>The majority of Chapter 9 of the book is of a theoretical nature. Therefore this section briefly reviews the concepts of internal and external validity in general and discusses examples of threats to internal and external validity of multiple regression models. We discuss consequences of</p>
<ul>
<li>misspecification of the functional form of the regression function</li>
<li>measurement errors</li>
<li>missing data and sample selection</li>
<li>simultaneous causality</li>
</ul>
<p>as well as sources of inconsistency of OLS standard errors. We also review concerns regarding internal validity and external validity in the context of forecasting using regression models.</p>
<p>The chapter closes with an application in <tt>R</tt> where we assess whether results found by multiple regression using the <tt>CASchools</tt> data can be generalized to school districts of another federal state of the United States.</p>
<p>For a more detailed treatment of these topics we encourage you to work through Chapter 9 of the book.</p>
<p>The following packages and their dependencies are needed for reproduction of the code chunks presented throughout this chapter:</p>
<ul>
<li><tt>AER</tt></li>
<li><tt>mvtnorm</tt></li>
<li><tt>stargazer</tt></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(AER)
<span class="kw">library</span>(mvtnorm)
<span class="kw">library</span>(stargazer)</code></pre></div>
<div id="internal-and-external-validity" class="section level2">
<h2><span class="header-section-number">9.1</span> Internal and External Validity</h2>
<div class="keyconcept">
<h3 class="right">
Key Concept 9.1
</h3>
<h3 class="left">
Internal and External Validity
</h3>
<p>A statistical analysis has <em>internal</em> validity if the statistical inference made about causal effects are valid for the considered population.</p>
<p>An analysis is said to have <em>external</em> validity if inferences and conclusion are valid for the studies’ population and can be generalized to other populations and settings.</p>
</div>
<div id="threats-to-internal-validity" class="section level4 unnumbered">
<h4>Threats to Internal Validity</h4>
<p>There are two conditions for internal validity to exist:</p>
<ol style="list-style-type: decimal">
<li><p>The estimator of the causal effect, which is measured the coefficient(s) of interest, should be unbiased and consistent.</p></li>
<li><p>Statistical inference is valid, that is, hypothesis tests should have the desired size and confidence intervals should have the desired coverage probability.</p></li>
</ol>
<p>In multiple regression, we estimate the model coefficients using OLS. Thus for condition 1. to be fulfilled we need the OLS estimator to be unbiased and consistent. For the second condition to be valid, the standard errors must be valid such that hypothesis testing and computation of confidence intervals yield results that are trustworthy. Remember that a sufficient condition for conditions 1. and 2. to be fulfilled is that the assumptions of Key Concept 6.4 hold.</p>
</div>
<div id="threats-to-external-validity" class="section level4 unnumbered">
<h4>Threats to External Validity</h4>
<p>External validity might be invalid</p>
<ul>
<li><p>if there are differences between the population studied and the population of interest.</p></li>
<li><p>if there are differences in the <em>settings</em> of the considered populations, e.g., the legal framework or the time of the investigation.</p></li>
</ul>
</div>
</div>
<div id="ttivomra" class="section level2">
<h2><span class="header-section-number">9.2</span> Threats to Internal Validity of Multiple Regression Analysis</h2>
<p>This section treats five sources that cause the OLS estimator in (multiple) regression models to be biased and inconsistent for the causal effect of interest and discusses possible remedies. All five sources imply a violation of the first least squares assumption presented in Key Concept 6.4.</p>
<p>This sections treats:</p>
<ul>
<li><p>omitted variable Bias</p></li>
<li><p>misspecification of the functional form</p></li>
<li><p>measurement errors</p></li>
<li><p>missing data and sample selection</p></li>
<li><p>simultaneous causality bias</p></li>
</ul>
<p>Beside these threats for consistency of the estimator, we also briefly discuss causes of inconsistent estimation of OLS standard errors.</p>
<div id="omitted-variable-bias-1" class="section level4 unnumbered">
<h4>Omitted Variable Bias</h4>
<div class="keyconcept">
<h3 class="right">
Key Concept 9.2
</h3>
<h3 class="left">
Omitted Variable Bias: Should I include More Variables in My Regression?
</h3>
<p>Inclusion of additional variables reduces the risk of omitted variable bias but may increase the variance of the estimator of the coefficient of interest.</p>
<p>We present some guidelines that help deciding whether to include an additional variable:</p>
<ol style="list-style-type: decimal">
<li><p>Specify the coefficient(s) of interest</p></li>
<li><p>Identify the most important potential sources of omitted variable bias by using knowledge available <em>before</em> estimating the model. You should end up with a baseline specification and a set of regressors that are questionable</p></li>
<li><p>Use different model specifications to test whether questionable regressors have coefficients different from zero</p></li>
<li><p>Use tables to provide full disclosure of your results, i.e., present different model specifications that both support your argument and enable the reader to see the effect of including questionable regressors</p></li>
</ol>
</div>
<p>By now you should be aware of omitted variable bias and its consequences. Key Concept 9.2 gives some guidelines on how to proceed if there are control variables that possibly allow to reduce omitted variable bias. If including additional variables to mitigate the bias is not an option because there are no adequate controls, there are different approaches to solve the problem:</p>
<ul>
<li><p>usage of panel data methods (discussed in Chapter <a href="rwpd.html#rwpd">10</a>)</p></li>
<li><p>usage of instrumental variables regression (discussed in Chapter <a href="ivr.html#ivr">12</a>)</p></li>
<li><p>usage of a randomized control experiment (discussed in Chapter <a href="eaqe.html#eaqe">13</a>)</p></li>
</ul>
</div>
<div id="misspecification-of-the-functional-form-of-the-regression-function" class="section level4 unnumbered">
<h4>Misspecification of the Functional Form of the Regression Function</h4>
<p>If the population regression function is nonlinear but the regression function is linear, the functional form of the regression model is misspecified. This leads to a bias of the OLS estimator.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 9.3
</h3>
<h3 class="left">
Functional Form Misspecification
</h3>
<p>A regression suffers from misspecification of the functional form when the functional form of the estimated regression model differs from the functional form of the population regression function. Functional form misspecification leads to biased and inconsistent coefficient estimators. A way to detect functional form misspecification is to plot the estimated regression function and the data. This may also be helpful to choose the correct functional form.</p>
</div>
<p>It is easy to come up with examples of misspecification of the functional form: consider the case where the population regression function is <span class="math display">\[Y_i = X_i^2\]</span> but the model used is <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + u_i.\]</span> Clearly, the regression function is misspecified here. We now simulate data and visualize this.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">3</span>)

<span class="co"># simulate data set</span>
X &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">100</span>, <span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)
Y &lt;-<span class="st"> </span>X<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>)

<span class="co"># estimate the regression function</span>
ms_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X)
ms_mod</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Coefficients:
## (Intercept)            X  
##     8.11363     -0.04684</code></pre>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the data</span>
<span class="kw">plot</span>(X, Y, 
     <span class="dt">main =</span> <span class="st">&quot;Misspecification of Functional Form&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>)

<span class="co"># plot the linear regression line</span>
<span class="kw">abline</span>(ms_mod, 
       <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>,
       <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-380-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>It is evident that the regression errors are relatively small for observations close to <span class="math inline">\(X=-3\)</span> and <span class="math inline">\(X=3\)</span> but that the errors increase for <span class="math inline">\(X\)</span> values closer to zero and even more for values beyond <span class="math inline">\(-4\)</span> and <span class="math inline">\(4\)</span>. Consequences are drastic: the intercept is estimated to be <span class="math inline">\(8.1\)</span> and for the slope parameter we obtain an estimate obviously very close to zero. This issue does not disappear as the number of observations is increased because OLS is biased <em>and</em> inconsistent due to the misspecification of the regression function.</p>
</div>
<div id="measurement-error-and-errors-in-variables-bias" class="section level4 unnumbered">
<h4>Measurement Error and Errors-in-Variables Bias</h4>
<div class="keyconcept">
<h3 class="right">
Key Concept 9.4
</h3>
<h3 class="left">
Errors-in-Variable Bias
</h3>
<p>When independent variables are measured imprecisely, we speak of errors-in-variables bias. This bias does not disappear if the sample size is large. If the measurement error has mean zero and is independent of the affected variable, the OLS estimator of the respective coefficient is biased towards zero.</p>
</div>
Suppose you are incorrectly measuring the single regressor <span class="math inline">\(X_i\)</span> so that there is a measurement error and you observe <span class="math inline">\(\overset{\sim}{X}_i\)</span> instead of <span class="math inline">\(X_i\)</span>. Then, instead of estimating the population the regression model <span class="math display">\[ Y_i = \beta_0 + \beta_1 X_i + u_i \]</span> you end up estimating
<span class="math display">\[\begin{align*}
  Y_i =&amp; \, \beta_0 + \beta_1 \overset{\sim}{X}_i + \underbrace{\beta_1 (X_i - \overset{\sim}{X}_i) + u_i}_{=v_i} \\
  Y_i =&amp; \, \beta_0 + \beta_1 \overset{\sim}{X}_i + v_i
\end{align*}\]</span>
<p>where <span class="math inline">\(\overset{\sim}{X}_i\)</span> and the error term <span class="math inline">\(v_i\)</span> are correlated. Thus OLS would be biased and inconsistent for the true <span class="math inline">\(\beta_1\)</span> in this example. One can show that direction and strength of the bias depend on the correlation between the observed regressor, <span class="math inline">\(\overset{\sim}{X}_i\)</span>, and the measurement error, <span class="math inline">\(w_i =X_i - \overset{\sim}{X}_i\)</span>. This correlation in turn depends on the type of the measurement error made.</p>
<p>The classical measurement error model assumes that the measurement error, <span class="math inline">\(w_i\)</span>, has zero mean and that it is uncorrelated with the variable, <span class="math inline">\(X_i\)</span>, and the error term of the population regression model, <span class="math inline">\(u_i\)</span>:</p>
<span class="math display">\[\begin{equation}
  \overset{\sim}{X}_i = X_i + w_i, \ \ \rho_{w_i,u_i}=0, \ \ \rho_{w_i,X_i}=0 
\end{equation}\]</span>
Then it holds that
<span class="math display" id="eq:cmembias">\[\begin{equation}
  \widehat{\beta}_1 \xrightarrow{p}{\frac{\sigma_{X}^2}{\sigma_{X}^2 + \sigma_{w}^2}} \beta_1 \tag{9.1}
\end{equation}\]</span>
<p>which implies inconsistency as <span class="math inline">\(\sigma_{X}^2, \sigma_{w}^2 &gt; 0\)</span> such that the fraction in <a href="asbomr.html#eq:cmembias">(9.1)</a> is smaller than <span class="math inline">\(1\)</span>. Note that there are two extreme cases:</p>
<ol style="list-style-type: decimal">
<li><p>If there is no measurement error, <span class="math inline">\(\sigma_{w}^2=0\)</span> such that <span class="math inline">\(\widehat{\beta}_1 \xrightarrow{p}{\beta_1}\)</span>.</p></li>
<li><p>if <span class="math inline">\(\sigma_{w}^2 \gg \sigma_{X}^2\)</span> we have <span class="math inline">\(\widehat{\beta}_1 \xrightarrow{p}{0}\)</span>. This is the case if the measurement error is so large that there essentially is no information on <span class="math inline">\(X\)</span> in the data that can be used to estimate <span class="math inline">\(\beta_1\)</span>.</p></li>
</ol>
<p>The most obvious way to deal with errors-in-variables bias is to use an accurately measured <span class="math inline">\(X\)</span>. If this not possible, instrumental variables regression is an option. One might also deal with the issue by using a mathematical model of the measurement error and adjust the estimates appropriately: if it is plausible that the classical measurement error model applies and if there is information that can be used to estimate the ratio in equation <a href="asbomr.html#eq:cmembias">(9.1)</a>, one could compute an estimate that corrects for the downward bias.</p>
For example, consider two bivariate normally distributed random variables <span class="math inline">\(X,Y\)</span>. It is a <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Bivariate_case">well known result</a> that the conditional expectation function of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> has the form
<span class="math display" id="eq:bnormexpfn">\[\begin{align}
  E(Y\vert X) = E(Y) + \rho_{X,Y} \frac{\sigma_{Y}}{\sigma_{X}}\left[X-E(X)\right]. \tag{9.2} 
\end{align}\]</span>
Thus for
<span class="math display" id="eq:bvnormd">\[\begin{align}
  (X, Y) \sim \mathcal{N}\left[\begin{pmatrix}50\\ 100\end{pmatrix},\begin{pmatrix}10 &amp; 5 \\ 5 &amp; 10 \end{pmatrix}\right] \tag{9.3}
\end{align}\]</span>
according to <a href="asbomr.html#eq:bnormexpfn">(9.2)</a>, the population regression function is
<span class="math display" id="eq:bnormregfun">\[\begin{align*}
  Y_i =&amp; \, 100 + 0.5 (X_i - 50) \\
      =&amp; \, 75 + 0.5 X_i. \tag{9.4}
\end{align*}\]</span>
<p>Now suppose you gather data on <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, but that you can only measure <span class="math inline">\(\overset{\sim}{X_i} = X_i + w_i\)</span> with <span class="math inline">\(w_i \overset{i.i.d.}{\sim} \mathcal{N}(0,10)\)</span>. Since the <span class="math inline">\(w_i\)</span> are independent of the <span class="math inline">\(X_i\)</span>, there is no correlation between the <span class="math inline">\(X_i\)</span> and the <span class="math inline">\(w_i\)</span> so that we have a case of the classical measurement error model. We now illustrate this example in <tt>R</tt> using the package <tt>mvtnorm</tt> <span class="citation">(Genz, Bretz, Miwa, Mi, &amp; Hothorn, <a href="#ref-R-mvtnorm">2018</a>)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># load the package &#39;mvtnorm&#39; and simulate bivariate normal data</span>
<span class="kw">library</span>(mvtnorm)
dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="kw">rmvnorm</span>(<span class="dv">1000</span>, <span class="kw">c</span>(<span class="dv">50</span>, <span class="dv">100</span>), 
          <span class="dt">sigma =</span> <span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">5</span>), <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">10</span>))))

<span class="co"># set columns names</span>
<span class="kw">colnames</span>(dat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X&quot;</span>, <span class="st">&quot;Y&quot;</span>)</code></pre></div>
<p>We now estimate a simple linear regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> using this sample data and run the same regression again but this time we add i.i.d. <span class="math inline">\(\mathcal{N}(0,10)\)</span> errors added to <span class="math inline">\(X\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the model (without measurement error)</span>
noerror_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> dat)

<span class="co"># estimate the model (with measurement error in X)</span>
dat<span class="op">$</span>X &lt;-<span class="st"> </span>dat<span class="op">$</span>X <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1000</span>, <span class="dt">sd =</span> <span class="kw">sqrt</span>(<span class="dv">10</span>))
error_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> dat)

<span class="co"># print estimated coefficients to console</span>
noerror_mod<span class="op">$</span>coefficients</code></pre></div>
<pre><code>## (Intercept)           X 
##  76.3002047   0.4755264</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">error_mod<span class="op">$</span>coefficients</code></pre></div>
<pre><code>## (Intercept)           X 
##   87.276004    0.255212</code></pre>
<p>Next, we visualize the results and compare with the population regression function.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot sample data</span>
<span class="kw">plot</span>(dat<span class="op">$</span>X, dat<span class="op">$</span>Y, 
     <span class="dt">pch =</span> <span class="dv">20</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)

<span class="co"># add population regression function</span>
<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(<span class="dv">75</span>, <span class="fl">0.5</span>), 
       <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>,
       <span class="dt">lwd  =</span> <span class="fl">1.5</span>)

<span class="co"># add estimated regression functions</span>
<span class="kw">abline</span>(noerror_mod, 
       <span class="dt">col =</span> <span class="st">&quot;purple&quot;</span>,
       <span class="dt">lwd  =</span> <span class="fl">1.5</span>)

<span class="kw">abline</span>(error_mod, 
       <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>,
       <span class="dt">lwd  =</span> <span class="fl">1.5</span>)

<span class="co"># add legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,
       <span class="dt">bg =</span> <span class="st">&quot;transparent&quot;</span>,
       <span class="dt">cex =</span> <span class="fl">0.8</span>,
       <span class="dt">lty =</span> <span class="dv">1</span>,
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;darkgreen&quot;</span>, <span class="st">&quot;purple&quot;</span>, <span class="st">&quot;darkred&quot;</span>), 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Population&quot;</span>, <span class="st">&quot;No Errors&quot;</span>, <span class="st">&quot;Errors&quot;</span>))</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-385-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>In the situation without measurement error, the estimated regression function is close to the population regression function. Things are different when we use the mismeasured regressor <span class="math inline">\(X\)</span>: both the estimate for the intercept and the estimate for the coefficient on <span class="math inline">\(X\)</span> differ considerably from results obtained using the “clean” data on <span class="math inline">\(X\)</span>. In particular <span class="math inline">\(\widehat{\beta}_1 = 0.255\)</span>, so there is a downward bias. We are in the comfortable situation to know <span class="math inline">\(\sigma_X^2\)</span> and <span class="math inline">\(\sigma^2_w\)</span>. This allows us to correct for the bias using <a href="asbomr.html#eq:cmembias">(9.1)</a>. Using this information we obtain the biased-corrected estimate <span class="math display">\[\frac{\sigma_X^2 + \sigma_w^2}{\sigma_X^2} \cdot \widehat{\beta}_1 = \frac{10+10}{10} \cdot 0.255 = 0.51\]</span> which is quite close to <span class="math inline">\(\beta_1=0.5\)</span>, the true coefficient from the population regression function.</p>
<p>Bear in mind that the above analysis uses a single sample. Thus one may argue that the results are just a coincidence. Can you show the contrary using a simulation study?</p>
</div>
<div id="missing-data-and-sample-selection" class="section level4 unnumbered">
<h4>Missing Data and Sample Selection</h4>
<div class="keyconcept">
<h3 class="right">
Key Concept 9.5
</h3>
<h3 class="left">
Sample Selection Bias
</h3>
<p>When the sampling process influences the availability of data and when there is a relation of this sampling process to the dependent variable that goes beyond the dependence on the regressors, we say that there is a sample selection bias. This bias is due to correlation between one or more regressors and the error term. Sample selection implies both bias and inconsistency of the OLS estimator.</p>
</div>
<p>There are three cases of sample selection. Only one of them poses a threat to internal validity of a regression study. The three cases are:</p>
<ol style="list-style-type: decimal">
<li><p>Data are missing at random.</p></li>
<li><p>Data are missing based on the value of a regressor.</p></li>
<li><p>Data are missing due to a selection process which is related to the dependent variable.</p></li>
</ol>
<p>Let us jump back to the example of variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> distributed as stated in equation <a href="asbomr.html#eq:bvnormd">(9.3)</a> and illustrate all three cases using <tt>R</tt>.</p>
<p>If data are missing at random, this is nothing but loosing observations. For example, loosing <span class="math inline">\(50\%\)</span> of the sample would be the same as never having seen the (randomly chosen) half of the sample observed. Therefore, missing data do not introduce an estimation bias and “only” lead to less efficient estimators.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># simulate data</span>
dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="kw">rmvnorm</span>(<span class="dv">1000</span>, <span class="kw">c</span>(<span class="dv">50</span>, <span class="dv">100</span>), 
          <span class="dt">sigma =</span> <span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">5</span>), <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">10</span>))))

<span class="kw">colnames</span>(dat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X&quot;</span>, <span class="st">&quot;Y&quot;</span>)

<span class="co"># mark 500 randomly selected observations</span>
id &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>, <span class="dt">size =</span> <span class="dv">500</span>)

<span class="kw">plot</span>(dat<span class="op">$</span>X[<span class="op">-</span>id], 
     dat<span class="op">$</span>Y[<span class="op">-</span>id], 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">cex =</span> <span class="fl">0.8</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)

<span class="kw">points</span>(dat<span class="op">$</span>X[id], 
       dat<span class="op">$</span>Y[id],
       <span class="dt">cex =</span> <span class="fl">0.8</span>,
       <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>, 
       <span class="dt">pch =</span> <span class="dv">20</span>)

<span class="co"># add the population regression function</span>
<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(<span class="dv">75</span>, <span class="fl">0.5</span>), 
       <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>,
       <span class="dt">lwd  =</span> <span class="fl">1.5</span>)

<span class="co"># add the estimated regression function for the full sample</span>
<span class="kw">abline</span>(noerror_mod)

<span class="co"># estimate model case 1 and add the regression line</span>
dat &lt;-<span class="st"> </span>dat[<span class="op">-</span>id, ]

c1_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(dat<span class="op">$</span>Y <span class="op">~</span><span class="st"> </span>dat<span class="op">$</span>X, <span class="dt">data =</span> dat)
<span class="kw">abline</span>(c1_mod, <span class="dt">col =</span> <span class="st">&quot;purple&quot;</span>)

<span class="co"># add a legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,
       <span class="dt">lty =</span> <span class="dv">1</span>,
       <span class="dt">bg =</span> <span class="st">&quot;transparent&quot;</span>,
       <span class="dt">cex =</span> <span class="fl">0.8</span>,
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;darkgreen&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;purple&quot;</span>), 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Population&quot;</span>, <span class="st">&quot;Full sample&quot;</span>, <span class="st">&quot;500 obs. randomly selected&quot;</span>))</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-388-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>The gray dots represent the <span class="math inline">\(500\)</span> discarded observations. When using the remaining observations, the estimation results deviate only marginally from the results obtained using the full sample.</p>
<p>Selecting data randomly based on the value of a regressor has also the effect of reducing the sample size and does not introduce estimation bias. We will now drop all observations with <span class="math inline">\(X &gt; 45\)</span>, estimate the model again and compare.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># simulate data</span>
dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="kw">rmvnorm</span>(<span class="dv">1000</span>, <span class="kw">c</span>(<span class="dv">50</span>, <span class="dv">100</span>), 
          <span class="dt">sigma =</span> <span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">5</span>), <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">10</span>))))

<span class="kw">colnames</span>(dat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X&quot;</span>, <span class="st">&quot;Y&quot;</span>)

<span class="co"># mark observations</span>
id &lt;-<span class="st"> </span>dat<span class="op">$</span>X <span class="op">&gt;=</span><span class="st"> </span><span class="dv">45</span>

<span class="kw">plot</span>(dat<span class="op">$</span>X[<span class="op">-</span>id], 
     dat<span class="op">$</span>Y[<span class="op">-</span>id], 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">cex =</span> <span class="fl">0.8</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)

<span class="kw">points</span>(dat<span class="op">$</span>X[id], 
       dat<span class="op">$</span>Y[id], 
       <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>,
       <span class="dt">cex =</span> <span class="fl">0.8</span>,
       <span class="dt">pch =</span> <span class="dv">20</span>)

<span class="co"># add population regression function</span>
<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(<span class="dv">75</span>, <span class="fl">0.5</span>), 
       <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>,
       <span class="dt">lwd  =</span> <span class="fl">1.5</span>)

<span class="co"># add estimated regression function for full sample</span>
<span class="kw">abline</span>(noerror_mod)

<span class="co"># estimate model case 1, add regression line</span>
dat &lt;-<span class="st"> </span>dat[<span class="op">-</span>id, ]

c2_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(dat<span class="op">$</span>Y <span class="op">~</span><span class="st"> </span>dat<span class="op">$</span>X, <span class="dt">data =</span> dat)
<span class="kw">abline</span>(c2_mod, <span class="dt">col =</span> <span class="st">&quot;purple&quot;</span>)

<span class="co"># add legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,
       <span class="dt">lty =</span> <span class="dv">1</span>,
       <span class="dt">bg =</span> <span class="st">&quot;transparent&quot;</span>,
       <span class="dt">cex =</span> <span class="fl">0.8</span>,
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;darkgreen&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;purple&quot;</span>), 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Population&quot;</span>, <span class="st">&quot;Full sample&quot;</span>, <span class="st">&quot;Obs. with X &lt;= 45&quot;</span>))</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-389-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>Note that although we dropped more than <span class="math inline">\(90\%\)</span> of all observations, the estimated regression function is very close to the line estimated based on the full sample.</p>
<p>In the third case we face sample selection bias. We can illustrate this by using only observations with <span class="math inline">\(X_i&lt;55\)</span> and <span class="math inline">\(Y_i&gt;100\)</span>. These observations are easily identified using the function <tt>which()</tt> and logical operators: <code>which(dat$X &lt; 55 &amp; dat$Y &gt; 100)</code></p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># simulate data</span>
dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="kw">rmvnorm</span>(<span class="dv">1000</span>, <span class="kw">c</span>(<span class="dv">50</span>,<span class="dv">100</span>), 
          <span class="dt">sigma =</span> <span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">5</span>), <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">10</span>))))

<span class="kw">colnames</span>(dat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X&quot;</span>,<span class="st">&quot;Y&quot;</span>)

<span class="co"># mark observations</span>
id &lt;-<span class="st"> </span><span class="kw">which</span>(dat<span class="op">$</span>X <span class="op">&lt;=</span><span class="st"> </span><span class="dv">55</span> <span class="op">&amp;</span><span class="st"> </span>dat<span class="op">$</span>Y <span class="op">&gt;=</span><span class="st"> </span><span class="dv">100</span>)

<span class="kw">plot</span>(dat<span class="op">$</span>X[<span class="op">-</span>id], 
       dat<span class="op">$</span>Y[<span class="op">-</span>id], 
       <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>,
       <span class="dt">cex =</span> <span class="fl">0.8</span>,
       <span class="dt">pch =</span> <span class="dv">20</span>,
       <span class="dt">xlab =</span> <span class="st">&quot;X&quot;</span>,
       <span class="dt">ylab =</span> <span class="st">&quot;Y&quot;</span>)

<span class="kw">points</span>(dat<span class="op">$</span>X[id], 
     dat<span class="op">$</span>Y[id], 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">cex =</span> <span class="fl">0.8</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>)

<span class="co"># add population regression function</span>
<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(<span class="dv">75</span>, <span class="fl">0.5</span>), 
       <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>,
       <span class="dt">lwd  =</span> <span class="fl">1.5</span>)

<span class="co"># add estimated regression function for full sample</span>
<span class="kw">abline</span>(noerror_mod)

<span class="co"># estimate model case 1, add regression line</span>
dat &lt;-<span class="st"> </span>dat[id, ]

c3_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(dat<span class="op">$</span>Y <span class="op">~</span><span class="st"> </span>dat<span class="op">$</span>X, <span class="dt">data =</span> dat)
<span class="kw">abline</span>(c3_mod, <span class="dt">col =</span> <span class="st">&quot;purple&quot;</span>)

<span class="co"># add legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,
       <span class="dt">lty =</span> <span class="dv">1</span>,
       <span class="dt">bg =</span> <span class="st">&quot;transparent&quot;</span>,
       <span class="dt">cex =</span> <span class="fl">0.8</span>,
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;darkgreen&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;purple&quot;</span>), 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Population&quot;</span>, <span class="st">&quot;Full sample&quot;</span>, <span class="st">&quot;X &lt;= 55 &amp; Y &gt;= 100&quot;</span>))</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-390-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>We see that the selection process leads to biased estimation results.</p>
<p>There are methods that allow to correct for sample selection bias. However, these methods are beyond the scope of the book and are therefore not considered here. The concept of sample selection bias is summarized in Key Concept 9.5.</p>
</div>
<div id="simultaneous-causality" class="section level4 unnumbered">
<h4>Simultaneous Causality</h4>
<div class="keyconcept">
<h3 class="right">
Key Concept 9.6
</h3>
<h3 class="left">
Simultaneous Causality Bias
</h3>
<p>So far we have assumed that the changes in the independent variable <span class="math inline">\(X\)</span> are responsible for changes in the dependent variable <span class="math inline">\(Y\)</span>. When the reverse is also true, we say that there is <em>simultaneous causality</em> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. This reverse causality leads to correlation between <span class="math inline">\(X\)</span> and the error in the population regression of interest such that the coefficient on <span class="math inline">\(X\)</span> is estimated with bias.</p>
</div>
<p>Suppose we are interested in estimating the effect of a <span class="math inline">\(20\%\)</span> increase in cigarettes prices on cigarette consumption in the United States using a multiple regression model. This may be investigated using the dataset <tt>CigarettesSW</tt> which is part of the <tt>AER</tt> package. <tt>CigarettesSW</tt> is a panel data set on cigarette consumption for all 48 continental U.S. federal states from 1985-1995 and provides data on economic indicators and average local prices, taxes and per capita pack consumption.</p>
<p>After loading the data set, we pick observations for the year 1995 and plot logarithms of the per pack price, <tt>price</tt>, against pack consumption, <tt>packs</tt>, and estimate a simple linear regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the data set</span>
<span class="kw">library</span>(AER)
<span class="kw">data</span>(<span class="st">&quot;CigarettesSW&quot;</span>)
c1995 &lt;-<span class="st"> </span><span class="kw">subset</span>(CigarettesSW, year <span class="op">==</span><span class="st"> &quot;1995&quot;</span>)

<span class="co"># estimate the model</span>
cigcon_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(packs) <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(price), <span class="dt">data =</span> c1995)
cigcon_mod</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(packs) ~ log(price), data = c1995)
## 
## Coefficients:
## (Intercept)   log(price)  
##      10.850       -1.213</code></pre>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the estimated regression line and the data</span>
<span class="kw">plot</span>(<span class="kw">log</span>(c1995<span class="op">$</span>price), <span class="kw">log</span>(c1995<span class="op">$</span>packs),
     <span class="dt">xlab =</span> <span class="st">&quot;ln(Price)&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;ln(Consumption)&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Demand for Cigarettes&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>)

<span class="kw">abline</span>(cigcon_mod, 
       <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>, 
       <span class="dt">lwd =</span> <span class="fl">1.5</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-394-1.png" width="672" /></p>
</div>
<p>Remember from Chapter <a href="nrf.html#nrf">8</a> that, due to the log-log specification, in the population regression the coefficient on the logarithm of price is interpreted as the price elasticity of consumption. The estimated coefficient suggests that a <span class="math inline">\(1\%\)</span> increase in cigarettes prices reduces cigarette consumption by about <span class="math inline">\(1.2\%\)</span>, on average. Have we estimated a demand curve? The answer is no: this is a classic example of simultaneous causality, see Key Concept 9.6. The observations are market equilibria which are determined by both changes in supply and changes in demand. Therefore the price is correlated with the error term and the OLS estimator is biased. We can neither estimate a demand nor a supply curve consistently using this approach.</p>
<p>We will return to this issue in Chapter <a href="ivr.html#ivr">12</a> which treats instrumental variables regression, an approach that allows consistent estimation when there is simultaneous causality.</p>
</div>
<div id="sources-of-inconsistency-of-ols-standard-errors" class="section level4 unnumbered">
<h4>Sources of Inconsistency of OLS Standard Errors</h4>
<p>There are two central threats to computation of consistent OLS standard errors:</p>
<ol style="list-style-type: decimal">
<li><p>Heteroskedasticity: implications of heteroskedasticiy have been discussed in Chapter <a href="htaciitslrm.html#htaciitslrm">5</a>. Heteroskedasticity-robust standard errors as computed by the function <tt>vcovHC()</tt> from the package <tt>sandwich</tt> produce valid standard errors under heteroskedasticity.</p></li>
<li><p>Serial correlation: if the population regression error is correlated across observations, we have serial correlation. This often happens in applications where repeated observations are used, e.g., in panel data studies. As for heteroskedasticity, <tt>vcovHC()</tt> can be used to obtain valid standard errors when there is serial correlation.</p></li>
</ol>
<p>Inconsistent standard errors will produce invalid hypothesis tests and wrong confidence intervals. For example, when testing the null that some model coefficient is zero, we cannot trust the outcome anymore because the test may fail to have a size of <span class="math inline">\(5\%\)</span> due to the wrongly computed standard error.</p>
<p>Key Concept 9.7 summarizes all threats to internal validity discussed above.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 9.7
</h3>
<h3 class="left">
Threats to Internal Validity of a Regression Study
</h3>
<p>The five primary threats to internal validity of a multiple regression study are:</p>
<ol style="list-style-type: decimal">
<li><p>Omitted variables</p></li>
<li><p>Misspecification of functional form</p></li>
<li><p>Errors in variables (measurement errors in the regressors)</p></li>
<li><p>Sample selection</p></li>
<li><p>Simultaneous causality</p></li>
</ol>
<p>All these threats lead to failure of the first least squares assumption <span class="math display">\[E(u_i\vert X_{1i},\dots ,X_{ki}) \neq 0\]</span> so that the OLS estimator is biased <em>and</em> inconsistent. <br></p>
<p>Furthermore, if one does not adjust for heteroskedasticity <em>and</em>/<em>or</em> serial correlation, incorrect standard errors may be a threat to internal validity of the study.</p>
</div>
</div>
</div>
<div id="internal-and-external-validity-when-the-regression-is-used-for-forecasting" class="section level2">
<h2><span class="header-section-number">9.3</span> Internal and External Validity when the Regression is Used for Forecasting</h2>
<p>Recall the regression of test scores on the student-teacher ratio (<span class="math inline">\(STR\)</span>) performed in Chapter <a href="lrwor.html#lrwor">4</a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">linear_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>STR, <span class="dt">data =</span> CASchools)
linear_model</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Coefficients:
## (Intercept)          STR  
##      698.93        -2.28</code></pre>
<p>The estimated regression function was</p>
<p><span class="math display">\[ \widehat{TestScore} = 698.9 - 2.28 \times STR.\]</span></p>
<p>The book discusses the example of a parent moving to a metropolitan area who plans to choose where to live based on the quality of local schools: a school district’s average test score is an adequate measure for the quality. However, the parent has information on the student-teacher ratio only such that test scores need to be predicted. Although we have established that there is omitted variable bias in this model due to omission of variables like student learning opportunities outside school, the share of English learners and so on, <tt>linear_model</tt> may in fact be useful for the parent:</p>
<p>The parent need not care if the coefficient on <span class="math inline">\(STR\)</span> has causal interpretation, she wants <span class="math inline">\(STR\)</span> to explain as much variation in test scores as possible. Therefore, despite the fact that <tt>linear_model</tt> cannot be used to estimate the <em>causal effect</em> of a change in <span class="math inline">\(STR\)</span> on test scores, it can be considered a <em>reliable predictor</em> of test scores in general.</p>
<p>Thus, the threats to internal validity as summarized in Key Concept 9.7 are negligible for the parent. This is, as instanced in the book, different for a superintendent who has been tasked to take measures that increase test scores: she requires a more reliable model that does not suffer from the threats listed in Key Concept 9.7.</p>
<p>Consult Chapter 9.3 of the book for the corresponding discussion.</p>
</div>
<div id="etsacs" class="section level2">
<h2><span class="header-section-number">9.4</span> Example: Test Scores and Class Size</h2>
<p>This section discusses internal and external validity of the results gained from analyzing the California test score data using multiple regression models.</p>
<div id="external-validity-of-the-study" class="section level4 unnumbered">
<h4>External Validity of the Study</h4>
<p>External validity of the California test score analysis means that its results can be generalized. Whether this is possible depends on the population and the setting. Following the book we conduct the same analysis using data for fourth graders in <span class="math inline">\(220\)</span> public school districts in Massachusetts in 1998. Like <tt>CASchools</tt>, the data set <tt>MASchools</tt> is part of the <tt>AER</tt> package <span class="citation">(Christian Kleiber &amp; Zeileis, <a href="#ref-R-AER">2017</a>)</span>. Use the help function (<code>?MASchools</code>) to get information on the definitions of all the variables contained.</p>
<p>We start by loading the data set and proceed by computing some summary statistics.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># attach the &#39;MASchools&#39; dataset</span>
<span class="kw">data</span>(<span class="st">&quot;MASchools&quot;</span>)
<span class="kw">summary</span>(MASchools)</code></pre></div>
<pre><code>##    district         municipality           expreg       expspecial   
##  Length:220         Length:220         Min.   :2905   Min.   : 3832  
##  Class :character   Class :character   1st Qu.:4065   1st Qu.: 7442  
##  Mode  :character   Mode  :character   Median :4488   Median : 8354  
##                                        Mean   :4605   Mean   : 8901  
##                                        3rd Qu.:4972   3rd Qu.: 9722  
##                                        Max.   :8759   Max.   :53569  
##                                                                      
##      expbil           expocc          exptot        scratio      
##  Min.   :     0   Min.   :    0   Min.   :3465   Min.   : 2.300  
##  1st Qu.:     0   1st Qu.:    0   1st Qu.:4730   1st Qu.: 6.100  
##  Median :     0   Median :    0   Median :5155   Median : 7.800  
##  Mean   :  3037   Mean   : 1104   Mean   :5370   Mean   : 8.107  
##  3rd Qu.:     0   3rd Qu.:    0   3rd Qu.:5789   3rd Qu.: 9.800  
##  Max.   :295140   Max.   :15088   Max.   :9868   Max.   :18.400  
##                                                  NA&#39;s   :9       
##     special          lunch          stratio          income      
##  Min.   : 8.10   Min.   : 0.40   Min.   :11.40   Min.   : 9.686  
##  1st Qu.:13.38   1st Qu.: 5.30   1st Qu.:15.80   1st Qu.:15.223  
##  Median :15.45   Median :10.55   Median :17.10   Median :17.128  
##  Mean   :15.97   Mean   :15.32   Mean   :17.34   Mean   :18.747  
##  3rd Qu.:17.93   3rd Qu.:20.02   3rd Qu.:19.02   3rd Qu.:20.376  
##  Max.   :34.30   Max.   :76.20   Max.   :27.00   Max.   :46.855  
##                                                                  
##      score4          score8          salary         english       
##  Min.   :658.0   Min.   :641.0   Min.   :24.96   Min.   : 0.0000  
##  1st Qu.:701.0   1st Qu.:685.0   1st Qu.:33.80   1st Qu.: 0.0000  
##  Median :711.0   Median :698.0   Median :35.88   Median : 0.0000  
##  Mean   :709.8   Mean   :698.4   Mean   :35.99   Mean   : 1.1177  
##  3rd Qu.:720.0   3rd Qu.:712.0   3rd Qu.:37.96   3rd Qu.: 0.8859  
##  Max.   :740.0   Max.   :747.0   Max.   :44.49   Max.   :24.4939  
##                  NA&#39;s   :40      NA&#39;s   :25</code></pre>
<p>It is fairly easy to replicate key components of Table 9.1 of the book using <tt>R</tt>. To be consistent with variable names used in the <tt>CASchools</tt> data set, we do some formatting beforehand.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Customized variables in MASchools</span>
MASchools<span class="op">$</span>score &lt;-<span class="st"> </span>MASchools<span class="op">$</span>score4 
MASchools<span class="op">$</span>STR &lt;-<span class="st"> </span>MASchools<span class="op">$</span>stratio

<span class="co"># Reproduce Table 9.1 of the book</span>
vars &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;score&quot;</span>, <span class="st">&quot;STR&quot;</span>, <span class="st">&quot;english&quot;</span>, <span class="st">&quot;lunch&quot;</span>, <span class="st">&quot;income&quot;</span>)

<span class="kw">cbind</span>(<span class="dt">CA_mean =</span> <span class="kw">sapply</span>(CASchools[, vars], mean),
      <span class="dt">CA_sd   =</span> <span class="kw">sapply</span>(CASchools[, vars], sd),
      <span class="dt">MA_mean =</span> <span class="kw">sapply</span>(MASchools[, vars], mean),
      <span class="dt">MA_sd   =</span> <span class="kw">sapply</span>(MASchools[, vars], sd))</code></pre></div>
<pre><code>##           CA_mean     CA_sd    MA_mean     MA_sd
## score   654.15655 19.053347 709.827273 15.126474
## STR      19.64043  1.891812  17.344091  2.276666
## english  15.76816 18.285927   1.117676  2.900940
## lunch    44.70524 27.123381  15.315909 15.060068
## income   15.31659  7.225890  18.746764  5.807637</code></pre>
<p>The summary statistics reveal that the average test score is higher for school districts in Massachusetts. The test used in Massachusetts is somewhat different from the one used in California (the Massachusetts test score also includes results for the school subject “Science”), therefore a direct comparison of test scores is not appropriate. We also see that, on average, classes are smaller in Massachusetts than in California and that the average district income, average percentage of English learners as well as the average share of students receiving subsidized lunch differ considerably from the averages computed for California. There are also notable differences in the observed dispersion of the variables.</p>
<p>Following the book we examine the relationship between district income and test scores in Massachusetts as we have done before in Chapter <a href="nrf.html#nrf">8</a> for the California data and reproduce Figure 9.2 of the book.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate linear model</span>
Linear_model_MA &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>income, <span class="dt">data =</span> MASchools)
Linear_model_MA</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ income, data = MASchools)
## 
## Coefficients:
## (Intercept)       income  
##     679.387        1.624</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate linear-log model</span>
Linearlog_model_MA &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(income), <span class="dt">data =</span> MASchools) 
Linearlog_model_MA</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ log(income), data = MASchools)
## 
## Coefficients:
## (Intercept)  log(income)  
##      600.80        37.71</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate Cubic model</span>
cubic_model_MA &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(income) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">3</span>), <span class="dt">data =</span> MASchools)
cubic_model_MA</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ I(income) + I(income^2) + I(income^3), data = MASchools)
## 
## Coefficients:
## (Intercept)    I(income)  I(income^2)  I(income^3)  
##  600.398531    10.635382    -0.296887     0.002762</code></pre>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot data</span>
<span class="kw">plot</span>(MASchools<span class="op">$</span>income, MASchools<span class="op">$</span>score,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;District income&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Test score&quot;</span>,
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">50</span>),
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">620</span>, <span class="dv">780</span>))

<span class="co"># add estimated regression line for the linear model</span>
<span class="kw">abline</span>(Linear_model_MA, <span class="dt">lwd =</span> <span class="dv">2</span>)

<span class="co"># add estimated regression function for Linear-log model</span>
order_id  &lt;-<span class="st"> </span><span class="kw">order</span>(MASchools<span class="op">$</span>income)

<span class="kw">lines</span>(MASchools<span class="op">$</span>income[order_id],
      <span class="kw">fitted</span>(Linearlog_model_MA)[order_id], 
      <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>)

<span class="co"># add estimated cubic regression function</span>
<span class="kw">lines</span>(<span class="dt">x =</span> MASchools<span class="op">$</span>income[order_id], 
      <span class="dt">y =</span> <span class="kw">fitted</span>(cubic_model_MA)[order_id],
      <span class="dt">col =</span> <span class="st">&quot;orange&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>) 

<span class="co"># add a legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Linear&quot;</span>, <span class="st">&quot;Linear-Log&quot;</span>, <span class="st">&quot;Cubic&quot;</span>),
       <span class="dt">lty =</span> <span class="dv">1</span>,
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;Black&quot;</span>, <span class="st">&quot;darkgreen&quot;</span>, <span class="st">&quot;orange&quot;</span>))</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-401-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>The plot indicates that the cubic specification fits the data best. Interestingly, this is different from the <tt>CASchools</tt> data where the pattern of nonlinearity is better described by the linear-log specification.</p>
<p>We continue by estimating most of the model specifications used for analysis of the <tt>CASchools</tt> data set in Chapter <a href="nrf.html#nrf">8</a> and use <tt>stargazer()</tt> <span class="citation">(Hlavac, <a href="#ref-R-stargazer">2018</a>)</span> to generate a tabular representation of the regression results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># add &#39;HiEL&#39; to &#39;MASchools&#39;</span>
MASchools<span class="op">$</span>HiEL &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(MASchools<span class="op">$</span>english <span class="op">&gt;</span><span class="st"> </span><span class="kw">median</span>(MASchools<span class="op">$</span>english))

<span class="co"># estimate the model specifications from Table 9.2 of the book</span>
TestScore_MA_mod1 &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>STR, <span class="dt">data =</span> MASchools)

TestScore_MA_mod2 &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>STR <span class="op">+</span><span class="st"> </span>english <span class="op">+</span><span class="st"> </span>lunch <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(income), 
                        <span class="dt">data =</span> MASchools)

TestScore_MA_mod3 &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>STR <span class="op">+</span><span class="st"> </span>english <span class="op">+</span><span class="st"> </span>lunch <span class="op">+</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">2</span>) 
                        <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">3</span>), <span class="dt">data =</span> MASchools)

TestScore_MA_mod4 &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>STR <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(STR<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(STR<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>english <span class="op">+</span><span class="st"> </span>lunch <span class="op">+</span><span class="st"> </span>income 
                        <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">3</span>), <span class="dt">data =</span> MASchools)

TestScore_MA_mod5 &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>STR <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>HiEL<span class="op">:</span>STR <span class="op">+</span><span class="st"> </span>lunch 
                        <span class="op">+</span><span class="st"> </span>income, <span class="dt">data =</span> MASchools)

TestScore_MA_mod6 &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>STR <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>HiEL <span class="op">+</span><span class="st"> </span>HiEL<span class="op">:</span>STR <span class="op">+</span><span class="st"> </span>lunch 
                        <span class="op">+</span><span class="st"> </span>income, <span class="dt">data =</span> MASchools)

<span class="co"># gather robust standard errors</span>
rob_se &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(TestScore_MA_mod1, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(TestScore_MA_mod2, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(TestScore_MA_mod3, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(TestScore_MA_mod4, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(TestScore_MA_mod5, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(TestScore_MA_mod6, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))))

<span class="co"># generate a table with &#39;stargazer()&#39;</span>
<span class="kw">library</span>(stargazer)

<span class="kw">stargazer</span>(Linear_model_MA, TestScore_MA_mod2, TestScore_MA_mod3, 
          TestScore_MA_mod4, TestScore_MA_mod5, TestScore_MA_mod6,
          <span class="dt">title =</span> <span class="st">&quot;Regressions Using Massachusetts Test Score Data&quot;</span>,
          <span class="dt">type =</span> <span class="st">&quot;latex&quot;</span>,
          <span class="dt">digits =</span> <span class="dv">3</span>,
          <span class="dt">header =</span> <span class="ot">FALSE</span>,
          <span class="dt">se =</span> rob_se,
          <span class="dt">object.names =</span> <span class="ot">TRUE</span>,
          <span class="dt">model.numbers =</span> <span class="ot">FALSE</span>,
          <span class="dt">column.labels =</span> <span class="kw">c</span>(<span class="st">&quot;(I)&quot;</span>, <span class="st">&quot;(II)&quot;</span>, <span class="st">&quot;(III)&quot;</span>, <span class="st">&quot;(IV)&quot;</span>, <span class="st">&quot;(V)&quot;</span>, <span class="st">&quot;(VI)&quot;</span>))</code></pre></div>



<table style="text-align:center"><tr><td colspan="7" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"></td><td colspan="6">Dependent Variable: Score</td></tr>
<tr><td></td><td colspan="6" style="border-bottom: 1px solid black"></td></tr>
<tr><td style="text-align:left"></td><td colspan="6">score</td></tr>
<tr><td style="text-align:left"></td><td>(I)</td><td>(II)</td><td>(III)</td><td>(IV)</td><td>(V)</td><td>(VI)</td></tr>
<tr><td colspan="7" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">STR</td><td>-1.718<sup>***</sup></td><td>-0.689<sup>**</sup></td><td>-0.641<sup>**</sup></td><td>12.426</td><td>-1.018<sup>***</sup></td><td>-0.672<sup>**</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.499)</td><td>(0.270)</td><td>(0.268)</td><td>(14.010)</td><td>(0.370)</td><td>(0.271)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(STR2)</td><td></td><td></td><td></td><td>-0.680</td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td>(0.737)</td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(STR3)</td><td></td><td></td><td></td><td>0.011</td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td>(0.013)</td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">english</td><td></td><td>-0.411</td><td>-0.437</td><td>-0.434</td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td>(0.306)</td><td>(0.303)</td><td>(0.300)</td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">HiEL</td><td></td><td></td><td></td><td></td><td>-12.561</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td>(9.793)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">lunch</td><td></td><td>-0.521<sup>***</sup></td><td>-0.582<sup>***</sup></td><td>-0.587<sup>***</sup></td><td>-0.709<sup>***</sup></td><td>-0.653<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td>(0.078)</td><td>(0.097)</td><td>(0.104)</td><td>(0.091)</td><td>(0.073)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">log(income)</td><td></td><td>16.529<sup>***</sup></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td>(3.146)</td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">income</td><td></td><td></td><td>-3.067</td><td>-3.382</td><td>-3.867</td><td>-3.218</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td>(2.353)</td><td>(2.491)</td><td>(2.488)</td><td>(2.306)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(income2)</td><td></td><td></td><td>0.164<sup>*</sup></td><td>0.174<sup>*</sup></td><td>0.184<sup>**</sup></td><td>0.165<sup>*</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td>(0.085)</td><td>(0.089)</td><td>(0.090)</td><td>(0.085)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(income3)</td><td></td><td></td><td>-0.002<sup>**</sup></td><td>-0.002<sup>**</sup></td><td>-0.002<sup>**</sup></td><td>-0.002<sup>**</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td>(0.001)</td><td>(0.001)</td><td>(0.001)</td><td>(0.001)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">STR:HiEL</td><td></td><td></td><td></td><td></td><td>0.799</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td>(0.555)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">Constant</td><td>739.621<sup>***</sup></td><td>682.432<sup>***</sup></td><td>744.025<sup>***</sup></td><td>665.496<sup>***</sup></td><td>759.914<sup>***</sup></td><td>747.364<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(8.607)</td><td>(11.497)</td><td>(21.318)</td><td>(81.332)</td><td>(23.233)</td><td>(20.278)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td colspan="7" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">Observations</td><td>220</td><td>220</td><td>220</td><td>220</td><td>220</td><td>220</td></tr>
<tr><td style="text-align:left">R<sup>2</sup></td><td>0.067</td><td>0.676</td><td>0.685</td><td>0.687</td><td>0.686</td><td>0.681</td></tr>
<tr><td style="text-align:left">Adjusted R<sup>2</sup></td><td>0.063</td><td>0.670</td><td>0.676</td><td>0.675</td><td>0.675</td><td>0.674</td></tr>
<tr><td style="text-align:left">Residual Std. Error</td><td>14.646 (df = 218)</td><td>8.686 (df = 215)</td><td>8.607 (df = 213)</td><td>8.626 (df = 211)</td><td>8.621 (df = 212)</td><td>8.637 (df = 214)</td></tr>
<tr><td style="text-align:left">F Statistic</td><td>15.616<sup>***</sup> (df = 1; 218)</td><td>112.284<sup>***</sup> (df = 4; 215)</td><td>77.232<sup>***</sup> (df = 6; 213)</td><td>57.803<sup>***</sup> (df = 8; 211)</td><td>66.023<sup>***</sup> (df = 7; 212)</td><td>91.560<sup>***</sup> (df = 5; 214)</td></tr>
<tr><td colspan="7" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"><em>Note:</em></td><td colspan="6" style="text-align:right"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>
</table>
<caption><p style='text-align:center'><span id="tab:rumtsd">Table 9.1: </span> Regressions Using Massachusetts Test Score Data</p></caption>


<p>Next we reproduce the <span class="math inline">\(F\)</span>-statistics and <span class="math inline">\(p\)</span>-values for testing exclusion of groups of variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># F-test model (3)</span>
<span class="kw">linearHypothesis</span>(TestScore_MA_mod3, 
                 <span class="kw">c</span>(<span class="st">&quot;I(income^2)=0&quot;</span>, <span class="st">&quot;I(income^3)=0&quot;</span>), 
                 <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## I(income^2) = 0
## I(income^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + english + lunch + income + I(income^2) + I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df     F   Pr(&gt;F)   
## 1    215                     
## 2    213  2 6.227 0.002354 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># F-tests model (4)</span>
<span class="kw">linearHypothesis</span>(TestScore_MA_mod4, 
                 <span class="kw">c</span>(<span class="st">&quot;STR=0&quot;</span>, <span class="st">&quot;I(STR^2)=0&quot;</span>, <span class="st">&quot;I(STR^3)=0&quot;</span>), 
                 <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## STR = 0
## I(STR^2) = 0
## I(STR^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + I(STR^2) + I(STR^3) + english + lunch + income + 
##     I(income^2) + I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F  Pr(&gt;F)  
## 1    214                    
## 2    211  3 2.3364 0.07478 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">linearHypothesis</span>(TestScore_MA_mod4, 
                 <span class="kw">c</span>(<span class="st">&quot;I(STR^2)=0&quot;</span>, <span class="st">&quot;I(STR^3)=0&quot;</span>), 
                 <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## I(STR^2) = 0
## I(STR^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + I(STR^2) + I(STR^3) + english + lunch + income + 
##     I(income^2) + I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F Pr(&gt;F)
## 1    213                 
## 2    211  2 0.3396 0.7124</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">linearHypothesis</span>(TestScore_MA_mod4, 
                 <span class="kw">c</span>(<span class="st">&quot;I(income^2)=0&quot;</span>, <span class="st">&quot;I(income^3)=0&quot;</span>), 
                 <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## I(income^2) = 0
## I(income^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + I(STR^2) + I(STR^3) + english + lunch + income + 
##     I(income^2) + I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F   Pr(&gt;F)   
## 1    213                      
## 2    211  2 5.7043 0.003866 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># F-tests model (5)</span>
<span class="kw">linearHypothesis</span>(TestScore_MA_mod5, 
                 <span class="kw">c</span>(<span class="st">&quot;STR=0&quot;</span>, <span class="st">&quot;STR:HiEL=0&quot;</span>), 
                 <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## STR = 0
## STR:HiEL = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + HiEL + HiEL:STR + lunch + income + I(income^2) + 
##     I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F Pr(&gt;F)  
## 1    214                   
## 2    212  2 3.7663 0.0247 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">linearHypothesis</span>(TestScore_MA_mod5, 
                 <span class="kw">c</span>(<span class="st">&quot;I(income^2)=0&quot;</span>, <span class="st">&quot;I(income^3)=0&quot;</span>), 
                 <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## I(income^2) = 0
## I(income^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + HiEL + HiEL:STR + lunch + income + I(income^2) + 
##     I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F  Pr(&gt;F)  
## 1    214                    
## 2    212  2 3.2201 0.04191 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">linearHypothesis</span>(TestScore_MA_mod5, 
                 <span class="kw">c</span>(<span class="st">&quot;HiEL=0&quot;</span>, <span class="st">&quot;STR:HiEL=0&quot;</span>), 
                 <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## HiEL = 0
## STR:HiEL = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + HiEL + HiEL:STR + lunch + income + I(income^2) + 
##     I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F Pr(&gt;F)
## 1    214                 
## 2    212  2 1.4674 0.2328</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># F-test Model (6)</span>
<span class="kw">linearHypothesis</span>(TestScore_MA_mod6, 
                 <span class="kw">c</span>(<span class="st">&quot;I(income^2)=0&quot;</span>, <span class="st">&quot;I(income^3)=0&quot;</span>), 
                 <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## I(income^2) = 0
## I(income^3) = 0
## 
## Model 1: restricted model
## Model 2: score ~ STR + lunch + income + I(income^2) + I(income^3)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F  Pr(&gt;F)  
## 1    216                    
## 2    214  2 4.2776 0.01508 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We see that, in terms of <span class="math inline">\(\bar{R}^2\)</span>, specification (3) which uses a cubic to model the relationship between district income and test scores indeed performs better than the linear-log specification (2). Using different <span class="math inline">\(F\)</span>-tests on models (4) and (5), we cannot reject the hypothesis that there is no nonlinear relationship between student teacher ratio and test score and also that the share of English learners has an influence on the relationship of interest. Furthermore, regression (6) shows that the percentage of English learners can be omitted as a regressor. Because of the model specifications made in (4) to (6) do not lead to substantially different results than those of regression (3), we choose model (3) as the most suitable specification.</p>
<p>In comparison to the California data, we observe the following results:</p>
<ol style="list-style-type: decimal">
<li><p>Controlling for the students’ background characteristics in model specification (2) reduces the coefficient of interest (student-teacher ratio) by roughly <span class="math inline">\(60\%\)</span>. The estimated coefficients are close to each other.</p></li>
<li><p>The coefficient on student-teacher ratio is always significantly different from zero at the level of <span class="math inline">\(1\%\)</span> for both data sets. This holds for all considered model specifications in both studies.</p></li>
<li><p>In both studies the share of English learners in a school district is of little importance for the estimated impact of a change in the student-teacher ratio on test score.</p></li>
</ol>
<p>The biggest difference is that, in contrast to the California results, we do not find evidence of a nonlinear relationship between test scores and the student teacher ratio for the Massachusetts data since the corresponding <span class="math inline">\(F\)</span>-tests for model (4) do not reject.</p>
<p>As pointed out in the book, the test scores for California and Massachusetts have different units because the underlying tests are different. Thus the estimated coefficients on the student-teacher ratio in both regressions cannot be compared before standardizing test scores to the same units as <span class="math display">\[\frac{Testscore - \overline{TestScore}}{\sigma_{TestScore}}\]</span> for all observations in both data sets and running the regressions of interest using the standardized data again. One can show that the coefficient on student-teacher ratio in the regression using standardized test scores is the coefficient of the original regression divided by the standard deviation of test scores.</p>
<p>For model (3) of the Massachusetts data, the estimated coefficient on the student-teacher ratio is <span class="math inline">\(-0.64\)</span>. A reduction of the student-teacher ratio by two students is predicted to increase test scores by <span class="math inline">\(-2 \cdot (-0.64) = 1.28\)</span> points. Thus we can compute the effect of a reduction of student-teacher ratio by two students on the standardized test scores as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TestScore_MA_mod3<span class="op">$</span>coefficients[<span class="dv">2</span>] <span class="op">/</span><span class="st"> </span><span class="kw">sd</span>(MASchools<span class="op">$</span>score) <span class="op">*</span><span class="st"> </span>(<span class="op">-</span><span class="dv">2</span>)</code></pre></div>
<pre><code>##        STR 
## 0.08474001</code></pre>
<p>For Massachusetts the predicted increase of test scores due to a reduction of the student-teacher ratio by two students is <span class="math inline">\(0.085\)</span> standard deviations of the distribution of the observed distribution of test scores.</p>
<p>Using the linear specification (2) for California, the estimated coefficient on the student-teacher ratio is <span class="math inline">\(-0.73\)</span> so the predicted increase of test scores induced by a reduction of the student-teacher ratio by two students is <span class="math inline">\(-0.73 \cdot (-2) = 1.46\)</span>. We use <tt>R</tt> to compute the predicted change in standard deviation units:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TestScore_mod2<span class="op">$</span>coefficients[<span class="dv">2</span>] <span class="op">/</span><span class="st"> </span><span class="kw">sd</span>(CASchools<span class="op">$</span>score) <span class="op">*</span><span class="st"> </span>(<span class="op">-</span><span class="dv">2</span>)</code></pre></div>
<pre><code>##        STR 
## 0.07708103</code></pre>
<p>This shows that the the predicted increase of test scores due to a reduction of the student-teacher ratio by two students is <span class="math inline">\(0.077\)</span> standard deviation of the observed distribution of test scores for the California data.</p>
<p>In terms of standardized test scores, the predicted change is essentially the same for school districts in California and Massachusetts.</p>
<p>Altogether, the results support external validity of the inferences made using data on Californian elementary school districts — at least for Massachusetts.</p>
</div>
<div id="internal-validity-of-the-study" class="section level4 unnumbered">
<h4>Internal Validity of the Study</h4>
<p>External validity of the study <em>does not</em> ensure their internal validity. Although the chosen model specification improves upon a simple linear regression model, internal validity may still be violated due to some of the threats listed in Key Concept 9.7. These threats are:</p>
<ul>
<li><p>omitted variable bias</p></li>
<li><p>misspecification of functional form</p></li>
<li><p>errors in variables</p></li>
<li><p>sample selection issues</p></li>
<li><p>simultaneous causality</p></li>
<li><p>heteroskedasticity</p></li>
<li><p>correlation of errors across observations</p></li>
</ul>
<p>Consult the book for an in-depth discussion of these threats in view of both test score studies.</p>
</div>
<div id="summary-1" class="section level4 unnumbered">
<h4>Summary</h4>
<p>We have found that there <em>is</em> a small but statistically significant effect of the student-teacher ratio on test scores. However, it remains unclear if we have indeed estimated the causal effect of interest since — despite that our approach including control variables, taking into account nonlinearities in the population regression function and statistical inference using robust standard errors — the results might still be biased for example if there are omitted factors which we have not considered. Thus <em>internal validity</em> of the study remains questionable. As we have concluded from comparison with the analysis of the Massachusetts data set, this result may be <em>externally valid</em>.</p>
<p>The following chapters address techniques that can be remedies to all the threats to internal validity listed in Key Concept 9.7 if multiple regression alone is insufficient. This includes regression using panel data and approaches that employ instrumental variables.</p>
</div>
</div>
<div id="exercises-7" class="section level2">
<h2><span class="header-section-number">9.5</span> Exercises</h2>
<div class="DCexercise">
<h4 id="simulation-study-misspecification-of-functional-form" class="unnumbered">1. Simulation Study: Misspecification of Functional Form</h4>
<p>As stated in Chapter <a href="asbomr.html#ttivomra">9.2</a>, misspecification of the regression function violates assumption 1 of Key Concept 6.3 so that the OLS estimator will be biased and inconsistent. We have illustrated the bias of <span class="math inline">\(\hat{\beta}_0\)</span> for the example of the quadratic population regression function <span class="math display">\[Y_i = X_i^2 \]</span> and the linear model <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + u_i, \, u_i \sim \mathcal{N}(0,1)\]</span> using 100 randomly generated observations. Strictly speaking, this finding could be just a coincidence because we consider just one estimate obtained using a single data set.</p>
<p>In this exercise, you have to generate simulation evidence for the bias of <span class="math inline">\(\hat{\beta}_0\)</span> in the model <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + u_i\]</span> if the population regression function is <span class="math display">\[Y_i = X_i^2.\]</span></p>
<p><strong>Instructions:</strong></p>
<p>Make sure to use the definitions suggested in the skeleton code in <tt>script.R</tt> to complete the following tasks:</p>
<ul>
<li><p>Generate 1000 OLS estimates of <span class="math inline">\(\beta_0\)</span> in the model above using a <tt>for()</tt> loop where <span class="math inline">\(X_i \sim \mathcal{U}[-5,5]\)</span>, <span class="math inline">\(u_i \sim \mathcal{N}(0,1)\)</span> using samples of size <span class="math inline">\(100\)</span>. Save the estimates in <tt>beta_hats</tt>.</p></li>
<li><p>Compare the sample mean of the estimates to the true parameter using the <tt>==</tt> operator.</p></li>
</ul>
<iframe src="DCL/ex9_1.html" frameborder="0" scrolling="no" style="width:100%;height:360px">
</iframe>
<p><strong>Hint:</strong></p>
<p>You can generate random numbers from a uniform distribution using <tt>runif()</tt>.</p>
</div>
<div class="DCexercise">
<h4 id="simulation-study-errors-in-variables-bias" class="unnumbered">2. Simulation Study: Errors-in-Variables Bias</h4>
<p>Consider again the application of the classical measurement error model introduced in Chapter <a href="asbomr.html#ttivomra">9.2</a>:</p>
The single regressor <span class="math inline">\(X_i\)</span> is measured with error so that <span class="math inline">\(\overset{\sim}{X}_i\)</span> is observed instead. Thus one estimates <span class="math inline">\(\beta_1\)</span> in
<span class="math display">\[\begin{align*}
  Y_i =&amp; \, \beta_0 + \beta_1 \overset{\sim}{X}_i + \underbrace{\beta_1 (X_i -\overset{\sim}{X}_i) + u_i}_{=v_i} \\
  Y_i =&amp; \, \beta_0 + \beta_1 \overset{\sim}{X}_i + v_i
\end{align*}\]</span>
<p>instead of <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + u_i,\]</span></p>
with the zero mean error <span class="math inline">\(w_i\)</span> being uncorrelated with <span class="math inline">\(X_i\)</span> and <span class="math inline">\(u_i\)</span>. Then <span class="math inline">\(\beta_1\)</span> is inconsistently estimated by OLS:
<span class="math display">\[\begin{equation}
  \widehat{\beta}_1 \xrightarrow{p}{\frac{\sigma_{X}^2}{\sigma_{X}^2 + \sigma_{w}^2}} \beta_1
\end{equation}\]</span>
<p>Let <span class="math display">\[(X, Y) \sim \mathcal{N}\left[\begin{pmatrix}50\\ 100\end{pmatrix},\begin{pmatrix}10 &amp; 5 \\ 5 &amp; 10 \end{pmatrix}\right].\]</span> Recall from <a href="asbomr.html#eq:bnormexpfn">(9.2)</a> that <span class="math inline">\(E(Y_i\vert X_i) = 75 + 0.5 X_i\)</span> in this case. Further Assume that <span class="math inline">\(\overset{\sim}{X_i} = X_i + w_i\)</span> with <span class="math inline">\(w_i \overset{i.i.d}{\sim} \mathcal{N}(0,10)\)</span>.</p>
<p>As mentioned in Exercise 1, Chapter <a href="asbomr.html#ttivomra">9.2</a> discusses the consequences of the measurement error for the OLS estimator of <span class="math inline">\(\beta_1\)</span> in this setting based on a <em>single</em> sample and and thus just one estimate. Strictly speaking, the conclusion made could be wrong because the oberseved bias may be due to random variation. A Monto Carlo simulation is more appropriate here.</p>
<p><strong>Instructions:</strong></p>
<p>Show that <span class="math inline">\(\beta_1\)</span> is estimated with a bias using a simulation study. Make sure to use the definitions suggested in the skeleton code in <tt>script.R</tt> to complete the following tasks:</p>
<ul>
<li><p>Generate 1000 estimates of <span class="math inline">\(\beta_1\)</span> in the simple regression model <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + u_i.\]</span> Use <tt>rmvnorm()</tt> to generate samples of 100 random observations from the bivariate normal distribution stated above.</p></li>
<li><p>Save the estimates in <tt>beta_hats</tt>.</p></li>
<li><p>Compute the sample mean of the estimates.</p></li>
</ul>
<iframe src="DCL/ex9_2.html" frameborder="0" scrolling="no" style="width:100%;height:400px">
</iframe>
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-mvtnorm">
<p>Genz, A., Bretz, F., Miwa, T., Mi, X., &amp; Hothorn, T. (2018). mvtnorm: Multivariate Normal and t Distributions (Version 1.0-8). Retrieved from <a href="https://CRAN.R-project.org/package=mvtnorm" class="uri">https://CRAN.R-project.org/package=mvtnorm</a></p>
</div>
<div id="ref-R-AER">
<p>Kleiber, C., &amp; Zeileis, A. (2017). AER: Applied Econometrics with R (Version 1.2-5). Retrieved from <a href="https://CRAN.R-project.org/package=AER" class="uri">https://CRAN.R-project.org/package=AER</a></p>
</div>
<div id="ref-R-stargazer">
<p>Hlavac, M. (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables (Version 5.2.2). Retrieved from <a href="https://CRAN.R-project.org/package=stargazer" class="uri">https://CRAN.R-project.org/package=stargazer</a></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="nrf.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="rwpd.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": true,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 1
},
"edit": null,
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
