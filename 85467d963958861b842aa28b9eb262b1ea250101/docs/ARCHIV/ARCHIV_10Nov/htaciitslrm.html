<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Econometrics with R</title>
  <meta name="description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Econometrics with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Econometrics with R" />
  
  <meta name="twitter:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2018-10-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="lrwor.html">
<link rel="next" href="rmwmr.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<!-- font families -->

<link href="https://fonts.googleapis.com/css?family=PT+Sans|Pacifico|Source+Sans+Pro" rel="stylesheet">

<!-- function to adjust height of iframes automatically depending on content loaded -->

<script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';
  }
</script>

<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/default.js"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110299877-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110299877-1');
</script>

<!-- open review block -->

<script async defer src="https://hypothes.is/embed.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://www.oek.wiwi.uni-due.de/en/">Chair of Econometrics at UDE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#a-very-short-introduction-to-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> A Very Short Introduction to <tt>R</tt> and <em>RStudio</em></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="pt.html"><a href="pt.html#random-variables-and-probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#expected-value-mean-and-variance"><i class="fa fa-check"></i>Expected Value, Mean and Variance</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#thetdist"><i class="fa fa-check"></i>The Student t Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-f-distribution"><i class="fa fa-check"></i>The F Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pt.html"><a href="pt.html#RSATDOSA"><i class="fa fa-check"></i><b>2.2</b> Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="pt.html"><a href="pt.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arosur.html"><a href="arosur.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="3.1" data-path="arosur.html"><a href="arosur.html#estimation-of-the-population-mean"><i class="fa fa-check"></i><b>3.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="3.2" data-path="arosur.html"><a href="arosur.html#potsm"><i class="fa fa-check"></i><b>3.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="3.3" data-path="arosur.html"><a href="arosur.html#hypothesis-tests-concerning-the-population-mean"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests Concerning the Population Mean</a><ul>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-p-value"><i class="fa fa-check"></i>The p-Value</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-known"><i class="fa fa-check"></i>Calculating the p-Value when the Standard Deviation is Known</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#SVSSDASE"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-unknown"><i class="fa fa-check"></i>Calculating the p-value When the Standard Deviation is Unknown</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-t-statistic"><i class="fa fa-check"></i>The t-statistic</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="arosur.html"><a href="arosur.html#confidence-intervals-for-the-population-mean"><i class="fa fa-check"></i><b>3.4</b> Confidence Intervals for the Population Mean</a></li>
<li class="chapter" data-level="3.5" data-path="arosur.html"><a href="arosur.html#cmfdp"><i class="fa fa-check"></i><b>3.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="3.6" data-path="arosur.html"><a href="arosur.html#aattggoe"><i class="fa fa-check"></i><b>3.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="3.7" data-path="arosur.html"><a href="arosur.html#scatterplots-sample-covariance-and-sample-correlation"><i class="fa fa-check"></i><b>3.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
<li class="chapter" data-level="3.8" data-path="arosur.html"><a href="arosur.html#exercises-1"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="4.1" data-path="lrwor.html"><a href="lrwor.html#simple-linear-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="4.2" data-path="lrwor.html"><a href="lrwor.html#estimating-the-coefficients-of-the-linear-regression-model"><i class="fa fa-check"></i><b>4.2</b> Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lrwor.html"><a href="lrwor.html#measures-of-fit"><i class="fa fa-check"></i><b>4.3</b> Measures of Fit</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-coefficient-of-determination"><i class="fa fa-check"></i>The Coefficient of Determination</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-standard-error-of-the-regression"><i class="fa fa-check"></i>The Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lrwor.html"><a href="lrwor.html#tlsa"><i class="fa fa-check"></i><b>4.4</b> The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption 1: The Error Term has Conditional Mean of Zero</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-2-independently-and-identically-distributed-data"><i class="fa fa-check"></i>Assumption 2: Independently and Identically Distributed Data</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption 3: Large Outliers are Unlikely</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="lrwor.html"><a href="lrwor.html#tsdotoe"><i class="fa fa-check"></i><b>4.5</b> The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-1"><i class="fa fa-check"></i>Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-2"><i class="fa fa-check"></i>Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-3"><i class="fa fa-check"></i>Simulation Study 3</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="lrwor.html"><a href="lrwor.html#exercises-2"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="htaciitslrm.html"><a href="htaciitslrm.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li class="chapter" data-level="5.1" data-path="htaciitslrm.html"><a href="htaciitslrm.html#testing-two-sided-hypotheses-concerning-the-slope-coefficient"><i class="fa fa-check"></i><b>5.1</b> Testing Two-Sided Hypotheses Concerning the Slope Coefficient</a></li>
<li class="chapter" data-level="5.2" data-path="htaciitslrm.html"><a href="htaciitslrm.html#cifrc"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-confidence-intervals"><i class="fa fa-check"></i>Simulation Study: Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="htaciitslrm.html"><a href="htaciitslrm.html#rwxiabv"><i class="fa fa-check"></i><b>5.3</b> Regression when X is a Binary Variable</a></li>
<li class="chapter" data-level="5.4" data-path="htaciitslrm.html"><a href="htaciitslrm.html#hah"><i class="fa fa-check"></i><b>5.4</b> Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="htaciitslrm.html"><a href="htaciitslrm.html#the-gauss-markov-theorem"><i class="fa fa-check"></i><b>5.5</b> The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-blue-estimator"><i class="fa fa-check"></i>Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="htaciitslrm.html"><a href="htaciitslrm.html#using-the-t-statistic-in-regression-when-the-sample-size-is-small"><i class="fa fa-check"></i><b>5.6</b> Using the t-Statistic in Regression When the Sample Size Is Small</a></li>
<li class="chapter" data-level="5.7" data-path="htaciitslrm.html"><a href="htaciitslrm.html#exercises-3"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rmwmr.html"><a href="rmwmr.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="6.1" data-path="rmwmr.html"><a href="rmwmr.html#omitted-variable-bias"><i class="fa fa-check"></i><b>6.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="6.2" data-path="rmwmr.html"><a href="rmwmr.html#tmrm"><i class="fa fa-check"></i><b>6.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="rmwmr.html"><a href="rmwmr.html#mofimr"><i class="fa fa-check"></i><b>6.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="6.4" data-path="rmwmr.html"><a href="rmwmr.html#ols-assumptions-in-multiple-regression"><i class="fa fa-check"></i><b>6.4</b> OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#simulation-study-imperfect-multicollinearity"><i class="fa fa-check"></i>Simulation Study: Imperfect Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="rmwmr.html"><a href="rmwmr.html#the-distribution-of-the-ols-estimators-in-multiple-regression"><i class="fa fa-check"></i><b>6.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
<li class="chapter" data-level="6.6" data-path="rmwmr.html"><a href="rmwmr.html#exercises-4"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="htaciimr.html"><a href="htaciimr.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence Intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="htaciimr.html"><a href="htaciimr.html#hypothesis-tests-and-confidence-intervals-for-a-single-coefficient"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="htaciimr.html"><a href="htaciimr.html#an-application-to-test-scores-and-the-student-teacher-ratio"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="htaciimr.html"><a href="htaciimr.html#joint-hypothesis-testing-using-the-f-statistic"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the F-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="htaciimr.html"><a href="htaciimr.html#confidence-sets-for-multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-for-multiple-regression"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="htaciimr.html"><a href="htaciimr.html#analysis-of-the-test-score-data-set"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
<li class="chapter" data-level="7.7" data-path="htaciimr.html"><a href="htaciimr.html#exercises-5"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nrf.html"><a href="nrf.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="8.1" data-path="nrf.html"><a href="nrf.html#a-general-strategy-for-modelling-nonlinear-regression-functions"><i class="fa fa-check"></i><b>8.1</b> A General Strategy for Modelling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="8.2" data-path="nrf.html"><a href="nrf.html#nfoasiv"><i class="fa fa-check"></i><b>8.2</b> Nonlinear Functions of a Single Independent Variable</a><ul>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="nrf.html"><a href="nrf.html#interactions-between-independent-variables"><i class="fa fa-check"></i><b>8.3</b> Interactions Between Independent Variables</a></li>
<li class="chapter" data-level="8.4" data-path="nrf.html"><a href="nrf.html#nonlinear-effects-on-test-scores-of-the-student-teacher-ratio"><i class="fa fa-check"></i><b>8.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
<li class="chapter" data-level="8.5" data-path="nrf.html"><a href="nrf.html#exercises-6"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="asbomr.html"><a href="asbomr.html"><i class="fa fa-check"></i><b>9</b> Assessing Studies Based on Multiple Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity"><i class="fa fa-check"></i><b>9.1</b> Internal and External Validity</a></li>
<li class="chapter" data-level="9.2" data-path="asbomr.html"><a href="asbomr.html#ttivomra"><i class="fa fa-check"></i><b>9.2</b> Threats to Internal Validity of Multiple Regression Analysis</a></li>
<li class="chapter" data-level="9.3" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity-when-the-regression-is-used-for-forecasting"><i class="fa fa-check"></i><b>9.3</b> Internal and External Validity when the Regression is Used for Forecasting</a></li>
<li class="chapter" data-level="9.4" data-path="asbomr.html"><a href="asbomr.html#etsacs"><i class="fa fa-check"></i><b>9.4</b> Example: Test Scores and Class Size</a></li>
<li class="chapter" data-level="9.5" data-path="asbomr.html"><a href="asbomr.html#exercises-7"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="rwpd.html"><a href="rwpd.html"><i class="fa fa-check"></i><b>10</b> Regression with Panel Data</a><ul>
<li class="chapter" data-level="10.1" data-path="rwpd.html"><a href="rwpd.html#panel-data"><i class="fa fa-check"></i><b>10.1</b> Panel Data</a></li>
<li class="chapter" data-level="10.2" data-path="rwpd.html"><a href="rwpd.html#PDWTTP"><i class="fa fa-check"></i><b>10.2</b> Panel Data with Two Time Periods: “Before and After” Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="rwpd.html"><a href="rwpd.html#fixed-effects-regression"><i class="fa fa-check"></i><b>10.3</b> Fixed Effects Regression</a><ul>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#estimation-and-inference"><i class="fa fa-check"></i>Estimation and Inference</a></li>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#application-to-traffic-deaths"><i class="fa fa-check"></i>Application to Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="rwpd.html"><a href="rwpd.html#regression-with-time-fixed-effects"><i class="fa fa-check"></i><b>10.4</b> Regression with Time Fixed Effects</a></li>
<li class="chapter" data-level="10.5" data-path="rwpd.html"><a href="rwpd.html#tferaaseffer"><i class="fa fa-check"></i><b>10.5</b> The Fixed Effects Regression Assumptions and Standard Errors for Fixed Effects Regression</a></li>
<li class="chapter" data-level="10.6" data-path="rwpd.html"><a href="rwpd.html#drunk-driving-laws-and-traffic-deaths"><i class="fa fa-check"></i><b>10.6</b> Drunk Driving Laws and Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="rwabdv.html"><a href="rwabdv.html"><i class="fa fa-check"></i><b>11</b> Regression with a Binary Dependent Variable</a><ul>
<li class="chapter" data-level="11.1" data-path="rwabdv.html"><a href="rwabdv.html#binary-dependent-variables-and-the-linear-probability-model"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables and the Linear Probability Model</a></li>
<li class="chapter" data-level="11.2" data-path="rwabdv.html"><a href="rwabdv.html#palr"><i class="fa fa-check"></i><b>11.2</b> Probit and Logit Regression</a><ul>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#probit-regression"><i class="fa fa-check"></i>Probit Regression</a></li>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#logit-regression"><i class="fa fa-check"></i>Logit Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="rwabdv.html"><a href="rwabdv.html#estimation-and-inference-in-the-logit-and-probit-models"><i class="fa fa-check"></i><b>11.3</b> Estimation and Inference in the Logit and Probit Models</a></li>
<li class="chapter" data-level="11.4" data-path="rwabdv.html"><a href="rwabdv.html#application-to-the-boston-hmda-data"><i class="fa fa-check"></i><b>11.4</b> Application to the Boston HMDA Data</a></li>
<li class="chapter" data-level="11.5" data-path="rwabdv.html"><a href="rwabdv.html#exercises-8"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ivr.html"><a href="ivr.html"><i class="fa fa-check"></i><b>12</b> Instrumental Variables Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="ivr.html"><a href="ivr.html#TIVEWASRAASI"><i class="fa fa-check"></i><b>12.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a></li>
<li class="chapter" data-level="12.2" data-path="ivr.html"><a href="ivr.html#TGIVRM"><i class="fa fa-check"></i><b>12.2</b> The General IV Regression Model</a></li>
<li class="chapter" data-level="12.3" data-path="ivr.html"><a href="ivr.html#civ"><i class="fa fa-check"></i><b>12.3</b> Checking Instrument Validity</a></li>
<li class="chapter" data-level="12.4" data-path="ivr.html"><a href="ivr.html#attdfc"><i class="fa fa-check"></i><b>12.4</b> Application to the Demand for Cigarettes</a></li>
<li class="chapter" data-level="12.5" data-path="ivr.html"><a href="ivr.html#where-do-valid-instruments-come-from"><i class="fa fa-check"></i><b>12.5</b> Where Do Valid Instruments Come From?</a></li>
<li class="chapter" data-level="12.6" data-path="ivr.html"><a href="ivr.html#exercises-9"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="eaqe.html"><a href="eaqe.html"><i class="fa fa-check"></i><b>13</b> Experiments and Quasi-Experiments</a><ul>
<li class="chapter" data-level="13.1" data-path="eaqe.html"><a href="eaqe.html#potential-outcomes-causal-effects-and-idealized-experiments"><i class="fa fa-check"></i><b>13.1</b> Potential Outcomes, Causal Effects and Idealized Experiments</a></li>
<li class="chapter" data-level="13.2" data-path="eaqe.html"><a href="eaqe.html#threats-to-validity-of-experiments"><i class="fa fa-check"></i><b>13.2</b> Threats to Validity of Experiments</a></li>
<li class="chapter" data-level="13.3" data-path="eaqe.html"><a href="eaqe.html#experimental-estimates-of-the-effect-of-class-size-reductions"><i class="fa fa-check"></i><b>13.3</b> Experimental Estimates of the Effect of Class Size Reductions</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#experimental-design-and-the-data-set"><i class="fa fa-check"></i>Experimental Design and the Data Set</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#analysis-of-the-star-data"><i class="fa fa-check"></i>Analysis of the STAR Data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="eaqe.html"><a href="eaqe.html#quasi-experiments"><i class="fa fa-check"></i><b>13.4</b> Quasi Experiments</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#the-differences-in-differences-estimator"><i class="fa fa-check"></i>The Differences-in-Differences Estimator</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#regression-discontinuity-estimators"><i class="fa fa-check"></i>Regression Discontinuity Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ittsraf.html"><a href="ittsraf.html"><i class="fa fa-check"></i><b>14</b> Introduction to Time Series Regression and Forecasting</a><ul>
<li class="chapter" data-level="14.1" data-path="ittsraf.html"><a href="ittsraf.html#using-regression-models-for-forecasting"><i class="fa fa-check"></i><b>14.1</b> Using Regression Models for Forecasting</a></li>
<li class="chapter" data-level="14.2" data-path="ittsraf.html"><a href="ittsraf.html#tsdasc"><i class="fa fa-check"></i><b>14.2</b> Time Series Data and Serial Correlation</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#notation-lags-differences-logarithms-and-growth-rates"><i class="fa fa-check"></i>Notation, Lags, Differences, Logarithms and Growth Rates</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ittsraf.html"><a href="ittsraf.html#autoregressions"><i class="fa fa-check"></i><b>14.3</b> Autoregressions</a><ul>
<li><a href="ittsraf.html#autoregressive-models-of-order-p">Autoregressive Models of Order <span class="math inline">\(p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ittsraf.html"><a href="ittsraf.html#cybtmpi"><i class="fa fa-check"></i><b>14.4</b> Can You Beat the Market? (Part I)</a></li>
<li class="chapter" data-level="14.5" data-path="ittsraf.html"><a href="ittsraf.html#apatadlm"><i class="fa fa-check"></i><b>14.5</b> Additional Predictors and The ADL Model</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#forecast-uncertainty-and-forecast-intervals"><i class="fa fa-check"></i>Forecast Uncertainty and Forecast Intervals</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="ittsraf.html"><a href="ittsraf.html#llsuic"><i class="fa fa-check"></i><b>14.6</b> Lag Length Selection Using Information Criteria</a></li>
<li class="chapter" data-level="14.7" data-path="ittsraf.html"><a href="ittsraf.html#nit"><i class="fa fa-check"></i><b>14.7</b> Nonstationarity I: Trends</a></li>
<li class="chapter" data-level="14.8" data-path="ittsraf.html"><a href="ittsraf.html#niib"><i class="fa fa-check"></i><b>14.8</b> Nonstationarity II: Breaks</a></li>
<li class="chapter" data-level="14.9" data-path="ittsraf.html"><a href="ittsraf.html#can-you-beat-the-market-part-ii"><i class="fa fa-check"></i><b>14.9</b> Can You Beat the Market? (Part II)</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="eodce.html"><a href="eodce.html"><i class="fa fa-check"></i><b>15</b> Estimation of Dynamic Causal Effects</a><ul>
<li class="chapter" data-level="15.1" data-path="eodce.html"><a href="eodce.html#the-orange-juice-data"><i class="fa fa-check"></i><b>15.1</b> The Orange Juice Data</a></li>
<li class="chapter" data-level="15.2" data-path="eodce.html"><a href="eodce.html#dynamic-causal-effects"><i class="fa fa-check"></i><b>15.2</b> Dynamic Causal Effects</a></li>
<li class="chapter" data-level="15.3" data-path="eodce.html"><a href="eodce.html#dynamic-multipliers-and-cumulative-dynamic-multipliers"><i class="fa fa-check"></i><b>15.3</b> Dynamic Multipliers and Cumulative Dynamic Multipliers</a></li>
<li class="chapter" data-level="15.4" data-path="eodce.html"><a href="eodce.html#hac-standard-errors"><i class="fa fa-check"></i><b>15.4</b> HAC Standard Errors</a></li>
<li class="chapter" data-level="15.5" data-path="eodce.html"><a href="eodce.html#estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors"><i class="fa fa-check"></i><b>15.5</b> Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors</a></li>
<li class="chapter" data-level="15.6" data-path="eodce.html"><a href="eodce.html#orange-juice-prices-and-cold-weather"><i class="fa fa-check"></i><b>15.6</b> Orange Juice Prices and Cold Weather</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="atitsr.html"><a href="atitsr.html"><i class="fa fa-check"></i><b>16</b> Additional Topics in Time Series Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="atitsr.html"><a href="atitsr.html#vector-autoregressions"><i class="fa fa-check"></i><b>16.1</b> Vector Autoregressions</a></li>
<li class="chapter" data-level="16.2" data-path="atitsr.html"><a href="atitsr.html#ooiatdfglsurt"><i class="fa fa-check"></i><b>16.2</b> Orders of Integration and the DF-GLS Unit Root Test</a></li>
<li class="chapter" data-level="16.3" data-path="atitsr.html"><a href="atitsr.html#cointegration"><i class="fa fa-check"></i><b>16.3</b> Cointegration</a></li>
<li class="chapter" data-level="16.4" data-path="atitsr.html"><a href="atitsr.html#volatility-clustering-and-autoregressive-conditional-heteroskedasticity"><i class="fa fa-check"></i><b>16.4</b> Volatility Clustering and Autoregressive Conditional Heteroskedasticity</a><ul>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#arch-and-garch-models"><i class="fa fa-check"></i>ARCH and GARCH Models</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#application-to-stock-price-volatility"><i class="fa fa-check"></i>Application to Stock Price Volatility</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#summary-8"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Econometrics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div class = rmdreview>
This book is in <b>Open Review</b>. We want your feedback to make the book better for you and other students. You may annotate some text by <span style="background-color: #3297FD; color: white">selecting it with the cursor</span> and then click the <i class="h-icon-annotate"></i> on the pop-up menu. You can also see the annotations of others: click the <i class="h-icon-chevron-left"></i> in the upper right hand corner of the page <i class="fa fa-arrow-circle-right  fa-rotate-315" aria-hidden="true"></i>
</div>
<div id="htaciitslrm" class="section level1">
<h1><span class="header-section-number">5</span> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</h1>
<p>This chapter, continues our treatment of the simple linear regression model. The following subsections discuss how we may use our knowledge about the sampling distribution of the OLS estimator in order to make statements regarding its uncertainty.</p>
<p>These subsections cover the following topics:</p>
<ul>
<li><p>Testing Hypotheses regarding regression coefficients.</p></li>
<li><p>Confidence intervals for regression coefficients.</p></li>
<li><p>Regression when <span class="math inline">\(X\)</span> is a dummy variable.</p></li>
<li><p>Heteroskedasticity and Homoskedasticity.</p></li>
</ul>
<p>The packages <tt>AER</tt> <span class="citation">(Christian Kleiber &amp; Zeileis, <a href="#ref-R-AER">2017</a>)</span> and <tt>scales</tt> <span class="citation">(Wickham, <a href="#ref-R-scales">2017</a>)</span> are required for reproduction of the code chunks presented throughout this chapter. The package <tt>scales</tt> provides additional generic plot scaling methods. Make sure both packages are installed before you proceed. The safest way to do so is by checking whether the following code chunk executes without any errors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(AER)
<span class="kw">library</span>(scales)</code></pre></div>
<div id="testing-two-sided-hypotheses-concerning-the-slope-coefficient" class="section level2">
<h2><span class="header-section-number">5.1</span> Testing Two-Sided Hypotheses Concerning the Slope Coefficient</h2>
<p>Using the fact that <span class="math inline">\(\hat{\beta}_1\)</span> is approximately normally distributed in large samples (see <a href="lrwor.html#tsdotoe">Key Concept 4.4</a>), testing hypotheses about the true value <span class="math inline">\(\beta_1\)</span> can be done as in Chapter <a href="arosur.html#potsm">3.2</a>.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.1
</h3>
<h3 class="left">
General Form of the <span class="math inline">\(t\)</span>-Statistic
</h3>
<p>Remember from Chapter <a href="arosur.html#arosur">3</a> that a general <span class="math inline">\(t\)</span>-statistic has the form <span class="math display">\[ t = \frac{\text{estimated value} - \text{hypothesized value}}{\text{standard error of the estimator}}.\]</span></p>
</div>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.2
</h3>
<h3 class="left">
Testing Hypotheses regarding <span class="math inline">\(\beta_1\)</span>
</h3>
<p>For testing the hypothesis <span class="math inline">\(H_0: \beta_1 = \beta_{1,0}\)</span>, we need to perform the following steps:</p>
<ol style="list-style-type: decimal">
<li>Compute the standard error of <span class="math inline">\(\hat{\beta}_1\)</span>, <span class="math inline">\(SE(\hat{\beta}_1)\)</span></li>
</ol>
<p><span class="math display">\[ SE(\hat{\beta}_1) = \sqrt{ \hat{\sigma}^2_{\hat{\beta}_1} } \ \ , \ \ 
  \hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \times \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u_i}^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \right]^2}.
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Compute the <span class="math inline">\(t\)</span>-statistic</li>
</ol>
<p><span class="math display">\[ t = \frac{\hat{\beta}_1 - \beta_{1,0}}{ SE(\hat{\beta}_1) }. \]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Given a two sided alternative (<span class="math inline">\(H_1:\beta_1 \neq \beta_{1,0}\)</span>) we reject at the <span class="math inline">\(5\%\)</span> level if <span class="math inline">\(|t^{act}| &gt; 1.96\)</span> or, equivalently, if the <span class="math inline">\(p\)</span>-value is less than <span class="math inline">\(0.05\)</span>.<br><br />
Recall the definition of the <span class="math inline">\(p\)</span>-value:</li>
</ol>
<span class="math display">\[\begin{align*}
    p \text{-value} =&amp; \, \text{Pr}_{H_0} \left[ \left| \frac{ \hat{\beta}_1 - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| &gt; \left|        \frac{ \hat{\beta}_1^{act} - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| \right] \\
    =&amp; \, \text{Pr}_{H_0} (|t| &gt; |t^{act}|) \\
    \approx&amp; \, 2 \cdot \Phi(-|t^{act}|)
  \end{align*}\]</span>
<p>       The last transformation is due to the normal approximation for large samples.</p>
</div>
<p>Consider again the OLS regression stored in <tt>linear_model</tt> from Chapter <a href="lrwor.html#lrwor">4</a> that gave us the regression line</p>
<p><span class="math display">\[ \widehat{TestScore} \ = \underset{(9.47)}{698.9} - \underset{(0.49)}{2.28} \times STR \ , \ R^2=0.051 \ , \ SER=18.6. \]</span></p>
<p>For testing a hypothesis concerning the slope parameter (the coefficient on <span class="math inline">\(STR\)</span>), we need <span class="math inline">\(SE(\hat{\beta}_1)\)</span>, the standard error of the respective point estimator. As is common in the literature, standard errors are presented in parentheses below the point estimates.</p>
<p>Key Concept 5.1 reveals that it is rather cumbersome to compute the standard error and thereby the <span class="math inline">\(t\)</span>-statistic by hand. The question you should be asking yourself right now is: can we obtain these values with minimum effort using <tt>R</tt>? Yes, we can. Let us first use <tt>summary()</tt> to get a summary on the estimated coefficients in <tt>linear_model</tt>.</p>
<p><strong>Note</strong>: Throughout the book, robust standard errors are reported. We consider it instructive keep things simple at the beginning and thus start out with simple examples that do not allow for robust inference. Standard errors that are robust to heteroskedasticity are introduced in Chapter <a href="htaciitslrm.html#hah">5.4</a> where we also demonstrate how they can be computed using <tt>R</tt>. A discussion of heteroskedasticity-autocorrelation robust standard errors takes place in Chapter <a href="eodce.html#eodce">15</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># print the summary of the coefficients to the console</span></code></pre></div>
<pre><code>##               Estimate Std. Error   t value      Pr(&gt;|t|)
## (Intercept) 698.932949  9.4674911 73.824516 6.569846e-242
## STR          -2.279808  0.4798255 -4.751327  2.783308e-06</code></pre>
<p>The second column of the coefficients’ summary, reports <span class="math inline">\(SE(\hat\beta_0)\)</span> and <span class="math inline">\(SE(\hat\beta_1)\)</span>. Also, in the third column <tt>t value</tt>, we find <span class="math inline">\(t\)</span>-statistics <span class="math inline">\(t^{act}\)</span> suitable for tests of the separate hypotheses <span class="math inline">\(H_0: \beta_0=0\)</span> and <span class="math inline">\(H_0: \beta_1=0\)</span>. Furthermore, the output provides us with <span class="math inline">\(p\)</span>-values corresponding to both tests against the two-sided alternatives <span class="math inline">\(H_1:\beta_0\neq0\)</span> respectively <span class="math inline">\(H_1:\beta_1\neq0\)</span> in the fourth column of the table.</p>
<p>Let us have a closer look at the test of</p>
<p><span class="math display">\[H_0: \beta_1=0 \ \ \ vs. \ \ \ H_1: \beta_1 \neq 0.\]</span></p>
<p>We have <span class="math display">\[ t^{act} = \frac{-2.279808 - 0}{0.4798255} \approx - 4.75. \]</span></p>
<p>What does this tell us about the significance of the estimated coefficient? We reject the null hypothesis at the <span class="math inline">\(5\%\)</span> level of significance since <span class="math inline">\(|t^{act}| &gt; 1.96\)</span>. That is, the observed test statistic falls into the rejection region as <span class="math inline">\(p\text{-value} = 2.78\cdot 10^{-6} &lt; 0.05\)</span>. We conclude that the coefficient is significantly different from zero. In other words, we reject the hypothesis that the class size <em>has no influence</em> on the students test scores at the <span class="math inline">\(5\%\)</span> level.</p>
<p>Note that although the difference is negligible in the present case as we will see later, <tt>summary()</tt> does not perform the normal approximation but calculates <span class="math inline">\(p\)</span>-values using the <span class="math inline">\(t\)</span>-distribution instead. Generally, the degrees of freedom of the assumed <span class="math inline">\(t\)</span>-distribution are determined in the following manner:</p>
<p><span class="math display">\[ \text{DF} = n - k - 1 \]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations used to estimate the model and <span class="math inline">\(k\)</span> is the number of regressors, excluding the intercept. In our case, we have <span class="math inline">\(n=420\)</span> observations and the only regressor is <span class="math inline">\(STR\)</span> so <span class="math inline">\(k=1\)</span>. The simplest way to determine the model degrees of freedom is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># determine residual degrees of freedom</span>
linear_model<span class="op">$</span>df.residual</code></pre></div>
<pre><code>## [1] 418</code></pre>
<p>Hence, for the assumed sampling distribution of <span class="math inline">\(\hat\beta_1\)</span> we have</p>
<p><span class="math display">\[\hat\beta_1 \sim t_{418}\]</span> such that the <span class="math inline">\(p\)</span>-value for a two-sided significance test can be obtained by executing the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pt</span>(<span class="op">-</span><span class="fl">4.751327</span>, <span class="dt">df =</span> <span class="dv">418</span>)</code></pre></div>
<pre><code>## [1] 2.78331e-06</code></pre>
<p>The result is very close to the value provided by <tt>summary()</tt>. However since <span class="math inline">\(n\)</span> is sufficiently large one could just as well use the standard normal density to compute the <span class="math inline">\(p\)</span>-value:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="op">-</span><span class="fl">4.751327</span>)</code></pre></div>
<pre><code>## [1] 2.02086e-06</code></pre>
<p>The difference is indeed negligible. These findings tell us that, if <span class="math inline">\(H_0: \beta_1 = 0\)</span> is true and we were to repeat the whole process of gathering observations and estimating the model, observing a <span class="math inline">\(\hat\beta_1 \geq |-2.28|\)</span> is very unlikely!</p>
<p>Using <tt>R</tt> we may visualize how such a statement is made when using the normal approximation. This reflects the principles depicted in figure 5.1 in the book. Do not let the following code chunk deter you: the code is somewhat longer than the usual examples and looks unappealing but there is <strong>a lot</strong> of repetition since color shadings and annotations are added on both tails of the normal distribution. We recommend to execute the code step by step in order to see how the graph is augmented with the annotations.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the standard normal on the support [-6,6]</span>
t &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="fl">0.01</span>)

<span class="kw">plot</span>(<span class="dt">x =</span> t, 
     <span class="dt">y =</span> <span class="kw">dnorm</span>(t, <span class="dv">0</span>, <span class="dv">1</span>), 
     <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">lwd =</span> <span class="dv">2</span>, 
     <span class="dt">yaxs =</span> <span class="st">&quot;i&quot;</span>, 
     <span class="dt">axes =</span> F, 
     <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, 
     <span class="dt">main =</span> <span class="kw">expression</span>(<span class="st">&quot;Calculating the p-value of a Two-sided Test when&quot;</span> <span class="op">~</span><span class="st"> </span>t<span class="op">^</span>act <span class="op">~</span><span class="st"> &quot;=-0.47&quot;</span>), 
     <span class="dt">cex.lab =</span> <span class="fl">0.7</span>,
     <span class="dt">cex.main =</span> <span class="dv">1</span>)

tact &lt;-<span class="st"> </span><span class="op">-</span><span class="fl">4.75</span>

<span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="op">-</span><span class="fl">1.96</span>, <span class="fl">1.96</span>, <span class="op">-</span>tact, tact), <span class="dt">cex.axis =</span> <span class="fl">0.7</span>)

<span class="co"># Shade the critical regions using polygon():</span>

<span class="co"># critical region in left tail</span>
<span class="kw">polygon</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">6</span>, <span class="kw">seq</span>(<span class="op">-</span><span class="dv">6</span>, <span class="op">-</span><span class="fl">1.96</span>, <span class="fl">0.01</span>), <span class="op">-</span><span class="fl">1.96</span>),
        <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">dnorm</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">6</span>, <span class="op">-</span><span class="fl">1.96</span>, <span class="fl">0.01</span>)), <span class="dv">0</span>), 
        <span class="dt">col =</span> <span class="st">&#39;orange&#39;</span>)

<span class="co"># critical region in right tail</span>

<span class="kw">polygon</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="fl">1.96</span>, <span class="kw">seq</span>(<span class="fl">1.96</span>, <span class="dv">6</span>, <span class="fl">0.01</span>), <span class="dv">6</span>),
        <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">dnorm</span>(<span class="kw">seq</span>(<span class="fl">1.96</span>, <span class="dv">6</span>, <span class="fl">0.01</span>)), <span class="dv">0</span>), 
        <span class="dt">col =</span> <span class="st">&#39;orange&#39;</span>)

<span class="co"># Add arrows and texts indicating critical regions and the p-value</span>
<span class="kw">arrows</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">0.2</span>, <span class="op">-</span><span class="fl">2.5</span>, <span class="fl">0.02</span>, <span class="dt">length =</span> <span class="fl">0.1</span>)
<span class="kw">arrows</span>(<span class="fl">3.5</span>, <span class="fl">0.2</span>, <span class="fl">2.5</span>, <span class="fl">0.02</span>, <span class="dt">length =</span> <span class="fl">0.1</span>)

<span class="kw">arrows</span>(<span class="op">-</span><span class="dv">5</span>, <span class="fl">0.16</span>, <span class="op">-</span><span class="fl">4.75</span>, <span class="dv">0</span>, <span class="dt">length =</span> <span class="fl">0.1</span>)
<span class="kw">arrows</span>(<span class="dv">5</span>, <span class="fl">0.16</span>, <span class="fl">4.75</span>, <span class="dv">0</span>, <span class="dt">length =</span> <span class="fl">0.1</span>)

<span class="kw">text</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">0.22</span>, 
     <span class="dt">labels =</span> <span class="kw">expression</span>(<span class="st">&quot;0.025&quot;</span><span class="op">~</span><span class="st">&quot;=&quot;</span><span class="op">~</span><span class="kw">over</span>(alpha, <span class="dv">2</span>)),
     <span class="dt">cex =</span> <span class="fl">0.7</span>)
<span class="kw">text</span>(<span class="fl">3.5</span>, <span class="fl">0.22</span>, 
     <span class="dt">labels =</span> <span class="kw">expression</span>(<span class="st">&quot;0.025&quot;</span><span class="op">~</span><span class="st">&quot;=&quot;</span><span class="op">~</span><span class="kw">over</span>(alpha, <span class="dv">2</span>)),
     <span class="dt">cex =</span> <span class="fl">0.7</span>)

<span class="kw">text</span>(<span class="op">-</span><span class="dv">5</span>, <span class="fl">0.18</span>, 
     <span class="dt">labels =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;-|&quot;</span>,t[act],<span class="st">&quot;|&quot;</span>)), 
     <span class="dt">cex =</span> <span class="fl">0.7</span>)
<span class="kw">text</span>(<span class="dv">5</span>, <span class="fl">0.18</span>, 
     <span class="dt">labels =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;|&quot;</span>,t[act],<span class="st">&quot;|&quot;</span>)), 
     <span class="dt">cex =</span> <span class="fl">0.7</span>)

<span class="co"># Add ticks indicating critical values at the 0.05-level, t^act and -t^act </span>
<span class="kw">rug</span>(<span class="kw">c</span>(<span class="op">-</span><span class="fl">1.96</span>, <span class="fl">1.96</span>), <span class="dt">ticksize  =</span> <span class="fl">0.145</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)
<span class="kw">rug</span>(<span class="kw">c</span>(<span class="op">-</span>tact, tact), <span class="dt">ticksize  =</span> <span class="op">-</span><span class="fl">0.0451</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-199-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>The <span class="math inline">\(p\)</span>-Value is the area under the curve to left of <span class="math inline">\(-4.75\)</span> plus the area under the curve to the right of <span class="math inline">\(4.75\)</span>. As we already know from the calculations above, this value is very small.</p>
</div>
<div id="cifrc" class="section level2">
<h2><span class="header-section-number">5.2</span> Confidence Intervals for Regression Coefficients</h2>
<p>As we already know, estimates of the regression coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are subject to sampling uncertainty, see Chapter <a href="lrwor.html#lrwor">4</a>. Therefore, we will <em>never</em> exactly estimate the true value of these parameters from sample data in an empirical application. However, we may construct confidence intervals for the intercept and the slope parameter.</p>
<p>A <span class="math inline">\(95\%\)</span> confidence interval for <span class="math inline">\(\beta_i\)</span> has two equivalent definitions:</p>
<ul>
<li>The interval is the set of values for which a hypothesis test to the level of <span class="math inline">\(5\%\)</span> cannot be rejected.</li>
<li>The interval has a probability of <span class="math inline">\(95\%\)</span> to contain the true value of <span class="math inline">\(\beta_i\)</span>. So in <span class="math inline">\(95\%\)</span> of all samples that could be drawn, the confidence interval will cover the true value of <span class="math inline">\(\beta_i\)</span>.</li>
</ul>
<p>We also say that the interval has a confidence level of <span class="math inline">\(95\%\)</span>. The idea of the confidence interval is summarized in Key Concept 5.3.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.3
</h3>
<h3 class="left">
A Confidence Interval for <span class="math inline">\(\beta_i\)</span>
</h3>
<p>Imagine you could draw all possible random samples of given size. The interval that contains the true value <span class="math inline">\(\beta_i\)</span> in <span class="math inline">\(95\%\)</span> of all samples is given by the expression</p>
<p><span class="math display">\[ \text{CI}_{0.95}^{\beta_i} = \left[ \hat{\beta}_i - 1.96 \times SE(\hat{\beta}_i) \, , \, \hat{\beta}_i + 1.96 \times SE(\hat{\beta}_i) \right]. \]</span></p>
<p>Equivalently, this interval can be seen as the set of null hypotheses for which a <span class="math inline">\(5\%\)</span> two-sided hypothesis test does not reject.</p>
</div>
<div id="simulation-study-confidence-intervals" class="section level3 unnumbered">
<h3>Simulation Study: Confidence Intervals</h3>
<p>To get a better understanding of confidence intervals we conduct another simulation study. For now, assume that we have the following sample of <span class="math inline">\(n=100\)</span> observations on a single variable <span class="math inline">\(Y\)</span> where</p>
<span class="math display">\[ Y_i \overset{i.i.d}{\sim} \mathcal{N}(5,25), \ i = 1, \dots, 100.\]</span>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">4</span>)

<span class="co"># generate and plot the sample data</span>
Y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, 
           <span class="dt">mean =</span> <span class="dv">5</span>, 
           <span class="dt">sd =</span> <span class="dv">5</span>)

<span class="kw">plot</span>(Y, 
     <span class="dt">pch =</span> <span class="dv">19</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-202-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>We assume that the data is generated by the model</p>
<p><span class="math display">\[ Y_i = \mu + \epsilon_i \]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is an unknown constant and we know that <span class="math inline">\(\epsilon_i \overset{i.i.d.}{\sim} \mathcal{N}(0,25)\)</span>. In this model, the OLS estimator for <span class="math inline">\(\mu\)</span> is given by <span class="math display">\[ \hat\mu = \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i, \]</span> i.e., the sample average of the <span class="math inline">\(Y_i\)</span>. It further holds that</p>
<p><span class="math display">\[ SE(\hat\mu) = \frac{\sigma_{\epsilon}}{\sqrt{n}} = \frac{5}{\sqrt{100}} \]</span></p>
<p>(see Chapter <a href="arosur.html#SVSSDASE">2</a>) A large-sample <span class="math inline">\(95\%\)</span> confidence interval for <span class="math inline">\(\mu\)</span> is then given by</p>
<span class="math display" id="eq:KI">\[\begin{equation} 
CI^{\mu}_{0.95} = \left[\hat\mu - 1.96 \times \frac{5}{\sqrt{100}} \ , \ \hat\mu + 1.96 \times \frac{5}{\sqrt{100}}  \right]. \tag{5.1}
\end{equation}\]</span>
<p>It is fairly easy to compute this interval in <tt>R</tt> by hand. The following code chunk generates a named vector containing the interval bounds:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cbind</span>(<span class="dt">CIlower =</span> <span class="kw">mean</span>(Y) <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span><span class="dv">5</span> <span class="op">/</span><span class="st"> </span><span class="dv">10</span>, <span class="dt">CIupper =</span> <span class="kw">mean</span>(Y) <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span><span class="dv">5</span> <span class="op">/</span><span class="st"> </span><span class="dv">10</span>)</code></pre></div>
<pre><code>##       CIlower  CIupper
## [1,] 4.502625 6.462625</code></pre>
<p>Knowing that <span class="math inline">\(\mu = 5\)</span> we see that, for our example data, the confidence interval covers true value.</p>
<p>As opposed to real world examples, we can use <tt>R</tt> to get a better understanding of confidence intervals by repeatedly sampling data, estimating <span class="math inline">\(\mu\)</span> and computing the confidence interval for <span class="math inline">\(\mu\)</span> as in <a href="htaciitslrm.html#eq:KI">(5.1)</a>.</p>
<p>The procedure is as follows:</p>
<ul>
<li>We initialize the vectors <tt>lower</tt> and <tt>upper</tt> in which the simulated interval limits are to be saved. We want to simulate <span class="math inline">\(10000\)</span> intervals so both vectors are set to have this length.</li>
<li>We use a <tt>for()</tt> loop to sample <span class="math inline">\(100\)</span> observations from the <span class="math inline">\(\mathcal{N}(5,25)\)</span> distribution and compute <span class="math inline">\(\hat\mu\)</span> as well as the boundaries of the confidence interval in every iteration of the loop.</li>
<li>At last we join <tt>lower</tt> and <tt>upper</tt> in a matrix.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set seed</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="co"># initialize vectors of lower and upper interval boundaries</span>
lower &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">10000</span>)
upper &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">10000</span>)

<span class="co"># loop sampling / estimation / CI</span>
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>) {
  
  Y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dt">mean =</span> <span class="dv">5</span>, <span class="dt">sd =</span> <span class="dv">5</span>)
  lower[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(Y) <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span><span class="dv">5</span> <span class="op">/</span><span class="st"> </span><span class="dv">10</span>
  upper[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(Y) <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span><span class="dv">5</span> <span class="op">/</span><span class="st"> </span><span class="dv">10</span>
  
}

<span class="co"># join vectors of interval bounds in a matrix</span>
CIs &lt;-<span class="st"> </span><span class="kw">cbind</span>(lower, upper)</code></pre></div>
<p>According to Key Concept 5.3 we expect that the fraction of the <span class="math inline">\(10000\)</span> simulated intervals saved in the matrix <tt>CIs</tt> that contain the true value <span class="math inline">\(\mu=5\)</span> should be roughly <span class="math inline">\(95\%\)</span>. We can easily check this using logical operators.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(CIs[, <span class="dv">1</span>] <span class="op">&lt;=</span><span class="st"> </span><span class="dv">5</span> <span class="op">&amp;</span><span class="st"> </span><span class="dv">5</span> <span class="op">&lt;=</span><span class="st"> </span>CIs[, <span class="dv">2</span>])</code></pre></div>
<pre><code>## [1] 0.9487</code></pre>
<p>The simulation shows that the fraction of intervals covering <span class="math inline">\(\mu=5\)</span>, i.e., those intervals for which <span class="math inline">\(H_0: \mu = 5\)</span> cannot be rejected is close to the theoretical value of <span class="math inline">\(95\%\)</span>.</p>
<p>Let us draw a plot of the first <span class="math inline">\(100\)</span> simulated confidence intervals and indicate those which <em>do not</em> cover the true value of <span class="math inline">\(\mu\)</span>. We do this via horizontal lines representing the confidence intervals on top of each other.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># identify intervals not covering mu</span>
<span class="co"># (4 intervals out of 100)</span>
ID &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="op">!</span>(CIs[<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>, <span class="dv">1</span>] <span class="op">&lt;=</span><span class="st"> </span><span class="dv">5</span> <span class="op">&amp;</span><span class="st"> </span><span class="dv">5</span> <span class="op">&lt;=</span><span class="st"> </span>CIs[<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>, <span class="dv">2</span>]))

<span class="co"># initialize the plot</span>
<span class="kw">plot</span>(<span class="dv">0</span>, 
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">7</span>), 
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">100</span>), 
     <span class="dt">ylab =</span> <span class="st">&quot;Sample&quot;</span>, 
     <span class="dt">xlab =</span> <span class="kw">expression</span>(mu), 
     <span class="dt">main =</span> <span class="st">&quot;Confidence Intervals&quot;</span>)

<span class="co"># set up color vector</span>
colors &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">gray</span>(<span class="fl">0.6</span>), <span class="dv">100</span>)
colors[ID] &lt;-<span class="st"> &quot;red&quot;</span>

<span class="co"># draw reference line at mu=5</span>
<span class="kw">abline</span>(<span class="dt">v =</span> <span class="dv">5</span>, <span class="dt">lty =</span> <span class="dv">2</span>)

<span class="co"># add horizontal bars representing the CIs</span>
<span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {
  
  <span class="kw">lines</span>(<span class="kw">c</span>(CIs[j, <span class="dv">1</span>], CIs[j, <span class="dv">2</span>]), 
        <span class="kw">c</span>(j, j), 
        <span class="dt">col =</span> colors[j], 
        <span class="dt">lwd =</span> <span class="dv">2</span>)
  
}</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-206-1.png" width="384" style="display: block; margin: auto;" /></p>
</div>
<p>For the first <span class="math inline">\(100\)</span> samples, the true null hypothesis is rejected in four cases so these intervals do not cover <span class="math inline">\(\mu=5\)</span>. We have indicated the intervals which lead to a rejection of the null red.</p>
<p>Let us now come back to the example of test scores and class sizes. The regression model from Chapter <a href="lrwor.html#lrwor">4</a> is stored in <tt>linear_model</tt>. An easy way to get <span class="math inline">\(95\%\)</span> confidence intervals for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the coefficients on <tt>(intercept)</tt> and <tt>STR</tt>, is to use the function <tt>confint()</tt>. We only have to provide a fitted model object as an input to this function. The confidence level is set to <span class="math inline">\(95\%\)</span> by default but can be modified by setting the argument <tt>level</tt>, see <tt>?confint</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute 95% confidence interval for coefficients in &#39;linear_model&#39;</span>
<span class="kw">confint</span>(linear_model)</code></pre></div>
<pre><code>##                 2.5 %     97.5 %
## (Intercept) 680.32312 717.542775
## STR          -3.22298  -1.336636</code></pre>
<p>Let us check if the calculation is done as we expect it to be for <span class="math inline">\(\beta_1\)</span>, the coefficient on <tt>STR</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute 95% confidence interval for coefficients in &#39;linear_model&#39; by hand</span>
lm_summ &lt;-<span class="st"> </span><span class="kw">summary</span>(linear_model)

<span class="kw">c</span>(<span class="st">&quot;lower&quot;</span> =<span class="st"> </span>lm_summ<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df =</span> lm_summ<span class="op">$</span>df[<span class="dv">2</span>]) <span class="op">*</span><span class="st"> </span>lm_summ<span class="op">$</span>coef[<span class="dv">2</span>, <span class="dv">2</span>],
  <span class="st">&quot;upper&quot;</span> =<span class="st"> </span>lm_summ<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df =</span> lm_summ<span class="op">$</span>df[<span class="dv">2</span>]) <span class="op">*</span><span class="st"> </span>lm_summ<span class="op">$</span>coef[<span class="dv">2</span>, <span class="dv">2</span>])</code></pre></div>
<pre><code>##     lower     upper 
## -3.222980 -1.336636</code></pre>
<p>The upper and the lower bounds coincide. We have used the <span class="math inline">\(0.975\)</span>-quantile of the <span class="math inline">\(t_{418}\)</span> distribution to get the exact result reported by <tt>confint</tt>. Obviously, this interval <em>does not</em> contain the value zero which, as we have already seen in the previous section, leads to the rejection of the null hypothesis <span class="math inline">\(\beta_{1,0} = 0\)</span>.</p>
</div>
</div>
<div id="rwxiabv" class="section level2">
<h2><span class="header-section-number">5.3</span> Regression when X is a Binary Variable</h2>
<p>Instead of using a continuous regressor <span class="math inline">\(X\)</span>, we might be interested in running the regression</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 D_i + u_i \tag{5.2} \]</span></p>
<p>where <span class="math inline">\(D_i\)</span> is a binary variable, a so-called <em>dummy variable</em>. For example, we may define <span class="math inline">\(D_i\)</span> as follows:</p>
<p><span class="math display">\[ D_i = \begin{cases}
        1 \ \ \text{if $STR$ in $i^{th}$ school district &lt; 20} \\
        0 \ \ \text{if $STR$ in $i^{th}$ school district $\geq$ 20} \\
      \end{cases} \tag{5.3} \]</span></p>
<p>The regression model now is</p>
<p><span class="math display">\[ TestScore_i = \beta_0 + \beta_1 D_i + u_i. \tag{5.4} \]</span></p>
<p>Let us see how these data look like in a scatter plot:</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the dummy variable as defined above</span>
CASchools<span class="op">$</span>D &lt;-<span class="st"> </span>CASchools<span class="op">$</span>STR <span class="op">&lt;</span><span class="st"> </span><span class="dv">20</span>

<span class="co"># Plot the data</span>
<span class="kw">plot</span>(CASchools<span class="op">$</span>D, CASchools<span class="op">$</span>score,            <span class="co"># provide the data to be plotted</span>
     <span class="dt">pch =</span> <span class="dv">20</span>,                                <span class="co"># use filled circles as plot symbols</span>
     <span class="dt">cex =</span> <span class="fl">0.5</span>,                               <span class="co"># set size of plot symbols to 0.5</span>
     <span class="dt">col =</span> <span class="st">&quot;Steelblue&quot;</span>,                       <span class="co"># set the symbols&#39; color to &quot;Steelblue&quot;</span>
     <span class="dt">xlab =</span> <span class="kw">expression</span>(D[i]),                 <span class="co"># Set title and axis names</span>
     <span class="dt">ylab =</span> <span class="st">&quot;Test Score&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Dummy Regression&quot;</span>)</code></pre></div>
</div>
<div class="unfolded">
<p><img src="ITER_files/figure-html/unnamed-chunk-210-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>With <span class="math inline">\(D\)</span> as the regressor, it is not useful to think of <span class="math inline">\(\beta_1\)</span> as a slope parameter since <span class="math inline">\(D_i \in \{0,1\}\)</span>, i.e., we only observe two discrete values instead of a continuum of regressor values. There is no continuous line depicting the conditional expectation function <span class="math inline">\(E(TestScore_i | D_i)\)</span> since this function is solely defined for <span class="math inline">\(x\)</span>-positions <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p>
<p>Therefore, the interpretation of the coefficients in this regression model is as follows:</p>
<ul>
<li><p><span class="math inline">\(E(Y_i | D_i = 0) = \beta_0\)</span>, so <span class="math inline">\(\beta_0\)</span> is the expected test score in districts where <span class="math inline">\(D_i=0\)</span> where <span class="math inline">\(STR\)</span> is below <span class="math inline">\(20\)</span>.</p></li>
<li><p><span class="math inline">\(E(Y_i | D_i = 1) = \beta_0 + \beta_1\)</span> or, using the result above, <span class="math inline">\(\beta_1 = E(Y_i | D_i = 1) - E(Y_i | D_i = 0)\)</span>. Thus, <span class="math inline">\(\beta_1\)</span> is <em>the difference in group specific expectations</em>, i.e., the difference in expected test score between districts with <span class="math inline">\(STR &lt; 20\)</span> and those with <span class="math inline">\(STR \geq 20\)</span>.</p></li>
</ul>
<p>We will now use <tt>R</tt> to estimate the dummy regression model as defined by the equations (<a href="#mjx-eqn-5.2">5.2</a>) and (<a href="#mjx-eqn-5.3">5.3</a>) .</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the dummy regression model</span>
dummy_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>D, <span class="dt">data =</span> CASchools)
<span class="kw">summary</span>(dummy_model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ D, data = CASchools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -50.496 -14.029  -0.346  12.884  49.504 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  650.077      1.393 466.666  &lt; 2e-16 ***
## DTRUE          7.169      1.847   3.882  0.00012 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 18.74 on 418 degrees of freedom
## Multiple R-squared:  0.0348, Adjusted R-squared:  0.0325 
## F-statistic: 15.07 on 1 and 418 DF,  p-value: 0.0001202</code></pre>
</div>

<div class="rmdnote">
<tt>summary()</tt> reports the <span class="math inline">\(p\)</span>-value of the test that the coefficient on <tt>(Intercept)</tt> is zero to to be <tt>&lt; 2e-16</tt>. This scientific notation states that the <span class="math inline">\(p\)</span>-value is smaller than <span class="math inline">\(\frac{2}{10^{16}}\)</span>, so a very small number. The reason for this is that computers cannot handle arbitrary small numbers. In fact, <span class="math inline">\(\frac{2}{10^{16}}\)</span> is the smallest possble number <tt>R</tt> can work with.
</div>

<p>The vector <tt>CASchools$D</tt> has the type <tt>logical</tt> (to see this, use <tt>typeof(CASchools$D)</tt>) which is shown in the output of <tt>summary(dummy_model)</tt>: the label <tt>DTRUE</tt> states that all entries <tt>TRUE</tt> are coded as <tt>1</tt> and all entries <tt>FALSE</tt> are coded as <tt>0</tt>. Thus, the interpretation of the coefficient <tt>DTRUE</tt> is as stated above for <span class="math inline">\(\beta_1\)</span>.</p>
<p>One can see that the expected test score in districts with <span class="math inline">\(STR &lt; 20\)</span> (<span class="math inline">\(D_i = 1\)</span>) is predicted to be <span class="math inline">\(650.1 + 7.17 = 657.27\)</span> while districts with <span class="math inline">\(STR \geq 20\)</span> (<span class="math inline">\(D_i = 0\)</span>) are expected to have an average test score of only <span class="math inline">\(650.1\)</span>.</p>
<p>Group specific predictions can be added to the plot by execution of the following code chunk.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># add group specific predictions to the plot</span>
<span class="kw">points</span>(<span class="dt">x =</span> CASchools<span class="op">$</span>D, 
       <span class="dt">y =</span> <span class="kw">predict</span>(dummy_model), 
       <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
       <span class="dt">pch =</span> <span class="dv">20</span>)</code></pre></div>
<p>Here we use the function <tt>predict()</tt> to obtain estimates of the group specific means. The red dots represent these sample group averages. Accordingly, <span class="math inline">\(\hat{\beta}_1 = 7.17\)</span> can be seen as the difference in group averages.</p>
<p><tt>summary(dummy_model)</tt> also answers the question whether there is a statistically significant difference in group means. This in turn would support the hypothesis that students perform differently when they are taught in small classes. We can assess this by a two-tailed test of the hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>. Conveniently, the <span class="math inline">\(t\)</span>-statistic and the corresponding <span class="math inline">\(p\)</span>-value for this test are computed by <tt>summary()</tt>.</p>
<p>Since <tt>t value</tt> <span class="math inline">\(= 3.88 &gt; 1.96\)</span> we reject the null hypothesis at the <span class="math inline">\(5\%\)</span> level of significance. The same conclusion results when using the <span class="math inline">\(p\)</span>-value, which reports significance up to the <span class="math inline">\(0.00012\%\)</span> level.</p>
<p>As done with <tt>linear_model</tt>, we may alternatively use the function <tt>confint()</tt> to compute a <span class="math inline">\(95\%\)</span> confidence interval for the true difference in means and see if the hypothesized value is an element of this confidence set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># confidence intervals for coefficients in the dummy regression model</span>
<span class="kw">confint</span>(dummy_model)</code></pre></div>
<pre><code>##                  2.5 %    97.5 %
## (Intercept) 647.338594 652.81500
## DTRUE         3.539562  10.79931</code></pre>
<p>We reject the hypothesis that there is no difference between group means at the <span class="math inline">\(5\%\)</span> significance level since <span class="math inline">\(\beta_{1,0} = 0\)</span> lies outside of <span class="math inline">\([3.54, 10.8]\)</span>, the <span class="math inline">\(95\%\)</span> confidence interval for the coefficient on <span class="math inline">\(D\)</span>.</p>
</div>
<div id="hah" class="section level2">
<h2><span class="header-section-number">5.4</span> Heteroskedasticity and Homoskedasticity</h2>
<p>All inference made in the previous chapters relies on the assumption that the error variance does not vary as regressor values change. But this will often not be the case in empirical applications.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.4
</h3>
<h3 class="left">
Heteroskedasticity and Homoskedasticity
</h3>
<ul>
<li><p>The error term of our regression model is homoskedastic if the variance of the conditional distribution of <span class="math inline">\(u_i\)</span> given <span class="math inline">\(X_i\)</span>, <span class="math inline">\(Var(u_i|X_i=x)\)</span>, is constant <em>for all</em> observations in our sample: <span class="math display">\[ \text{Var}(u_i|X_i=x) = \sigma^2 \ \forall \ i=1,\dots,n. \]</span></p></li>
<li><p>If instead there is dependence of the conditional variance of <span class="math inline">\(u_i\)</span> on <span class="math inline">\(X_i\)</span>, the error term is said to be heteroskedastic. We then write <span class="math display">\[ \text{Var}(u_i|X_i=x) = \sigma_i^2 \ \forall \ i=1,\dots,n. \]</span></p></li>
<li>Homoskedasticity is a <em>special case</em> of heteroskedasticity.</li>
</ul>
</div>
<p>For a better understanding of heteroskedasticity, we generate some bivariate heteroskedastic data, estimate a linear regression model and then use box plots to depict the conditional distributions of the residuals.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load scales package for adjusting color opacities</span>
<span class="kw">library</span>(scales)

<span class="co"># generate some heteroskedastic data:</span>

<span class="co"># set seed for reproducibility</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>) 

<span class="co"># set up vector of x coordinates</span>
x &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>, <span class="dv">25</span>), <span class="dt">each =</span> <span class="dv">25</span>)

<span class="co"># initialize vector of errors</span>
e &lt;-<span class="st"> </span><span class="kw">c</span>()

<span class="co"># sample 100 errors such that the variance increases with x</span>
e[<span class="dv">1</span><span class="op">:</span><span class="dv">25</span>] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">25</span>, <span class="dt">sd =</span> <span class="dv">10</span>)
e[<span class="dv">26</span><span class="op">:</span><span class="dv">50</span>] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">25</span>, <span class="dt">sd =</span> <span class="dv">15</span>)
e[<span class="dv">51</span><span class="op">:</span><span class="dv">75</span>] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">25</span>, <span class="dt">sd =</span> <span class="dv">20</span>)
e[<span class="dv">76</span><span class="op">:</span><span class="dv">100</span>] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">25</span>, <span class="dt">sd =</span> <span class="dv">25</span>)

<span class="co"># set up y</span>
y &lt;-<span class="st"> </span><span class="dv">720</span> <span class="op">-</span><span class="st"> </span><span class="fl">3.3</span> <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>e

<span class="co"># Estimate the model </span>
mod &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x)

<span class="co"># Plot the data</span>
<span class="kw">plot</span>(<span class="dt">x =</span> x, 
     <span class="dt">y =</span> y, 
     <span class="dt">main =</span> <span class="st">&quot;An Example of Heteroskedasticity&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Student-Teacher Ratio&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Test Score&quot;</span>,
     <span class="dt">cex =</span> <span class="fl">0.5</span>, 
     <span class="dt">pch =</span> <span class="dv">19</span>, 
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">8</span>, <span class="dv">27</span>), 
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">600</span>, <span class="dv">710</span>))

<span class="co"># Add the regression line to the plot</span>
<span class="kw">abline</span>(mod, <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)

<span class="co"># Add boxplots to the plot</span>
<span class="kw">boxplot</span>(<span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>x, 
        <span class="dt">add =</span> <span class="ot">TRUE</span>, 
        <span class="dt">at =</span> <span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>, <span class="dv">25</span>), 
        <span class="dt">col =</span> <span class="kw">alpha</span>(<span class="st">&quot;gray&quot;</span>, <span class="fl">0.4</span>), 
        <span class="dt">border =</span> <span class="st">&quot;black&quot;</span>
        )</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-216-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>We have used the <tt>formula</tt> argument <tt>y ~ x</tt> in <tt>boxplot()</tt> to specify that we want to split up the vector <tt>y</tt> into groups according to <tt>x</tt>. <tt>boxplot(y ~ x)</tt> generates a boxplot for each of the groups in <tt>y</tt> defined by <tt>x</tt>.</p>
<p>For this artificial data it is clear that the conditional error variances differ. Specifically, we observe that the variance in test scores (and therefore the variance of the errors committed) <em>increases</em> with the student teacher ratio.</p>
<div id="a-real-world-example-for-heteroskedasticity" class="section level3 unnumbered">
<h3>A Real-World Example for Heteroskedasticity</h3>
<p>Think about the economic value of education: if there were no expected economic value-added to receiving university education, you probably would not be reading this script right now. A starting point to empirically verify such a relation is to have data on working individuals. More precisely, we need data on wages and education of workers in order to estimate a model like</p>
<p><span class="math display">\[ wage_i = \beta_0 + \beta_1 \cdot education_i + u_i. \]</span></p>
<p>What can be presumed about this relation? It is likely that, on average, higher educated workers earn more than workers with less education, so we expect to estimate an upward sloping regression line. Also, it seems plausible that earnings of better educated workers have a higher dispersion then those of low-skilled workers: solid education is not a guarantee for a high salary so even highly qualified workers take on low-income jobs. However, they are more likely to meet the requirements for the well-paid jobs than workers with less education for whom opportunities in the labor market are much more limited.</p>
<p>To verify this empirically we may use real data on hourly earnings and the number of years of education of employees. Such data can be found in <tt>CPSSWEducation</tt>. This data set is part of the package <tt>AER</tt> and comes from the Current Population Survey (CPS) which is conducted periodically by the <a href="http://www.bls.gov/">Bureau of Labor Statistics</a> in the United States.</p>
<p>The subsequent code chunks demonstrate how to import the data into <tt>R</tt> and how to produce a plot in the fashion of Figure 5.3 in the book.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load package and attach data</span>
<span class="kw">library</span>(AER)
<span class="kw">data</span>(<span class="st">&quot;CPSSWEducation&quot;</span>)
<span class="kw">attach</span>(CPSSWEducation)

<span class="co"># get an overview</span>
<span class="kw">summary</span>(CPSSWEducation)</code></pre></div>
<pre><code>##       age          gender        earnings        education    
##  Min.   :29.0   female:1202   Min.   : 2.137   Min.   : 6.00  
##  1st Qu.:29.0   male  :1748   1st Qu.:10.577   1st Qu.:12.00  
##  Median :29.0                 Median :14.615   Median :13.00  
##  Mean   :29.5                 Mean   :16.743   Mean   :13.55  
##  3rd Qu.:30.0                 3rd Qu.:20.192   3rd Qu.:16.00  
##  Max.   :30.0                 Max.   :97.500   Max.   :18.00</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate a simple regression model</span>
labor_model &lt;-<span class="st"> </span><span class="kw">lm</span>(earnings <span class="op">~</span><span class="st"> </span>education)

<span class="co"># plot observations and add the regression line</span>
<span class="kw">plot</span>(education, 
     earnings, 
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">150</span>))

<span class="kw">abline</span>(labor_model, 
       <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
       <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-217-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The plot reveals that the mean of the distribution of earnings increases with the level of education. This is also supported by a formal analysis: the estimated regression model stored in <tt>labor_mod</tt> shows that there is a positive relation between years of education and earnings.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># print the contents of labor_model to the console</span>
labor_model</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = earnings ~ education)
## 
## Coefficients:
## (Intercept)    education  
##      -3.134        1.467</code></pre>
<p>The estimated regression equation states that, on average, an additional year of education increases a worker’s hourly earnings by about <span class="math inline">\(\$ 1.47\)</span>. Once more we use <tt>confint()</tt> to obtain a <span class="math inline">\(95\%\)</span> confidence interval for both regression coefficients.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute a 95% confidence interval for the coefficients in the model</span>
<span class="kw">confint</span>(labor_model)</code></pre></div>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) -5.015248 -1.253495
## education    1.330098  1.603753</code></pre>
<p>Since the interval is <span class="math inline">\([1.33, 1.60]\)</span> we can reject the hypothesis that the coefficient on <tt>education</tt> is zero at the <span class="math inline">\(5\%\)</span> level.</p>
<p>Furthermore, the plot indicates that there is heteroskedasticity: if we assume the regression line to be a reasonably good representation of the conditional mean function <span class="math inline">\(E(earnings_i\vert education_i)\)</span>, the dispersion of hourly earnings around that function clearly increases with the level of education, i.e., the variance of the distribution of earnings increases. In other words: the variance of the errors (the errors made in explaining earnings by education) increases with education so that the regression errors are heteroskedastic.</p>
<p>This example makes a case that the assumption of homoskedasticity is doubtful in economic applications. Should we care about heteroskedasticity? Yes, we should. As explained in the next section, heteroskedasticity can have serious negative consequences in hypothesis testing, if we ignore it.</p>
</div>
<div id="should-we-care-about-heteroskedasticity" class="section level3 unnumbered">
<h3>Should We Care About Heteroskedasticity?</h3>
<p>To answer the question whether we should worry about heteroskedasticity being present, consider the variance of <span class="math inline">\(\hat\beta_1\)</span> under the assumption of homoskedasticity. In this case we have</p>
<p><span class="math display">\[ \sigma^2_{\hat\beta_1} = \frac{\sigma^2_u}{n \cdot \sigma^2_X} \tag{5.5} \]</span></p>
<p>which is a simplified version of the general equation (<a href="#mjx-eqn-4.1">4.1</a>) presented in Key Concept 4.4. See Appendix 5.1 of the book for details on the derivation. <tt>summary()</tt> estimates (<a href="#mjx-eqn-5.5">5.5</a>) by</p>
<p><span class="math display">\[ \overset{\sim}{\sigma}^2_{\hat\beta_1} = \frac{SER^2}{\sum_{i=1}^n (X_i - \overline{X})^2} \ \ \text{where} \ \ SER=\frac{1}{n-2} \sum_{i=1}^n \hat u_i^2. \]</span></p>
<p>Thus <tt>summary()</tt> estimates the <em>homoskedasticity-only</em> standard error</p>
<p><span class="math display">\[ \sqrt{ \overset{\sim}{\sigma}^2_{\hat\beta_1} } = \sqrt{ \frac{SER^2}{\sum_{i=1}^n(X_i - \overline{X})^2} }. \]</span></p>
<p>This is in fact an estimator for the standard deviation of the estimator <span class="math inline">\(\hat{\beta}_1\)</span> that is <em>inconsistent</em> for the true value <span class="math inline">\(\sigma^2_{\hat\beta_1}\)</span> when there is heteroskedasticity. The implication is that <span class="math inline">\(t\)</span>-statistics computed in the manner of Key Concept 5.1 do not follow a standard normal distribution, even in large samples. This issue may invalidate inference when using the previously treated tools for hypothesis testing: we should be cautious when making statements about the significance of regression coefficients on the basis of <span class="math inline">\(t\)</span>-statistics as computed by <tt>summary()</tt> or confidence intervals produced by <tt>confint()</tt> if it is doubtful for the assumption of homoskedasticity to hold!</p>
<p>We will now use <tt>R</tt> to compute the homoskedasticity-only standard error for <span class="math inline">\(\hat{\beta}_1\)</span> in the test score regression model <tt>linear_model</tt> by hand and see that it matches the value produced by <tt>summary()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Store model summary in &#39;model&#39;</span>
model &lt;-<span class="st"> </span><span class="kw">summary</span>(linear_model)

<span class="co"># Extract the standard error of the regression from model summary</span>
SER &lt;-<span class="st"> </span>model<span class="op">$</span>sigma

<span class="co"># Compute the variation in &#39;size&#39;</span>
V &lt;-<span class="st"> </span>(<span class="kw">nrow</span>(CASchools)<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="kw">var</span>(CASchools<span class="op">$</span>STR)

<span class="co"># Compute the standard error of the slope parameter&#39;s estimator and print it</span>
SE.beta_<span class="fl">1.</span>hat &lt;-<span class="st"> </span><span class="kw">sqrt</span>(SER<span class="op">^</span><span class="dv">2</span><span class="op">/</span>V)
SE.beta_<span class="fl">1.</span>hat</code></pre></div>
<pre><code>## [1] 0.4798255</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use logical operators to see if the value computed by hand matches the one provided </span>
<span class="co"># in mod$coefficients. Round estimates to four decimal places</span>
<span class="kw">round</span>(model<span class="op">$</span>coefficients[<span class="dv">2</span>, <span class="dv">2</span>], <span class="dv">4</span>) <span class="op">==</span><span class="st"> </span><span class="kw">round</span>(SE.beta_<span class="fl">1.</span>hat, <span class="dv">4</span>)</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Indeed, the estimated values are equal.</p>
</div>
<div id="computation-of-heteroskedasticity-robust-standard-errors" class="section level3 unnumbered">
<h3>Computation of Heteroskedasticity-Robust Standard Errors</h3>
<p>Consistent estimation of <span class="math inline">\(\sigma_{\hat{\beta}_1}\)</span> under heteroskedasticity is granted when the following <em>robust</em> estimator is used.</p>
<p><span class="math display">\[ SE(\hat{\beta}_1) = \sqrt{ \frac{1}{n} \cdot \frac{ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u}_i^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2  \right]^2} } \tag{5.6} \]</span></p>
<p>Standard error estimates computed this way are also referred to as <a href="https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors">Eicker-Huber-White standard errors</a>, the most frequently cited paper on this is <span class="citation">White (<a href="#ref-white1980">1980</a>)</span>.</p>
<p>It can be quite cumbersome to do this calculation by hand. Luckily, there are <tt>R</tt> function for that purpose. A convenient one named <tt>vcovHC()</tt> is part of the package <tt>sandwich</tt>.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> This function can compute a variety of standard errors. The one brought forward in (<a href="#mjx-eqn-5.6">5.6</a>) is computed when the argument <tt>type</tt> is set to <tt>“HC0”</tt>. Most of the examples presented in the book rely on a slightly different formula which is the default in the statistics package <em>STATA</em>:</p>
<span class="math display" id="eq:hc1">\[\begin{align}
SE(\hat{\beta}_1)_{HC1} = \sqrt{ \frac{1}{n} \cdot \frac{ \frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u}_i^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2  \right]^2}} \tag{5.2}
\end{align}\]</span>
<p>The difference is that we multiply by <span class="math inline">\(\frac{1}{n-2}\)</span> in the numerator of <a href="htaciitslrm.html#eq:hc1">(5.2)</a>. This is a degrees of freedom correction and was considered by <span class="citation">MacKinnon &amp; White (<a href="#ref-mackinnon1985">1985</a>)</span>. To get <tt>vcovHC()</tt> to use <a href="htaciitslrm.html#eq:hc1">(5.2)</a>, we have to set <tt>type = “HC1”</tt>.</p>
<p>Let us now compute robust standard error estimates for the coefficients in <tt>linear_model</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute heteroskedasticity-robust standard errors</span>
vcov &lt;-<span class="st"> </span><span class="kw">vcovHC</span>(linear_model, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)
vcov</code></pre></div>
<pre><code>##             (Intercept)        STR
## (Intercept)  107.419993 -5.3639114
## STR           -5.363911  0.2698692</code></pre>
<p>The output of <tt>vcovHC()</tt> is the variance-covariance matrix of coefficient estimates. We are interested in the square root of the diagonal elements of this matrix, i.e., the standard error estimates.</p>

<div class="rmdknit">
<p>When we have k &gt; 1 regressors, writing down the equations for a regression model becomes very messy. A more convinient way to denote and estimate so-called multiple regression models (see Chapter <a href="rmwmr.html#rmwmr">6</a>) is by using matrix algebra. This is why functions like <tt>vcovHC()</tt> produce matrices. In the simple linear regression model, the variances and covariances of the estimators can be gathered in the symmetric variance-covariance matrix</p>
<span class="math display">\[\begin{equation}
\text{Var}
  \begin{pmatrix}
    \hat\beta_0 \\
    \hat\beta_1
  \end{pmatrix} = 
\begin{pmatrix}
  \text{Var}(\hat\beta_0) &amp; \text{Cov}(\hat\beta_0,\hat\beta_1) \\
\text{Cov}(\hat\beta_0,\hat\beta_1) &amp; \text{Var}(\hat\beta_1)
\end{pmatrix},
\end{equation}\]</span>
<p>so <tt>vcovHC()</tt> gives us <span class="math inline">\(\widehat{\text{Var}}(\hat\beta_0)\)</span>, <span class="math inline">\(\widehat{\text{Var}}(\hat\beta_1)\)</span> and <span class="math inline">\(\widehat{\text{Cov}}(\hat\beta_0,\hat\beta_1)\)</span>, but most of the time we are interested in the diagonal elements of the estimated matrix.</p>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the square root of the diagonal elements in vcov</span>
robust_se &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">diag</span>(vcov))
robust_se</code></pre></div>
<pre><code>## (Intercept)         STR 
##  10.3643617   0.5194893</code></pre>
<p>Now assume we want to generate a coefficient summary as provided by <tt>summary()</tt> but with <em>robust</em> standard errors of the coefficient estimators, robust <span class="math inline">\(t\)</span>-statistics and corresponding <span class="math inline">\(p\)</span>-values for the regression model <tt>linear_model</tt>. This can be done using <tt>coeftest()</tt> from the package <tt>lmtest</tt>, see <code>?coeftest</code>. Further we specify in the argument <tt>vcov.</tt> that <tt>vcov</tt>, the Eicker-Huber-White estimate of the variance matrix we have computed before, should be used.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># we invoke the function `coeftest()` on our model</span>
<span class="kw">coeftest</span>(linear_model, <span class="dt">vcov. =</span> vcov)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 698.93295   10.36436 67.4362 &lt; 2.2e-16 ***
## STR          -2.27981    0.51949 -4.3886 1.447e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We see that the values reported in the column <tt>Std. Error</tt> are equal those from <tt>sqrt(diag(vcov))</tt>.</p>
<p>How severe are the implications of using homoskedasticity-only standard errors in the presence of heteroskedasticity? The answer is: it depends. As mentioned above we face the risk of drawing wrong conclusions when conducting significance tests. <br> Let us illustrate this by generating another example of a heteroskedastic data set and using it to estimate a simple regression model. We take</p>
<p><span class="math display">\[ Y_i = \beta_1 \cdot X_i + u_i \ \ , \ \ u_i \overset{i.i.d.}{\sim} \mathcal{N}(0,0.36 \cdot X_i^2)  \]</span></p>
<p>with <span class="math inline">\(\beta_1=1\)</span> as the data generating process. Clearly, the assumption of homoskedasticity is violated here since the variance of the errors is a nonlinear, increasing function of <span class="math inline">\(X_i\)</span> but the errors have zero mean and are i.i.d. such that the assumptions made in Key Concept 4.3 are not violated. As before, we are interested in estimating <span class="math inline">\(\beta_1\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate heteroskedastic data </span>
X &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">500</span>
Y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">500</span>, <span class="dt">mean =</span> X, <span class="dt">sd =</span> <span class="fl">0.6</span> <span class="op">*</span><span class="st"> </span>X)

<span class="co"># estimate a simple regression model</span>
reg &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X)</code></pre></div>
<p>We plot the data and add the regression line.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the data</span>
<span class="kw">plot</span>(<span class="dt">x =</span> X, <span class="dt">y =</span> Y, 
     <span class="dt">pch =</span> <span class="dv">19</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">cex =</span> <span class="fl">0.8</span>)

<span class="co"># add the regression line to the plot</span>
<span class="kw">abline</span>(reg, 
       <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>, 
       <span class="dt">lwd =</span> <span class="fl">1.5</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-225-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>The plot shows that the data are heteroskedastic as the variance of <span class="math inline">\(Y\)</span> grows with <span class="math inline">\(X\)</span>. We next conduct a significance test of the (true) null hypothesis <span class="math inline">\(H_0: \beta_1 = 1\)</span> twice, once using the homoskedasticity-only standard error formula and once with the robust version (<a href="#mjx-eqn-5.6">5.6</a>). An easy way to do this in <tt>R</tt> is the function <tt>linearHypothesis()</tt> from the package <tt>car</tt>, see <code>?linearHypothesis</code>. It allows to test linear hypotheses about parameters in linear models in a similar way as done with a <span class="math inline">\(t\)</span>-statistic and offers various robust covariance matrix estimators. We test by comparing the tests’ <span class="math inline">\(p\)</span>-values to the significance level of <span class="math inline">\(5\%\)</span>.</p>

<div class="rmdknit">
<p><tt>linearHypothesis()</tt> computes a test statistic that follows an <span class="math inline">\(F\)</span>-distribution under the null hypothesis. We will not loose too much words on the underlying theory. In general, the idea of the <span class="math inline">\(F\)</span>-test is to compare the fit of different models. When testing a hypothesis about a <em>single</em> coefficient using an <span class="math inline">\(F\)</span>-test, one can show that the test statistic is simply the square of the corresponding <span class="math inline">\(t\)</span>-statistic:</p>
<p><span class="math display">\[F = t^2 = \left(\frac{\hat\beta_i - \beta_{i,0}}{SE(\hat\beta_i)}\right)^2 \sim F_{1,n-k-1}\]</span></p>
In <tt>linearHypothesis()</tt>, there are different ways to specify the hypothesis to be tested, e.g., using a vector of the type <tt>character</tt> (as done in the next code chunk), see <tt>?linearHypothesis</tt> for alternatives. The function returns an object of class <tt>anova</tt> which contains further information on the test that can be accessed using the <tt>$</tt> operator.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># test hypthesis using the default standard error formula</span>
<span class="kw">linearHypothesis</span>(reg, <span class="dt">hypothesis.matrix =</span> <span class="st">&quot;X = 1&quot;</span>)<span class="op">$</span><span class="st">&#39;Pr(&gt;F)&#39;</span>[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># test hypothesis using the robust standard error formula</span>
<span class="kw">linearHypothesis</span>(reg, <span class="dt">hypothesis.matrix =</span> <span class="st">&quot;X = 1&quot;</span>, <span class="dt">white.adjust =</span> <span class="st">&quot;hc1&quot;</span>)<span class="op">$</span><span class="st">&#39;Pr(&gt;F)&#39;</span>[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<p>This is a good example of what can go wrong if we ignore heteroskedasticity: for the data set at hand the default method rejects the null hypothesis <span class="math inline">\(\beta_1 = 1\)</span> although it is true. When using the robust standard error formula the test does not reject the null. Of course, we could this might just be a coincidence and both tests do equally well in maintaining the type I error rate of <span class="math inline">\(5\%\)</span>. This can be further investigated by computing <em>Monte Carlo</em> estimates of the rejection frequencies of both tests on the basis of a large number of random samples. We proceed as follows:</p>
<ul>
<li>initialize vectors <tt>t</tt> and <tt>t.rob</tt>.</li>
<li>Using a <tt>for()</tt> loop, we generate <span class="math inline">\(10000\)</span> heteroskedastic random samples of size <span class="math inline">\(1000\)</span>, estimate the regression model and check whether the tests falsely reject the null at the level of <span class="math inline">\(5\%\)</span> using comparison operators. The results are stored in the respective vectors <tt>t</tt> and <tt>t.rob</tt>.</li>
<li>After the simulation, we compute the fraction of false rejections for both tests.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize vectors t and t.rob</span>
t &lt;-<span class="st"> </span><span class="kw">c</span>()
t.rob &lt;-<span class="st"> </span><span class="kw">c</span>()

<span class="co"># loop sampling and estimation</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>) {
  
  <span class="co"># sample data</span>
  X &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>
  Y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1000</span>, <span class="dt">mean =</span> X, <span class="dt">sd =</span> <span class="fl">0.6</span><span class="op">*</span>X)

  <span class="co"># estimate regression model</span>
  reg &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X)

  <span class="co"># homoskedasdicity-only significance test</span>
  t[i] &lt;-<span class="st"> </span><span class="kw">linearHypothesis</span>(reg, <span class="st">&quot;X = 1&quot;</span>)<span class="op">$</span><span class="st">&#39;Pr(&gt;F)&#39;</span>[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>

  <span class="co"># robust significance test</span>
  t.rob[i] &lt;-<span class="st"> </span><span class="kw">linearHypothesis</span>(reg, <span class="st">&quot;X = 1&quot;</span>, <span class="dt">white.adjust =</span> <span class="st">&quot;hc1&quot;</span>)<span class="op">$</span><span class="st">&#39;Pr(&gt;F)&#39;</span>[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>

}

<span class="co"># compute the fraction of false rejections</span>
<span class="kw">round</span>(<span class="kw">cbind</span>(<span class="dt">t =</span> <span class="kw">mean</span>(t), <span class="dt">t.rob =</span> <span class="kw">mean</span>(t.rob)), <span class="dv">3</span>)</code></pre></div>
<pre><code>##          t t.rob
## [1,] 0.073  0.05</code></pre>
<p>These results reveal the increased risk of falsely rejecting the null using the homoskedasticity-only standard error for the testing problem at hand: with the common standard error estimator, <span class="math inline">\(7.28\%\)</span> of all tests falsely reject the null hypothesis. In contrast, with the robust test statistic we are closer to the nominal level of <span class="math inline">\(5\%\)</span>.</p>
</div>
</div>
<div id="the-gauss-markov-theorem" class="section level2">
<h2><span class="header-section-number">5.5</span> The Gauss-Markov Theorem</h2>
<p>When estimating regression models, we know that the results of the estimation procedure are random. However, when using unbiased estimators, at least on average, we estimate the true parameter. When comparing different unbiased estimators, it is therefore interesting to know which one has the highest precision: being aware that the likelihood of estimating the <em>exact</em> value of the parameter of interest is <span class="math inline">\(0\)</span> in an empirical application, we want to make sure that the likelihood of obtaining an estimate very close to the true value is as high as possible. This means we want to use the estimator with the lowest variance of all unbiased estimators, provided we care about unbiasedness. The Gauss-Markov theorem states that, in the class of conditionally unbiased linear estimators, the OLS estimator has this property under certain conditions.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 5.5
</h3>
<h3 class="left">
The Gauss-Markov Theorem for <span class="math inline">\(\hat{\beta}_1\)</span>
</h3>
<p>Suppose that the assumptions made in Key Concept 4.3 hold <em>and</em> that the errors are <em>homoskedastic</em>. The OLS estimator is the best (in the sense of smallest variance) linear conditionally unbiased estimator (BLUE) in this setting.</p>
<p>Let us have a closer look at what this means:</p>
<ul>
<li><p>Estimators of <span class="math inline">\(\beta_1\)</span> that are linear functions of the <span class="math inline">\(Y_1, \dots, Y_n\)</span> and that are unbiased conditionally on the regressor <span class="math inline">\(X_1, \dots, X_n\)</span> can be written as <span class="math display">\[ \overset{\sim}{\beta}_1 = \sum_{i=1}^n a_i Y_i \]</span> where the <span class="math inline">\(a_i\)</span> are weights that are allowed to depend on the <span class="math inline">\(X_i\)</span> but <em>not</em> on the <span class="math inline">\(Y_i\)</span>.</p></li>
<li><p>We already know that <span class="math inline">\(\overset{\sim}{\beta}_1\)</span> has a sampling distribution: <span class="math inline">\(\overset{\sim}{\beta}_1\)</span> is a linear function of the <span class="math inline">\(Y_i\)</span> which are random variables. If now <span class="math display">\[ E(\overset{\sim}{\beta}_1 | X_1, \dots, X_n) = \beta_1, \]</span> <span class="math inline">\(\overset{\sim}{\beta}_1\)</span> is a linear unbiased estimator of <span class="math inline">\(\beta_1\)</span>, conditionally on the <span class="math inline">\(X_1, \dots, X_n\)</span>.</p></li>
<li>We may ask if <span class="math inline">\(\overset{\sim}{\beta}_1\)</span> is also the <em>best</em> estimator in this class, i.e., the most efficient one of all linear conditionally unbiased estimators where “most efficient” means smallest variance. The weights <span class="math inline">\(a_i\)</span> play an important role here and it turns out that OLS uses just the right weights to have the BLUE property.</li>
</ul>
</div>
<div id="simulation-study-blue-estimator" class="section level3 unnumbered">
<h3>Simulation Study: BLUE Estimator</h3>
<p>Consider the case of a regression of <span class="math inline">\(Y_i,\dots,Y_n\)</span> only on a constant. Here, the <span class="math inline">\(Y_i\)</span> are assumed to be a random sample from a population with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. The OLS estimator in this model is simply the sample mean, see Chapter <a href="arosur.html#potsm">3.2</a>.</p>
<span class="math display" id="eq:bluemean">\[\begin{equation}
\hat{\beta}_1 = \sum_{i=1}^n \underbrace{\frac{1}{n}}_{=a_i} Y_i \tag{5.3}
\end{equation}\]</span>
<p>Clearly, each observation is weighted by</p>
<p><span class="math display">\[a_i = \frac{1}{n}.\]</span></p>
<p>and we also know that <span class="math inline">\(\text{Var}(\hat{\beta}_1)=\frac{\sigma^2}{n}\)</span>.</p>
<p>We now use <tt>R</tt> to conduct a simulation study that demonstrates what happens to the variance of <a href="htaciitslrm.html#eq:bluemean">(5.3)</a> if different weights <span class="math display">\[ w_i = \frac{1 \pm \epsilon}{n} \]</span> are assigned to either half of the sample <span class="math inline">\(Y_1, \dots, Y_n\)</span> instead of using <span class="math inline">\(\frac{1}{n}\)</span>, the OLS weights.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set sample size and number of repetitions</span>
n &lt;-<span class="st"> </span><span class="dv">100</span>      
reps &lt;-<span class="st"> </span><span class="fl">1e5</span>

<span class="co"># choose epsilon and create a vector of weights as defined above</span>
epsilon &lt;-<span class="st"> </span><span class="fl">0.8</span>
w &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>((<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>epsilon) <span class="op">/</span><span class="st"> </span>n, n <span class="op">/</span><span class="st"> </span><span class="dv">2</span>), 
       <span class="kw">rep</span>((<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>epsilon) <span class="op">/</span><span class="st"> </span>n, n <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) )

<span class="co"># draw a random sample y_1,...,y_n from the standard normal distribution, </span>
<span class="co"># use both estimators 1e5 times and store the result in the vectors &#39;ols&#39; and </span>
<span class="co"># &#39;weightedestimator&#39;</span>

ols &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, reps)
weightedestimator &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, reps)

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>reps) {
  
  y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n)
  ols[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(y)
  weightedestimator[i] &lt;-<span class="st"> </span><span class="kw">crossprod</span>(w, y)
  
}

<span class="co"># plot kernel density estimates of the estimators&#39; distributions: </span>

<span class="co"># OLS</span>
<span class="kw">plot</span>(<span class="kw">density</span>(ols), 
     <span class="dt">col =</span> <span class="st">&quot;purple&quot;</span>, 
     <span class="dt">lwd =</span> <span class="dv">3</span>, 
     <span class="dt">main =</span> <span class="st">&quot;Density of OLS and Weighted Estimator&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Estimates&quot;</span>)

<span class="co"># weighted</span>
<span class="kw">lines</span>(<span class="kw">density</span>(weightedestimator), 
      <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">3</span>) 

<span class="co"># add a dashed line at 0 and add a legend to the plot</span>
<span class="kw">abline</span>(<span class="dt">v =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>)

<span class="kw">legend</span>(<span class="st">&#39;topright&#39;</span>, 
       <span class="kw">c</span>(<span class="st">&quot;OLS&quot;</span>, <span class="st">&quot;Weighted&quot;</span>), 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;purple&quot;</span>, <span class="st">&quot;steelblue&quot;</span>), 
       <span class="dt">lwd =</span> <span class="dv">3</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-230-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>What conclusion can we draw from the result?</p>
<ul>
<li>Both estimators seem to be unbiased: the means of their estimated distributions are zero.</li>
<li>The estimator using weights that deviate from those implied by OLS is less efficient than the OLS estimator: there is higher dispersion when weights are <span class="math inline">\(w_i = \frac{1 \pm 0.8}{100}\)</span> instead of <span class="math inline">\(w_i=\frac{1}{100}\)</span> as required by the OLS solution.</li>
</ul>
<p>Hence, the simulation results support the Gauss-Markov Theorem.</p>
</div>
</div>
<div id="using-the-t-statistic-in-regression-when-the-sample-size-is-small" class="section level2">
<h2><span class="header-section-number">5.6</span> Using the t-Statistic in Regression When the Sample Size Is Small</h2>
<p>The three OLS assumptions discussed in Chapter <a href="lrwor.html#lrwor">4</a> (see Key Concept 4.3) are the foundation for the results on the large sample distribution of the OLS estimators in the simple regression model. What can be said about the distribution of the estimators and their <span class="math inline">\(t\)</span>-statistics when the sample size is small and the population distribution of the data is unknown? Provided that the three least squares assumptions hold and the errors are normally distributed and homoskedastic (we refer to these conditions as the homoskedastic normal regression assumptions), we have normally distributed estimators and <span class="math inline">\(t\)</span>-distributed test statistics in small samples.</p>
<p>Recall the <a href="pt.html#thetdist">definition</a> of a <span class="math inline">\(t\)</span>-distributed variable</p>
<p><span class="math display">\[ \frac{Z}{\sqrt{W/M}} \sim t_M\]</span></p>
<p>where <span class="math inline">\(Z\)</span> is a standard normal random variable, <span class="math inline">\(W\)</span> is <span class="math inline">\(\chi^2\)</span> distributed with <span class="math inline">\(M\)</span> degrees of freedom and <span class="math inline">\(Z\)</span> and <span class="math inline">\(W\)</span> are independent. See section 5.6 in the book for a more detailed discussion of the small sample distribution of <span class="math inline">\(t\)</span>-statistics in regression methods.</p>
<p>Let us simulate the distribution of regression <span class="math inline">\(t\)</span>-statistics based on a large number of small random samples, say <span class="math inline">\(n=20\)</span>, and compare the simulated distributions to the theoretical distributions which should be <span class="math inline">\(t_{18}\)</span>, the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(18\)</span> degrees of freedom (recall that <span class="math inline">\(\text{DF}=n-k-1\)</span>).</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize two vectors</span>
beta_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">c</span>()
beta_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">c</span>()

<span class="co"># loop sampling / estimation / t statistics</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>) {
  
  X &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">20</span>, <span class="dv">0</span>, <span class="dv">20</span>)
  Y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">20</span>, <span class="dt">mean =</span> X)
  reg &lt;-<span class="st"> </span><span class="kw">summary</span>(<span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X))
  beta_<span class="dv">0</span>[i] &lt;-<span class="st"> </span>(reg<span class="op">$</span>coefficients[<span class="dv">1</span>, <span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="dv">0</span>)<span class="op">/</span>(reg<span class="op">$</span>coefficients[<span class="dv">1</span>, <span class="dv">2</span>])
  beta_<span class="dv">1</span>[i] &lt;-<span class="st"> </span>(reg<span class="op">$</span>coefficients[<span class="dv">2</span>, <span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span>(reg<span class="op">$</span>coefficients[<span class="dv">2</span>, <span class="dv">2</span>])
  
}

<span class="co"># plot the distributions and compare with t_18 density:</span>

<span class="co"># divide plotting area</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))

<span class="co"># plot the simulated density of beta_0</span>
<span class="kw">plot</span>(<span class="kw">density</span>(beta_<span class="dv">0</span>), 
     <span class="dt">lwd =</span> <span class="dv">2</span> , 
     <span class="dt">main =</span> <span class="kw">expression</span>(<span class="kw">widehat</span>(beta)[<span class="dv">0</span>]), 
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>))

<span class="co"># add the t_18 density to the plot</span>
<span class="kw">curve</span>(<span class="kw">dt</span>(x, <span class="dt">df =</span> <span class="dv">18</span>), 
      <span class="dt">add =</span> T, 
      <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>, 
      <span class="dt">lty =</span> <span class="dv">2</span>)

<span class="co"># plot the simulated density of beta_1</span>
<span class="kw">plot</span>(<span class="kw">density</span>(beta_<span class="dv">1</span>), 
     <span class="dt">lwd =</span> <span class="dv">2</span>, 
     <span class="dt">main =</span> <span class="kw">expression</span>(<span class="kw">widehat</span>(beta)[<span class="dv">1</span>]), <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>)
     )

<span class="co"># add the t_18 density to the plot</span>
<span class="kw">curve</span>(<span class="kw">dt</span>(x, <span class="dt">df =</span> <span class="dv">18</span>), 
      <span class="dt">add =</span> T, 
      <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>, 
      <span class="dt">lty =</span> <span class="dv">2</span>) </code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-231-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>The outcomes are consistent with our expectations: the empirical distributions of both estimators seem to track the theoretical <span class="math inline">\(t_{18}\)</span> distribution quite closely.</p>
</div>
<div id="exercises-3" class="section level2">
<h2><span class="header-section-number">5.7</span> Exercises</h2>
<div class="DCexercise">
<h4 id="testing-two-null-hypotheses-separately" class="unnumbered">1. Testing Two Null Hypotheses Separately</h4>
<p>Consider the estimated regression model</p>
<p><span class="math display">\[ \widehat{TestScore} = \underset{(23.96)}{567.43} - \underset{(0.85)}{7.15} \times STR, \, R^2 = 0.8976, \, SER=15.19 \]</span></p>
<p>with standard errors in parentheses.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Compute the <span class="math inline">\(p\)</span>-value for a <span class="math inline">\(t\)</span>-test of the hypothesis that the intercept is zero against the two-sided alternative that it is non-zero. Save the result to <tt>p_int</tt></li>
<li>Compute the <span class="math inline">\(p\)</span>-value for a <span class="math inline">\(t\)</span>-test of the hypothesis that the coefficient of <tt>STR</tt> is zero against the two-sided alternative that it is non-zero. Save the result to <tt>p_STR</tt></li>
</ul>
<iframe src="DCL/ex5_1.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hint:</strong></p>
<p>Both hypotheses can be tested individually using a two-sided test. Use <tt>pnorm()</tt> to obtain cumulated probabilities for standard normally distributed outcomes.</p>
</div>
<div class="DCexercise">
<h4 id="two-null-hypotheses-you-cannot-reject-can-you" class="unnumbered">2. Two Null Hypotheses You Cannot Reject, Can You?</h4>
<p>Consider again the estimated regression model</p>
<p><span class="math display">\[\widehat{TestScore} = \underset{(23.96)}{567.43} - \underset{(0.85)}{7.15} \times STR, \, R^2 = 0.8976, \,SER=15.19\]</span></p>
<p>Can you reject the null hypotheses discussed in the previous code exercise using individual <span class="math inline">\(t\)</span>-tests at the <span class="math inline">\(5\%\)</span> significance level?</p>
<p>The variables <tt>t_int</tt> and <tt>t_STR</tt> are the <span class="math inline">\(t\)</span>-statistics. Both are available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Gather <tt>t_int</tt> and <tt>t_STR</tt> in a vector <tt>test</tt> and use logical operators to check whether the corresponding rejection rule applies.</li>
</ul>
<iframe src="DCL/ex5_2.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li>Both tests are two-sided <span class="math inline">\(t\)</span>-tests. Key Concept 5.2 recaps how a two-sided <span class="math inline">\(t\)</span>-test is conducted.</li>
<li>Use <tt>qnorm()</tt> to obtain standard normal critical values.</li>
</ul>
</div>
<div class="DCexercise">
<h4 id="confidence-intervals" class="unnumbered">3. Confidence Intervals</h4>
<p><tt>mod</tt>, the object of class <tt>lm</tt> which contains the estimated regression model <span class="math display">\[\widehat{TestScore} = \underset{(23.96)}{567.43} - \underset{(0.85)}{7.15} \times STR, \, R^2 = 0.8976, \,SER=15.19\]</span> is available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<p>Compute <span class="math inline">\(90\%\)</span> confidence intervals for both coefficients.</p>
<iframe src="DCL/ex5_3.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hint:</strong></p>
<p>Use the function <tt>confint()</tt>, see <tt>?confint</tt>. The argument <tt>level</tt> sets the confidence level to be used.</p>
</div>
<div class="DCexercise">
<h4 id="a-confidence-interval-for-the-mean-i" class="unnumbered">4. A Confidence Interval for the Mean I</h4>
<p>Consider the regression model <span class="math display">\[Y_i = \beta_1 + u_i\]</span> where <span class="math inline">\(Y_i \sim \mathcal{N}(\mu, \sigma^2)\)</span>. Following the discussion preceding equation <a href="htaciitslrm.html#eq:KI">(5.1)</a>, a <span class="math inline">\(95\%\)</span> confidence interval for the mean of the <span class="math inline">\(Y_i\)</span> can be computed as</p>
<p><span class="math display">\[CI^{\mu}_{0.95} = \left[\hat\mu - 1.96 \times \frac{\sigma}{\sqrt{n}}; \, \hat\mu + 1.96 \times \frac{\sigma}{\sqrt{n}} \right].\]</span></p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Sample <span class="math inline">\(n=100\)</span> observations from a normal distribution with variance <span class="math inline">\(100\)</span> and mean <span class="math inline">\(10\)</span>.</li>
<li>Use the sample to estimate <span class="math inline">\(\beta_1\)</span>. Save the estimate in <tt>mu_hat</tt>.</li>
<li>Assume that <span class="math inline">\(\sigma^2 = 100\)</span> is known. Replace the <tt>NA</tt>s in the code below to obtain a <span class="math inline">\(95\%\)</span> confidence interval for the mean of the <span class="math inline">\(Y_i\)</span>.</li>
</ul>
<iframe src="DCL/ex5_4.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hint:</strong></p>
<p>Use the function <tt>confint()</tt>, see <tt>?confint</tt>. The argument <tt>level</tt> sets the confidence level.</p>
</div>
<div class="DCexercise">
<h4 id="a-confidence-interval-for-the-mean-ii" class="unnumbered">5. A Confidence Interval for the Mean II</h4>
<p>For historical reasons, some <tt>R</tt> functions which we use to obtain inference on model parameters, among them <tt>confint()</tt> and <tt>summary()</tt>, rely on the <span class="math inline">\(t\)</span>-distribution instead of using the large-sample normal approximation. This is why for small sample sizes (and hence small degrees of freedom), <span class="math inline">\(p\)</span>-values and confidence intervals reported by these functions deviate from those computed using critical values or cumulative probabilities of the standard normal distribution.</p>
<p>The <span class="math inline">\(95\%\)</span> confidence interval for the mean in the previous exercise is <span class="math inline">\([9.13, 13.05]\)</span>.</p>
<p><strong>Instructions:</strong></p>
<p>100 observations sampled from a normal distribution with <span class="math inline">\(\mu=10\)</span> and <span class="math inline">\(\sigma^2=100\)</span> have been assigned to the vector <tt>s</tt> which is available in your environment.</p>
<p>Set up a suitable regression model to estimate the mean of the observations in <tt>s</tt>. Then use <tt>confint()</tt> to compute a <span class="math inline">\(95\%\)</span> confidence interval for the mean.</p>
<p>(Check that the result is different from the interval reported above.)</p>
<iframe src="DCL/ex5_5.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="regression-on-a-dummy-variable-i" class="unnumbered">6. Regression on a Dummy Variable I</h4>
<p>Chapter <a href="htaciitslrm.html#rwxiabv">5.3</a> discusses regression when <span class="math inline">\(X\)</span> is a dummy variable. We have used a <tt>for()</tt> loop to generate a binary variable indicating whether a schooling district in the <tt>CASchools</tt> data set has a student-teacher ratio below <span class="math inline">\(20\)</span>. Though it is instructive to use a loop for this, there are alternate ways to achieve the same with fewer lines of code.</p>
<p>A <tt>data.frame</tt> <tt>DF</tt> with <span class="math inline">\(100\)</span> observations of a variable <tt>X</tt> is available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Use <tt>ifelse()</tt> to generate a binary vector <tt>dummy</tt> indicating whether the observations in <tt>X</tt> are <em>positive</em>.</p></li>
<li><p>Append <tt>dummy</tt> to the <tt>data.frame</tt> <tt>DF</tt>.</p></li>
</ul>
<iframe src="DCL/ex5_6.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="regression-on-a-dummy-variable-ii" class="unnumbered">7. Regression on a Dummy Variable II</h4>
<p>A <tt>data.frame</tt> <tt>DF</tt> with 100 observations on <tt>Y</tt> and the binary variable <tt>D</tt> from the previous exercise is available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Compute the group-specific sample means of the observations in <tt>Y</tt>: save the mean of observations in <tt>Y</tt> where <tt>dummy == 1</tt> to <tt>mu_Y_D1</tt> and assign the mean of those observations with <tt>D == 0</tt> to <tt>mu_Y_D0</tt>.</p></li>
<li><p>Use <tt>lm()</tt> to regress <tt>Y</tt> on <tt>D</tt>, i.e., estimate the coefficients in the model <span class="math display">\[Y_i = \beta_0 + \beta_1 \times D_i + u_i.\]</span></p></li>
</ul>
<p>Also check that the estimates of the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> reflect specific sample means. Can you tell which (no code submission needed)?</p>
<iframe src="DCL/ex5_7.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="regression-on-a-dummy-variable-iii" class="unnumbered">8. Regression on a Dummy Variable III</h4>
<p>In this exercise, you have to visualize some of the results from the dummy regression model <span class="math display">\[\widehat{Y}_i = -0.66 + 1.43 \times D_i\]</span> estimated in the previous exercise.</p>
<p>A <tt>data.frame</tt> <tt>DF</tt> with 100 observations on <tt>X</tt> and the binary variable <tt>dummy</tt> as well as the model object <tt>dummy_mod</tt> from the previous exercise are available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Start by drawing a visually appealing plot of the observations on <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span> based on the code chunk provided in <tt>Script.R</tt>. Replace the <tt>???</tt> by the correct expressions!</p></li>
<li><p>Add the regression line to the plot.</p></li>
</ul>
<iframe src="DCL/ex5_8.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="gender-wage-gap-i" class="unnumbered">9. Gender Wage Gap I</h4>
<p>The cross-section data set <tt>CPS1985</tt> is a subsample from the May 1985 <em>Current Population Survey</em> conducted by the <em>US Census Bureau</em> which contains observations on, among others things, wage and the gender of employees.</p>
<p><tt>CPS1985</tt> is part of the package <tt>AER</tt>.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Attach the package <tt>AER</tt> and load the data set <tt>CPS1985</tt>.</p></li>
<li><p>Estimate the dummy regression model <span class="math display">\[wage_i = \beta_0 + \beta_1 \cdot female_i + u_i\]</span> where</p>
<span class="math display">\[\begin{align*}
  female_i = 
  \begin{cases}
    1, &amp; \text{if employee} \, i \, \text{is female,} \\
    0, &amp; \text{if employee} \, i \,  \text{is male.}
  \end{cases}
\end{align*}\]</span>
<p>Save the result in <tt>wage_mod</tt>.</p></li>
</ul>
<iframe src="DCL/ex5_9.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="gender-wage-gap-ii" class="unnumbered">10. Gender Wage Gap II</h4>
<p>The wage regression from the previous exercise yields <span class="math display">\[\widehat{wage}_i = 9.995 - 2.116 \cdot female_i.\]</span></p>
<p>The model object <tt>dummy_mod</tt> is available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Test the hypothesis that the coefficient on <span class="math inline">\(female_i\)</span> is zero against the alternative that it is non-zero. The null hypothesis implies that there is no gender wage gap. Use the heteroskedasticity-robust estimator proposed by <span class="citation">White (<a href="#ref-white1980">1980</a>)</span>.</li>
</ul>
<iframe src="DCL/ex5_10.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p><tt>vcovHC()</tt> computes heteroskedasticity-robust estimates of the covariance matrix of the coefficient estimators for the model supplied. The estimator proposed by <span class="citation">White (<a href="#ref-white1980">1980</a>)</span> is computed if you set <tt>type = “HC0”</tt>.</p></li>
<li><p>The function <tt>coeftest()</tt> performs significance tests for the coefficients in model objects. A covariance matrix can be supplied using the argument <tt>vcov.</tt>.</p></li>
</ul>
</div>
<div class="DCexercise">
<h4 id="computation-of-heteroskedasticity-robust-standard-errors-1" class="unnumbered">11. Computation of Heteroskedasticity-Robust Standard Errors</h4>
<p>In the simple regression model, the covariance matrix of the coefficient estimators is denoted</p>
<span class="math display">\[\begin{equation}
\text{Var}
  \begin{pmatrix}
    \hat\beta_0 \
    \hat\beta_1
  \end{pmatrix} = 
\begin{pmatrix}
  \text{Var}(\hat\beta_0) &amp; \text{Cov}(\hat\beta_0,\hat\beta_1) \\
\text{Cov}(\hat\beta_0,\hat\beta_1) &amp; \text{Var}(\hat\beta_1)
\end{pmatrix}
\end{equation}\]</span>
<p>The function <tt>vcovHC</tt> can be used to obtain estimates of this matrix for a model object of interest.</p>
<p><tt>dummy_mod</tt>, a model object containing the wage regression dealt with in Exercises 9 and 10 is available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Compute robust standard errors of the type <tt>HC1</tt> for the coefficients estimators in the model object <tt>dummy_mod</tt>. Store the standard errors in a vector named <tt>rob_SEs</tt>.</li>
</ul>
<iframe src="DCL/ex5_11.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints</strong></p>
<ul>
<li>The standard errors we seek can be obtained by taking the square root of the diagonal elements of the estimated covariance matrix.</li>
<li><tt>diag(A)</tt> returns the diagonal elements of the matrix <tt>A</tt>.</li>
</ul>
</div>
<div class="DCexercise">
<h4 id="robust-confidence-intervals" class="unnumbered">12. Robust Confidence Intervals</h4>
<p>The function <tt>confint()</tt> computes confidence intervals for regression models using homoskedasticity-only standard errors so this function is not an option when there is heteroskedasticity.</p>
<p>The function <tt>Rob_CI()</tt> in <tt>script.R</tt> is meant to compute and report heteroskedasticity-robust confidence intervals for both model coefficients in a simple regression model.</p>
<p><tt>gender_mod</tt>, a model object containing the wage regression dealt with in the previous exercises is available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Complete the code of <tt>Rob_CI()</tt> given in <tt>Script.R</tt> such that lower and upper bounds of <span class="math inline">\(95\%\)</span> robust confidence intervals are returned. Use standard errors of the type <tt>HC1</tt>.</p></li>
<li><p>Use the function <tt>Rob_CI()</tt> to obtain <span class="math inline">\(95\%\)</span> confidence intervals for the model coefficients in <tt>dummy_mod</tt>.</p></li>
</ul>
<iframe src="DCL/ex5_12.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="a-small-simulation-study-i" class="unnumbered">13. A Small Simulation Study — I</h4>
Consider the data generating process (DGP)
<span class="math display" id="eq:asss">\[\begin{align}
  X_i \sim&amp; \, \mathcal{U}[2,10], \notag \\
  e_i \sim&amp; \, \mathcal{N}(0, X_i), \notag \\
  Y_i =&amp; \, \beta_1 X_i + e_i, \tag{5.4}  
\end{align}\]</span>
<p>where <span class="math inline">\(\mathcal{U}[2,10]\)</span> denotes the uniform distribution on the interval <span class="math inline">\([2,10]\)</span> and <span class="math inline">\(\beta_1=2\)</span>.</p>
<p>Notice that the errors <span class="math inline">\(e_i\)</span> are heteroskedastic since the variance of the <span class="math inline">\(e_i\)</span> is a function of <span class="math inline">\(X_i\)</span>.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Write a function <tt>DGP_OLS</tt> that generates a sample <span class="math inline">\((X_i,Y_i)\)</span>, <span class="math inline">\(i=1,…,100\)</span> using the DGP above and returns the OLS estimate of <span class="math inline">\(\beta_1\)</span> based on this sample.</li>
</ul>
<iframe src="DCL/ex5_13.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hint:</strong></p>
<p><tt>runif()</tt> can be used to obtain random samples from a uniform distribution, see <tt>?runif</tt>.</p>
</div>
<div class="DCexercise">
<h4 id="a-small-simulation-study-ii" class="unnumbered">14. A Small Simulation Study — II</h4>
<p>The function <tt>DGP_OLS()</tt> from the previous exercise is available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Use <tt>replicate()</tt> to generate a sample of <span class="math inline">\(1000\)</span> OLS estimates <span class="math inline">\(\widehat{\beta}_1\)</span> using the function <tt>DGP_OLS</tt>. Store the estimates in a vector named <tt>estimates</tt>.</p></li>
<li><p>Next, estimate the variance of <span class="math inline">\(\widehat{\beta}_1\)</span> in <a href="htaciitslrm.html#eq:asss">(5.4)</a>: compute the sample variance of the <span class="math inline">\(1000\)</span> OLS estimates in <tt>estimates</tt>. Store the result in <tt>est_var_OLS</tt>.</p></li>
</ul>
<iframe src="DCL/ex5_14.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="a-small-simulation-study-iii" class="unnumbered">15. A Small Simulation Study — III</h4>
<p>According to the the Gauss-Markov theorem, the OLS estimator in linear regression models is no longer the most efficient estimator among the conditionally unbiased linear estimators when there is heteroskedasticity. In other words, the OLS estimator loses the BLUE property when the assumption of homoskedasticity is violated.</p>
<p>It turns out that OLS applied to the weighted observations <span class="math inline">\((w_i X_i, w_i Y_i)\)</span> where <span class="math inline">\(w_i=\frac{1}{\sigma_i}\)</span> is the BLUE estimator under heteroskedasticity. This estimator is called the <em>weighted least squares</em> (WLS) estimator. Thus, when there is heteroskedasticity, the WLS estimator has lower variance than OLS.</p>
<p>The function <tt>DGP_OLS()</tt> and the estimated variance <tt>est_var_OLS</tt> from the previous exercises are available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Write a function <tt>DGP_WLS()</tt> that generates <span class="math inline">\(100\)</span> samples using the DGP introduced in Exercise 13 and returns the WLS estimate of <span class="math inline">\(\beta_1\)</span>. Treat <span class="math inline">\(\sigma_i\)</span> as known, i.e., set <span class="math inline">\(w_i=\frac{1}{\sqrt{X_i}}\)</span>.</p></li>
<li><p>Repeat exercise 14 using <tt>DGP_WLS()</tt>. Store the variance estimate in <tt>est_var_GLS</tt>.</p></li>
<li><p>Compare the estimated variances <tt>est_var_OLS</tt> and <tt>est_var_GLS</tt> using logical operators (<tt>&lt;</tt> or <tt>&gt;</tt>).</p></li>
</ul>
<iframe src="DCL/ex5_15.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p><tt>DGP_WLS()</tt> can be obtained using a modified code of <tt>DGP_OLS()</tt>.</p></li>
<li><p>Remember that functions are objects and you may print the code of a function to the console.</p></li>
</ul>
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-AER">
<p>Kleiber, C., &amp; Zeileis, A. (2017). AER: Applied Econometrics with R (Version 1.2-5). Retrieved from <a href="https://CRAN.R-project.org/package=AER" class="uri">https://CRAN.R-project.org/package=AER</a></p>
</div>
<div id="ref-R-scales">
<p>Wickham, H. (2017). scales: Scale Functions for Visualization (Version 0.5.0). Retrieved from <a href="https://CRAN.R-project.org/package=scales" class="uri">https://CRAN.R-project.org/package=scales</a></p>
</div>
<div id="ref-white1980">
<p>White, H. (1980). A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity. <em>Econometrica</em>, <em>48</em>(4), pp. 817–838.</p>
</div>
<div id="ref-mackinnon1985">
<p>MacKinnon, J. G., &amp; White, H. (1985). Some heteroskedasticity-consistent covariance matrix estimators with improved finite sample properties. <em>Journal of Econometrics</em>, <em>29</em>(3), 305–325.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>The package <tt>sandwich</tt> is a dependency of the package <tt>AER</tt>, meaning that it is attached automatically if you load <tt>AER</tt>.<a href="htaciitslrm.html#fnref6">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lrwor.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="rmwmr.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": true,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 1
},
"edit": null,
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
