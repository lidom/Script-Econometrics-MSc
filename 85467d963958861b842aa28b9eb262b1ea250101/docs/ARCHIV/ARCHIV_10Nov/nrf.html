<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Econometrics with R</title>
  <meta name="description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Econometrics with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Econometrics with R" />
  
  <meta name="twitter:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2018-10-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="htaciimr.html">
<link rel="next" href="asbomr.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<!-- font families -->

<link href="https://fonts.googleapis.com/css?family=PT+Sans|Pacifico|Source+Sans+Pro" rel="stylesheet">

<!-- function to adjust height of iframes automatically depending on content loaded -->

<script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';
  }
</script>

<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/default.js"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110299877-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110299877-1');
</script>

<!-- open review block -->

<script async defer src="https://hypothes.is/embed.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://www.oek.wiwi.uni-due.de/en/">Chair of Econometrics at UDE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#a-very-short-introduction-to-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> A Very Short Introduction to <tt>R</tt> and <em>RStudio</em></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="pt.html"><a href="pt.html#random-variables-and-probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#expected-value-mean-and-variance"><i class="fa fa-check"></i>Expected Value, Mean and Variance</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#thetdist"><i class="fa fa-check"></i>The Student t Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-f-distribution"><i class="fa fa-check"></i>The F Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pt.html"><a href="pt.html#RSATDOSA"><i class="fa fa-check"></i><b>2.2</b> Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="pt.html"><a href="pt.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arosur.html"><a href="arosur.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="3.1" data-path="arosur.html"><a href="arosur.html#estimation-of-the-population-mean"><i class="fa fa-check"></i><b>3.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="3.2" data-path="arosur.html"><a href="arosur.html#potsm"><i class="fa fa-check"></i><b>3.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="3.3" data-path="arosur.html"><a href="arosur.html#hypothesis-tests-concerning-the-population-mean"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests Concerning the Population Mean</a><ul>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-p-value"><i class="fa fa-check"></i>The p-Value</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-known"><i class="fa fa-check"></i>Calculating the p-Value when the Standard Deviation is Known</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#SVSSDASE"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-unknown"><i class="fa fa-check"></i>Calculating the p-value When the Standard Deviation is Unknown</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-t-statistic"><i class="fa fa-check"></i>The t-statistic</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="arosur.html"><a href="arosur.html#confidence-intervals-for-the-population-mean"><i class="fa fa-check"></i><b>3.4</b> Confidence Intervals for the Population Mean</a></li>
<li class="chapter" data-level="3.5" data-path="arosur.html"><a href="arosur.html#cmfdp"><i class="fa fa-check"></i><b>3.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="3.6" data-path="arosur.html"><a href="arosur.html#aattggoe"><i class="fa fa-check"></i><b>3.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="3.7" data-path="arosur.html"><a href="arosur.html#scatterplots-sample-covariance-and-sample-correlation"><i class="fa fa-check"></i><b>3.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
<li class="chapter" data-level="3.8" data-path="arosur.html"><a href="arosur.html#exercises-1"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="4.1" data-path="lrwor.html"><a href="lrwor.html#simple-linear-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="4.2" data-path="lrwor.html"><a href="lrwor.html#estimating-the-coefficients-of-the-linear-regression-model"><i class="fa fa-check"></i><b>4.2</b> Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lrwor.html"><a href="lrwor.html#measures-of-fit"><i class="fa fa-check"></i><b>4.3</b> Measures of Fit</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-coefficient-of-determination"><i class="fa fa-check"></i>The Coefficient of Determination</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-standard-error-of-the-regression"><i class="fa fa-check"></i>The Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lrwor.html"><a href="lrwor.html#tlsa"><i class="fa fa-check"></i><b>4.4</b> The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption 1: The Error Term has Conditional Mean of Zero</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-2-independently-and-identically-distributed-data"><i class="fa fa-check"></i>Assumption 2: Independently and Identically Distributed Data</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption 3: Large Outliers are Unlikely</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="lrwor.html"><a href="lrwor.html#tsdotoe"><i class="fa fa-check"></i><b>4.5</b> The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-1"><i class="fa fa-check"></i>Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-2"><i class="fa fa-check"></i>Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-3"><i class="fa fa-check"></i>Simulation Study 3</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="lrwor.html"><a href="lrwor.html#exercises-2"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="htaciitslrm.html"><a href="htaciitslrm.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li class="chapter" data-level="5.1" data-path="htaciitslrm.html"><a href="htaciitslrm.html#testing-two-sided-hypotheses-concerning-the-slope-coefficient"><i class="fa fa-check"></i><b>5.1</b> Testing Two-Sided Hypotheses Concerning the Slope Coefficient</a></li>
<li class="chapter" data-level="5.2" data-path="htaciitslrm.html"><a href="htaciitslrm.html#cifrc"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-confidence-intervals"><i class="fa fa-check"></i>Simulation Study: Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="htaciitslrm.html"><a href="htaciitslrm.html#rwxiabv"><i class="fa fa-check"></i><b>5.3</b> Regression when X is a Binary Variable</a></li>
<li class="chapter" data-level="5.4" data-path="htaciitslrm.html"><a href="htaciitslrm.html#hah"><i class="fa fa-check"></i><b>5.4</b> Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="htaciitslrm.html"><a href="htaciitslrm.html#the-gauss-markov-theorem"><i class="fa fa-check"></i><b>5.5</b> The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-blue-estimator"><i class="fa fa-check"></i>Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="htaciitslrm.html"><a href="htaciitslrm.html#using-the-t-statistic-in-regression-when-the-sample-size-is-small"><i class="fa fa-check"></i><b>5.6</b> Using the t-Statistic in Regression When the Sample Size Is Small</a></li>
<li class="chapter" data-level="5.7" data-path="htaciitslrm.html"><a href="htaciitslrm.html#exercises-3"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rmwmr.html"><a href="rmwmr.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="6.1" data-path="rmwmr.html"><a href="rmwmr.html#omitted-variable-bias"><i class="fa fa-check"></i><b>6.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="6.2" data-path="rmwmr.html"><a href="rmwmr.html#tmrm"><i class="fa fa-check"></i><b>6.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="rmwmr.html"><a href="rmwmr.html#mofimr"><i class="fa fa-check"></i><b>6.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="6.4" data-path="rmwmr.html"><a href="rmwmr.html#ols-assumptions-in-multiple-regression"><i class="fa fa-check"></i><b>6.4</b> OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#simulation-study-imperfect-multicollinearity"><i class="fa fa-check"></i>Simulation Study: Imperfect Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="rmwmr.html"><a href="rmwmr.html#the-distribution-of-the-ols-estimators-in-multiple-regression"><i class="fa fa-check"></i><b>6.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
<li class="chapter" data-level="6.6" data-path="rmwmr.html"><a href="rmwmr.html#exercises-4"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="htaciimr.html"><a href="htaciimr.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence Intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="htaciimr.html"><a href="htaciimr.html#hypothesis-tests-and-confidence-intervals-for-a-single-coefficient"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="htaciimr.html"><a href="htaciimr.html#an-application-to-test-scores-and-the-student-teacher-ratio"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="htaciimr.html"><a href="htaciimr.html#joint-hypothesis-testing-using-the-f-statistic"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the F-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="htaciimr.html"><a href="htaciimr.html#confidence-sets-for-multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-for-multiple-regression"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="htaciimr.html"><a href="htaciimr.html#analysis-of-the-test-score-data-set"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
<li class="chapter" data-level="7.7" data-path="htaciimr.html"><a href="htaciimr.html#exercises-5"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nrf.html"><a href="nrf.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="8.1" data-path="nrf.html"><a href="nrf.html#a-general-strategy-for-modelling-nonlinear-regression-functions"><i class="fa fa-check"></i><b>8.1</b> A General Strategy for Modelling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="8.2" data-path="nrf.html"><a href="nrf.html#nfoasiv"><i class="fa fa-check"></i><b>8.2</b> Nonlinear Functions of a Single Independent Variable</a><ul>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="nrf.html"><a href="nrf.html#interactions-between-independent-variables"><i class="fa fa-check"></i><b>8.3</b> Interactions Between Independent Variables</a></li>
<li class="chapter" data-level="8.4" data-path="nrf.html"><a href="nrf.html#nonlinear-effects-on-test-scores-of-the-student-teacher-ratio"><i class="fa fa-check"></i><b>8.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
<li class="chapter" data-level="8.5" data-path="nrf.html"><a href="nrf.html#exercises-6"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="asbomr.html"><a href="asbomr.html"><i class="fa fa-check"></i><b>9</b> Assessing Studies Based on Multiple Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity"><i class="fa fa-check"></i><b>9.1</b> Internal and External Validity</a></li>
<li class="chapter" data-level="9.2" data-path="asbomr.html"><a href="asbomr.html#ttivomra"><i class="fa fa-check"></i><b>9.2</b> Threats to Internal Validity of Multiple Regression Analysis</a></li>
<li class="chapter" data-level="9.3" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity-when-the-regression-is-used-for-forecasting"><i class="fa fa-check"></i><b>9.3</b> Internal and External Validity when the Regression is Used for Forecasting</a></li>
<li class="chapter" data-level="9.4" data-path="asbomr.html"><a href="asbomr.html#etsacs"><i class="fa fa-check"></i><b>9.4</b> Example: Test Scores and Class Size</a></li>
<li class="chapter" data-level="9.5" data-path="asbomr.html"><a href="asbomr.html#exercises-7"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="rwpd.html"><a href="rwpd.html"><i class="fa fa-check"></i><b>10</b> Regression with Panel Data</a><ul>
<li class="chapter" data-level="10.1" data-path="rwpd.html"><a href="rwpd.html#panel-data"><i class="fa fa-check"></i><b>10.1</b> Panel Data</a></li>
<li class="chapter" data-level="10.2" data-path="rwpd.html"><a href="rwpd.html#PDWTTP"><i class="fa fa-check"></i><b>10.2</b> Panel Data with Two Time Periods: “Before and After” Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="rwpd.html"><a href="rwpd.html#fixed-effects-regression"><i class="fa fa-check"></i><b>10.3</b> Fixed Effects Regression</a><ul>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#estimation-and-inference"><i class="fa fa-check"></i>Estimation and Inference</a></li>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#application-to-traffic-deaths"><i class="fa fa-check"></i>Application to Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="rwpd.html"><a href="rwpd.html#regression-with-time-fixed-effects"><i class="fa fa-check"></i><b>10.4</b> Regression with Time Fixed Effects</a></li>
<li class="chapter" data-level="10.5" data-path="rwpd.html"><a href="rwpd.html#tferaaseffer"><i class="fa fa-check"></i><b>10.5</b> The Fixed Effects Regression Assumptions and Standard Errors for Fixed Effects Regression</a></li>
<li class="chapter" data-level="10.6" data-path="rwpd.html"><a href="rwpd.html#drunk-driving-laws-and-traffic-deaths"><i class="fa fa-check"></i><b>10.6</b> Drunk Driving Laws and Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="rwabdv.html"><a href="rwabdv.html"><i class="fa fa-check"></i><b>11</b> Regression with a Binary Dependent Variable</a><ul>
<li class="chapter" data-level="11.1" data-path="rwabdv.html"><a href="rwabdv.html#binary-dependent-variables-and-the-linear-probability-model"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables and the Linear Probability Model</a></li>
<li class="chapter" data-level="11.2" data-path="rwabdv.html"><a href="rwabdv.html#palr"><i class="fa fa-check"></i><b>11.2</b> Probit and Logit Regression</a><ul>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#probit-regression"><i class="fa fa-check"></i>Probit Regression</a></li>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#logit-regression"><i class="fa fa-check"></i>Logit Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="rwabdv.html"><a href="rwabdv.html#estimation-and-inference-in-the-logit-and-probit-models"><i class="fa fa-check"></i><b>11.3</b> Estimation and Inference in the Logit and Probit Models</a></li>
<li class="chapter" data-level="11.4" data-path="rwabdv.html"><a href="rwabdv.html#application-to-the-boston-hmda-data"><i class="fa fa-check"></i><b>11.4</b> Application to the Boston HMDA Data</a></li>
<li class="chapter" data-level="11.5" data-path="rwabdv.html"><a href="rwabdv.html#exercises-8"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ivr.html"><a href="ivr.html"><i class="fa fa-check"></i><b>12</b> Instrumental Variables Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="ivr.html"><a href="ivr.html#TIVEWASRAASI"><i class="fa fa-check"></i><b>12.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a></li>
<li class="chapter" data-level="12.2" data-path="ivr.html"><a href="ivr.html#TGIVRM"><i class="fa fa-check"></i><b>12.2</b> The General IV Regression Model</a></li>
<li class="chapter" data-level="12.3" data-path="ivr.html"><a href="ivr.html#civ"><i class="fa fa-check"></i><b>12.3</b> Checking Instrument Validity</a></li>
<li class="chapter" data-level="12.4" data-path="ivr.html"><a href="ivr.html#attdfc"><i class="fa fa-check"></i><b>12.4</b> Application to the Demand for Cigarettes</a></li>
<li class="chapter" data-level="12.5" data-path="ivr.html"><a href="ivr.html#where-do-valid-instruments-come-from"><i class="fa fa-check"></i><b>12.5</b> Where Do Valid Instruments Come From?</a></li>
<li class="chapter" data-level="12.6" data-path="ivr.html"><a href="ivr.html#exercises-9"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="eaqe.html"><a href="eaqe.html"><i class="fa fa-check"></i><b>13</b> Experiments and Quasi-Experiments</a><ul>
<li class="chapter" data-level="13.1" data-path="eaqe.html"><a href="eaqe.html#potential-outcomes-causal-effects-and-idealized-experiments"><i class="fa fa-check"></i><b>13.1</b> Potential Outcomes, Causal Effects and Idealized Experiments</a></li>
<li class="chapter" data-level="13.2" data-path="eaqe.html"><a href="eaqe.html#threats-to-validity-of-experiments"><i class="fa fa-check"></i><b>13.2</b> Threats to Validity of Experiments</a></li>
<li class="chapter" data-level="13.3" data-path="eaqe.html"><a href="eaqe.html#experimental-estimates-of-the-effect-of-class-size-reductions"><i class="fa fa-check"></i><b>13.3</b> Experimental Estimates of the Effect of Class Size Reductions</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#experimental-design-and-the-data-set"><i class="fa fa-check"></i>Experimental Design and the Data Set</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#analysis-of-the-star-data"><i class="fa fa-check"></i>Analysis of the STAR Data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="eaqe.html"><a href="eaqe.html#quasi-experiments"><i class="fa fa-check"></i><b>13.4</b> Quasi Experiments</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#the-differences-in-differences-estimator"><i class="fa fa-check"></i>The Differences-in-Differences Estimator</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#regression-discontinuity-estimators"><i class="fa fa-check"></i>Regression Discontinuity Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ittsraf.html"><a href="ittsraf.html"><i class="fa fa-check"></i><b>14</b> Introduction to Time Series Regression and Forecasting</a><ul>
<li class="chapter" data-level="14.1" data-path="ittsraf.html"><a href="ittsraf.html#using-regression-models-for-forecasting"><i class="fa fa-check"></i><b>14.1</b> Using Regression Models for Forecasting</a></li>
<li class="chapter" data-level="14.2" data-path="ittsraf.html"><a href="ittsraf.html#tsdasc"><i class="fa fa-check"></i><b>14.2</b> Time Series Data and Serial Correlation</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#notation-lags-differences-logarithms-and-growth-rates"><i class="fa fa-check"></i>Notation, Lags, Differences, Logarithms and Growth Rates</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ittsraf.html"><a href="ittsraf.html#autoregressions"><i class="fa fa-check"></i><b>14.3</b> Autoregressions</a><ul>
<li><a href="ittsraf.html#autoregressive-models-of-order-p">Autoregressive Models of Order <span class="math inline">\(p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ittsraf.html"><a href="ittsraf.html#cybtmpi"><i class="fa fa-check"></i><b>14.4</b> Can You Beat the Market? (Part I)</a></li>
<li class="chapter" data-level="14.5" data-path="ittsraf.html"><a href="ittsraf.html#apatadlm"><i class="fa fa-check"></i><b>14.5</b> Additional Predictors and The ADL Model</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#forecast-uncertainty-and-forecast-intervals"><i class="fa fa-check"></i>Forecast Uncertainty and Forecast Intervals</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="ittsraf.html"><a href="ittsraf.html#llsuic"><i class="fa fa-check"></i><b>14.6</b> Lag Length Selection Using Information Criteria</a></li>
<li class="chapter" data-level="14.7" data-path="ittsraf.html"><a href="ittsraf.html#nit"><i class="fa fa-check"></i><b>14.7</b> Nonstationarity I: Trends</a></li>
<li class="chapter" data-level="14.8" data-path="ittsraf.html"><a href="ittsraf.html#niib"><i class="fa fa-check"></i><b>14.8</b> Nonstationarity II: Breaks</a></li>
<li class="chapter" data-level="14.9" data-path="ittsraf.html"><a href="ittsraf.html#can-you-beat-the-market-part-ii"><i class="fa fa-check"></i><b>14.9</b> Can You Beat the Market? (Part II)</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="eodce.html"><a href="eodce.html"><i class="fa fa-check"></i><b>15</b> Estimation of Dynamic Causal Effects</a><ul>
<li class="chapter" data-level="15.1" data-path="eodce.html"><a href="eodce.html#the-orange-juice-data"><i class="fa fa-check"></i><b>15.1</b> The Orange Juice Data</a></li>
<li class="chapter" data-level="15.2" data-path="eodce.html"><a href="eodce.html#dynamic-causal-effects"><i class="fa fa-check"></i><b>15.2</b> Dynamic Causal Effects</a></li>
<li class="chapter" data-level="15.3" data-path="eodce.html"><a href="eodce.html#dynamic-multipliers-and-cumulative-dynamic-multipliers"><i class="fa fa-check"></i><b>15.3</b> Dynamic Multipliers and Cumulative Dynamic Multipliers</a></li>
<li class="chapter" data-level="15.4" data-path="eodce.html"><a href="eodce.html#hac-standard-errors"><i class="fa fa-check"></i><b>15.4</b> HAC Standard Errors</a></li>
<li class="chapter" data-level="15.5" data-path="eodce.html"><a href="eodce.html#estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors"><i class="fa fa-check"></i><b>15.5</b> Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors</a></li>
<li class="chapter" data-level="15.6" data-path="eodce.html"><a href="eodce.html#orange-juice-prices-and-cold-weather"><i class="fa fa-check"></i><b>15.6</b> Orange Juice Prices and Cold Weather</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="atitsr.html"><a href="atitsr.html"><i class="fa fa-check"></i><b>16</b> Additional Topics in Time Series Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="atitsr.html"><a href="atitsr.html#vector-autoregressions"><i class="fa fa-check"></i><b>16.1</b> Vector Autoregressions</a></li>
<li class="chapter" data-level="16.2" data-path="atitsr.html"><a href="atitsr.html#ooiatdfglsurt"><i class="fa fa-check"></i><b>16.2</b> Orders of Integration and the DF-GLS Unit Root Test</a></li>
<li class="chapter" data-level="16.3" data-path="atitsr.html"><a href="atitsr.html#cointegration"><i class="fa fa-check"></i><b>16.3</b> Cointegration</a></li>
<li class="chapter" data-level="16.4" data-path="atitsr.html"><a href="atitsr.html#volatility-clustering-and-autoregressive-conditional-heteroskedasticity"><i class="fa fa-check"></i><b>16.4</b> Volatility Clustering and Autoregressive Conditional Heteroskedasticity</a><ul>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#arch-and-garch-models"><i class="fa fa-check"></i>ARCH and GARCH Models</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#application-to-stock-price-volatility"><i class="fa fa-check"></i>Application to Stock Price Volatility</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#summary-8"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Econometrics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div class = rmdreview>
This book is in <b>Open Review</b>. We want your feedback to make the book better for you and other students. You may annotate some text by <span style="background-color: #3297FD; color: white">selecting it with the cursor</span> and then click the <i class="h-icon-annotate"></i> on the pop-up menu. You can also see the annotations of others: click the <i class="h-icon-chevron-left"></i> in the upper right hand corner of the page <i class="fa fa-arrow-circle-right  fa-rotate-315" aria-hidden="true"></i>
</div>
<div id="nrf" class="section level1">
<h1><span class="header-section-number">8</span> Nonlinear Regression Functions</h1>
<p>Until now we assumed the regression function to be linear, i.e., we have treated the slope parameter of the regression function as a constant. This implies that the effect on <span class="math inline">\(Y\)</span> of a one unit change in <span class="math inline">\(X\)</span> does not depend on the level of <span class="math inline">\(X\)</span>. If, however, the effect of a change in <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> does depend on the value of <span class="math inline">\(X\)</span>, we should use a nonlinear regression function.</p>
<p>Just like for the previous chapter, the packages <tt>AER</tt> <span class="citation">(Christian Kleiber &amp; Zeileis, <a href="#ref-R-AER">2017</a>)</span> and <tt>stargazer</tt> <span class="citation">(Hlavac, <a href="#ref-R-stargazer">2018</a>)</span> are required for reproduction of the code presented in this chapter. Check whether the code chunk below executes without any error messages.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(AER)
<span class="kw">library</span>(stargazer)</code></pre></div>
<div id="a-general-strategy-for-modelling-nonlinear-regression-functions" class="section level2">
<h2><span class="header-section-number">8.1</span> A General Strategy for Modelling Nonlinear Regression Functions</h2>
<p>Let us have a look at an example where using a nonlinear regression function is better suited for estimating the population relationship between the regressor, <span class="math inline">\(X\)</span>, and the regressand, <span class="math inline">\(Y\)</span>: the relationship between the income of schooling districts and their test scores.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># prepare the data</span>
<span class="kw">library</span>(AER)                                                     
<span class="kw">data</span>(CASchools)
CASchools<span class="op">$</span>size &lt;-<span class="st"> </span>CASchools<span class="op">$</span>students<span class="op">/</span>CASchools<span class="op">$</span>teachers
CASchools<span class="op">$</span>score &lt;-<span class="st"> </span>(CASchools<span class="op">$</span>read <span class="op">+</span><span class="st"> </span>CASchools<span class="op">$</span>math) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>       </code></pre></div>
<p>We start our analysis by computing the correlation between both variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(CASchools<span class="op">$</span>income, CASchools<span class="op">$</span>score)</code></pre></div>
<pre><code>## [1] 0.7124308</code></pre>
<p>Here, income and test scores are positively related: school districts with above average income tend to achieve above average test scores. Does a linear regression function model the data adequately? Let us plot the data and add a linear regression line.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit a simple linear model</span>
linear_model&lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>income, <span class="dt">data =</span> CASchools)

<span class="co"># plot the observations</span>
<span class="kw">plot</span>(CASchools<span class="op">$</span>income, CASchools<span class="op">$</span>score,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;District Income (thousands of dollars)&quot;</span>, 
     <span class="dt">ylab =</span> <span class="st">&quot;Test Score&quot;</span>,
     <span class="dt">cex.main =</span> <span class="fl">0.9</span>,
     <span class="dt">main =</span> <span class="st">&quot;Test Score vs. District Income and a Linear OLS Regression Function&quot;</span>)

<span class="co"># add the regression line to the plot</span>
<span class="kw">abline</span>(linear_model, 
       <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
       <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-316-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>As pointed out in the book, the linear regression line seems to overestimate the true relationship when income is very high or very low and underestimates it for the middle income group.</p>
<p>Fortunately, OLS does not only handle linear functions of the regressors. We can for example model test scores as a function of income and the square of income. The corresponding regression model is</p>
<p><span class="math display">\[TestScore_i = \beta_0 + \beta_1 \times income_i + \beta_2 \times income_i^2 + u_i,\]</span> called a <em>quadratic regression model</em>. That is, <span class="math inline">\(income^2\)</span> is treated as an additional explanatory variable. Hence, the quadratic model is a special case of a multivariate regression model. When fitting the model with <tt>lm()</tt> we have to use the <tt>^</tt> operator in conjunction with the function <tt>I()</tt> to add the quadratic term as an additional regressor to the argument <tt>formula</tt>. This is because the regression formula we pass to <tt>formula</tt> is converted to an object of the class <tt>formula</tt>. For objects of this class, the operators <tt>+</tt>, <tt>-</tt>, <tt>*</tt> and <tt>^</tt> have a nonarithmetic interpretation. <tt>I()</tt> ensures that they are used as arithmetical operators, see <tt>?I</tt>,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit the quadratic Model</span>
quadratic_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">2</span>), <span class="dt">data =</span> CASchools)

<span class="co"># obtain the model summary</span>
<span class="kw">coeftest</span>(quadratic_model, <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##                Estimate  Std. Error  t value  Pr(&gt;|t|)    
## (Intercept) 607.3017435   2.9017544 209.2878 &lt; 2.2e-16 ***
## income        3.8509939   0.2680942  14.3643 &lt; 2.2e-16 ***
## I(income^2)  -0.0423084   0.0047803  -8.8505 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The output tells us that the estimated regression function is</p>
<p><span class="math display">\[\widehat{TestScore}_i = \underset{(2.90)}{607.3} + \underset{(0.27)}{3.85} \times income_i - \underset{(0.0048)}{0.0423} \times income_i^2.\]</span></p>
<p>This model allows us to test the hypothesis that the relationship between test scores and district income is linear against the alternative that it is quadratic. This corresponds to testing</p>
<p><span class="math display">\[H_0: \beta_2 = 0 \ \ \text{vs.} \ \  H_1: \beta_2\neq0,\]</span></p>
<p>since <span class="math inline">\(\beta_2=0\)</span> corresponds to a simple linear equation and <span class="math inline">\(\beta_2\neq0\)</span> implies a quadratic relationship. We find that <span class="math inline">\(t=(\hat\beta_2 - 0)/SE(\hat\beta_2) = -0.0423/0.0048 = -8.81\)</span> so the null is rejected at any common level of significance and we conclude that the relationship is nonlinear. This is consistent with the impression gained from the plot.</p>
<p>We now draw the same scatter plot as for the linear model and add the regression line for the quadratic model. Because <tt>abline()</tt> can only draw straight lines, it cannot be used here. <tt>lines()</tt> is a function which allows to draw nonstraight lines, see <code>?lines</code>. The most basic call of <tt>lines()</tt> is <tt>lines(x_values, y_values)</tt> where <tt>x_values</tt> and <tt>y_values</tt> are vectors of the same length that provide coordinates of the points to be <em>sequentially</em> connected by a line. This makes it necessary to sort the coordinate pairs according to the X-values. Here we use the function <tt>order()</tt> to sort the fitted values of <tt>score</tt> according to the observations of <tt>income</tt>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># draw a scatterplot of the observations for income and test score</span>
<span class="kw">plot</span>(CASchools<span class="op">$</span>income, CASchools<span class="op">$</span>score,
     <span class="dt">col  =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;District Income (thousands of dollars)&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Test Score&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Estimated Linear and Quadratic Regression Functions&quot;</span>)

<span class="co"># add a linear function to the plot</span>
<span class="kw">abline</span>(linear_model, <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)

<span class="co"># add quatratic function to the plot</span>
order_id &lt;-<span class="st"> </span><span class="kw">order</span>(CASchools<span class="op">$</span>income)

<span class="kw">lines</span>(<span class="dt">x =</span> CASchools<span class="op">$</span>income[order_id], 
      <span class="dt">y =</span> <span class="kw">fitted</span>(quadratic_model)[order_id],
      <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>) </code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-318-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>We see that the quadratic function does fit the data much better than the linear function.</p>
</div>
<div id="nfoasiv" class="section level2">
<h2><span class="header-section-number">8.2</span> Nonlinear Functions of a Single Independent Variable</h2>
<div id="polynomials" class="section level3 unnumbered">
<h3>Polynomials</h3>
<p>The approach used to obtain a quadratic model can be generalized to polynomial models of arbitrary degree <span class="math inline">\(r\)</span>, <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \cdots + \beta_r X_i^r + u_i.\]</span></p>
<p>A cubic model for instance can be estimated in the same way as the quadratic model; we just have to use a polynomial of degree <span class="math inline">\(r=3\)</span> in <tt>income</tt>. This is conveniently done using the function <tt>poly()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate a cubic model</span>
cubic_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(income, <span class="dt">degree =</span> <span class="dv">3</span>, <span class="dt">raw =</span> <span class="ot">TRUE</span>), <span class="dt">data =</span> CASchools)</code></pre></div>
<p><tt>poly()</tt> generates orthogonal polynomials which are orthogonal to the constant by default. Here, we set <tt>raw = TRUE</tt> such that raw polynomials are evaluated, see <code>?poly</code>.</p>
<p>In practice the question will arise which polynomial order should be chosen. First, similarly as for <span class="math inline">\(r=2\)</span>, we can test the null hypothesis that the true relation is linear against the alternative hypothesis that the relationship is a polynomial of degree <span class="math inline">\(r\)</span>:</p>
<p><span class="math display">\[ H_0: \beta_2=0, \ \beta_3=0,\dots,\beta_r=0 \ \ \ \text{vs.} \ \ \ H_1: \text{at least one} \ \beta_j\neq0, \ j=2,\dots,r \]</span></p>
<p>This is a joint null hypothesis with <span class="math inline">\(r-1\)</span> restrictions so it can be tested using the <span class="math inline">\(F\)</span>-test presented in Chapter <a href="htaciimr.html#htaciimr">7</a>. <tt>linearHypothesis()</tt> can be used to conduct such tests. For example, we may test the null of a linear model against the alternative of a polynomial of a maximal degree <span class="math inline">\(r=3\)</span> as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># test the hypothesis of a linear model against quadratic or polynomial</span>
<span class="co"># alternatives</span>

<span class="co"># set up hypothesis matrix</span>
R &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>),
            <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>))

<span class="co"># do the test</span>
<span class="kw">linearHypothesis</span>(cubic_model,
                 <span class="dt">hypothesis.matrix =</span> R,
                 <span class="dt">white.adj =</span> <span class="st">&quot;hc1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## poly(income, degree = 3, raw = TRUE)2 = 0
## poly(income, degree = 3, raw = TRUE)3 = 0
## 
## Model 1: restricted model
## Model 2: score ~ poly(income, degree = 3, raw = TRUE)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F    Pr(&gt;F)    
## 1    418                        
## 2    416  2 37.691 9.043e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We provide a hypothesis matrix as the argument <tt>hypothesis.matrix</tt>. This is useful when the coefficients have long names, as is the case here due to using <tt>poly()</tt>, or when the restrictions include multiple coefficients. How the hypothesis matrix <span class="math inline">\(\mathbf{R}\)</span> is interpreted by <tt>linearHypothesis()</tt> is best seen using matrix algebra:</p>
For the two linear constrains above, we have
<span class="math display">\[\begin{align*}
  \mathbf{R}\boldsymbol{\beta} =&amp; \mathbf{s} \\
  \begin{pmatrix}
    0 &amp; 0 &amp; 1 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 1
  \end{pmatrix}
  \begin{pmatrix}
    \beta_0 \\
    \beta_1 \\
    \beta_2 \\
    \beta_3 \\
  \end{pmatrix} =&amp;
  \begin{pmatrix}
   0 \\
   0
  \end{pmatrix} \\
  \begin{pmatrix}
    \beta_2 \\
    \beta_3
  \end{pmatrix}= &amp;
  \begin{pmatrix}
    0 \\
    0
  \end{pmatrix}.
\end{align*}\]</span>
<p><tt>linearHypothesis()</tt> uses the zero vector for <span class="math inline">\(\mathbf{s}\)</span> by default, see <code>?linearHypothesis</code>.</p>
<p>The <span class="math inline">\(p\)</span>-value for is very small so that we reject the null hypothesis. However, this does not tell us <em>which</em> <span class="math inline">\(r\)</span> to choose. In practice, one approach to determine the degree of the polynomial is to use <em>sequential testing</em>:</p>
<ol style="list-style-type: decimal">
<li>Estimate a polynomial model for some maximum value <span class="math inline">\(r\)</span>.</li>
<li>Use a <span class="math inline">\(t\)</span>-test to test <span class="math inline">\(\beta_r = 0\)</span>. <em>Rejection</em> of the null means that <span class="math inline">\(X^r\)</span> belongs in the regression equation.</li>
<li><em>Acceptance</em> of the null in step 2 means that <span class="math inline">\(X^r\)</span> can be eliminated from the model. Continue by repeating step 1 with order <span class="math inline">\(r-1\)</span> and test whether <span class="math inline">\(\beta_{r-1}=0\)</span>. If the test rejects, use a polynomial model of order <span class="math inline">\(r-1\)</span>.</li>
<li>If the tests from step 3 rejects, continue with the procedure until the coefficient on the highest power is statistically significant.</li>
</ol>
<p>There is no unambiguous guideline how to choose <span class="math inline">\(r\)</span> in step one. However, as pointed out in <span class="citation">J. Stock &amp; Watson (<a href="#ref-stock2015">2015</a>)</span>, economic data is often smooth such that it is appropriate to choose small orders like <span class="math inline">\(2\)</span>, <span class="math inline">\(3\)</span>, or <span class="math inline">\(4\)</span>.</p>
<p>We will demonstrate how to apply sequential testing by the example of the cubic model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(cubic_model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ poly(income, degree = 3, raw = TRUE), data = CASchools)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -44.28  -9.21   0.20   8.32  31.16 
## 
## Coefficients:
##                                         Estimate Std. Error t value
## (Intercept)                            6.001e+02  5.830e+00 102.937
## poly(income, degree = 3, raw = TRUE)1  5.019e+00  8.595e-01   5.839
## poly(income, degree = 3, raw = TRUE)2 -9.581e-02  3.736e-02  -2.564
## poly(income, degree = 3, raw = TRUE)3  6.855e-04  4.720e-04   1.452
##                                       Pr(&gt;|t|)    
## (Intercept)                            &lt; 2e-16 ***
## poly(income, degree = 3, raw = TRUE)1 1.06e-08 ***
## poly(income, degree = 3, raw = TRUE)2   0.0107 *  
## poly(income, degree = 3, raw = TRUE)3   0.1471    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.71 on 416 degrees of freedom
## Multiple R-squared:  0.5584, Adjusted R-squared:  0.5552 
## F-statistic: 175.4 on 3 and 416 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The estimated cubic model stored in <tt>cubic_model</tt> is</p>
<p><span class="math display">\[ \widehat{TestScore}_i = \underset{(5.83)}{600.1} + \underset{(0.86)}{5.02} \times income -\underset{(0.03)}{0.96} \times income^2 - \underset{(0.00047)}{0.00069} \times income^3. \]</span></p>
<p>The <span class="math inline">\(t\)</span>-statistic on <span class="math inline">\(income^3\)</span> is <span class="math inline">\(1.42\)</span> so the null that the relationship is quadratic cannot be rejected, even at the <span class="math inline">\(10\%\)</span> level. This is contrary to the result presented book which reports robust standard errors throughout so we will also use robust variance-covariance estimation to reproduce these results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># test the hypothesis using robust standard errors</span>
<span class="kw">coeftest</span>(cubic_model, <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##                                          Estimate  Std. Error  t value
## (Intercept)                            6.0008e+02  5.1021e+00 117.6150
## poly(income, degree = 3, raw = TRUE)1  5.0187e+00  7.0735e-01   7.0950
## poly(income, degree = 3, raw = TRUE)2 -9.5805e-02  2.8954e-02  -3.3089
## poly(income, degree = 3, raw = TRUE)3  6.8549e-04  3.4706e-04   1.9751
##                                        Pr(&gt;|t|)    
## (Intercept)                           &lt; 2.2e-16 ***
## poly(income, degree = 3, raw = TRUE)1 5.606e-12 ***
## poly(income, degree = 3, raw = TRUE)2  0.001018 ** 
## poly(income, degree = 3, raw = TRUE)3  0.048918 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The reported standard errors have changed. Furthermore, the coefficient for <code>income^3</code> is now significant at the <span class="math inline">\(5\%\)</span> level. This means we reject the hypothesis that the regression function is quadratic against the alternative that it is cubic. Furthermore, we can also test if the coefficients for <tt>income^2</tt> and <tt>income^3</tt> are jointly significant using a robust version of the <span class="math inline">\(F\)</span>-test.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># perform robust F-test </span>
<span class="kw">linearHypothesis</span>(cubic_model, 
                 <span class="dt">hypothesis.matrix =</span> R,
                 <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## poly(income, degree = 3, raw = TRUE)2 = 0
## poly(income, degree = 3, raw = TRUE)3 = 0
## 
## Model 1: restricted model
## Model 2: score ~ poly(income, degree = 3, raw = TRUE)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F    Pr(&gt;F)    
## 1    418                        
## 2    416  2 29.678 8.945e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>With a <span class="math inline">\(p\)</span>-value of <span class="math inline">\(9.043e^{-16}\)</span>, i.e., much less than <span class="math inline">\(0.05\)</span>, the null hypothesis of linearity is rejected in favor of the alternative that the relationship is quadratic <em>or</em> cubic.</p>
<div id="interpretation-of-coefficients-in-nonlinear-regression-models" class="section level4 unnumbered">
<h4>Interpretation of Coefficients in Nonlinear Regression Models</h4>
<p>The coefficients in polynomial regression do not have a simple interpretation. Why? Think of a quadratic model: it is not helpful to think of the coefficient on <span class="math inline">\(X\)</span> as the expected change in <span class="math inline">\(Y\)</span> associated with a change in <span class="math inline">\(X\)</span> holding the other regressors constant because <span class="math inline">\(X^2\)</span> changes as <span class="math inline">\(X\)</span> varies. This is also the case for other deviations from linearity, for example in models where regressors and/or the dependent variable are log-transformed. A way to approach this is to calculate the estimated effect on <span class="math inline">\(Y\)</span> associated with a change in <span class="math inline">\(X\)</span> for one or more values of <span class="math inline">\(X\)</span>. This idea is summarized in Key Concept 8.1.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 8.1
</h3>
<h3 class="left">
The Expected Effect on <span class="math inline">\(Y\)</span> of a Change in <span class="math inline">\(X_1\)</span> in a Nonlinear Regression Model
</h3>
<p>Consider the nonlinear population regression model</p>
<p><span class="math display">\[ Y_i = f(X_{1i}, X_{2i}, \dots, X_{ki}) + u_i \ , \ i=1,\dots,n,\]</span></p>
<p>where <span class="math inline">\(f(X_{1i}, X_{2i}, \dots, X_{ki})\)</span> is the population regression function and <span class="math inline">\(u_i\)</span> is the error term.</p>
<p>Denote by <span class="math inline">\(\Delta Y\)</span> the expected change in <span class="math inline">\(Y\)</span> associated with <span class="math inline">\(\Delta X_1\)</span>, the change in <span class="math inline">\(X_1\)</span> while holding <span class="math inline">\(X_2, \cdots , X_k\)</span> constant. That is, the expected change in <span class="math inline">\(Y\)</span> is the difference</p>
<p><span class="math display">\[\Delta Y = f(X_1 + \Delta X_1, X_2, \cdots, X_k) - f(X_1, X_2, \cdots, X_k).\]</span></p>
<p>The estimator of this unknown population difference is the difference between the predicted values for these two cases. Let <span class="math inline">\(\hat{f}(X_1, X_2, \cdots, X_k)\)</span> be the predicted value of of <span class="math inline">\(Y\)</span> based on the estimator <span class="math inline">\(\hat{f}\)</span> of the population regression function. Then the predicted change in <span class="math inline">\(Y\)</span> is</p>
<span class="math display">\[\Delta \widehat{Y} = \hat{f}(X_1 + \Delta X_1, X_2, \cdots, X_k) - \hat{f}(X_1, X_2, \cdots, X_k).\]</span>
</p>
</div>
<p>For example, we may ask the following: what is the predicted change in test scores associated with a one unit change (i.e., <span class="math inline">\(\$1000\)</span>) in income, based on the estimated quadratic regression function</p>
<p><span class="math display">\[\widehat{TestScore} = 607.3 + 3.85 \times income - 0.0423 \times income^2\ ?\]</span></p>
<p>Because the regression function is quadratic, this effect depends on the initial district income. We therefore consider two cases:</p>
<ol style="list-style-type: decimal">
<li><p>An increase in district income form <span class="math inline">\(10\)</span> to <span class="math inline">\(11\)</span> (from <span class="math inline">\(\$10000\)</span> per capita to <span class="math inline">\(\$11000\)</span>).</p></li>
<li><p>An increase in district income from <span class="math inline">\(40\)</span> to <span class="math inline">\(41\)</span> (that is from <span class="math inline">\(\$40000\)</span> to <span class="math inline">\(\$41000\)</span>).</p></li>
</ol>
<p>In order to obtain the <span class="math inline">\(\Delta \widehat{Y}\)</span> associated with a change in income form <span class="math inline">\(10\)</span> to <span class="math inline">\(11\)</span>, we use the following formula:</p>
<p><span class="math display">\[\Delta \widehat{Y} = \left(\hat{\beta}_0 + \hat{\beta}_1 \times 11 + \hat{\beta}_2 \times 11^2\right) - \left(\hat{\beta}_0 + \hat{\beta}_1 \times 10 + \hat{\beta}_2 \times 10^2\right) \]</span> To compute <span class="math inline">\(\widehat{Y}\)</span> using <tt>R</tt> we may use <tt>predict()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute and assign the quadratic model</span>
quadriatic_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(income<span class="op">^</span><span class="dv">2</span>), <span class="dt">data =</span> CASchools)

<span class="co"># set up data for prediction</span>
new_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">income =</span> <span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">11</span>))

<span class="co"># do the prediction</span>
Y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(quadriatic_model, <span class="dt">newdata =</span> new_data)

<span class="co"># compute the difference</span>
<span class="kw">diff</span>(Y_hat)</code></pre></div>
<pre><code>##        2 
## 2.962517</code></pre>
<p>Analogously we can compute the effect of a change in district income from <span class="math inline">\(40\)</span> to <span class="math inline">\(41\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set up data for prediction</span>
new_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">income =</span> <span class="kw">c</span>(<span class="dv">40</span>, <span class="dv">41</span>))

<span class="co"># do the prediction</span>
Y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(quadriatic_model, <span class="dt">newdata =</span> new_data)

<span class="co"># compute the difference</span>
<span class="kw">diff</span>(Y_hat)</code></pre></div>
<pre><code>##         2 
## 0.4240097</code></pre>
<p>So for the quadratic model, the expected change in <span class="math inline">\(TestScore\)</span> induced by an increase in <span class="math inline">\(income\)</span> from <span class="math inline">\(10\)</span> to <span class="math inline">\(11\)</span> is about <span class="math inline">\(2.96\)</span> points but an increase in <span class="math inline">\(income\)</span> from <span class="math inline">\(40\)</span> to <span class="math inline">\(41\)</span> increases the predicted score by only <span class="math inline">\(0.42\)</span>. Hence, the slope of the estimated quadratic regression function is <em>steeper</em> at low levels of income than at higher levels.</p>
</div>
</div>
<div id="logarithms" class="section level3 unnumbered">
<h3>Logarithms</h3>
<p>Another way to specify a nonlinear regression function is to use the natural logarithm of <span class="math inline">\(Y\)</span> and/or <span class="math inline">\(X\)</span>. Logarithms convert changes in variables into percentage changes. This is convenient as many relationships are naturally expressed in terms of percentages.</p>
<p>There are three different cases in which logarithms might be used.</p>
<ol style="list-style-type: decimal">
<li><p>Transform <span class="math inline">\(X\)</span> with its logarithm, but not <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Analogously we could transform <span class="math inline">\(Y\)</span> to its logarithm but leave <span class="math inline">\(X\)</span> at level.</p></li>
<li><p>Both <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are transformed to their logarithms.</p></li>
</ol>
<p>The interpretation of the regression coefficients is different in each case.</p>
<div id="case-i-x-is-in-logarithm-y-is-not." class="section level4 unnumbered">
<h4>Case I: <span class="math inline">\(X\)</span> is in Logarithm, <span class="math inline">\(Y\)</span> is not.</h4>
<p>The regression model then is <span class="math display">\[Y_i = \beta_0 + \beta_1 \times \ln(X_i) + u_i \text{, } i=1,...,n. \]</span> Similar as for polynomial regression we do not have to create a new variable before using <tt>lm()</tt>. We can simply adjust the <tt>formula</tt> argument of <tt>lm()</tt> to tell <tt>R</tt> that the log-transformation of a variable should be used.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate a level-log model</span>
LinearLog_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(income), <span class="dt">data =</span> CASchools)

<span class="co"># compute robust summary</span>
<span class="kw">coeftest</span>(LinearLog_model, 
         <span class="dt">vcov =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 557.8323     3.8399 145.271 &lt; 2.2e-16 ***
## log(income)  36.4197     1.3969  26.071 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Hence, the estimated regression function is</p>
<p><span class="math display">\[\widehat{TestScore} = 557.8 + 36.42 \times \ln(income).\]</span></p>
<p>Let us draw a plot of this function.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># draw a scatterplot</span>
<span class="kw">plot</span>(score <span class="op">~</span><span class="st"> </span>income, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">data =</span> CASchools,
     <span class="dt">main =</span> <span class="st">&quot;Linear-Log Regression Line&quot;</span>)

<span class="co"># add the linear-log regression line</span>
order_id  &lt;-<span class="st"> </span><span class="kw">order</span>(CASchools<span class="op">$</span>income)

<span class="kw">lines</span>(CASchools<span class="op">$</span>income[order_id],
      <span class="kw">fitted</span>(LinearLog_model)[order_id], 
      <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-326-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>We can interpret <span class="math inline">\(\hat{\beta}_1\)</span> as follows: a <span class="math inline">\(1\%\)</span> increase in income is associated with an increase in test scores of <span class="math inline">\(0.01 \times 36.42 = 0.36\)</span> points. In order to get the estimated effect of a one unit change in income (that is, a change in the original units, thousands of dollars) on test scores, the method presented in Key Concept 8.1 can be used.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set up new data</span>
new_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">income =</span> <span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">11</span>, <span class="dv">40</span>, <span class="dv">41</span>))

<span class="co"># predict the outcomes </span>
Y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(LinearLog_model, <span class="dt">newdata =</span> new_data)

<span class="co"># compute the expected difference</span>
Y_hat_matrix &lt;-<span class="st"> </span><span class="kw">matrix</span>(Y_hat, <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
Y_hat_matrix[, <span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>Y_hat_matrix[, <span class="dv">1</span>]</code></pre></div>
<pre><code>## [1] 3.471166 0.899297</code></pre>
<p>By setting <tt>nrow = 2</tt> and <tt>byrow = TRUE</tt> in <tt>matrix()</tt> we ensure that <tt>Y_hat_matrix</tt> is a <span class="math inline">\(2\times2\)</span> matrix filled row-wise with the entries of <tt>Y_hat</tt>.</p>
<p>The estimated model states that for an income increase from <span class="math inline">\(\$10000\)</span> to <span class="math inline">\(\$11000\)</span>, test scores increase by an expected amount of <span class="math inline">\(3.47\)</span> points. When income increases from <span class="math inline">\(\$40000\)</span> to <span class="math inline">\(\$41000\)</span>, the expected increase in test scores is only about <span class="math inline">\(0.90\)</span> points.</p>
</div>
<div id="case-ii-y-is-in-logarithm-x-is-not" class="section level4 unnumbered">
<h4>Case II: <span class="math inline">\(Y\)</span> is in Logarithm, <span class="math inline">\(X\)</span> is not</h4>
<p>There are cases where it is useful to regress <span class="math inline">\(\ln(Y)\)</span>.</p>
<p>The corresponding regression model then is</p>
<p><span class="math display">\[ \ln(Y_i) = \beta_0 + \beta_1 \times X_i + u_i , \ \ i=1,...,n. \]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate a log-linear model </span>
LogLinear_model &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(score) <span class="op">~</span><span class="st"> </span>income, <span class="dt">data =</span> CASchools)

<span class="co"># obtain a robust coefficient summary</span>
<span class="kw">coeftest</span>(LogLinear_model, 
         <span class="dt">vcov =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##               Estimate Std. Error  t value  Pr(&gt;|t|)    
## (Intercept) 6.43936234 0.00289382 2225.210 &lt; 2.2e-16 ***
## income      0.00284407 0.00017509   16.244 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The estimated regression function is <span class="math display">\[\widehat{\ln(TestScore)} = 6.439 + 0.00284 \times income.\]</span> An increase in district income by <span class="math inline">\(\$1000\)</span> is expected to increase test scores by <span class="math inline">\(100\times 0.00284 \% = 0.284\%\)</span>.</p>
<p>When the dependent variable in logarithm, one cannot simply use <span class="math inline">\(e^{\log(\cdot)}\)</span> to transform predictions back to the original scale, see page of the book.</p>
</div>
<div id="case-iii-x-and-y-are-in-logarithms" class="section level4 unnumbered">
<h4>Case III: <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are in Logarithms</h4>
<p>The log-log regression model is <span class="math display">\[\ln(Y_i) = \beta_0 + \beta_1 \times \ln(X_i) + u_i, \ \ i=1,...,n.\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the log-log model</span>
LogLog_model &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(score) <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(income), <span class="dt">data =</span> CASchools)

<span class="co"># print robust coefficient summary to the console</span>
<span class="kw">coeftest</span>(LogLog_model, 
         <span class="dt">vcov =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error  t value  Pr(&gt;|t|)    
## (Intercept) 6.3363494  0.0059246 1069.501 &lt; 2.2e-16 ***
## log(income) 0.0554190  0.0021446   25.841 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The estimated regression function hence is <span class="math display">\[\widehat{\ln(TestScore)} = 6.336 + 0.0554 \times \ln(income).\]</span> In a log-log model, a <span class="math inline">\(1\%\)</span> change in <span class="math inline">\(X\)</span> is associated with an estimated <span class="math inline">\(\hat\beta_1 \%\)</span> change in <span class="math inline">\(Y\)</span>.</p>
<p>We now reproduce Figure 8.5 of the book.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate a scatterplot</span>
<span class="kw">plot</span>(<span class="kw">log</span>(score) <span class="op">~</span><span class="st"> </span>income, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">pch =</span> <span class="dv">20</span>, 
     <span class="dt">data =</span> CASchools,
     <span class="dt">main =</span> <span class="st">&quot;Log-Linear Regression Function&quot;</span>)

<span class="co"># add the log-linear regression line</span>
order_id  &lt;-<span class="st"> </span><span class="kw">order</span>(CASchools<span class="op">$</span>income)

<span class="kw">lines</span>(CASchools<span class="op">$</span>income[order_id], 
      <span class="kw">fitted</span>(LogLinear_model)[order_id], 
      <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>)

<span class="co"># add the log-log regression line</span>
<span class="kw">lines</span>(<span class="kw">sort</span>(CASchools<span class="op">$</span>income), 
      <span class="kw">fitted</span>(LogLog_model)[<span class="kw">order</span>(CASchools<span class="op">$</span>income)], 
      <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>)

<span class="co"># add a legend</span>
<span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>,
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;log-linear model&quot;</span>, <span class="st">&quot;log-log model&quot;</span>),
       <span class="dt">lwd =</span> <span class="dv">2</span>,
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;green&quot;</span>))</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-329-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>Key Concept 8.2 summarizes the three logarithmic regression models.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 8.2
</h3>
<h3 class="left">
Logarithms in Regression: Three Cases
</h3>
<p>
Logarithms can be used to transform the dependent variable <span class="math inline">\(Y\)</span> or the independent variable <span class="math inline">\(X\)</span>, or both (the variable being transformed must be positive). The following table summarizes these three cases and the interpretation of the regression coefficient <span class="math inline">\(\beta_1\)</span>. In each case, <span class="math inline">\(\beta_1\)</span>, can be estimated by applying OLS after taking the logarithm(s) of the dependent and/or the independent variable.
</p>
<table>
<thead>
<tr class="header">
<th align="left">
Case
</th>
<th align="left">
Model Specification
</th>
<th align="left">
Interpretation of <span class="math inline">\(\beta_1\)</span>
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">
<span class="math inline">\((I)\)</span>
</td>
<td align="left">
<span class="math inline">\(Y_i = \beta_0 + \beta_1 \ln(X_i) + u_i\)</span>
</td>
<td align="left">
A <span class="math inline">\(1 \%\)</span> change in <span class="math inline">\(X\)</span> is associated with a change in <span class="math inline">\(Y\)</span> of <span class="math inline">\(0.01 \times \beta_1\)</span>.
</td>
</tr>
<tr class="even">
<td align="left">
<span class="math inline">\((II)\)</span>
</td>
<td align="left">
<span class="math inline">\(\ln(Y_i) = \beta_0 + \beta_1 X_i + u_i\)</span>
</td>
<td align="left">
A change in <span class="math inline">\(X\)</span> by one unit (<span class="math inline">\(\Delta X = 1\)</span>) is associated with a <span class="math inline">\(100 \times \beta_1 \%\)</span> change in <span class="math inline">\(Y\)</span>.
</td>
</tr>
<tr class="odd">
<td align="left">
<span class="math inline">\((III)\)</span>
</td>
<td align="left">
<span class="math inline">\(\ln(Y_i) = \beta_0 + \beta_1 \ln(X_i) + u_i\)</span>
</td>
<td align="left">
A <span class="math inline">\(1\%\)</span> change in <span class="math inline">\(X\)</span> is associated with a <span class="math inline">\(\beta_1\%\)</span> change in <span class="math inline">\(Y\)</span>, so <span class="math inline">\(\beta_1\)</span> is the elasticity of <span class="math inline">\(Y\)</span> with respect to <span class="math inline">\(X\)</span>.
</td>
</tr>
</tbody>
</table>
</div>
<p>Of course we can also estimate a <em>polylog</em> model like</p>
<p><span class="math display">\[ TestScore_i = \beta_0 + \beta_1 \times \ln(income_i) + \beta_2 \times \ln(income_i)^2 + \beta_3 \times \ln(income_i)^3 + u_i \]</span></p>
<p>which models the dependent variable <span class="math inline">\(TestScore\)</span> by a third-degree polynomial of the log-transformed regressor <span class="math inline">\(income\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the polylog model</span>
polyLog_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(income) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(income)<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(income)<span class="op">^</span><span class="dv">3</span>), 
                    <span class="dt">data =</span> CASchools)

<span class="co"># print robust summary to the console</span>
<span class="kw">coeftest</span>(polyLog_model, 
         <span class="dt">vcov =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##                  Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept)      486.1341    79.3825  6.1239 2.115e-09 ***
## log(income)      113.3820    87.8837  1.2901    0.1977    
## I(log(income)^2) -26.9111    31.7457 -0.8477    0.3971    
## I(log(income)^3)   3.0632     3.7369  0.8197    0.4128    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Comparing by <span class="math inline">\(\bar{R}^2\)</span> we find that, leaving out the log-linear model, all models have a similar adjusted fit. In the class of polynomial models, the cubic specification has the highest <span class="math inline">\(\bar{R}^2\)</span> whereas the linear-log specification is the best of the log-models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the adj. R^2 for the nonlinear models</span>
adj_R2 &lt;-<span class="kw">rbind</span>(<span class="st">&quot;quadratic&quot;</span> =<span class="st"> </span><span class="kw">summary</span>(quadratic_model)<span class="op">$</span>adj.r.squared,
               <span class="st">&quot;cubic&quot;</span> =<span class="st"> </span><span class="kw">summary</span>(cubic_model)<span class="op">$</span>adj.r.squared,
               <span class="st">&quot;LinearLog&quot;</span> =<span class="st"> </span><span class="kw">summary</span>(LinearLog_model)<span class="op">$</span>adj.r.squared,
               <span class="st">&quot;LogLinear&quot;</span> =<span class="st"> </span><span class="kw">summary</span>(LogLinear_model)<span class="op">$</span>adj.r.squared,
               <span class="st">&quot;LogLog&quot;</span> =<span class="st"> </span><span class="kw">summary</span>(LogLog_model)<span class="op">$</span>adj.r.squared,
               <span class="st">&quot;polyLog&quot;</span> =<span class="st"> </span><span class="kw">summary</span>(polyLog_model)<span class="op">$</span>adj.r.squared)

<span class="co"># assign column names</span>
<span class="kw">colnames</span>(adj_R2) &lt;-<span class="st"> &quot;adj_R2&quot;</span>

adj_R2</code></pre></div>
<pre><code>##              adj_R2
## quadratic 0.5540444
## cubic     0.5552279
## LinearLog 0.5614605
## LogLinear 0.4970106
## LogLog    0.5567251
## polyLog   0.5599944</code></pre>
<p>Let us now compare the cubic and the linear-log model by plotting the corresponding estimated regression functions.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate a scatterplot</span>
<span class="kw">plot</span>(score <span class="op">~</span><span class="st"> </span>income, 
     <span class="dt">data =</span> CASchools,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, 
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">main =</span> <span class="st">&quot;Linear-Log and Cubic Regression Functions&quot;</span>)

<span class="co"># add the linear-log regression line</span>
order_id  &lt;-<span class="st"> </span><span class="kw">order</span>(CASchools<span class="op">$</span>income)

<span class="kw">lines</span>(CASchools<span class="op">$</span>income[order_id],
      <span class="kw">fitted</span>(LinearLog_model)[order_id], 
      <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>)

<span class="co"># add the cubic regression line</span>
<span class="kw">lines</span>(<span class="dt">x =</span> CASchools<span class="op">$</span>income[order_id], 
      <span class="dt">y =</span> <span class="kw">fitted</span>(cubic_model)[order_id],
      <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>, 
      <span class="dt">lwd =</span> <span class="dv">2</span>) </code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-333-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>Both regression lines look nearly identical. Altogether the linear-log model may be preferable since it is more parsimonious in terms of regressors: it does not include higher-degree polynomials.</p>
</div>
</div>
</div>
<div id="interactions-between-independent-variables" class="section level2">
<h2><span class="header-section-number">8.3</span> Interactions Between Independent Variables</h2>
<p>There are research questions where it is interesting to learn how the effect on <span class="math inline">\(Y\)</span> of a change in an independent variable depends on the value of another independent variable. For example, we may ask if districts with many English learners benefit differentially from a decrease in class sizes to those with few English learning students. To assess this using a multiple regression model, we include an interaction term. We consider three cases:</p>
<ol style="list-style-type: decimal">
<li><p>Interactions between two binary variables.</p></li>
<li><p>Interactions between a binary and a continuous variable.</p></li>
<li><p>Interactions between two continuous variables.</p></li>
</ol>
<p>The following subsections discuss these cases briefly and demonstrate how to perform such regressions in <tt>R</tt>.</p>
<div id="interactions-between-two-binary-variables" class="section level4 unnumbered">
<h4>Interactions Between Two Binary Variables</h4>
<p>Take two binary variables <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> and the population regression model</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 \times D_{1i} + \beta_2 \times D_{2i} + u_i. \]</span></p>
<p>Now assume that</p>
<span class="math display">\[\begin{align*}
  Y_i=&amp; \, \ln(Earnings_i),\\
  D_{1i} =&amp; \,
   \begin{cases}
      1 &amp; \text{if $i^{th}$ person has a college degree,} \\
      0 &amp; \text{else}.
    \end{cases} \\
  D_{2i} =&amp; \, 
    \begin{cases}
      1 &amp; \text{if $i^{th}$ person is female,} \\
      0 &amp; \text{if $i^{th}$ person is male}.
    \end{cases}
\end{align*}\]</span>
<p>We know that <span class="math inline">\(\beta_1\)</span> measures the average difference in <span class="math inline">\(\ln(Earnings)\)</span> between individuals with and without a college degree and <span class="math inline">\(\beta_2\)</span> is the gender differential in <span class="math inline">\(\ln(Earnings)\)</span>, ceteris paribus. This model <em>does not</em> allow us to determine if there is a gender specific effect of having a college degree and, if so, <em>how strong</em> this effect is. It is easy to come up with a model specification that allows to investigate this:</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 \times D_{1i} + \beta_2 \times D_{2i} + \beta_3 \times (D_{1i} \times D_{2i}) + u_i \]</span></p>
<p><span class="math inline">\((D_{1i} \times D_{2i})\)</span> is called an interaction term and <span class="math inline">\(\beta_3\)</span> measures the difference in the effect of having a college degree for women versus men.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 8.3
</h3>
<h3 class="left">
A Method for Interpreting Coefficients in Regression with Binary Variables
</h3>
<p>Compute expected values of <span class="math inline">\(Y\)</span> for each possible set described by the set of binary variables. Compare the expected values. The coefficients can be expressed either as expected values or as the difference between at least two expected values.</p>
</div>
<p>Following Key Concept 8.3 we have</p>
<span class="math display">\[\begin{align*}
  E(Y_i\vert D_{1i}=0, D_{2i} = d_2) =&amp; \, \beta_0 + \beta_1 \times 0 + \beta_2 \times d_2 + \beta_3 \times (0 \times d_2) \\
  =&amp; \, \beta_0 + \beta_2 \times d_2.
\end{align*}\]</span>
<p>If <span class="math inline">\(D_{1i}\)</span> switches from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span> we obtain</p>
<span class="math display">\[\begin{align*}
  E(Y_i\vert D_{1i}=1, D_{2i} = d_2) =&amp; \, \beta_0 + \beta_1 \times 1 + \beta_2 \times d_2 + \beta_3 \times (1 \times d_2) \\
  =&amp; \, \beta_0 + \beta_1 + \beta_2 \times d_2 + \beta_3 \times d_2.
\end{align*}\]</span>
<p>Hence, the overall effect is</p>
<p><span class="math display">\[ E(Y_i\vert D_{1i}=1, D_{2i} = d_2) - E(Y_i\vert D_{1i}=0, D_{2i} = d_2) = \beta_1 + \beta_3 \times d_2 \]</span> so the effect is a difference of expected values.</p>
</div>
<div id="application-to-the-student-teacher-ratio-and-the-percentage-of-english-learners" class="section level4 unnumbered">
<h4>Application to the Student-Teacher Ratio and the Percentage of English Learners</h4>
<p>Now let</p>
<span class="math display">\[\begin{align*}
  HiSTR =&amp; \, 
    \begin{cases}
      1, &amp; \text{if $STR \geq 20$} \\
      0, &amp; \text{else},
    \end{cases} \\
  \\
  HiEL =&amp; \,
    \begin{cases}
      1, &amp; \text{if $PctEL \geq 10$} \\
      0, &amp; \text{else}.
    \end{cases}
\end{align*}\]</span>
<p>We may use <tt>R</tt> to construct the variables above as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># append HiSTR to CASchools</span>
CASchools<span class="op">$</span>HiSTR &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(CASchools<span class="op">$</span>size <span class="op">&gt;=</span><span class="st"> </span><span class="dv">20</span>)

<span class="co"># append HiEL to CASchools</span>
CASchools<span class="op">$</span>HiEL &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(CASchools<span class="op">$</span>english <span class="op">&gt;=</span><span class="st"> </span><span class="dv">10</span>)</code></pre></div>
<p>We proceed by estimating the model</p>
<span class="math display" id="eq:im">\[\begin{align}
TestScore = \beta_0 + \beta_1 \times HiSTR + \beta_2 \times HiEL + \beta_3 \times (HiSTR \times HiEL) + u_i. \tag{8.1}
\end{align}\]</span>
<p>There are several ways to add the interaction term to the <tt>formula</tt> argument when using <tt>lm()</tt> but the most intuitive way is to use <tt>HiEL * HiSTR</tt>.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the model with a binary interaction term</span>
bi_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>HiSTR <span class="op">*</span><span class="st"> </span>HiEL, <span class="dt">data =</span> CASchools)

<span class="co"># print a robust summary of the coefficients</span>
<span class="kw">coeftest</span>(bi_model, <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error  t value  Pr(&gt;|t|)    
## (Intercept) 664.1433     1.3881 478.4589 &lt; 2.2e-16 ***
## HiSTR        -1.9078     1.9322  -0.9874    0.3240    
## HiEL        -18.3155     2.3340  -7.8472 3.634e-14 ***
## HiSTR:HiEL   -3.2601     3.1189  -1.0453    0.2965    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The estimated regression model is <span class="math display">\[\widehat{TestScore} = \underset{(1.39)}{664.1} - \underset{(1.93)}{1.9} \times HiSTR - \underset{(2.33)}{18.3} \times HiEL - \underset{(3.12)}{3.3} \times (HiSTR \times HiEL)\]</span> and it predicts that the effect of moving from a school district with a low student-teacher ratio to a district with a high student-teacher ratio, depending on high or low percentage of english learners is <span class="math inline">\(-1.9-3.3\times HiEL\)</span>. So for districts with a low share of english learners (<span class="math inline">\(HiEL = 0\)</span>), the estimated effect is a decrease of <span class="math inline">\(1.9\)</span> points in test scores while for districts with a large fraction of English learners (<span class="math inline">\(HiEL = 1\)</span>), the predicted decrease in test scores amounts to <span class="math inline">\(1.9 + 3.3 = 5.2\)</span> points.</p>
<p>We can also use the model to estimate the mean test score for each possible combination of the included binary variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate means for all combinations of HiSTR and HiEL</span>

<span class="co"># 1.</span>
<span class="kw">predict</span>(bi_model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="st">&quot;HiSTR&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;HiEL&quot;</span> =<span class="st"> </span><span class="dv">0</span>))</code></pre></div>
<pre><code>##        1 
## 664.1433</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 2.</span>
<span class="kw">predict</span>(bi_model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="st">&quot;HiSTR&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;HiEL&quot;</span> =<span class="st"> </span><span class="dv">1</span>))</code></pre></div>
<pre><code>##        1 
## 645.8278</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 3.</span>
<span class="kw">predict</span>(bi_model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="st">&quot;HiSTR&quot;</span> =<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;HiEL&quot;</span> =<span class="st"> </span><span class="dv">0</span>))</code></pre></div>
<pre><code>##        1 
## 662.2354</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 4.</span>
<span class="kw">predict</span>(bi_model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="st">&quot;HiSTR&quot;</span> =<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;HiEL&quot;</span> =<span class="st"> </span><span class="dv">1</span>))</code></pre></div>
<pre><code>##        1 
## 640.6598</code></pre>
<p>We now verify that these predictions are differences in the coefficient estimates presented in equation <a href="nrf.html#eq:im">(8.1)</a>:</p>
<span class="math display">\[\begin{align*}
\widehat{TestScore} = \hat\beta_0 = 664.1 \quad &amp;\Leftrightarrow \quad HiSTR = 0, \, HIEL = 0\\
\widehat{TestScore} = \hat\beta_0 + \hat\beta_2 = 664.1 - 18.3 = 645.8 \quad &amp;\Leftrightarrow \quad HiSTR = 0, \, HIEL = 1\\
\widehat{TestScore} = \hat\beta_0 + \hat\beta_1 = 664.1 - 1.9 = 662.2 \quad &amp;\Leftrightarrow \quad HiSTR = 1, \, HIEL = 0\\
\widehat{TestScore} = \hat\beta_0 + \hat\beta_1 + \hat\beta_2 + \hat\beta_3  = 664.1 - 1.9 - 18.3 - 3.3 = 640.6 \quad &amp;\Leftrightarrow \quad HiSTR = 1, \, HIEL = 1
\end{align*}\]</span>
</div>
<div id="interactions-between-a-continuous-and-a-binary-variable" class="section level4 unnumbered">
<h4>Interactions Between a Continuous and a Binary Variable</h4>
Now let <span class="math inline">\(X_i\)</span> denote the years of working experience of person <span class="math inline">\(i\)</span>, which is a continuous variable. We have
<span class="math display">\[\begin{align*}
  Y_i =&amp; \, \ln(Earnings_i), \\
  \\
  X_i =&amp; \, \text{working experience of person }i, \\
  \\
  D_i =&amp; \,  
    \begin{cases}
      1, &amp; \text{if $i^{th}$ person has a college degree} \\
      0, &amp; \text{else}.
    \end{cases}
\end{align*}\]</span>
<p>The baseline model thus is</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 X_i + \beta_2 D_i + u_i, \]</span></p>
<p>a multiple regression model that allows us to estimate the average benefit of having a college degree holding working experience constant as well as the average effect on earnings of a change in working experience holding college degree constant.</p>
<p>By adding the interaction term <span class="math inline">\(X_i \times D_i\)</span> we allow the effect of an additional year of work experience to differ between individuals with and without college degree,</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 X_i + \beta_2 D_i + \beta_3  (X_i \times D_i) + u_i. \]</span></p>
<p>Here, <span class="math inline">\(\beta_3\)</span> is the expected difference in the effect of an additional year of work experience for college graduates versus non-graduates. Another possible specification is</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 X_i + \beta_2 (X_i \times D_i) + u_i. \]</span></p>
<p>This model states that the expected impact of an additional year of work experience on earnings differs for for college graduates and non-graduates but that graduating on its own does not increase earnings.</p>
<p>All three regression functions can be visualized by straight lines. Key Concept 8.4 summarizes the differences.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 8.4
</h3>
<h3 class="left">
Interactions Between Binary and Continuous Variables
</h3>
<p>An interaction term like <span class="math inline">\(X_i \times D_i\)</span> (where <span class="math inline">\(X_i\)</span> is continuous and <span class="math inline">\(D_i\)</span> is binary) allows for the slope to depend on the binary variable <span class="math inline">\(D_i\)</span>. There are three possibilities:</p>
<ol style="list-style-type: decimal">
<li>Different intercept and same slope: <span class="math display">\[ Y_i = \beta_0 + \beta_1 X_i + \beta_2 D_i + u_i \]</span></li>
<li><p>Different intercept and different slope: <span class="math display">\[ Y_i = \beta_0 + \beta_1 X_i + \beta_2 D_i + \beta_3 \times (X_i \times D_i) + u_i \]</span></p></li>
<li>Same intercept and different slope: <span class="math display">\[ Y_i = \beta_0 + \beta_1 X_i + \beta_2 (X_i \times D_i) + u_i \]</span></li>
</ol>
</div>
<p>The following code chunk demonstrates how to replicate the results shown in Figure 8.8 of the book using artificial data.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate artificial data</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)

X &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">200</span>,<span class="dv">0</span>, <span class="dv">15</span>)
D &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, <span class="dv">200</span>, <span class="dt">replace =</span> T)
Y &lt;-<span class="st"> </span><span class="dv">450</span> <span class="op">+</span><span class="st">  </span><span class="dv">150</span> <span class="op">*</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span><span class="dv">500</span> <span class="op">*</span><span class="st"> </span>D <span class="op">+</span><span class="st"> </span><span class="dv">50</span> <span class="op">*</span><span class="st"> </span>(X <span class="op">*</span><span class="st"> </span>D) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">200</span>, <span class="dt">sd =</span> <span class="dv">300</span>)

<span class="co"># divide plotting area accordingly</span>
m &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">0</span>))
graphics<span class="op">::</span><span class="kw">layout</span>(m)

<span class="co"># estimate the models and plot the regression lines</span>

<span class="co"># 1. (baseline model)</span>
<span class="kw">plot</span>(X, <span class="kw">log</span>(Y),
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Different Intercepts, Same Slope&quot;</span>)

mod1_coef &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Y) <span class="op">~</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span>D)<span class="op">$</span>coefficients

<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(mod1_coef[<span class="dv">1</span>], mod1_coef[<span class="dv">2</span>]), 
       <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>,
       <span class="dt">lwd =</span> <span class="fl">1.5</span>)

<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(mod1_coef[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>mod1_coef[<span class="dv">3</span>], mod1_coef[<span class="dv">2</span>]), 
       <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>,
       <span class="dt">lwd =</span> <span class="fl">1.5</span>)
       
<span class="co"># 2. (baseline model + interaction term)</span>
<span class="kw">plot</span>(X, <span class="kw">log</span>(Y),
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Different Intercepts, Different Slopes&quot;</span>)

mod2_coef &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Y) <span class="op">~</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span>D <span class="op">+</span><span class="st"> </span>X<span class="op">:</span>D)<span class="op">$</span>coefficients

<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(mod2_coef[<span class="dv">1</span>], mod2_coef[<span class="dv">2</span>]), 
       <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>,
       <span class="dt">lwd =</span> <span class="fl">1.5</span>)

<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(mod2_coef[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>mod2_coef[<span class="dv">3</span>], mod2_coef[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>mod2_coef[<span class="dv">4</span>]), 
       <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>,
       <span class="dt">lwd =</span> <span class="fl">1.5</span>)

<span class="co"># 3. (omission of D as regressor + interaction term)</span>
<span class="kw">plot</span>(X, <span class="kw">log</span>(Y),
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Same Intercept, Different Slopes&quot;</span>)

mod3_coef &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Y) <span class="op">~</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span>X<span class="op">:</span>D)<span class="op">$</span>coefficients

<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(mod3_coef[<span class="dv">1</span>], mod3_coef[<span class="dv">2</span>]), 
       <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>,
       <span class="dt">lwd =</span> <span class="fl">1.5</span>)

<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(mod3_coef[<span class="dv">1</span>], mod3_coef[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>mod3_coef[<span class="dv">3</span>]), 
       <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>,
       <span class="dt">lwd =</span> <span class="fl">1.5</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-341-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="application-to-the-student-teacher-ratio-and-the-percentage-of-english-learners-1" class="section level4 unnumbered">
<h4>Application to the Student-Teacher Ratio and the Percentage of English Learners</h4>
<p>Using a model specification like the second one discussed in Key Concept 8.3 (different slope, different intercept) we may answer the question whether the effect on test scores of decreasing the student-teacher ratio depends on whether there are many or few English learners. We estimate the regression model</p>
<p><span class="math display">\[ \widehat{TestScore_i} = \beta_0 + \beta_1 \times size_i + \beta_2 \times HiEL_i + \beta_2 (size_i \times HiEL_i) + u_i. \]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the model</span>
bci_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>size <span class="op">+</span><span class="st"> </span>HiEL <span class="op">+</span><span class="st"> </span>size <span class="op">*</span><span class="st"> </span>HiEL, <span class="dt">data =</span> CASchools)

<span class="co"># print robust summary of coefficients to the console</span>
<span class="kw">coeftest</span>(bci_model, <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 682.24584   11.86781 57.4871   &lt;2e-16 ***
## size         -0.96846    0.58910 -1.6440   0.1009    
## HiEL          5.63914   19.51456  0.2890   0.7727    
## size:HiEL    -1.27661    0.96692 -1.3203   0.1875    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The estimated regression model is <span class="math display">\[ \widehat{TestScore} = \underset{(11.87)}{682.2} - \underset{(0.59)}{0.97} \times size + \underset{(19.51)}{5.6} \times HiEL - \underset{(0.97)}{1.28} \times (size \times HiEL). \]</span> The estimated regression line for districts with a low fraction of English learners (<span class="math inline">\(HiEL_i=0\)</span>) is <span class="math display">\[ \widehat{TestScore} = 682.2 - 0.97\times size_i. \]</span></p>
<p>For districts with a high fraction of English learners we have</p>
<span class="math display">\[\begin{align*} 
  \widehat{TestScore} =&amp; \, 682.2 + 5.6 - 0.97\times size_i - 1.28 \times size_i \\
   =&amp; \, 687.8 - 2.25 \times size_i.
\end{align*}\]</span>
<p>The predicted increase in test scores following a reduction of the student-teacher ratio by <span class="math inline">\(1\)</span> unit is about <span class="math inline">\(0.97\)</span> points in districts where the fraction of English learners is low but <span class="math inline">\(2.25\)</span> in districts with a high share of English learners. From the coefficient on the interaction term <span class="math inline">\(size \times HiEL\)</span> we see that the difference between both effects is <span class="math inline">\(1.28\)</span> points.</p>
<p>The next code chunk draws both lines belonging to the model. In order to make observations with <span class="math inline">\(HiEL = 0\)</span> distinguishable from those with <span class="math inline">\(HiEL = 1\)</span>, we use different colors.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># identify observations with PctEL &gt;= 10</span>
id &lt;-<span class="st"> </span>CASchools<span class="op">$</span>english <span class="op">&gt;=</span><span class="st"> </span><span class="dv">10</span>

<span class="co"># plot observations with HiEL = 0 as red dots</span>
<span class="kw">plot</span>(CASchools<span class="op">$</span>size[<span class="op">!</span>id], CASchools<span class="op">$</span>score[<span class="op">!</span>id],
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">27</span>),
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">600</span>, <span class="dv">720</span>),
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Class Size&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Test Score&quot;</span>)

<span class="co"># plot observations with HiEL = 1 as green dots</span>
<span class="kw">points</span>(CASchools<span class="op">$</span>size[id], CASchools<span class="op">$</span>score[id],
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>)

<span class="co"># read out estimated coefficients of bci_model</span>
coefs &lt;-<span class="st"> </span>bci_model<span class="op">$</span>coefficients

<span class="co"># draw the estimated regression line for HiEL = 0</span>
<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(coefs[<span class="dv">1</span>], coefs[<span class="dv">2</span>]),
       <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>,
       <span class="dt">lwd =</span> <span class="fl">1.5</span>)

<span class="co"># draw the estimated regression line for HiEL = 1</span>
<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(coefs[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>coefs[<span class="dv">3</span>], coefs[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>coefs[<span class="dv">4</span>]),
       <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>, 
       <span class="dt">lwd =</span> <span class="fl">1.5</span> )

<span class="co"># add a legend to the plot</span>
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, 
       <span class="dt">pch =</span> <span class="kw">c</span>(<span class="dv">20</span>, <span class="dv">20</span>), 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;green&quot;</span>), 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;HiEL = 0&quot;</span>, <span class="st">&quot;HiEL = 1&quot;</span>))</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-343-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="interactions-between-two-continuous-variables" class="section level4 unnumbered">
<h4>Interactions Between Two Continuous Variables</h4>
<p>Consider a regression model with <span class="math inline">\(Y\)</span> the log earnings and two continuous regressors <span class="math inline">\(X_1\)</span>, the years of work experience, and <span class="math inline">\(X_2\)</span>, the years of schooling. We want to estimate the effect on wages of an additional year of work experience depending on a given level of schooling. This effect can be assessed by including the interaction term <span class="math inline">\((X_{1i} \times X_{2i})\)</span> in the model:</p>
<p><span class="math display">\[ \Delta Y_i = \beta_0 + \beta_1 \times X_{1i} + \beta_2 \times X_{2i} + \beta_3 \times (X_{1i} \times X_{2i}) + u_i \]</span></p>
<p>Following Key Concept 8.1 we find that the effect on <span class="math inline">\(Y\)</span> of a change on <span class="math inline">\(X_1\)</span> given <span class="math inline">\(X_2\)</span> is <span class="math display">\[ \frac{\Delta Y}{\Delta X_1} = \beta_1 + \beta_3 X_2. \]</span></p>
<p>In the earnings example, a positive <span class="math inline">\(\beta_3\)</span> implies that the effect on log earnings of an additional year of work experience grows linearly with years of schooling. Vice versa we have <span class="math display">\[ \frac{\Delta Y}{\Delta X_2} = \beta_2 + \beta_3 X_1 \]</span> as the effect on log earnings of an additional year of schooling holding work experience constant.</p>
<p>Altogether we find that <span class="math inline">\(\beta_3\)</span> measures the effect of a unit increase in <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> <em>beyond</em> the effects of increasing <span class="math inline">\(X_1\)</span> alone and <span class="math inline">\(X_2\)</span> alone by one unit. The overall change in <span class="math inline">\(Y\)</span> is thus</p>
<span class="math display" id="eq:generalinteraction">\[\begin{align}
Y_i = (\beta_1 + \beta_3 X_2) \Delta X_1 + (\beta_2 + \beta_3 X_1) \Delta X_2 + \beta_3\Delta X_1 \Delta X_2. \tag{8.2}
\end{align}\]</span>
<p>Key Concept 8.5 summarizes interactions between two regressors in multiple regression.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 8.5
</h3>
<h3 class="left">
Interactions in Multiple Regression
</h3>
<p>The interaction term between the two regressors <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is given by their product <span class="math inline">\(X_1 \times X_2\)</span>. Adding this interaction term as a regressor to the model <span class="math display">\[ Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + u_i \]</span> allows the effect on <span class="math inline">\(Y\)</span> of a change in <span class="math inline">\(X_2\)</span> to depend on the value of <span class="math inline">\(X_1\)</span> and vice versa. Thus the coefficient <span class="math inline">\(\beta_3\)</span> in the model <span class="math display">\[ Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 (X_1 \times X_2) + u_i \]</span> measures the effect of a one-unit increase in both <span class="math inline">\(X_1\)</span> <it>and</it> <span class="math inline">\(X_2\)</span> above and beyond the sum of both individual effects. This holds for continuous <em>and</em> binary regressors.</p>
</div>
</div>
<div id="application-to-the-student-teacher-ratio-and-the-percentage-of-english-learners-2" class="section level4">
<h4><span class="header-section-number">8.3.0.1</span> Application to the Student-Teacher Ratio and the Percentage of English Learners</h4>
<p>We now examine the interaction between the continuous variables student-teacher ratio and the percentage of English learners.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate regression model including the interaction between &#39;PctEL&#39; and &#39;size&#39;</span>
cci_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>size <span class="op">+</span><span class="st"> </span>english <span class="op">+</span><span class="st"> </span>english <span class="op">*</span><span class="st"> </span>size, <span class="dt">data =</span> CASchools) 

<span class="co"># print a summary to the console</span>
<span class="kw">coeftest</span>(cci_model, <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##                 Estimate  Std. Error t value Pr(&gt;|t|)    
## (Intercept)  686.3385268  11.7593466 58.3654  &lt; 2e-16 ***
## size          -1.1170184   0.5875136 -1.9013  0.05796 .  
## english       -0.6729119   0.3741231 -1.7986  0.07280 .  
## size:english   0.0011618   0.0185357  0.0627  0.95005    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The estimated model equation is <span class="math display">\[ \widehat{TestScore} = \underset{(11.76)}{686.3} - \underset{(0.59)}{1.12} \times STR - \underset{(0.37)}{0.67} \times PctEL + \underset{(0.02)}{0.0012} \times (STR\times PctEL). \]</span></p>
<p>For the interpretation, let us consider the quartiles of <span class="math inline">\(PctEL\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(CASchools<span class="op">$</span>english)</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   1.941   8.778  15.768  22.970  85.540</code></pre>
<p>According to <a href="nrf.html#eq:generalinteraction">(8.2)</a>, if <span class="math inline">\(PctEL\)</span> is at its median value of <span class="math inline">\(8.78\)</span>, the slope of the regression function relating test scores and the student teacher ratio is predicted to be <span class="math inline">\(-1.12 + 0.0012 \times 8.78 = -1.11\)</span>. This means that increasing the student-teacher ratio by one unit is expected to deteriorate test scores by <span class="math inline">\(1.11\)</span> points. For the <span class="math inline">\(75\%\)</span> quantile, the estimated change on <span class="math inline">\(TestScore\)</span> of a one-unit increase in <span class="math inline">\(STR\)</span> is estimated by <span class="math inline">\(-1.12 + 0.0012 \times 23.0 = -1.09\)</span> so the slope is somewhat lower. The interpretation is that for a school district with a share of <span class="math inline">\(23\%\)</span> English learners, a reduction of the student-teacher ratio by one unit is expected to increase test scores by only <span class="math inline">\(1.09\)</span> points.</p>
<p>However, the output of <tt>summary()</tt> indicates that the difference of the effect for the median and the <span class="math inline">\(75\%\)</span> quantile is not statistically significant. <span class="math inline">\(H_0: \beta_3 = 0\)</span> cannot be rejected at the <span class="math inline">\(5\%\)</span> level of significance (the <span class="math inline">\(p\)</span>-value is <span class="math inline">\(0.95\)</span>).</p>
</div>
<div id="example-the-demand-for-economic-journals" class="section level4 unnumbered">
<h4>Example: The Demand for Economic Journals</h4>
<p>In this section we replicate the empirical example  presented at pages 336 - 337 of the book. The central question is: how elastic is the demand by libraries for economic journals? The idea here is to analyze the relationship between the number of subscription to a journal at U.S. libraries and the journal’s subscription price. The study uses the data set <tt>Journals</tt> which is provided with the <tt>AER</tt> package and contains observations for <span class="math inline">\(180\)</span> economic journals for the year 2000. You can use the help function (<code>?Journals</code>) to get more information on the data after loading the package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load package and the data set</span>
<span class="kw">library</span>(AER)
<span class="kw">data</span>(<span class="st">&quot;Journals&quot;</span>)</code></pre></div>
<p>We measure the price as “price per citation” and compute journal age and the number of characters manually. For consistency with the book we also rename the variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define and rename variables</span>
Journals<span class="op">$</span>PricePerCitation &lt;-<span class="st"> </span>Journals<span class="op">$</span>price<span class="op">/</span>Journals<span class="op">$</span>citations
Journals<span class="op">$</span>Age &lt;-<span class="st"> </span><span class="dv">2000</span> <span class="op">-</span><span class="st"> </span>Journals<span class="op">$</span>foundingyear
Journals<span class="op">$</span>Characters &lt;-<span class="st"> </span>Journals<span class="op">$</span>charpp <span class="op">*</span><span class="st"> </span>Journals<span class="op">$</span>pages<span class="op">/</span><span class="dv">10</span><span class="op">^</span><span class="dv">6</span>
Journals<span class="op">$</span>Subscriptions &lt;-<span class="st"> </span>Journals<span class="op">$</span>subs</code></pre></div>
<p>The range of “price per citation” is quite large:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute summary statistics for price per citation</span>
<span class="kw">summary</span>(Journals<span class="op">$</span>PricePerCitation)</code></pre></div>
<pre><code>##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
##  0.005223  0.464495  1.320513  2.548455  3.440171 24.459459</code></pre>
<p>The lowest price observed is a mere <span class="math inline">\(0.5\)</span>¢ per citation while the highest price is more than <span class="math inline">\(20\)</span>¢ per citation.</p>
<p>We now estimate four different model specifications. All models are log-log models. This is useful because it allows us to directly interpret the coefficients as elasticities, see Key Concept 8.2. <span class="math inline">\((I)\)</span> is a linear model. To alleviate a possible omitted variable bias, <span class="math inline">\((II)\)</span> augments <span class="math inline">\((I)\)</span> by the covariates <span class="math inline">\(\ln(Age)\)</span> and <span class="math inline">\(\ln(Characters)\)</span>. The largest model <span class="math inline">\((III)\)</span> attempts to capture nonlinearities in the relationship of <span class="math inline">\(\ln(Subscriptions)\)</span> and <span class="math inline">\(\ln(PricePerCitation)\)</span> using a cubic regression function of <span class="math inline">\(\ln(PricePerCitation)\)</span> and also adds the interaction term <span class="math inline">\((PricePerCitation \times Age)\)</span> while specification <span class="math inline">\((IV)\)</span> does not include the cubic term.</p>
<span class="math display">\[\begin{align*}
  (I)\quad \ln(Subscriptions_i) =&amp; \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + u_i \\
  \\
  (II)\quad \ln(Subscriptions_i) =&amp; \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + \beta_4 \ln(Age_i) + \beta_6 \ln(Characters_i) + u_i \\
  \\
  (III)\quad \ln(Subscriptions_i) =&amp; \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + \beta_2 \ln(PricePerCitation_i)^2 \\
  +&amp; \, \beta_3 \ln(PricePerCitation_i)^3 + \beta_4 \ln(Age_i) + \beta_5 \left[\ln(Age_i) \times \ln(PricePerCitation_i)\right] \\ +&amp; \, \beta_6 \ln(Characters_i) + u_i \\
  \\
  (IV)\quad \ln(Subscriptions_i) =&amp; \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + \beta_4 \ln(Age_i) + \beta_6 \ln(Characters_i) + u_i
\end{align*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Estimate models (I) - (IV)</span>
Journals_mod1 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Subscriptions) <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(PricePerCitation), 
                    <span class="dt">data =</span> Journals)

Journals_mod2 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Subscriptions) <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(PricePerCitation) 
                    <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(Age) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(Characters), 
                    <span class="dt">data =</span> Journals)

Journals_mod3 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Subscriptions) <span class="op">~</span><span class="st"> </span>
<span class="st">                    </span><span class="kw">log</span>(PricePerCitation) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(PricePerCitation)<span class="op">^</span><span class="dv">2</span>) 
                    <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">log</span>(PricePerCitation)<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(Age) 
                    <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(Age)<span class="op">:</span><span class="kw">log</span>(PricePerCitation) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(Characters), 
                    <span class="dt">data =</span> Journals)

Journals_mod4 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Subscriptions) <span class="op">~</span><span class="st"> </span>
<span class="st">                    </span><span class="kw">log</span>(PricePerCitation) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(Age) 
                    <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(Age)<span class="op">:</span><span class="kw">log</span>(PricePerCitation) <span class="op">+</span><span class="st"> </span>
<span class="st">                    </span><span class="kw">log</span>(Characters), 
                    <span class="dt">data =</span> Journals)</code></pre></div>
<p>Using <tt>summary()</tt>, we obtain the following estimated models:</p>
<span class="math display">\[\begin{align*}
  (I)\quad \widehat{\ln(Subscriptions_i)} =&amp; \, 4.77 - 0.53 \ln(PricePerCitation_i) \\
  \\
  (II)\quad \widehat{\ln(Subscriptions_i)} =&amp; \, 3.21 - 0.41 \ln(PricePerCitation_i) + 0.42 \ln(Age_i) + 0.21 \ln(Characters_i) \\
  \\
  (III)\quad \widehat{\ln(Subscriptions_i)} =&amp; \, 3.41 - 0.96 \ln(PricePerCitation_i) + 0.02 \ln(PricePerCitation_i)^2 \\
  &amp;+ 0.004 \ln(PricePerCitation_i)^3 + 0.37 \ln(Age_i) \\
  &amp;+ 0.16 \left[\ln(Age_i) \times \ln(PricePerCitation_i)\right] \\ &amp;+ 0.23 \ln(Characters_i) \\
  \\
  (IV)\quad \widehat{\ln(Subscriptions_i)} =&amp; \, 3.43 - 0.90 \ln(PricePerCitation_i) + 0.37 \ln(Age_i) \\ 
  &amp;+ 0.14 \left[\ln(Age_i) \times \ln(PricePerCitation_i)\right] + 0.23 \ln(Characters_i)
\end{align*}\]</span>
<p>We use an <span class="math inline">\(F\)</span>-Test to test if the transformations of <span class="math inline">\(\ln(PricePerCitation)\)</span> in Model <span class="math inline">\((III)\)</span> are statistically significant.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># F-Test for significance of cubic terms</span>
<span class="kw">linearHypothesis</span>(Journals_mod3, 
                 <span class="kw">c</span>(<span class="st">&quot;I(log(PricePerCitation)^2)=0&quot;</span>, <span class="st">&quot;I(log(PricePerCitation)^3)=0&quot;</span>),
                 <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## I(log(PricePerCitation)^2) = 0
## I(log(PricePerCitation)^3) = 0
## 
## Model 1: restricted model
## Model 2: log(Subscriptions) ~ log(PricePerCitation) + I(log(PricePerCitation)^2) + 
##     I(log(PricePerCitation)^3) + log(Age) + log(Age):log(PricePerCitation) + 
##     log(Characters)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F Pr(&gt;F)
## 1    175                 
## 2    173  2 0.1943 0.8236</code></pre>
<p>Clearly, we cannot reject the null hypothesis <span class="math inline">\(H_0: \beta_3=\beta_4=0\)</span> in model <span class="math inline">\((III)\)</span>.</p>
<p>We now demonstrate how the function <tt>stargazer()</tt> can be used to generate a tabular representation of all four models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the stargazer package</span>
<span class="kw">library</span>(stargazer)

<span class="co"># gather robust standard errors in a list</span>
rob_se &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(Journals_mod1, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(Journals_mod2, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(Journals_mod3, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(Journals_mod4, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))))

<span class="co"># generate a Latex table using stargazer</span>
<span class="kw">stargazer</span>(Journals_mod1, Journals_mod2, Journals_mod3, Journals_mod4,
          <span class="dt">se =</span> rob_se,
          <span class="dt">digits =</span> <span class="dv">3</span>,
          <span class="dt">column.labels =</span> <span class="kw">c</span>(<span class="st">&quot;(I)&quot;</span>, <span class="st">&quot;(II)&quot;</span>, <span class="st">&quot;(III)&quot;</span>, <span class="st">&quot;(IV)&quot;</span>))</code></pre></div>



<table style="text-align:center"><tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"></td><td colspan="4">Dependent Variable: Logarithm of Subscriptions</td></tr>
<tr><td></td><td colspan="4" style="border-bottom: 1px solid black"></td></tr>
<tr><td style="text-align:left"></td><td colspan="4">log(Subscriptions)</td></tr>
<tr><td style="text-align:left"></td><td>(I)</td><td>(II)</td><td>(III)</td><td>(IV)</td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">log(PricePerCitation)</td><td>-0.533<sup>***</sup></td><td>-0.408<sup>***</sup></td><td>-0.961<sup>***</sup></td><td>-0.899<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.034)</td><td>(0.044)</td><td>(0.160)</td><td>(0.145)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(log(PricePerCitation)2)</td><td></td><td></td><td>0.017</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td>(0.025)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(log(PricePerCitation)3)</td><td></td><td></td><td>0.004</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td>(0.006)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">log(Age)</td><td></td><td>0.424<sup>***</sup></td><td>0.373<sup>***</sup></td><td>0.374<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td>(0.119)</td><td>(0.118)</td><td>(0.118)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">log(Characters)</td><td></td><td>0.206<sup>**</sup></td><td>0.235<sup>**</sup></td><td>0.229<sup>**</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td>(0.098)</td><td>(0.098)</td><td>(0.096)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">log(PricePerCitation):log(Age)</td><td></td><td></td><td>0.156<sup>***</sup></td><td>0.141<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td>(0.052)</td><td>(0.040)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">Constant</td><td>4.766<sup>***</sup></td><td>3.207<sup>***</sup></td><td>3.408<sup>***</sup></td><td>3.434<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.055)</td><td>(0.380)</td><td>(0.374)</td><td>(0.367)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">Observations</td><td>180</td><td>180</td><td>180</td><td>180</td></tr>
<tr><td style="text-align:left">R<sup>2</sup></td><td>0.557</td><td>0.613</td><td>0.635</td><td>0.634</td></tr>
<tr><td style="text-align:left">Adjusted R<sup>2</sup></td><td>0.555</td><td>0.607</td><td>0.622</td><td>0.626</td></tr>
<tr><td style="text-align:left">Residual Std. Error</td><td>0.750 (df = 178)</td><td>0.705 (df = 176)</td><td>0.691 (df = 173)</td><td>0.688 (df = 175)</td></tr>
<tr><td style="text-align:left">F Statistic</td><td>224.037<sup>***</sup> (df = 1; 178)</td><td>93.009<sup>***</sup> (df = 3; 176)</td><td>50.149<sup>***</sup> (df = 6; 173)</td><td>75.749<sup>***</sup> (df = 4; 175)</td></tr>
<tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"><em>Note:</em></td><td colspan="4" style="text-align:right"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>
</table>
<caption><p style='text-align:center'><span id="tab:nrmojs">Table 8.1: </span> Nonlinear Regression Models of Journal Subscribtions</p></caption>


<p>The subsequent code chunk reproduces Figure 8.9 of the book.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># divide plotting area</span>
m &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">0</span>))
graphics<span class="op">::</span><span class="kw">layout</span>(m)

<span class="co"># scatterplot</span>
<span class="kw">plot</span>(Journals<span class="op">$</span>PricePerCitation, 
     Journals<span class="op">$</span>Subscriptions, 
     <span class="dt">pch =</span> <span class="dv">20</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Subscriptions&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;ln(Price per ciation)&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;(a)&quot;</span>)

<span class="co"># log-log scatterplot and estimated regression line (I)</span>
<span class="kw">plot</span>(<span class="kw">log</span>(Journals<span class="op">$</span>PricePerCitation), 
     <span class="kw">log</span>(Journals<span class="op">$</span>Subscriptions), 
     <span class="dt">pch =</span> <span class="dv">20</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;ln(Subscriptions)&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;ln(Price per ciation)&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;(b)&quot;</span>)

<span class="kw">abline</span>(Journals_mod1,
       <span class="dt">lwd =</span> <span class="fl">1.5</span>)

<span class="co"># log-log scatterplot and regression lines (IV) for Age = 5 and Age = 80</span>
<span class="kw">plot</span>(<span class="kw">log</span>(Journals<span class="op">$</span>PricePerCitation), 
     <span class="kw">log</span>(Journals<span class="op">$</span>Subscriptions), 
     <span class="dt">pch =</span> <span class="dv">20</span>, 
     <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;ln(Subscriptions)&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;ln(Price per ciation)&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;(c)&quot;</span>)

JM4C &lt;-Journals_mod4<span class="op">$</span>coefficients

<span class="co"># Age = 80</span>
<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(JM4C[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>JM4C[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">80</span>), 
                JM4C[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>JM4C[<span class="dv">5</span>] <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">80</span>)),
       <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>,
       <span class="dt">lwd =</span> <span class="fl">1.5</span>)

<span class="co"># Age = 5</span>
<span class="kw">abline</span>(<span class="dt">coef =</span> <span class="kw">c</span>(JM4C[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>JM4C[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">5</span>), 
                JM4C[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>JM4C[<span class="dv">5</span>] <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">5</span>)),
       <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>,
       <span class="dt">lwd =</span> <span class="fl">1.5</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-356-1.png" width="672" /></p>
</div>
<p>As can be seen from plots (a) and (b), the relation between subscriptions and the citation price is adverse and nonlinear. Log-transforming both variables makes it approximately linear. Plot (c) shows that the price elasticity of journal subscriptions depends on the journal’s age: the red line shows the estimated relationship for <span class="math inline">\(Age=80\)</span> while the green line represents the prediction from model <span class="math inline">\((IV)\)</span> for <span class="math inline">\(Age=5\)</span>.</p>
<p>Which conclusion can be drawn?</p>
<ol style="list-style-type: decimal">
<li><p>We conclude that the demand for journals is more elastic for young journals than for old journals.</p></li>
<li><p>For model <span class="math inline">\((III)\)</span> we cannot reject the null hypothesis that the coefficients on <span class="math inline">\(\ln(PricePerCitation)^2\)</span> and <span class="math inline">\(\ln(PricePerCitation)^3\)</span> are both zero using an <span class="math inline">\(F\)</span>-test. This is evidence compatible with a linear relation between log-subscriptions and log-price.</p></li>
<li><p>Demand is greater for Journals with more characters, holding price and age constant.</p></li>
</ol>
<p>Altogether our estimates suggest that the demand is very inelastic, i.e., the libraries’ demand for economic journals is quite insensitive to the price: using model <span class="math inline">\((IV)\)</span>, even for a young journal (<span class="math inline">\(Age=5\)</span>) we estimate the price elasticity to be <span class="math inline">\(-0.899+0.374\times\ln(5)+0.141\times\left[\ln(1)\times\ln(5)\right] \approx -0.3\)</span> so a one percent increase in price is predicted to reduce the demand by only <span class="math inline">\(0.3\)</span> percent.</p>
<p>This finding comes at no surprise since providing the most recent publications is a necessity for libraries.</p>
</div>
</div>
<div id="nonlinear-effects-on-test-scores-of-the-student-teacher-ratio" class="section level2">
<h2><span class="header-section-number">8.4</span> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</h2>
<p>In this section we will discuss three specific questions about the relationship between test scores and the student-teacher ratio:</p>
<ol style="list-style-type: decimal">
<li><p>Does the effect on test scores of decreasing the student-teacher ratio depend on the fraction of English learners when we control for economic idiosyncrasies of the different districts?</p></li>
<li><p>Does this effect depend on the the student-teacher ratio?</p></li>
<li><p><em>How strong</em> is the effect of decreasing the student-teacher ratio (by two students per teacher) if we take into account economic characteristics and nonlinearities?</p></li>
</ol>
<p>Too answer these questions we consider a total of seven models, some of which are nonlinear regression specifications of the types that have been discussed before. As measures for the students’ economic backgrounds, we additionally consider the regressors <span class="math inline">\(lunch\)</span> and <span class="math inline">\(\ln(income)\)</span>. We use the logarithm of <span class="math inline">\(income\)</span> because the analysis in Chapter <a href="nrf.html#nfoasiv">8.2</a> showed that the nonlinear relationship between <span class="math inline">\(income\)</span> and <span class="math inline">\(TestScores\)</span> is approximately logarithmic. We do not include expenditure per pupil (<span class="math inline">\(expenditure\)</span>) because doing so would imply that expenditure varies with the student-teacher ratio (see Chapter 7.2 of the book for a detailed argument).</p>
<div id="nonlinear-regression-models-of-test-scores" class="section level4 unnumbered">
<h4>Nonlinear Regression Models of Test Scores</h4>
<p>The considered model specifications are:</p>
<span class="math display">\[\begin{align}
 TestScore_i =&amp; \beta_0 + \beta_1 size_i + \beta_4 english_i + \beta_9 lunch_i + u_i \\
 TestScore_i =&amp; \beta_0 + \beta_1 size_i + \beta_4 english_i + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i \\
  TestScore_i =&amp; \beta_0 + \beta_1 size_i + \beta_5 HiEL_i + \beta_6 (HiEL_i\times size_i) + u_i \\
  TestScore_i =&amp; \beta_0 + \beta_1 size_i + \beta_5 HiEL_i + \beta_6 (HiEL_i\times size_i) + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i \\
  TestScore_i =&amp; \beta_0 + \beta_1 size_i + \beta_2 size_i^2 + \beta_5 HiEL_i + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i \\
  TestScore_i =&amp; \beta_0 + \beta_1 size_i + \beta_2 size_i^2 + \beta_3 size_i^3 + \beta_5 HiEL_i + \beta_6 (HiEL\times size) \\  &amp;+ \beta_7 (HiEL_i\times size_i^2) + \beta_8 (HiEL_i\times size_i^3) + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i \\
  TestScore_i =&amp; \beta_0 + \beta_1 size_i + \beta_2 size_i^2 + \beta_3 size_i^3 + \beta_4 english + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i
\end{align}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate all models</span>
TestScore_mod1 &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>size <span class="op">+</span><span class="st"> </span>english <span class="op">+</span><span class="st"> </span>lunch, <span class="dt">data =</span> CASchools)

TestScore_mod2 &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>size <span class="op">+</span><span class="st"> </span>english <span class="op">+</span><span class="st"> </span>lunch <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(income), <span class="dt">data =</span> CASchools)

TestScore_mod3 &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>size <span class="op">+</span><span class="st"> </span>HiEL <span class="op">+</span><span class="st"> </span>HiEL<span class="op">:</span>size, <span class="dt">data =</span> CASchools)

TestScore_mod4 &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>size <span class="op">+</span><span class="st"> </span>HiEL <span class="op">+</span><span class="st"> </span>HiEL<span class="op">:</span>size <span class="op">+</span><span class="st"> </span>lunch <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(income), 
    <span class="dt">data =</span> CASchools)

TestScore_mod5 &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>size <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(size<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(size<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>HiEL <span class="op">+</span><span class="st"> </span>lunch <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(income), 
    <span class="dt">data =</span> CASchools)

TestScore_mod6 &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>size <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(size<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(size<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>HiEL <span class="op">+</span><span class="st"> </span>HiEL<span class="op">:</span>size <span class="op">+</span><span class="st"> </span>
<span class="st">    </span>HiEL<span class="op">:</span><span class="kw">I</span>(size<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>HiEL<span class="op">:</span><span class="kw">I</span>(size<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>lunch <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(income), <span class="dt">data =</span> CASchools)

TestScore_mod7 &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>size <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(size<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(size<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>english <span class="op">+</span><span class="st"> </span>lunch <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">log</span>(income), <span class="dt">data =</span> CASchools)</code></pre></div>
<p>We may use <tt>summary()</tt> to assess the models’ fit. Using <tt>stargazer()</tt> we may also obtain a tabular representation of all regression outputs and which is more convenient for comparison of the models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># gather robust standard errors in a list</span>
rob_se &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(TestScore_mod1, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(TestScore_mod2, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(TestScore_mod3, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(TestScore_mod4, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(TestScore_mod5, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(TestScore_mod6, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(TestScore_mod7, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))))

<span class="co"># generate a LaTeX table of regression outputs</span>
<span class="kw">stargazer</span>(TestScore_mod1, 
          TestScore_mod2, 
          TestScore_mod3, 
          TestScore_mod4, 
          TestScore_mod5, 
          TestScore_mod6, 
          TestScore_mod7,
          <span class="dt">digits =</span> <span class="dv">3</span>,
          <span class="dt">dep.var.caption =</span> <span class="st">&quot;Dependent Variable: Test Score&quot;</span>,
          <span class="dt">se =</span> rob_se,
          <span class="dt">column.labels =</span> <span class="kw">c</span>(<span class="st">&quot;(1)&quot;</span>, <span class="st">&quot;(2)&quot;</span>, <span class="st">&quot;(3)&quot;</span>, <span class="st">&quot;(4)&quot;</span>, <span class="st">&quot;(5)&quot;</span>, <span class="st">&quot;(6)&quot;</span>, <span class="st">&quot;(7)&quot;</span>))</code></pre></div>



<table style="text-align:center"><tr><td colspan="8" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"></td><td colspan="7">Dependent Variable: Test Score</td></tr>
<tr><td></td><td colspan="7" style="border-bottom: 1px solid black"></td></tr>
<tr><td style="text-align:left"></td><td colspan="7">score</td></tr>
<tr><td style="text-align:left"></td><td>(1)</td><td>(2)</td><td>(3)</td><td>(4)</td><td>(5)</td><td>(6)</td><td>(7)</td></tr>
<tr><td colspan="8" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">size</td><td>-0.998<sup>***</sup></td><td>-0.734<sup>***</sup></td><td>-0.968</td><td>-0.531</td><td>64.339<sup>***</sup></td><td>83.702<sup>***</sup></td><td>65.285<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.270)</td><td>(0.257)</td><td>(0.589)</td><td>(0.342)</td><td>(24.861)</td><td>(28.497)</td><td>(25.259)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">english</td><td>-0.122<sup>***</sup></td><td>-0.176<sup>***</sup></td><td></td><td></td><td></td><td></td><td>-0.166<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.033)</td><td>(0.034)</td><td></td><td></td><td></td><td></td><td>(0.034)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(size2)</td><td></td><td></td><td></td><td></td><td>-3.424<sup>***</sup></td><td>-4.381<sup>***</sup></td><td>-3.466<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td>(1.250)</td><td>(1.441)</td><td>(1.271)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(size3)</td><td></td><td></td><td></td><td></td><td>0.059<sup>***</sup></td><td>0.075<sup>***</sup></td><td>0.060<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td>(0.021)</td><td>(0.024)</td><td>(0.021)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">lunch</td><td>-0.547<sup>***</sup></td><td>-0.398<sup>***</sup></td><td></td><td>-0.411<sup>***</sup></td><td>-0.420<sup>***</sup></td><td>-0.418<sup>***</sup></td><td>-0.402<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.024)</td><td>(0.033)</td><td></td><td>(0.029)</td><td>(0.029)</td><td>(0.029)</td><td>(0.033)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">log(income)</td><td></td><td>11.569<sup>***</sup></td><td></td><td>12.124<sup>***</sup></td><td>11.748<sup>***</sup></td><td>11.800<sup>***</sup></td><td>11.509<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td>(1.819)</td><td></td><td>(1.798)</td><td>(1.771)</td><td>(1.778)</td><td>(1.806)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">HiEL</td><td></td><td></td><td>5.639</td><td>5.498</td><td>-5.474<sup>***</sup></td><td>816.076<sup>**</sup></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td>(19.515)</td><td>(9.795)</td><td>(1.034)</td><td>(327.674)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">size:HiEL</td><td></td><td></td><td>-1.277</td><td>-0.578</td><td></td><td>-123.282<sup>**</sup></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td>(0.967)</td><td>(0.496)</td><td></td><td>(50.213)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(size2):HiEL</td><td></td><td></td><td></td><td></td><td></td><td>6.121<sup>**</sup></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td>(2.542)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(size3):HiEL</td><td></td><td></td><td></td><td></td><td></td><td>-0.101<sup>**</sup></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td>(0.043)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">Constant</td><td>700.150<sup>***</sup></td><td>658.552<sup>***</sup></td><td>682.246<sup>***</sup></td><td>653.666<sup>***</sup></td><td>252.050</td><td>122.353</td><td>244.809</td></tr>
<tr><td style="text-align:left"></td><td>(5.568)</td><td>(8.642)</td><td>(11.868)</td><td>(9.869)</td><td>(163.634)</td><td>(185.519)</td><td>(165.722)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td colspan="8" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">Observations</td><td>420</td><td>420</td><td>420</td><td>420</td><td>420</td><td>420</td><td>420</td></tr>
<tr><td style="text-align:left">R<sup>2</sup></td><td>0.775</td><td>0.796</td><td>0.310</td><td>0.797</td><td>0.801</td><td>0.803</td><td>0.801</td></tr>
<tr><td style="text-align:left">Adjusted R<sup>2</sup></td><td>0.773</td><td>0.794</td><td>0.305</td><td>0.795</td><td>0.798</td><td>0.799</td><td>0.798</td></tr>
<tr><td style="text-align:left">Residual Std. Error</td><td>9.080 (df = 416)</td><td>8.643 (df = 415)</td><td>15.880 (df = 416)</td><td>8.629 (df = 414)</td><td>8.559 (df = 413)</td><td>8.547 (df = 410)</td><td>8.568 (df = 413)</td></tr>
<tr><td style="text-align:left">F Statistic</td><td>476.306<sup>***</sup> (df = 3; 416)</td><td>405.359<sup>***</sup> (df = 4; 415)</td><td>62.399<sup>***</sup> (df = 3; 416)</td><td>325.803<sup>***</sup> (df = 5; 414)</td><td>277.212<sup>***</sup> (df = 6; 413)</td><td>185.777<sup>***</sup> (df = 9; 410)</td><td>276.515<sup>***</sup> (df = 6; 413)</td></tr>
<tr><td colspan="8" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"><em>Note:</em></td><td colspan="7" style="text-align:right"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>
</table>
<caption><p style='text-align:center'><span id="tab:nmots">Table 8.2: </span> Nonlinear Models of Test Scores</p></caption>


<p>Let us summarize what can be concluded from the results presented in Table <a href="nrf.html#tab:nmots">8.2</a>.</p>
<p>First of all, the coefficient on <span class="math inline">\(size\)</span> is statistically significant in all seven models. Adding <span class="math inline">\(\ln(income)\)</span> to model (1) we find that the corresponding coefficient is statistically significant at <span class="math inline">\(1\%\)</span> while all other coefficients remain at their significance level. Furthermore, the estimate for the coefficient on <span class="math inline">\(size\)</span> is roughly <span class="math inline">\(0.27\)</span> points larger, which may be a sign of attenuated omitted variable bias. We consider this a reason to include <span class="math inline">\(\ln(income)\)</span> as a regressor in other models, too.</p>
<p>Regressions (3) and (4) aim to assess the effect of allowing for an interaction between <span class="math inline">\(size\)</span> and <span class="math inline">\(HiEL\)</span>, without and with economic control variables. In both models, both the coefficient on the interaction term and the coefficient on the dummy are not statistically significant. Thus, even with economic controls we cannot reject the null hypotheses, that the effect of the student-teacher ratio on test scores is the same for districts with high and districts with low share of English learning students.</p>
<p>Regression (5) includes a cubic term for the student-teacher ratio and omits the interaction between <span class="math inline">\(size\)</span> and <span class="math inline">\(HiEl\)</span>. The results indicate that there is a nonlinear effect of the student-teacher ratio on test scores (Can you verify this using an <span class="math inline">\(F\)</span>-test of <span class="math inline">\(H_0: \beta_2=\beta_3=0\)</span>?)</p>
<p>Consequently, regression (6) further explores whether the fraction of English learners impacts the student-teacher ratio by using <span class="math inline">\(HiEL \times size\)</span> and the interactions <span class="math inline">\(HiEL \times size^2\)</span> and <span class="math inline">\(HiEL \times size^3\)</span>. All individual <span class="math inline">\(t\)</span>-tests indicate that that there are significant effects. We check this using a robust <span class="math inline">\(F\)</span>-test of <span class="math inline">\(H_0: \beta_6=\beta_7=\beta_8=0\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># check joint significance of the interaction terms</span>
<span class="kw">linearHypothesis</span>(TestScore_mod6, 
                 <span class="kw">c</span>(<span class="st">&quot;size:HiEL=0&quot;</span>, <span class="st">&quot;I(size^2):HiEL=0&quot;</span>, <span class="st">&quot;I(size^3):HiEL=0&quot;</span>),
                 <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## size:HiEL = 0
## I(size^2):HiEL = 0
## I(size^3):HiEL = 0
## 
## Model 1: restricted model
## Model 2: score ~ size + I(size^2) + I(size^3) + HiEL + HiEL:size + HiEL:I(size^2) + 
##     HiEL:I(size^3) + lunch + log(income)
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F  Pr(&gt;F)  
## 1    413                    
## 2    410  3 2.1885 0.08882 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We find that the null can be rejected at the level of <span class="math inline">\(5\%\)</span> and conclude that the regression function differs for districts with high and low percentage of English learners.</p>
<p>Specification (7) uses a continuous measure for the share of English learners instead of a dummy variable (and thus does not include interaction terms). We observe only small changes to the coefficient estimates on the other regressors and thus conclude that the results observed for specification (5) are not sensitive to the way the percentage of English learners is measured.</p>
<p>We continue by reproducing Figure 8.10 of the book for interpretation of the nonlinear specifications (2), (5) and (7).</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># scatterplot</span>
<span class="kw">plot</span>(CASchools<span class="op">$</span>size, 
     CASchools<span class="op">$</span>score, 
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">12</span>, <span class="dv">28</span>),
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">600</span>, <span class="dv">740</span>),
     <span class="dt">pch =</span> <span class="dv">20</span>, 
     <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;Student-Teacher Ratio&quot;</span>, 
     <span class="dt">ylab =</span> <span class="st">&quot;Test Score&quot;</span>)

<span class="co"># add a legend</span>
<span class="kw">legend</span>(<span class="st">&quot;top&quot;</span>, 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Linear Regression (2)&quot;</span>, 
                  <span class="st">&quot;Cubic Regression (5)&quot;</span>, 
                  <span class="st">&quot;Cubic Regression (7)&quot;</span>),
       <span class="dt">cex =</span> <span class="fl">0.8</span>,
       <span class="dt">ncol =</span> <span class="dv">3</span>,
       <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>),
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>))

<span class="co"># data for use with predict()</span>
new_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">16</span>, <span class="dv">24</span>, <span class="fl">0.05</span>), 
                       <span class="st">&quot;english&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(CASchools<span class="op">$</span>english),
                       <span class="st">&quot;lunch&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(CASchools<span class="op">$</span>lunch),
                       <span class="st">&quot;income&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(CASchools<span class="op">$</span>income),
                       <span class="st">&quot;HiEL&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(CASchools<span class="op">$</span>HiEL))

<span class="co"># add estimated regression function for model (2)</span>
fitted &lt;-<span class="st"> </span><span class="kw">predict</span>(TestScore_mod2, <span class="dt">newdata =</span> new_data)

<span class="kw">lines</span>(new_data<span class="op">$</span>size, 
      fitted,
      <span class="dt">lwd =</span> <span class="fl">1.5</span>,
      <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>)

<span class="co"># add estimated regression function for model (5)</span>
fitted &lt;-<span class="st"> </span><span class="kw">predict</span>(TestScore_mod5, <span class="dt">newdata =</span> new_data)

<span class="kw">lines</span>(new_data<span class="op">$</span>size, 
      fitted, 
      <span class="dt">lwd =</span> <span class="fl">1.5</span>,
      <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)

<span class="co"># add estimated regression function for model (7)</span>
fitted &lt;-<span class="st"> </span><span class="kw">predict</span>(TestScore_mod7, <span class="dt">newdata =</span> new_data)

<span class="kw">lines</span>(new_data<span class="op">$</span>size, 
      fitted, 
      <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>,
      <span class="dt">lwd =</span> <span class="fl">1.5</span>,
      <span class="dt">lty =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-362-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>For the above figure all regressors except <span class="math inline">\(size\)</span> are set to their sample averages. We see that the cubic regressions (5) and (7) are almost identical. They indicate that the relation between test scores and the student-teacher ratio only has a small amount of nonlinearity since they do not deviate much from the regression function of (2).</p>
<p>The next code chunk reproduces Figure 8.11 of the book. We use <tt>plot()</tt> and <tt>points()</tt> to color observations depending on <span class="math inline">\(HiEL\)</span>. Again, the regression lines are drawn based on predictions using average sample averages of all regressors except for <span class="math inline">\(size\)</span>.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># draw scatterplot</span>

<span class="co"># observations with HiEL = 0</span>
<span class="kw">plot</span>(CASchools<span class="op">$</span>size[CASchools<span class="op">$</span>HiEL <span class="op">==</span><span class="st"> </span><span class="dv">0</span>], 
     CASchools<span class="op">$</span>score[CASchools<span class="op">$</span>HiEL <span class="op">==</span><span class="st"> </span><span class="dv">0</span>], 
     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">12</span>, <span class="dv">28</span>),
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">600</span>, <span class="dv">730</span>),
     <span class="dt">pch =</span> <span class="dv">20</span>, 
     <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;Student-Teacher Ratio&quot;</span>, 
     <span class="dt">ylab =</span> <span class="st">&quot;Test Score&quot;</span>)

<span class="co"># observations with HiEL = 1</span>
<span class="kw">points</span>(CASchools<span class="op">$</span>size[CASchools<span class="op">$</span>HiEL <span class="op">==</span><span class="st"> </span><span class="dv">1</span>], 
       CASchools<span class="op">$</span>score[CASchools<span class="op">$</span>HiEL <span class="op">==</span><span class="st"> </span><span class="dv">1</span>],
       <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>,
       <span class="dt">pch =</span> <span class="dv">20</span>)

<span class="co"># add a legend</span>
<span class="kw">legend</span>(<span class="st">&quot;top&quot;</span>, 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Regression (6) with HiEL=0&quot;</span>, <span class="st">&quot;Regression (6) with HiEL=1&quot;</span>),
       <span class="dt">cex =</span> <span class="fl">0.7</span>,
       <span class="dt">ncol =</span> <span class="dv">2</span>,
       <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>))

<span class="co"># data for use with &#39;predict()&#39;</span>
new_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">12</span>, <span class="dv">28</span>, <span class="fl">0.05</span>), 
                       <span class="st">&quot;english&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(CASchools<span class="op">$</span>english),
                       <span class="st">&quot;lunch&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(CASchools<span class="op">$</span>lunch),
                       <span class="st">&quot;income&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(CASchools<span class="op">$</span>income),
                       <span class="st">&quot;HiEL&quot;</span> =<span class="st"> </span><span class="dv">0</span>)

<span class="co"># add estimated regression function for model (6) with HiEL=0</span>
fitted &lt;-<span class="st"> </span><span class="kw">predict</span>(TestScore_mod6, <span class="dt">newdata =</span> new_data)

<span class="kw">lines</span>(new_data<span class="op">$</span>size, 
      fitted, 
      <span class="dt">lwd =</span> <span class="fl">1.5</span>,
      <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>)

<span class="co"># add estimated regression function for model (6) with HiEL=1</span>
new_data<span class="op">$</span>HiEL &lt;-<span class="st"> </span><span class="dv">1</span>

fitted &lt;-<span class="st"> </span><span class="kw">predict</span>(TestScore_mod6, <span class="dt">newdata =</span> new_data)

<span class="kw">lines</span>(new_data<span class="op">$</span>size, 
      fitted, 
      <span class="dt">lwd =</span> <span class="fl">1.5</span>,
      <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-363-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>The regression output shows that model (6) finds statistically significant coefficients on the interaction terms <span class="math inline">\(HiEL:size\)</span>, <span class="math inline">\(HiEL:size^2\)</span> and <span class="math inline">\(HiEL:size^3\)</span>, i.e., there is evidence that the nonlinear relationship connecting test scores and student-teacher ratio depends on the fraction of English learning students in the district. However, the above figure shows that this difference is not of practical importance and is a good example for why one should be careful when interpreting nonlinear models: although the two regression functions look different, we see that the slope of both functions is almost identical for student-teacher ratios between <span class="math inline">\(17\)</span> and <span class="math inline">\(23\)</span>. Since this range includes almost <span class="math inline">\(90\%\)</span> of all observations, we can be confident that nonlinear interactions between the fraction of English learners and the student-teacher ratio can be neglected.</p>
<p>One might be tempted to object since both functions show opposing slopes for student-teacher ratios below <span class="math inline">\(15\)</span> and beyond <span class="math inline">\(24\)</span>. There are at least to possible objections:</p>
<ol style="list-style-type: decimal">
<li><p>There are only few observations with low and high values of the student-teacher ratio, so there is only little information to be exploited when estimating the model. This means the estimated function is less precise in the tails of the data set.</p></li>
<li><p>The above described behavior of the regression function, is a typical caveat when using cubic functions since they generally show extreme behavior for extreme regressor values. Think of the graph of <span class="math inline">\(f(x) = x^3\)</span>.</p></li>
</ol>
<p>We thus find no clear evidence for a relation between class size and test scores on the percentage of English learners in the district.</p>
</div>
<div id="summary" class="section level4 unnumbered">
<h4>Summary</h4>
<p>We are now able to answer the three question posed at the beginning of this section.</p>
<ol style="list-style-type: decimal">
<li><p>In the linear models, the percentage of English learners has only little influence on the effect on test scores from changing the student-teacher ratio. This result stays valid if we control for economic background of the students. While the cubic specification (6) provides evidence that the effect the student-teacher ratio on test score depends on the share of English learners, the strength of this effect is negligible.</p></li>
<li><p>When controlling for the students’ economic background we find evidence of nonlinearities in the relationship between student-teacher ratio and test scores.</p></li>
<li><p>The linear specification (2) predicts that a reduction of the student-teacher ratio by two students per teacher leads to an improvement in test scores of about <span class="math inline">\(-0.73 \times (-2) = 1.46\)</span> points. Since the model is linear, this effect is independent of the class size. Assume that the student-teacher ratio is <span class="math inline">\(20\)</span>. For example, the nonlinear model (5) predicts that the reduction increases test scores by <span class="math display">\[64.33\cdot18+18^2\cdot(-3.42)+18^3\cdot(0.059) - (64.33\cdot20+20^2\cdot(-3.42)+20^3\cdot(0.059)) \approx 3.3\]</span> points. If the ratio is <span class="math inline">\(22\)</span>, a reduction to <span class="math inline">\(20\)</span> leads to a predicted improvement in test scores of <span class="math display">\[64.33\cdot20+20^2\cdot(-3.42)+20^3\cdot(0.059) - (64.33\cdot22+22^2\cdot(-3.42)+22^3\cdot(0.059)) \approx 2.4\]</span> points. This suggests that the effect is stronger in smaller classes.</p></li>
</ol>
</div>
</div>
<div id="exercises-6" class="section level2">
<h2><span class="header-section-number">8.5</span> Exercises</h2>
<div class="DCexercise">
<h4 id="correlation-and-nonlinearity-i" class="unnumbered">1. Correlation and (Non)linearity I</h4>
<p>Consider the estimated simple linear regression model <span class="math display">\[\widehat{medv_i} = 34.554 - 0.95\times lstat_i,\]</span></p>
<p>with <tt>medv</tt> (the median house value in the suburb) and <tt>lstat</tt> (the percent of households with low socioeconomic status in the suburb) being variables from the already known <tt>Boston</tt> dataset.</p>
<p>The <tt>lm()</tt> object for the above model is available as <tt>mod</tt> in your working environment. The package <tt>MASS</tt> has been loaded.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Compute the correlation coefficient between <tt>medv</tt> and <tt>lstat</tt> and save it to <tt>corr</tt>.</p></li>
<li><p>Plot <tt>medv</tt> against <tt>lstat</tt> and add the regression line using the model object <tt>mod</tt>. What do you notice?</p></li>
</ul>
<iframe src="DCL/ex8_1.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p>You can use <tt>cor()</tt> to compute the correlation between variables.</p></li>
<li><p>You may use <tt>plot()</tt> and <tt>abline()</tt> to visualize regression results.</p></li>
</ul>
</div>
<div class="DCexercise">
<h4 id="correlation-and-nonlinearity-ii" class="unnumbered">2. Correlation and (Non)linearity II</h4>
<p>In the previous exercise we saw an example where the correlation between the dependent variable <tt>medv</tt> and the regressor <tt>medv</tt> is not useful for choosing the functional form of the regression since correlation captures only the linear relationship.</p>
<p>As an alternative, consider the nonlinear specification</p>
<p><span class="math display">\[medv_i = \beta_0 + \beta_1\times\log(lstat_i) + u_i.\]</span></p>
<p>The package <tt>MASS</tt> has been loaded.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Conduct the regression from above and assign the result to <tt>log_mod</tt>.</p></li>
<li><p>Visualize your results using a scatterplot and add the regression line. In comparison to the previous exercise, what do you notice now?</p></li>
</ul>
<iframe src="DCL/ex8_2.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p>Use <tt>lm()</tt> to conduct the regression.</p></li>
<li><p>Use <tt>plot()</tt> and <tt>abline()</tt> to visualize regression results.</p></li>
</ul>
</div>
<div class="DCexercise">
<h4 id="the-optimal-polynomial-order-sequential-testing" class="unnumbered">3. The Optimal Polynomial Order — Sequential Testing</h4>
<p>Recall the following model from the previous exercise <span class="math display">\[medv_i = \beta_0 + \beta_1\times\log(lstat_i) + u_i.\]</span></p>
<p>We saw that this model specification seems to be a reasonable choice. However, a higher order polynomial in <span class="math inline">\(\log(lstat_i)\)</span> may be more suited for explaining <span class="math inline">\(medv\)</span>.</p>
<p>The packages <tt>AER</tt> and <tt>MASS</tt> have been loaded.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Determine the optimal order of a polylog model using sequential testing. Use a maximum polynomial order of <span class="math inline">\(r=4\)</span> and the significance level <span class="math inline">\(\alpha=0.05\)</span>. We would like you to use a <tt>for()</tt> loop and recommend the following approach:</p>
<ol style="list-style-type: decimal">
<li>Estimate a model, say <tt>mod</tt>, which starts with the highest polynomial order</li>
<li>Save the <span class="math inline">\(p\)</span>-value (use robust standard errors) of the relevant parameter and compare it to the significance level <span class="math inline">\(\alpha\)</span></li>
<li>If you cannot reject the null, repeat steps 1 and 2 for the next lowest polynomial order, otherwise stop the loop and print out the polynomial order</li>
</ol></li>
<li><p>Compute the <span class="math inline">\(R^2\)</span> of the selected model and assign it to <tt>R2</tt>.</p></li>
</ul>
<iframe src="DCL/ex8_3.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p>The index for the <tt>for()</tt> loop should start at 4 and end at 1.</p></li>
<li><p>Using <tt>poly()</tt> in the argument <tt>formula</tt> of <tt>lm()</tt> is a generic way to incorporate higher orders of a certain variable in the model. Besides the variable, you have to specify the degree of the polynomial via the argument <tt>degree</tt> and set <tt>raw = TRUE</tt>.</p></li>
<li><p>Use <tt>coeftest()</tt> together with the argument <tt>vcov.</tt> to obtain <span class="math inline">\(p\)</span>-values (use robust standard errors!). Use the structure of the resulting object to extract the relevant <span class="math inline">\(p\)</span>-value.</p></li>
<li><p>An <tt>if()</tt> statement may be useful to check whether the condition for acceptance of the null in step 3 is fulfilled.</p></li>
<li><p>A <tt>for()</tt> loop is stopped using <tt>break</tt>.</p></li>
<li><p>Use <tt>summary()</tt> to obtain the <span class="math inline">\(R^2\)</span>. You may extract it by appending <tt>$r.squared</tt> to the function call.</p></li>
</ul>
</div>
<div class="DCexercise">
<h4 id="the-estimated-effect-of-a-unit-change" class="unnumbered">4. The Estimated Effect of a Unit Change</h4>
<p>Reconsider the polylog model from the previous exercise that was selected by the sequential testing approach. As this model is logarithmic and of quadratic form, we cannot simply read off the estimated effect of a unit change (that is, one percent) in <tt>lstat</tt> from the coefficient summary because this effect depends on the level of <tt>lstat</tt>. We may compute this manually.</p>
<p>The selected polylog model <tt>mod_pl</tt> is available in your working environment. The package <tt>MASS</tt> has been loaded.</p>
<p><strong>Instructions:</strong></p>
<p>Assume that we are interested in the effect on <tt>medv</tt> of an increase in <tt>lstat</tt> from <span class="math inline">\(10\%\)</span> to <span class="math inline">\(11\%\)</span>.</p>
<ul>
<li><p>Set up a <tt>data.frame</tt> with the relevant observations of <tt>lstat</tt>.</p></li>
<li><p>Use the new observations to predict the corresponding values of <tt>medv</tt>.</p></li>
<li><p>Compute the expected effect with the help of <tt>diff()</tt>.</p></li>
</ul>
<iframe src="DCL/ex8_4.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p>You may use <tt>predict()</tt> together with the new data to obtain the predicted values of <tt>medv</tt>. Note that the column names of the <tt>data.frame</tt> must match the names of the regressors when using <tt>predict()</tt>.</p></li>
<li><p><tt>diff()</tt> expects a vector. It computes the differences between all entries of this vector.</p></li>
</ul>
</div>
<div class="DCexercise">
<h4 id="interactions-between-independent-variables-i" class="unnumbered">5. Interactions between Independent Variables I</h4>
<p>Consider the following regression model</p>
<p><span class="math display">\[medv_i=\beta_0+\beta_1\times chas_i+\beta_2\times old_i+\beta_3\times (chas_i\cdot old_i)+u_i\]</span></p>
<p>where <span class="math inline">\(chas_i\)</span> and <span class="math inline">\(old_i\)</span> are dummy variables. The former takes the value <span class="math inline">\(1\)</span>, if the Charles River (a short river in the proximity of Boston) passes through suburb <span class="math inline">\(i\)</span> and is <span class="math inline">\(0\)</span> otherwise. The latter indicates for a high proportion of old buildings and is constructed as</p>
<span class="math display">\[\begin{align}
old_i = &amp; \,
    \begin{cases}
      1 &amp; \text{if $age_i\geq 95$},\\
      0 &amp; \text{else},
    \end{cases}
\end{align}\]</span>
<p>with <span class="math inline">\(age_i\)</span> being the proportion of owner-occupied units built prior to 1940 in suburb <span class="math inline">\(i\)</span>.</p>
<p>The packages <tt>MASS</tt> and <tt>AER</tt> have been loaded.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Generate an append the binary variable <tt>old</tt> to the dataset <tt>Boston</tt>.</p></li>
<li><p>Conduct the regression stated above and assign the result to <tt>mod_bb</tt>.</p></li>
<li><p>Obtain a robust coefficient summary of the model. How do you interpret the results?</p></li>
</ul>
<iframe src="DCL/ex8_5.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p>The operator <tt>&gt;=</tt> can be used to generate a logical vector. Transform a logical vector to the numeric type via <tt>as.numeric()</tt>.</p></li>
<li><p>In <tt>lm()</tt> there are two ways to include interaction terms using the argument <tt>formula</tt>:</p>
<ol style="list-style-type: decimal">
<li><p><tt>Var1*Var2</tt> to add <tt>Var1</tt>, <tt>Var2</tt> and the corresponding interaction term at once</p></li>
<li><p><tt>Var1:Var2</tt> to manually add the interaction term (which of course requires you to add the remaining terms manually as well)</p></li>
</ol></li>
</ul>
</div>
<div class="DCexercise">
<h4 id="interactions-between-independent-variables-ii" class="unnumbered">6. Interactions between Independent Variables II</h4>
<p>Now consider the regression model</p>
<p><span class="math display">\[medv_i=\beta_0+\beta_1\times indus_i+\beta_2\times old_i+\beta_3\times (indus_i\cdot old_i)+u_i\]</span></p>
<p>with <span class="math inline">\(old_i\)</span> defined as in the previous exercise and <span class="math inline">\(indus_i\)</span> being the proportion of non-retail business acres in suburb <span class="math inline">\(i\)</span>.</p>
<p>The vector <tt>old</tt> from the previous exercise has been appended to the dataset. The package <tt>MASS</tt> has been loaded.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Estimate the above regression model and assign the result to <tt>mod_bc</tt>.</p></li>
<li><p>Extract the estimated coefficients of the model and assign them to <tt>params</tt>.</p></li>
<li><p>Plot <tt>medv</tt> against <tt>indus</tt> and add the regression lines for both states of the binary variable <span class="math inline">\(old\)</span>.</p></li>
</ul>
<iframe src="DCL/ex8_6.html" frameborder="0" scrolling="no" style="width:100%;height:340px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p>Make use the structure of <tt>mod_bc</tt> the output generated by <tt>coef()</tt> to extract the estimated coefficients.</p></li>
<li><p>Apart from passing an <tt>lm()</tt> object to <tt>abline()</tt> one may also specify intercept and slope manually using the arguments <tt>a</tt> and <tt>b</tt>, respectively.</p></li>
</ul>
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-AER">
<p>Kleiber, C., &amp; Zeileis, A. (2017). AER: Applied Econometrics with R (Version 1.2-5). Retrieved from <a href="https://CRAN.R-project.org/package=AER" class="uri">https://CRAN.R-project.org/package=AER</a></p>
</div>
<div id="ref-R-stargazer">
<p>Hlavac, M. (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables (Version 5.2.2). Retrieved from <a href="https://CRAN.R-project.org/package=stargazer" class="uri">https://CRAN.R-project.org/package=stargazer</a></p>
</div>
<div id="ref-stock2015">
<p>Stock, J., &amp; Watson, M. (2015). <em>Introduction to Econometrics, Third Update, Global Edition</em>. Pearson Education Limited.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>Appending <tt>HiEL * HiSTR</tt> to the formula will add <tt>HiEL</tt>, <tt>HiSTR</tt> <em>and</em> their interaction as regressors while <tt>HiEL:HiSTR</tt> only adds the interaction term.<a href="nrf.html#fnref7">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="htaciimr.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="asbomr.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": true,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 1
},
"edit": null,
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
