<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Econometrics with R</title>
  <meta name="description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Econometrics with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="github-repo" content="Emwikts1970/URFITE-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Econometrics with R" />
  
  <meta name="twitter:description" content="Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programing and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Christoph Hanck, Martin Arnold, Alexander Gerber and Martin Schmelzer">


<meta name="date" content="2018-10-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="rwpd.html">
<link rel="next" href="ivr.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<!-- font families -->

<link href="https://fonts.googleapis.com/css?family=PT+Sans|Pacifico|Source+Sans+Pro" rel="stylesheet">

<!-- function to adjust height of iframes automatically depending on content loaded -->

<script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';
  }
</script>

<script src="js/hideOutput.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/default.js"></script>

 <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSmath.js"],
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        jax: ["input/TeX","output/CommonHTML"]
      });
      MathJax.Hub.processSectionDelay = 0;
  </script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110299877-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110299877-1');
</script>

<!-- open review block -->

<script async defer src="https://hypothes.is/embed.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://www.oek.wiwi.uni-due.de/en/">Chair of Econometrics at UDE</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#a-very-short-introduction-to-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> A Very Short Introduction to <tt>R</tt> and <em>RStudio</em></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pt.html"><a href="pt.html"><i class="fa fa-check"></i><b>2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.1" data-path="pt.html"><a href="pt.html#random-variables-and-probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Random Variables and Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-discrete-random-variables"><i class="fa fa-check"></i>Probability Distributions of Discrete Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#bernoulli-trials"><i class="fa fa-check"></i>Bernoulli Trials</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#expected-value-mean-and-variance"><i class="fa fa-check"></i>Expected Value, Mean and Variance</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#probability-distributions-of-continuous-random-variables"><i class="fa fa-check"></i>Probability Distributions of Continuous Random Variables</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-normal-distribution"><i class="fa fa-check"></i>The Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-chi-squared-distribution"><i class="fa fa-check"></i>The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#thetdist"><i class="fa fa-check"></i>The Student t Distribution</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#the-f-distribution"><i class="fa fa-check"></i>The F Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pt.html"><a href="pt.html#RSATDOSA"><i class="fa fa-check"></i><b>2.2</b> Random Sampling and the Distribution of Sample Averages</a><ul>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i>Mean and Variance of the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="pt.html"><a href="pt.html#large-sample-approximations-to-sampling-distributions"><i class="fa fa-check"></i>Large Sample Approximations to Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="pt.html"><a href="pt.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arosur.html"><a href="arosur.html"><i class="fa fa-check"></i><b>3</b> A Review of Statistics using R</a><ul>
<li class="chapter" data-level="3.1" data-path="arosur.html"><a href="arosur.html#estimation-of-the-population-mean"><i class="fa fa-check"></i><b>3.1</b> Estimation of the Population Mean</a></li>
<li class="chapter" data-level="3.2" data-path="arosur.html"><a href="arosur.html#potsm"><i class="fa fa-check"></i><b>3.2</b> Properties of the Sample Mean</a></li>
<li class="chapter" data-level="3.3" data-path="arosur.html"><a href="arosur.html#hypothesis-tests-concerning-the-population-mean"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests Concerning the Population Mean</a><ul>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-p-value"><i class="fa fa-check"></i>The p-Value</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-known"><i class="fa fa-check"></i>Calculating the p-Value when the Standard Deviation is Known</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#SVSSDASE"><i class="fa fa-check"></i>Sample Variance, Sample Standard Deviation and Standard Error</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#calculating-the-p-value-when-the-standard-deviation-is-unknown"><i class="fa fa-check"></i>Calculating the p-value When the Standard Deviation is Unknown</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#the-t-statistic"><i class="fa fa-check"></i>The t-statistic</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#hypothesis-testing-with-a-prespecified-significance-level"><i class="fa fa-check"></i>Hypothesis Testing with a Prespecified Significance Level</a></li>
<li class="chapter" data-level="" data-path="arosur.html"><a href="arosur.html#one-sided-alternatives"><i class="fa fa-check"></i>One-sided Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="arosur.html"><a href="arosur.html#confidence-intervals-for-the-population-mean"><i class="fa fa-check"></i><b>3.4</b> Confidence Intervals for the Population Mean</a></li>
<li class="chapter" data-level="3.5" data-path="arosur.html"><a href="arosur.html#cmfdp"><i class="fa fa-check"></i><b>3.5</b> Comparing Means from Different Populations</a></li>
<li class="chapter" data-level="3.6" data-path="arosur.html"><a href="arosur.html#aattggoe"><i class="fa fa-check"></i><b>3.6</b> An Application to the Gender Gap of Earnings</a></li>
<li class="chapter" data-level="3.7" data-path="arosur.html"><a href="arosur.html#scatterplots-sample-covariance-and-sample-correlation"><i class="fa fa-check"></i><b>3.7</b> Scatterplots, Sample Covariance and Sample Correlation</a></li>
<li class="chapter" data-level="3.8" data-path="arosur.html"><a href="arosur.html#exercises-1"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lrwor.html"><a href="lrwor.html"><i class="fa fa-check"></i><b>4</b> Linear Regression with One Regressor</a><ul>
<li class="chapter" data-level="4.1" data-path="lrwor.html"><a href="lrwor.html#simple-linear-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="4.2" data-path="lrwor.html"><a href="lrwor.html#estimating-the-coefficients-of-the-linear-regression-model"><i class="fa fa-check"></i><b>4.2</b> Estimating the Coefficients of the Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-ordinary-least-squares-estimator"><i class="fa fa-check"></i>The Ordinary Least Squares Estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lrwor.html"><a href="lrwor.html#measures-of-fit"><i class="fa fa-check"></i><b>4.3</b> Measures of Fit</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-coefficient-of-determination"><i class="fa fa-check"></i>The Coefficient of Determination</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#the-standard-error-of-the-regression"><i class="fa fa-check"></i>The Standard Error of the Regression</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#application-to-the-test-score-data"><i class="fa fa-check"></i>Application to the Test Score Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lrwor.html"><a href="lrwor.html#tlsa"><i class="fa fa-check"></i><b>4.4</b> The Least Squares Assumptions</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-1-the-error-term-has-conditional-mean-of-zero"><i class="fa fa-check"></i>Assumption 1: The Error Term has Conditional Mean of Zero</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-2-independently-and-identically-distributed-data"><i class="fa fa-check"></i>Assumption 2: Independently and Identically Distributed Data</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#assumption-3-large-outliers-are-unlikely"><i class="fa fa-check"></i>Assumption 3: Large Outliers are Unlikely</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="lrwor.html"><a href="lrwor.html#tsdotoe"><i class="fa fa-check"></i><b>4.5</b> The Sampling Distribution of the OLS Estimator</a><ul>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-1"><i class="fa fa-check"></i>Simulation Study 1</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-2"><i class="fa fa-check"></i>Simulation Study 2</a></li>
<li class="chapter" data-level="" data-path="lrwor.html"><a href="lrwor.html#simulation-study-3"><i class="fa fa-check"></i>Simulation Study 3</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="lrwor.html"><a href="lrwor.html#exercises-2"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="htaciitslrm.html"><a href="htaciitslrm.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests and Confidence Intervals in the Simple Linear Regression Model</a><ul>
<li class="chapter" data-level="5.1" data-path="htaciitslrm.html"><a href="htaciitslrm.html#testing-two-sided-hypotheses-concerning-the-slope-coefficient"><i class="fa fa-check"></i><b>5.1</b> Testing Two-Sided Hypotheses Concerning the Slope Coefficient</a></li>
<li class="chapter" data-level="5.2" data-path="htaciitslrm.html"><a href="htaciitslrm.html#cifrc"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals for Regression Coefficients</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-confidence-intervals"><i class="fa fa-check"></i>Simulation Study: Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="htaciitslrm.html"><a href="htaciitslrm.html#rwxiabv"><i class="fa fa-check"></i><b>5.3</b> Regression when X is a Binary Variable</a></li>
<li class="chapter" data-level="5.4" data-path="htaciitslrm.html"><a href="htaciitslrm.html#hah"><i class="fa fa-check"></i><b>5.4</b> Heteroskedasticity and Homoskedasticity</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#a-real-world-example-for-heteroskedasticity"><i class="fa fa-check"></i>A Real-World Example for Heteroskedasticity</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#should-we-care-about-heteroskedasticity"><i class="fa fa-check"></i>Should We Care About Heteroskedasticity?</a></li>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#computation-of-heteroskedasticity-robust-standard-errors"><i class="fa fa-check"></i>Computation of Heteroskedasticity-Robust Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="htaciitslrm.html"><a href="htaciitslrm.html#the-gauss-markov-theorem"><i class="fa fa-check"></i><b>5.5</b> The Gauss-Markov Theorem</a><ul>
<li class="chapter" data-level="" data-path="htaciitslrm.html"><a href="htaciitslrm.html#simulation-study-blue-estimator"><i class="fa fa-check"></i>Simulation Study: BLUE Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="htaciitslrm.html"><a href="htaciitslrm.html#using-the-t-statistic-in-regression-when-the-sample-size-is-small"><i class="fa fa-check"></i><b>5.6</b> Using the t-Statistic in Regression When the Sample Size Is Small</a></li>
<li class="chapter" data-level="5.7" data-path="htaciitslrm.html"><a href="htaciitslrm.html#exercises-3"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="rmwmr.html"><a href="rmwmr.html"><i class="fa fa-check"></i><b>6</b> Regression Models with Multiple Regressors</a><ul>
<li class="chapter" data-level="6.1" data-path="rmwmr.html"><a href="rmwmr.html#omitted-variable-bias"><i class="fa fa-check"></i><b>6.1</b> Omitted Variable Bias</a></li>
<li class="chapter" data-level="6.2" data-path="rmwmr.html"><a href="rmwmr.html#tmrm"><i class="fa fa-check"></i><b>6.2</b> The Multiple Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="rmwmr.html"><a href="rmwmr.html#mofimr"><i class="fa fa-check"></i><b>6.3</b> Measures of Fit in Multiple Regression</a></li>
<li class="chapter" data-level="6.4" data-path="rmwmr.html"><a href="rmwmr.html#ols-assumptions-in-multiple-regression"><i class="fa fa-check"></i><b>6.4</b> OLS Assumptions in Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#multicollinearity"><i class="fa fa-check"></i>Multicollinearity</a></li>
<li class="chapter" data-level="" data-path="rmwmr.html"><a href="rmwmr.html#simulation-study-imperfect-multicollinearity"><i class="fa fa-check"></i>Simulation Study: Imperfect Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="rmwmr.html"><a href="rmwmr.html#the-distribution-of-the-ols-estimators-in-multiple-regression"><i class="fa fa-check"></i><b>6.5</b> The Distribution of the OLS Estimators in Multiple Regression</a></li>
<li class="chapter" data-level="6.6" data-path="rmwmr.html"><a href="rmwmr.html#exercises-4"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="htaciimr.html"><a href="htaciimr.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Tests and Confidence Intervals in Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="htaciimr.html"><a href="htaciimr.html#hypothesis-tests-and-confidence-intervals-for-a-single-coefficient"><i class="fa fa-check"></i><b>7.1</b> Hypothesis Tests and Confidence Intervals for a Single Coefficient</a></li>
<li class="chapter" data-level="7.2" data-path="htaciimr.html"><a href="htaciimr.html#an-application-to-test-scores-and-the-student-teacher-ratio"><i class="fa fa-check"></i><b>7.2</b> An Application to Test Scores and the Student-Teacher Ratio</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#another-augmentation-of-the-model"><i class="fa fa-check"></i>Another Augmentation of the Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="htaciimr.html"><a href="htaciimr.html#joint-hypothesis-testing-using-the-f-statistic"><i class="fa fa-check"></i><b>7.3</b> Joint Hypothesis Testing Using the F-Statistic</a></li>
<li class="chapter" data-level="7.4" data-path="htaciimr.html"><a href="htaciimr.html#confidence-sets-for-multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Confidence Sets for Multiple Coefficients</a></li>
<li class="chapter" data-level="7.5" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-for-multiple-regression"><i class="fa fa-check"></i><b>7.5</b> Model Specification for Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="htaciimr.html"><a href="htaciimr.html#model-specification-in-theory-and-in-practice"><i class="fa fa-check"></i>Model Specification in Theory and in Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="htaciimr.html"><a href="htaciimr.html#analysis-of-the-test-score-data-set"><i class="fa fa-check"></i><b>7.6</b> Analysis of the Test Score Data Set</a></li>
<li class="chapter" data-level="7.7" data-path="htaciimr.html"><a href="htaciimr.html#exercises-5"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nrf.html"><a href="nrf.html"><i class="fa fa-check"></i><b>8</b> Nonlinear Regression Functions</a><ul>
<li class="chapter" data-level="8.1" data-path="nrf.html"><a href="nrf.html#a-general-strategy-for-modelling-nonlinear-regression-functions"><i class="fa fa-check"></i><b>8.1</b> A General Strategy for Modelling Nonlinear Regression Functions</a></li>
<li class="chapter" data-level="8.2" data-path="nrf.html"><a href="nrf.html#nfoasiv"><i class="fa fa-check"></i><b>8.2</b> Nonlinear Functions of a Single Independent Variable</a><ul>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#polynomials"><i class="fa fa-check"></i>Polynomials</a></li>
<li class="chapter" data-level="" data-path="nrf.html"><a href="nrf.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="nrf.html"><a href="nrf.html#interactions-between-independent-variables"><i class="fa fa-check"></i><b>8.3</b> Interactions Between Independent Variables</a></li>
<li class="chapter" data-level="8.4" data-path="nrf.html"><a href="nrf.html#nonlinear-effects-on-test-scores-of-the-student-teacher-ratio"><i class="fa fa-check"></i><b>8.4</b> Nonlinear Effects on Test Scores of the Student-Teacher Ratio</a></li>
<li class="chapter" data-level="8.5" data-path="nrf.html"><a href="nrf.html#exercises-6"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="asbomr.html"><a href="asbomr.html"><i class="fa fa-check"></i><b>9</b> Assessing Studies Based on Multiple Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity"><i class="fa fa-check"></i><b>9.1</b> Internal and External Validity</a></li>
<li class="chapter" data-level="9.2" data-path="asbomr.html"><a href="asbomr.html#ttivomra"><i class="fa fa-check"></i><b>9.2</b> Threats to Internal Validity of Multiple Regression Analysis</a></li>
<li class="chapter" data-level="9.3" data-path="asbomr.html"><a href="asbomr.html#internal-and-external-validity-when-the-regression-is-used-for-forecasting"><i class="fa fa-check"></i><b>9.3</b> Internal and External Validity when the Regression is Used for Forecasting</a></li>
<li class="chapter" data-level="9.4" data-path="asbomr.html"><a href="asbomr.html#etsacs"><i class="fa fa-check"></i><b>9.4</b> Example: Test Scores and Class Size</a></li>
<li class="chapter" data-level="9.5" data-path="asbomr.html"><a href="asbomr.html#exercises-7"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="rwpd.html"><a href="rwpd.html"><i class="fa fa-check"></i><b>10</b> Regression with Panel Data</a><ul>
<li class="chapter" data-level="10.1" data-path="rwpd.html"><a href="rwpd.html#panel-data"><i class="fa fa-check"></i><b>10.1</b> Panel Data</a></li>
<li class="chapter" data-level="10.2" data-path="rwpd.html"><a href="rwpd.html#PDWTTP"><i class="fa fa-check"></i><b>10.2</b> Panel Data with Two Time Periods: “Before and After” Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="rwpd.html"><a href="rwpd.html#fixed-effects-regression"><i class="fa fa-check"></i><b>10.3</b> Fixed Effects Regression</a><ul>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#estimation-and-inference"><i class="fa fa-check"></i>Estimation and Inference</a></li>
<li class="chapter" data-level="" data-path="rwpd.html"><a href="rwpd.html#application-to-traffic-deaths"><i class="fa fa-check"></i>Application to Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="rwpd.html"><a href="rwpd.html#regression-with-time-fixed-effects"><i class="fa fa-check"></i><b>10.4</b> Regression with Time Fixed Effects</a></li>
<li class="chapter" data-level="10.5" data-path="rwpd.html"><a href="rwpd.html#tferaaseffer"><i class="fa fa-check"></i><b>10.5</b> The Fixed Effects Regression Assumptions and Standard Errors for Fixed Effects Regression</a></li>
<li class="chapter" data-level="10.6" data-path="rwpd.html"><a href="rwpd.html#drunk-driving-laws-and-traffic-deaths"><i class="fa fa-check"></i><b>10.6</b> Drunk Driving Laws and Traffic Deaths</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="rwabdv.html"><a href="rwabdv.html"><i class="fa fa-check"></i><b>11</b> Regression with a Binary Dependent Variable</a><ul>
<li class="chapter" data-level="11.1" data-path="rwabdv.html"><a href="rwabdv.html#binary-dependent-variables-and-the-linear-probability-model"><i class="fa fa-check"></i><b>11.1</b> Binary Dependent Variables and the Linear Probability Model</a></li>
<li class="chapter" data-level="11.2" data-path="rwabdv.html"><a href="rwabdv.html#palr"><i class="fa fa-check"></i><b>11.2</b> Probit and Logit Regression</a><ul>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#probit-regression"><i class="fa fa-check"></i>Probit Regression</a></li>
<li class="chapter" data-level="" data-path="rwabdv.html"><a href="rwabdv.html#logit-regression"><i class="fa fa-check"></i>Logit Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="rwabdv.html"><a href="rwabdv.html#estimation-and-inference-in-the-logit-and-probit-models"><i class="fa fa-check"></i><b>11.3</b> Estimation and Inference in the Logit and Probit Models</a></li>
<li class="chapter" data-level="11.4" data-path="rwabdv.html"><a href="rwabdv.html#application-to-the-boston-hmda-data"><i class="fa fa-check"></i><b>11.4</b> Application to the Boston HMDA Data</a></li>
<li class="chapter" data-level="11.5" data-path="rwabdv.html"><a href="rwabdv.html#exercises-8"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ivr.html"><a href="ivr.html"><i class="fa fa-check"></i><b>12</b> Instrumental Variables Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="ivr.html"><a href="ivr.html#TIVEWASRAASI"><i class="fa fa-check"></i><b>12.1</b> The IV Estimator with a Single Regressor and a Single Instrument</a></li>
<li class="chapter" data-level="12.2" data-path="ivr.html"><a href="ivr.html#TGIVRM"><i class="fa fa-check"></i><b>12.2</b> The General IV Regression Model</a></li>
<li class="chapter" data-level="12.3" data-path="ivr.html"><a href="ivr.html#civ"><i class="fa fa-check"></i><b>12.3</b> Checking Instrument Validity</a></li>
<li class="chapter" data-level="12.4" data-path="ivr.html"><a href="ivr.html#attdfc"><i class="fa fa-check"></i><b>12.4</b> Application to the Demand for Cigarettes</a></li>
<li class="chapter" data-level="12.5" data-path="ivr.html"><a href="ivr.html#where-do-valid-instruments-come-from"><i class="fa fa-check"></i><b>12.5</b> Where Do Valid Instruments Come From?</a></li>
<li class="chapter" data-level="12.6" data-path="ivr.html"><a href="ivr.html#exercises-9"><i class="fa fa-check"></i><b>12.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="eaqe.html"><a href="eaqe.html"><i class="fa fa-check"></i><b>13</b> Experiments and Quasi-Experiments</a><ul>
<li class="chapter" data-level="13.1" data-path="eaqe.html"><a href="eaqe.html#potential-outcomes-causal-effects-and-idealized-experiments"><i class="fa fa-check"></i><b>13.1</b> Potential Outcomes, Causal Effects and Idealized Experiments</a></li>
<li class="chapter" data-level="13.2" data-path="eaqe.html"><a href="eaqe.html#threats-to-validity-of-experiments"><i class="fa fa-check"></i><b>13.2</b> Threats to Validity of Experiments</a></li>
<li class="chapter" data-level="13.3" data-path="eaqe.html"><a href="eaqe.html#experimental-estimates-of-the-effect-of-class-size-reductions"><i class="fa fa-check"></i><b>13.3</b> Experimental Estimates of the Effect of Class Size Reductions</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#experimental-design-and-the-data-set"><i class="fa fa-check"></i>Experimental Design and the Data Set</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#analysis-of-the-star-data"><i class="fa fa-check"></i>Analysis of the STAR Data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="eaqe.html"><a href="eaqe.html#quasi-experiments"><i class="fa fa-check"></i><b>13.4</b> Quasi Experiments</a><ul>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#the-differences-in-differences-estimator"><i class="fa fa-check"></i>The Differences-in-Differences Estimator</a></li>
<li class="chapter" data-level="" data-path="eaqe.html"><a href="eaqe.html#regression-discontinuity-estimators"><i class="fa fa-check"></i>Regression Discontinuity Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ittsraf.html"><a href="ittsraf.html"><i class="fa fa-check"></i><b>14</b> Introduction to Time Series Regression and Forecasting</a><ul>
<li class="chapter" data-level="14.1" data-path="ittsraf.html"><a href="ittsraf.html#using-regression-models-for-forecasting"><i class="fa fa-check"></i><b>14.1</b> Using Regression Models for Forecasting</a></li>
<li class="chapter" data-level="14.2" data-path="ittsraf.html"><a href="ittsraf.html#tsdasc"><i class="fa fa-check"></i><b>14.2</b> Time Series Data and Serial Correlation</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#notation-lags-differences-logarithms-and-growth-rates"><i class="fa fa-check"></i>Notation, Lags, Differences, Logarithms and Growth Rates</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ittsraf.html"><a href="ittsraf.html#autoregressions"><i class="fa fa-check"></i><b>14.3</b> Autoregressions</a><ul>
<li><a href="ittsraf.html#autoregressive-models-of-order-p">Autoregressive Models of Order <span class="math inline">\(p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ittsraf.html"><a href="ittsraf.html#cybtmpi"><i class="fa fa-check"></i><b>14.4</b> Can You Beat the Market? (Part I)</a></li>
<li class="chapter" data-level="14.5" data-path="ittsraf.html"><a href="ittsraf.html#apatadlm"><i class="fa fa-check"></i><b>14.5</b> Additional Predictors and The ADL Model</a><ul>
<li class="chapter" data-level="" data-path="ittsraf.html"><a href="ittsraf.html#forecast-uncertainty-and-forecast-intervals"><i class="fa fa-check"></i>Forecast Uncertainty and Forecast Intervals</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="ittsraf.html"><a href="ittsraf.html#llsuic"><i class="fa fa-check"></i><b>14.6</b> Lag Length Selection Using Information Criteria</a></li>
<li class="chapter" data-level="14.7" data-path="ittsraf.html"><a href="ittsraf.html#nit"><i class="fa fa-check"></i><b>14.7</b> Nonstationarity I: Trends</a></li>
<li class="chapter" data-level="14.8" data-path="ittsraf.html"><a href="ittsraf.html#niib"><i class="fa fa-check"></i><b>14.8</b> Nonstationarity II: Breaks</a></li>
<li class="chapter" data-level="14.9" data-path="ittsraf.html"><a href="ittsraf.html#can-you-beat-the-market-part-ii"><i class="fa fa-check"></i><b>14.9</b> Can You Beat the Market? (Part II)</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="eodce.html"><a href="eodce.html"><i class="fa fa-check"></i><b>15</b> Estimation of Dynamic Causal Effects</a><ul>
<li class="chapter" data-level="15.1" data-path="eodce.html"><a href="eodce.html#the-orange-juice-data"><i class="fa fa-check"></i><b>15.1</b> The Orange Juice Data</a></li>
<li class="chapter" data-level="15.2" data-path="eodce.html"><a href="eodce.html#dynamic-causal-effects"><i class="fa fa-check"></i><b>15.2</b> Dynamic Causal Effects</a></li>
<li class="chapter" data-level="15.3" data-path="eodce.html"><a href="eodce.html#dynamic-multipliers-and-cumulative-dynamic-multipliers"><i class="fa fa-check"></i><b>15.3</b> Dynamic Multipliers and Cumulative Dynamic Multipliers</a></li>
<li class="chapter" data-level="15.4" data-path="eodce.html"><a href="eodce.html#hac-standard-errors"><i class="fa fa-check"></i><b>15.4</b> HAC Standard Errors</a></li>
<li class="chapter" data-level="15.5" data-path="eodce.html"><a href="eodce.html#estimation-of-dynamic-causal-effects-with-strictly-exogeneous-regressors"><i class="fa fa-check"></i><b>15.5</b> Estimation of Dynamic Causal Effects with Strictly Exogeneous Regressors</a></li>
<li class="chapter" data-level="15.6" data-path="eodce.html"><a href="eodce.html#orange-juice-prices-and-cold-weather"><i class="fa fa-check"></i><b>15.6</b> Orange Juice Prices and Cold Weather</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="atitsr.html"><a href="atitsr.html"><i class="fa fa-check"></i><b>16</b> Additional Topics in Time Series Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="atitsr.html"><a href="atitsr.html#vector-autoregressions"><i class="fa fa-check"></i><b>16.1</b> Vector Autoregressions</a></li>
<li class="chapter" data-level="16.2" data-path="atitsr.html"><a href="atitsr.html#ooiatdfglsurt"><i class="fa fa-check"></i><b>16.2</b> Orders of Integration and the DF-GLS Unit Root Test</a></li>
<li class="chapter" data-level="16.3" data-path="atitsr.html"><a href="atitsr.html#cointegration"><i class="fa fa-check"></i><b>16.3</b> Cointegration</a></li>
<li class="chapter" data-level="16.4" data-path="atitsr.html"><a href="atitsr.html#volatility-clustering-and-autoregressive-conditional-heteroskedasticity"><i class="fa fa-check"></i><b>16.4</b> Volatility Clustering and Autoregressive Conditional Heteroskedasticity</a><ul>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#arch-and-garch-models"><i class="fa fa-check"></i>ARCH and GARCH Models</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#application-to-stock-price-volatility"><i class="fa fa-check"></i>Application to Stock Price Volatility</a></li>
<li class="chapter" data-level="" data-path="atitsr.html"><a href="atitsr.html#summary-8"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Econometrics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div class = rmdreview>
This book is in <b>Open Review</b>. We want your feedback to make the book better for you and other students. You may annotate some text by <span style="background-color: #3297FD; color: white">selecting it with the cursor</span> and then click the <i class="h-icon-annotate"></i> on the pop-up menu. You can also see the annotations of others: click the <i class="h-icon-chevron-left"></i> in the upper right hand corner of the page <i class="fa fa-arrow-circle-right  fa-rotate-315" aria-hidden="true"></i>
</div>
<div id="rwabdv" class="section level1">
<h1><span class="header-section-number">11</span> Regression with a Binary Dependent Variable</h1>
<p>This chapter, we discusses a special class of regression models that aim to explain a limited dependent variable. In particular, we consider models where the dependent variable is binary. We will see that in such models, the regression function can be interpreted as a conditional probability function of the binary dependent variable.</p>
<p>We review the following concepts:</p>
<ul>
<li>the linear probability model</li>
<li>the Probit model</li>
<li>the Logit model</li>
<li>maximum likelihood estimation of nonlinear regression models</li>
</ul>
<p>Of course, we will also see how to estimate above models using <tt>R</tt> and discuss an application where we examine the question whether there is racial discrimination in the U.S. mortgage market.</p>
<p>The following packages and their dependencies are needed for reproduction of the code chunks presented throughout this chapter on your computer:</p>
<ul>
<li><tt>AER</tt> <span class="citation">(Christian Kleiber &amp; Zeileis, <a href="#ref-R-AER">2017</a>)</span></li>
<li><tt>stargazer</tt> <span class="citation">(Hlavac, <a href="#ref-R-stargazer">2018</a>)</span></li>
</ul>
<p>Check whether the following code chunk runs without any errors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(AER)
<span class="kw">library</span>(stargazer)</code></pre></div>
<div id="binary-dependent-variables-and-the-linear-probability-model" class="section level2">
<h2><span class="header-section-number">11.1</span> Binary Dependent Variables and the Linear Probability Model</h2>
<div class="keyconcept">
<h3 class="right">
Key Concept 11.1
</h3>
<h3 class="left">
The Linear Probability Model
</h3>
<p>The linear regression model</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 + X_{1i} + \beta_2 X_{2i} + \dots + \beta_k X_{ki} + u_i\]</span> with a binary dependent variable <span class="math inline">\(Y_i\)</span> is called the linear probability model. In the linear probability model we have <span class="math display">\[E(Y\vert X_1,X_2,\dots,X_k) = P(Y=1\vert X_1, X_2,\dots, X_3)\]</span> where <span class="math display">\[ P(Y = 1 \vert X_1, X_2, \dots, X_k) = \beta_0 + \beta_1 + X_{1i} + \beta_2 X_{2i} + \dots + \beta_k X_{ki}.\]</span></p>
<p>Thus, <span class="math inline">\(\beta_j\)</span> can be interpreted as the change in the probability that <span class="math inline">\(Y_i=1\)</span>, holding constant the other <span class="math inline">\(k-1\)</span> regressors. Just as in common multiple regression, the <span class="math inline">\(\beta_j\)</span> can be estimated using OLS and the robust standard error formulas can be used for hypothesis testing and computation of confidence intervals.</p>
<p>In most linear probability models, <span class="math inline">\(R^2\)</span> has no meaningful interpretation since the regression line can never fit the data perfectly if the dependent variable is binary and the regressors are continuous. This can be seen in the application below.</p>
<p>It is <em>essential</em> to use robust standard errors since the <span class="math inline">\(u_i\)</span> in a linear probability model are always heteroskedastic.</p>
<p>Linear probability models are easily estimated in <tt>R</tt> using the function <tt>lm()</tt>.</p>
</div>
<div id="mortgage-data" class="section level4 unnumbered">
<h4>Mortgage Data</h4>
<p>Following the book, we start by loading the data set <tt>HMDA</tt> which provides data that relate to mortgage applications filed in Boston in the year of 1990.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load `AER` package and attach the HMDA data</span>
<span class="kw">library</span>(AER)
<span class="kw">data</span>(HMDA)</code></pre></div>
<p>We continue by inspecting the first few observations and compute summary statistics afterwards.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># inspect the data</span>
<span class="kw">head</span>(HMDA)</code></pre></div>
<pre><code>##   deny pirat hirat     lvrat chist mhist phist unemp selfemp insurance
## 1   no 0.221 0.221 0.8000000     5     2    no   3.9      no        no
## 2   no 0.265 0.265 0.9218750     2     2    no   3.2      no        no
## 3   no 0.372 0.248 0.9203980     1     2    no   3.2      no        no
## 4   no 0.320 0.250 0.8604651     1     2    no   4.3      no        no
## 5   no 0.360 0.350 0.6000000     1     1    no   3.2      no        no
## 6   no 0.240 0.170 0.5105263     1     1    no   3.9      no        no
##   condomin afam single hschool
## 1       no   no     no     yes
## 2       no   no    yes     yes
## 3       no   no     no     yes
## 4       no   no     no     yes
## 5       no   no     no     yes
## 6       no   no     no     yes</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(HMDA)</code></pre></div>
<pre><code>##   deny          pirat            hirat            lvrat        chist   
##  no :2095   Min.   :0.0000   Min.   :0.0000   Min.   :0.0200   1:1353  
##  yes: 285   1st Qu.:0.2800   1st Qu.:0.2140   1st Qu.:0.6527   2: 441  
##             Median :0.3300   Median :0.2600   Median :0.7795   3: 126  
##             Mean   :0.3308   Mean   :0.2553   Mean   :0.7378   4:  77  
##             3rd Qu.:0.3700   3rd Qu.:0.2988   3rd Qu.:0.8685   5: 182  
##             Max.   :3.0000   Max.   :3.0000   Max.   :1.9500   6: 201  
##  mhist    phist          unemp        selfemp    insurance  condomin  
##  1: 747   no :2205   Min.   : 1.800   no :2103   no :2332   no :1694  
##  2:1571   yes: 175   1st Qu.: 3.100   yes: 277   yes:  48   yes: 686  
##  3:  41              Median : 3.200                                   
##  4:  21              Mean   : 3.774                                   
##                      3rd Qu.: 3.900                                   
##                      Max.   :10.600                                   
##   afam      single     hschool   
##  no :2041   no :1444   no :  39  
##  yes: 339   yes: 936   yes:2341  
##                                  
##                                  
##                                  
## </code></pre>
<p>The variable we are interested in modelling is <tt>deny</tt>, an indicator for whether an applicant’s mortgage application has been accepted (<tt>deny = no</tt>) or denied (<tt>deny = yes</tt>). A regressor that ought to have power in explaining whether a mortgage application has been denied is <tt>pirat</tt>, the size of the anticipated total monthly loan payments relative to the the applicant’s income. It is straightforward to translate this into the simple regression model</p>
<span class="math display" id="eq:denymod1">\[\begin{align}
  deny = \beta_0 + \beta_1 \times P/I\ ratio + u. \tag{11.1}
\end{align}\]</span>
<p>We estimate this model just as any other linear regression model using <tt>lm()</tt>. Before we do so, the variable <tt>deny</tt> must be converted to a numeric variable using <tt>as.numeric()</tt> as <tt>lm()</tt> does not accepts the <em>dependent variable</em> to be of class <tt>factor</tt>. Note that <code>as.numeric(HMDA$deny)</code> will turn <tt>deny = no</tt> into <tt>deny = 1</tt> and <tt>deny = yes</tt> into <tt>deny = 2</tt>, so using <tt>as.numeric(HMDA$deny)-1</tt> we obtain the values <tt>0</tt> and <tt>1</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># convert &#39;deny&#39; to numeric</span>
HMDA<span class="op">$</span>deny &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(HMDA<span class="op">$</span>deny) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>

<span class="co"># estimate a simple linear probabilty model</span>
denymod1 &lt;-<span class="st"> </span><span class="kw">lm</span>(deny <span class="op">~</span><span class="st"> </span>pirat, <span class="dt">data =</span> HMDA)
denymod1</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = deny ~ pirat, data = HMDA)
## 
## Coefficients:
## (Intercept)        pirat  
##    -0.07991      0.60353</code></pre>
<p>Next, we plot the data and the regression line to reproduce Figure 11.1 of the book.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the data</span>
<span class="kw">plot</span>(<span class="dt">x =</span> HMDA<span class="op">$</span>pirat, 
     <span class="dt">y =</span> HMDA<span class="op">$</span>deny,
     <span class="dt">main =</span> <span class="st">&quot;Scatterplot Mortgage Application Denial and the Payment-to-Income Ratio&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;P/I ratio&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Deny&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.4</span>, <span class="fl">1.4</span>),
     <span class="dt">cex.main =</span> <span class="fl">0.8</span>)

<span class="co"># add horizontal dashed lines and text</span>
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">1</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)
<span class="kw">text</span>(<span class="fl">2.5</span>, <span class="fl">0.9</span>, <span class="dt">cex =</span> <span class="fl">0.8</span>, <span class="st">&quot;Mortgage denied&quot;</span>)
<span class="kw">text</span>(<span class="fl">2.5</span>, <span class="op">-</span><span class="fl">0.1</span>, <span class="dt">cex=</span> <span class="fl">0.8</span>, <span class="st">&quot;Mortgage approved&quot;</span>)

<span class="co"># add the estimated regression line</span>
<span class="kw">abline</span>(denymod1, 
       <span class="dt">lwd =</span> <span class="fl">1.8</span>, 
       <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-449-1.png" width="672" /></p>
<p>According to the estimated model, a payment-to-income ratio of <span class="math inline">\(1\)</span> is associated with an expected probability of mortgage application denial of roughly <span class="math inline">\(50\%\)</span>. The model indicates that there is a positive relation between the payment-to-income ratio and the probability of a denied mortgage application so individuals with a high ratio of loan payments to income are more likely to be rejected.</p>
<p>We may use <tt>coeftest()</tt> to obtain robust standard errors for both coefficient estimates.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># print robust coefficient summary</span>
<span class="kw">coeftest</span>(denymod1, <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) -0.079910   0.031967 -2.4998   0.01249 *  
## pirat        0.603535   0.098483  6.1283 1.036e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
The estimated regression line is
<span class="math display" id="eq:lpm">\[\begin{align}
\widehat{deny} = -\underset{(0.032)}{0.080} + \underset{(0.098)}{0.604} P/I \ ratio. \tag{11.2}
\end{align}\]</span>
<p>The true coefficient on <span class="math inline">\(P/I \ ratio\)</span> is statistically different from <span class="math inline">\(0\)</span> at the <span class="math inline">\(1\%\)</span> level. Its estimate can be interpreted as follows: a 1 percentage point increase in <span class="math inline">\(P/I \ ratio\)</span> leads to an increase in the probability of a loan denial by <span class="math inline">\(0.604 \cdot 0.01 = 0.00604 \approx 0.6\%\)</span>.</p>
<p>Following the book we augment the simple model <a href="rwabdv.html#eq:denymod1">(11.1)</a> by an additional regressor <span class="math inline">\(black\)</span> which equals <span class="math inline">\(1\)</span> if the applicant is an African American and equals <span class="math inline">\(0\)</span> otherwise. Such a specification is the baseline for investigating if there is racial discrimination in the mortgage market: if being black has a significant (positive) influence on the probability of a loan denial when we control for factors that allow for an objective assessment of an applicants credit worthiness, this is an indicator for discrimination.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># rename the variable &#39;afam&#39; for consistency</span>
<span class="kw">colnames</span>(HMDA)[<span class="kw">colnames</span>(HMDA) <span class="op">==</span><span class="st"> &quot;afam&quot;</span>] &lt;-<span class="st"> &quot;black&quot;</span>

<span class="co"># estimate the model</span>
denymod2 &lt;-<span class="st"> </span><span class="kw">lm</span>(deny <span class="op">~</span><span class="st"> </span>pirat <span class="op">+</span><span class="st"> </span>black, <span class="dt">data =</span> HMDA)
<span class="kw">coeftest</span>(denymod2, <span class="dt">vcov. =</span> vcovHC)</code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) -0.090514   0.033430 -2.7076  0.006826 ** 
## pirat        0.559195   0.103671  5.3939 7.575e-08 ***
## blackyes     0.177428   0.025055  7.0815 1.871e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
The estimated regression function is
<span class="math display" id="eq:denymod2">\[\begin{align}
  \widehat{deny} =&amp; \, -\underset{(0.029)}{0.091} + \underset{(0.089)}{0.559} P/I \ ratio + \underset{(0.025)}{0.177} black. \tag{11.3}
\end{align}\]</span>
<p>The coefficient on <span class="math inline">\(black\)</span> is positive and significantly different from zero at the <span class="math inline">\(0.01\%\)</span> level. The interpretation is that, holding constant the <span class="math inline">\(P/I \ ratio\)</span>, being black increases the probability of a mortgage application denial by about <span class="math inline">\(17.7\%\)</span>. This finding is compatible with racial discrimination. However, it might be distorted by omitted variable bias so discrimination could be a premature conclusion.</p>
</div>
</div>
<div id="palr" class="section level2">
<h2><span class="header-section-number">11.2</span> Probit and Logit Regression</h2>
<p>The linear probability model has a major flaw: it assumes the conditional probability function to be linear. This does not restrict <span class="math inline">\(P(Y=1\vert X_1,\dots,X_k)\)</span> to lie between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. We can easily see this in our reproduction of Figure 11.1 of the book: for <span class="math inline">\(P/I \ ratio \geq 1.75\)</span>, <a href="rwabdv.html#eq:lpm">(11.2)</a> predicts the probability of a mortgage application denial to be bigger than <span class="math inline">\(1\)</span>. For applications with <span class="math inline">\(P/I \ ratio\)</span> close to <span class="math inline">\(0\)</span>, the predicted probability of denial is even negative so that the model has no meaningful interpretation here.</p>
<p>This circumstance calls for an approach that uses a nonlinear function to model the conditional probability function of a binary dependent variable. Commonly used methods are Probit and Logit regression.</p>
<div id="probit-regression" class="section level3 unnumbered">
<h3>Probit Regression</h3>
In Probit regression, the cumulative standard normal distribution function <span class="math inline">\(\Phi(\cdot)\)</span> is used to model the regression function when the dependent variable is binary, that is, we assume
<span class="math display" id="eq:probitmodel">\[\begin{align}
  E(Y\vert X) = P(Y=1\vert X) = \Phi(\beta_0 + \beta_1 X). \tag{11.4}
\end{align}\]</span>
<p><span class="math inline">\(\beta_0 + \beta_1 X\)</span> in <a href="rwabdv.html#eq:probitmodel">(11.4)</a> plays the role of a quantile <span class="math inline">\(z\)</span>. Remember that <span class="math display">\[\Phi(z) = P(Z \leq z) \ , \ Z \sim \mathcal{N}(0,1)\]</span> such that the Probit coefficient <span class="math inline">\(\beta_1\)</span> in <a href="rwabdv.html#eq:probitmodel">(11.4)</a> is the change in <span class="math inline">\(z\)</span> associated with a one unit change in <span class="math inline">\(X\)</span>. Although the effect on <span class="math inline">\(z\)</span> of a change in <span class="math inline">\(X\)</span> is linear, the link between <span class="math inline">\(z\)</span> and the dependent variable <span class="math inline">\(Y\)</span> is nonlinear since <span class="math inline">\(\Phi\)</span> is a nonlinear function of <span class="math inline">\(X\)</span>.</p>
<p>Since the dependent variable is a nonlinear function of the regressors, the coefficient on <span class="math inline">\(X\)</span> has no simple interpretation. According to Key Concept 8.1, the expected change in the probability that <span class="math inline">\(Y=1\)</span> due to a change in <span class="math inline">\(P/I \ ratio\)</span> can be computed as follows:</p>
<ol style="list-style-type: decimal">
<li>Compute the predicted probability that <span class="math inline">\(Y=1\)</span> for the original value of <span class="math inline">\(X\)</span>.</li>
<li>Compute the predicted probability that <span class="math inline">\(Y=1\)</span> for <span class="math inline">\(X + \Delta X\)</span>.</li>
<li>Compute the difference between both predicted probabilities.</li>
</ol>
<p>Of course we can generalize <a href="rwabdv.html#eq:probitmodel">(11.4)</a> to Probit regression with multiple regressors to mitigate the risk of facing omitted variable bias. Probit regression essentials are summarized in Key Concept 11.2.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 11.2
</h3>
<h3 class="left">
Probit Model, Predicted Probabilities and Estimated Effects
</h3>
<p>Assume that <span class="math inline">\(Y\)</span> is a binary variable. The model</p>
<p><span class="math display">\[ Y= \beta_0 + \beta_1 + X_1 + \beta_2 X_2 + \dots + \beta_k X_k + u \]</span> with <span class="math display">\[P(Y = 1 \vert X_1, X_2, \dots ,X_k) = \Phi(\beta_0 + \beta_1 + X_1 + \beta_2 X_2 + \dots + \beta_k X_k)\]</span> is the population Probit model with multiple regressors <span class="math inline">\(X_1, X_2, \dots, X_k\)</span> and <span class="math inline">\(\Phi(\cdot)\)</span> is the cumulative standard normal distribution function.</p>
<p>The predicted probability that <span class="math inline">\(Y=1\)</span> given <span class="math inline">\(X_1, X_2, \dots, X_k\)</span> can be calculated in two steps:</p>
<ol style="list-style-type: decimal">
<li><p>Compute <span class="math inline">\(z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_k X_k\)</span></p></li>
<li><p>Look up <span class="math inline">\(\Phi(z)\)</span> by calling <tt>pnorm()</tt>.</p></li>
</ol>
<p><span class="math inline">\(\beta_j\)</span> is the effect on <span class="math inline">\(z\)</span> of a one unit change in regressor <span class="math inline">\(X_j\)</span>, holding constant all other <span class="math inline">\(k-1\)</span> regressors.</p>
<p>The effect on the predicted probability of a change in a regressor can be computed as in Key Concept 8.1.</p>
<p>In <tt>R</tt>, Probit models can be estimated using the function <tt>glm()</tt> from the package <tt>stats</tt>. Using the argument <tt>family</tt> we specify that we want to use a Probit link function.</p>
</div>
<p>We now estimate a simple Probit model of the probability of a mortgage denial.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the simple probit model</span>
denyprobit &lt;-<span class="st"> </span><span class="kw">glm</span>(deny <span class="op">~</span><span class="st"> </span>pirat, 
                  <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;probit&quot;</span>), 
                  <span class="dt">data =</span> HMDA)

<span class="kw">coeftest</span>(denyprobit, <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## z test of coefficients:
## 
##             Estimate Std. Error  z value  Pr(&gt;|z|)    
## (Intercept) -2.19415    0.18901 -11.6087 &lt; 2.2e-16 ***
## pirat        2.96787    0.53698   5.5269 3.259e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The estimated model is</p>
<span class="math display" id="eq:denyprobit">\[\begin{align}
  \widehat{P(deny\vert P/I \ ratio}) = \Phi(-\underset{(0.19)}{2.19} + \underset{(0.54)}{2.97} P/I \ ratio). \tag{11.5}
\end{align}\]</span>
<p>Just as in the linear probability model we find that the relation between the probability of denial and the payments-to-income ratio is positive and that the corresponding coefficient is highly significant.</p>
<p>The following code chunk reproduces Figure 11.2 of the book.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot data</span>
<span class="kw">plot</span>(<span class="dt">x =</span> HMDA<span class="op">$</span>pirat, 
     <span class="dt">y =</span> HMDA<span class="op">$</span>deny,
     <span class="dt">main =</span> <span class="st">&quot;Probit Model of the Probability of Denial, Given P/I Ratio&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;P/I ratio&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Deny&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.4</span>, <span class="fl">1.4</span>),
     <span class="dt">cex.main =</span> <span class="fl">0.85</span>)

<span class="co"># add horizontal dashed lines and text</span>
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">1</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)
<span class="kw">text</span>(<span class="fl">2.5</span>, <span class="fl">0.9</span>, <span class="dt">cex =</span> <span class="fl">0.8</span>, <span class="st">&quot;Mortgage denied&quot;</span>)
<span class="kw">text</span>(<span class="fl">2.5</span>, <span class="op">-</span><span class="fl">0.1</span>, <span class="dt">cex=</span> <span class="fl">0.8</span>, <span class="st">&quot;Mortgage approved&quot;</span>)

<span class="co"># add estimated regression line</span>
x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="fl">0.01</span>)
y &lt;-<span class="st"> </span><span class="kw">predict</span>(denyprobit, <span class="kw">list</span>(<span class="dt">pirat =</span> x), <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)

<span class="kw">lines</span>(x, y, <span class="dt">lwd =</span> <span class="fl">1.5</span>, <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>)</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-455-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>The estimated regression function has a stretched “S”-shape which is typical for the CDF of a continuous random variable with symmetric PDF like that of a normal random variable. The function is clearly nonlinear and flattens out for large and small values of <span class="math inline">\(P/I \ ratio\)</span>. The functional form thus also ensures that the predicted conditional probabilities of a denial lie between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p>
<p>We use <tt>predict()</tt> to compute the predicted change in the denial probability when <span class="math inline">\(P/I \ ratio\)</span> is increased from <span class="math inline">\(0.3\)</span> to <span class="math inline">\(0.4\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 1. compute predictions for P/I ratio = 0.3, 0.4</span>
predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(denyprobit, 
                       <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="st">&quot;pirat&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.4</span>)),
                       <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)

<span class="co"># 2. Compute difference in probabilities</span>
<span class="kw">diff</span>(predictions)</code></pre></div>
<pre><code>##          2 
## 0.06081433</code></pre>
<p>We find that an increase in the payment-to-income ratio from <span class="math inline">\(0.3\)</span> to <span class="math inline">\(0.4\)</span> is predicted to increase the probability of denial by approximately <span class="math inline">\(6.2\%\)</span>.</p>
<p>We continue by using an augmented Probit model to estimate the effect of race on the probability of a mortgage application denial.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">denyprobit2 &lt;-<span class="st"> </span><span class="kw">glm</span>(deny <span class="op">~</span><span class="st"> </span>pirat <span class="op">+</span><span class="st"> </span>black, 
                   <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;probit&quot;</span>), 
                   <span class="dt">data =</span> HMDA)

<span class="kw">coeftest</span>(denyprobit2, <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## z test of coefficients:
## 
##              Estimate Std. Error  z value  Pr(&gt;|z|)    
## (Intercept) -2.258787   0.176608 -12.7898 &lt; 2.2e-16 ***
## pirat        2.741779   0.497673   5.5092 3.605e-08 ***
## blackyes     0.708155   0.083091   8.5227 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The estimated model equation is</p>
<span class="math display" id="eq:denyprobit2">\[\begin{align}
  \widehat{P(deny\vert P/I \ ratio, black)} = \Phi (-\underset{(0.18)}{2.26} + \underset{(0.50)}{2.74} P/I \ ratio + \underset{(0.08)}{0.71} black). \tag{11.6} 
\end{align}\]</span>
<p>While all coefficients are highly significant, both the estimated coefficients on the payments-to-income ratio and the indicator for African American descent are positive. Again, the coefficients are difficult to interpret but they indicate that, first, African Americans have a higher probability of denial than white applicants, holding constant the payments-to-income ratio and second, applicants with a high payments-to-income ratio face a higher risk of being rejected.</p>
<p>How big is the estimated difference in denial probabilities between two hypothetical applicants with the same payments-to-income ratio? As before, we may use <tt>predict()</tt> to compute this difference.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 1. compute predictions for P/I ratio = 0.3</span>
predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(denyprobit2, 
                       <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="st">&quot;black&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>), 
                                            <span class="st">&quot;pirat&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>)),
                       <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)

<span class="co"># 2. compute difference in probabilities</span>
<span class="kw">diff</span>(predictions)</code></pre></div>
<pre><code>##         2 
## 0.1578117</code></pre>
<p>In this case, the estimated difference in denial probabilities is about <span class="math inline">\(15.8\%\)</span>.</p>
</div>
<div id="logit-regression" class="section level3 unnumbered">
<h3>Logit Regression</h3>
<p>Key Concept 11.3 summarizes the Logit regression function.</p>
<div class="keyconcept">
<h3 class="right">
Key Concept 11.3
</h3>
<h3 class="left">
Logit Regression
</h3>
<p>The population Logit regression function is</p>
<span class="math display">\[\begin{align*}
  P(Y=1\vert X_1, X_2, \dots, X_k) =&amp; \, F(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_k X_k) \\
  =&amp; \, \frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_k X_k)}}.
\end{align*}\]</span>
<p>The idea is similar to Probit regression except that a different CDF is used: <span class="math display">\[F(x) = \frac{1}{1+e^{-x}}\]</span> is the CDF of a standard logistically distributed random variable.</p>
</div>
<p>As for Probit regression, there is no simple interpretation of the model coefficients and it is best to consider predicted probabilities or differences in predicted probabilities. Here again, <span class="math inline">\(t\)</span>-statistics and confidence intervals based on large sample normal approximations can be computed as usual.</p>
<p>It is fairly easy to estimate a Logit regression model using <tt>R</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">denylogit &lt;-<span class="st"> </span><span class="kw">glm</span>(deny <span class="op">~</span><span class="st"> </span>pirat, 
                 <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>), 
                 <span class="dt">data =</span> HMDA)

<span class="kw">coeftest</span>(denylogit, <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## z test of coefficients:
## 
##             Estimate Std. Error  z value  Pr(&gt;|z|)    
## (Intercept) -4.02843    0.35898 -11.2218 &lt; 2.2e-16 ***
## pirat        5.88450    1.00015   5.8836 4.014e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The subsequent code chunk reproduces Figure 11.3 of the book.</p>
<div class="unfolded">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot data</span>
<span class="kw">plot</span>(<span class="dt">x =</span> HMDA<span class="op">$</span>pirat, 
     <span class="dt">y =</span> HMDA<span class="op">$</span>deny,
     <span class="dt">main =</span> <span class="st">&quot;Probit and Logit Models Model of the Probability of Denial, Given P/I Ratio&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;P/I ratio&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Deny&quot;</span>,
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.4</span>, <span class="fl">1.4</span>),
     <span class="dt">cex.main =</span> <span class="fl">0.9</span>)

<span class="co"># add horizontal dashed lines and text</span>
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">1</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkred&quot;</span>)
<span class="kw">text</span>(<span class="fl">2.5</span>, <span class="fl">0.9</span>, <span class="dt">cex =</span> <span class="fl">0.8</span>, <span class="st">&quot;Mortgage denied&quot;</span>)
<span class="kw">text</span>(<span class="fl">2.5</span>, <span class="op">-</span><span class="fl">0.1</span>, <span class="dt">cex=</span> <span class="fl">0.8</span>, <span class="st">&quot;Mortgage approved&quot;</span>)

<span class="co"># add estimated regression line of Probit and Logit models</span>
x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="fl">0.01</span>)
y_probit &lt;-<span class="st"> </span><span class="kw">predict</span>(denyprobit, <span class="kw">list</span>(<span class="dt">pirat =</span> x), <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
y_logit &lt;-<span class="st"> </span><span class="kw">predict</span>(denylogit, <span class="kw">list</span>(<span class="dt">pirat =</span> x), <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)

<span class="kw">lines</span>(x, y_probit, <span class="dt">lwd =</span> <span class="fl">1.5</span>, <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>)
<span class="kw">lines</span>(x, y_logit, <span class="dt">lwd =</span> <span class="fl">1.5</span>, <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)

<span class="co"># add a legend</span>
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,
       <span class="dt">horiz =</span> <span class="ot">TRUE</span>,
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Probit&quot;</span>, <span class="st">&quot;Logit&quot;</span>),
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;steelblue&quot;</span>, <span class="st">&quot;black&quot;</span>), 
       <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</code></pre></div>
<p><img src="ITER_files/figure-html/unnamed-chunk-462-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>Both models produce very similar estimates of the probability that a mortgage application will be denied depending on the applicants payment-to-income ratio.</p>
<p>Following the book we extend the simple Logit model of mortgage denial with the additional regressor <span class="math inline">\(black\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate a Logit regression with multiple regressors</span>
denylogit2 &lt;-<span class="st"> </span><span class="kw">glm</span>(deny <span class="op">~</span><span class="st"> </span>pirat <span class="op">+</span><span class="st"> </span>black, 
                  <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>), 
                  <span class="dt">data =</span> HMDA)

<span class="kw">coeftest</span>(denylogit2, <span class="dt">vcov. =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## 
## z test of coefficients:
## 
##             Estimate Std. Error  z value  Pr(&gt;|z|)    
## (Intercept) -4.12556    0.34597 -11.9245 &lt; 2.2e-16 ***
## pirat        5.37036    0.96376   5.5723 2.514e-08 ***
## blackyes     1.27278    0.14616   8.7081 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We obtain</p>
<span class="math display" id="eq:denylogit2">\[\begin{align}
  \widehat{P(deny=1 \vert P/I ratio, black)} = F(-\underset{(0.35)}{4.13} + \underset{(0.96)}{5.37} P/I \ ratio + \underset{(0.15)}{1.27} black). \tag{11.7}
\end{align}\]</span>
<p>As for the Probit model <a href="rwabdv.html#eq:denyprobit2">(11.6)</a> all model coefficients are highly significant and we obtain positive estimates for the coefficients on <span class="math inline">\(P/I \ ratio\)</span> and <span class="math inline">\(black\)</span>. For comparison we compute the predicted probability of denial for two hypothetical applicants that differ in race and have a <span class="math inline">\(P/I \ ratio\)</span> of <span class="math inline">\(0.3\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 1. compute predictions for P/I ratio = 0.3</span>
predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(denylogit2, 
                       <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="st">&quot;black&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>), 
                                            <span class="st">&quot;pirat&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>)),
                       <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)

predictions</code></pre></div>
<pre><code>##          1          2 
## 0.07485143 0.22414592</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 2. Compute difference in probabilities</span>
<span class="kw">diff</span>(predictions)</code></pre></div>
<pre><code>##         2 
## 0.1492945</code></pre>
<p>We find that the white applicant faces a denial probability of only <span class="math inline">\(7.5\%\)</span>, while the African American is rejected with a probability of <span class="math inline">\(22.4\%\)</span>, a difference of <span class="math inline">\(14.9\%\)</span>.</p>
<div id="comparison-of-the-models" class="section level4 unnumbered">
<h4>Comparison of the Models</h4>
<p>The Probit model and the Logit model deliver only approximations to the unknown population regression function <span class="math inline">\(E(Y\vert X)\)</span>. It is not obvious how to decide which model to use in practice. The linear probability model has the clear drawback of not being able to capture the nonlinear nature of the population regression function and it may predict probabilities to lie outside the interval <span class="math inline">\([0,1]\)</span>. Probit and Logit models are harder to interpret but capture the nonlinearities better than the linear approach: both models produce predictions of probabilities that lie inside the interval <span class="math inline">\([0,1]\)</span>. Predictions of all three models are often close to each other. The book suggests to use the method that is easiest to use in the statistical software of choice. As we have seen, it is equally easy to estimate Probit and Logit model using <tt>R</tt>. We can therefore give no general recommendation which method to use.</p>
</div>
</div>
</div>
<div id="estimation-and-inference-in-the-logit-and-probit-models" class="section level2">
<h2><span class="header-section-number">11.3</span> Estimation and Inference in the Logit and Probit Models</h2>
<p>So far nothing has been said about <em>how</em> Logit and Probit models are estimated by statistical software. The reason why this is interesting is that both models are <em>nonlinear in the parameters</em> and thus cannot be estimated using OLS. Instead one relies on <em>maximum likelihood estimation</em> (MLE). Another approach is estimation by <em>nonlinear least squares</em> (NLS).</p>
<div id="nonlinear-least-squares" class="section level4 unnumbered">
<h4>Nonlinear Least Squares</h4>
Consider the multiple regression Probit model
<span class="math display" id="eq:multprobit">\[\begin{align}
  E(Y_i\vert X_{1i}, \dots, X_{ki}) = P(Y_i=1\vert X_{1i}, \dots, X_{ki}) = \Phi(\beta_0 + \beta_1 X_{1i} + \dots + \beta_k X_{ki}). \tag{11.8}
\end{align}\]</span>
<p>Similarly to OLS, NLS estimates the parameters <span class="math inline">\(\beta_0,\beta_1,\dots,\beta_k\)</span> by minimizing the sum of squared mistakes <span class="math display">\[\sum_{i=1}^n\left[ Y_i - \Phi(b_0 + b_1 X_{1i} + \dots + b_k X_{ki}) \right]^2.\]</span> NLS estimation is a consistent approach that produces estimates which are normally distributed in large samples. In <tt>R</tt> there are functions like <tt>nls()</tt> from package <tt>stats</tt> which provide algorithms for solving nonlinear least squares problems. However, NLS is inefficient, meaning that there are estimation techniques that have a smaller variance which is why we will not dwell any further on this topic.</p>
</div>
<div id="maximum-likelihood-estimation" class="section level4 unnumbered">
<h4>Maximum Likelihood Estimation</h4>
<p>In MLE we seek to estimate the unknown parameters choosing them such that the likelihood of drawing the sample observed is maximized. This probability is measured by means of the likelihood function, the joint probability distribution of the data treated as a function of the unknown parameters. Put differently, the maximum likelihood estimates of the unknown parameters are the values that result in a model which is most likely to produce the data observed. It turns out that MLE is more efficient than NLS.</p>
<p>As maximum likelihood estimates are normally distributed in large samples, statistical inference for coefficients in nonlinear models like Logit and Probit regression can be made using the same tools that are used for linear regression models: we can compute <span class="math inline">\(t\)</span>-statistics and confidence intervals.</p>
<p>Many software packages use an MLE algorithm for estimation of nonlinear models. The function <tt>glm()</tt> uses an algorithm named <em>iteratively reweighted least squares</em>.</p>
</div>
<div id="measures-of-fit-1" class="section level4 unnumbered">
<h4>Measures of Fit</h4>
<p>It is important to be aware that the usual <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\bar{R}^2\)</span> are <em>invalid</em> for nonlinear regression models. The reason for this is simple: both measures assume that the relation between the dependent and the explanatory variable(s) is linear. This obviously does not hold for Probit and Logit models. Thus <span class="math inline">\(R^2\)</span> need not lie between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> and there is no meaningful interpretation. However, statistical software sometimes reports these measures anyway.</p>
<p>There are many measures of fit for nonlinear regression models and there is no consensus which one should be reported. The situation is even more complicated because there is no measure of fit that is generally meaningful. For models with a binary response variable like <span class="math inline">\(deny\)</span> one could use the following rule: If <span class="math inline">\(Y_i = 1\)</span> and <span class="math inline">\(\widehat{P(Y_i|X_{i1}, \dots, X_{ik})} &gt; 0.5\)</span> or if <span class="math inline">\(Y_i = 0\)</span> and <span class="math inline">\(\widehat{P(Y_i|X_{i1}, \dots, X_{ik})} &lt; 0.5\)</span>, consider the <span class="math inline">\(Y_i\)</span> as correctly predicted. Otherwise <span class="math inline">\(Y_i\)</span> is said to be incorrectly predicted. The measure of fit is the share of correctly predicted observations. The downside of such an approach is that it does not mirror the quality of the prediction: whether <span class="math inline">\(\widehat{P(Y_i = 1|X_{i1}, \dots, X_{ik}) = 0.51}\)</span> or <span class="math inline">\(\widehat{P(Y_i =1|X_{i1}, \dots, X_{ik}) = 0.99}\)</span> is not reflected, we just predict <span class="math inline">\(Y_i = 1\)</span>.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
<p>An alternative to the latter are so called pseudo-<span class="math inline">\(R^2\)</span> measures. In order to measure the quality of the fit, these measures compare the value of the maximized (log-)likelihood of the model with all regressors (the <em>full model</em>) to the likelihood of a model with no regressors (<em>null model</em>, regression on a constant).</p>
<p>For example, consider a Probit regression. The <span class="math inline">\(\text{pseudo-}R^2\)</span> is given by <span class="math display">\[\text{pseudo-}R^2 = 1 - \frac{\ln(f^{max}_{full})}{\ln(f^{max}_{null})}\]</span> where <span class="math inline">\(f^{max}_j \in [0,1]\)</span> denotes the maximized likelihood for model <span class="math inline">\(j\)</span>.</p>
<p>The reasoning behind this is that the maximized likelihood increases as additional regressors are added to the model, similarly to the decrease in <span class="math inline">\(SSR\)</span> when regressors are added in a linear regression model. If the full model has a similar maximized likelihood as the null model, the full model does not really improve upon a model that uses only the information in the dependent variable, so <span class="math inline">\(\text{pseudo-}R^2 \approx 0\)</span>. If the full model fits the data very well, the maximized likelihood should be close to <span class="math inline">\(1\)</span> such that <span class="math inline">\(\ln(f^{max}_{full}) \approx 0\)</span> and <span class="math inline">\(\text{pseudo-}R^2 \approx 1\)</span>. See Appendix 11.2 of the book for more on MLE and pseudo-<span class="math inline">\(R^2\)</span> measures.</p>
<p><tt>summary()</tt> does not report <span class="math inline">\(\text{pseudo-}R^2\)</span> for models estimated by <tt>glm()</tt> but we can use the entries <em>residual deviance</em> (<tt>deviance</tt>) and <em>null deviance</em> (<tt>null.deviance</tt>) instead. These are computed as <span class="math display">\[\text{deviance} = -2 \times \left[\ln(f^{max}_{saturated}) - \ln(f^{max}_{full}) \right]\]</span> and</p>
<p><span class="math display">\[\text{null deviance} = -2 \times \left[\ln(f^{max}_{saturated}) - \ln(f^{max}_{null}) \right]\]</span> where <span class="math inline">\(f^{max}_{saturated}\)</span> is the maximized likelihood for a model which assumes that each observation has its own parameter (there are <span class="math inline">\(n+1\)</span> parameters to be estimated which leads to a perfect fit). For models with a binary dependent variable, it holds that <span class="math display">\[\text{pseudo-}R^2 = 1 - \frac{\text{deviance}}{\text{null deviance}} = 1- \frac{\ln(f^{max}_{full})}{\ln(f^{max}_{null})}.\]</span></p>
<p>We now compute the <span class="math inline">\(\text{pseudo-}R^2\)</span> for the augmented Probit model of mortgage denial.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute pseudo-R2 for the probit model of mortgage denial</span>
pseudoR2 &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(denyprobit2<span class="op">$</span>deviance) <span class="op">/</span><span class="st"> </span>(denyprobit2<span class="op">$</span>null.deviance)
pseudoR2</code></pre></div>
<pre><code>## [1] 0.08594259</code></pre>
<p>Another way to obtain the <span class="math inline">\(\text{pseudo-}R^2\)</span> is to estimate the null model using <tt>glm()</tt> and extract the maximized log-likelihoods for both the null and the full model using the function <tt>logLik()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the null model</span>
denyprobit_null &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="dt">formula =</span> deny <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, 
                       <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;probit&quot;</span>), 
                       <span class="dt">data =</span> HMDA)

<span class="co"># compute the pseudo-R2 using &#39;logLik&#39;</span>
<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">logLik</span>(denyprobit2)[<span class="dv">1</span>]<span class="op">/</span><span class="kw">logLik</span>(denyprobit_null)[<span class="dv">1</span>]</code></pre></div>
<pre><code>## [1] 0.08594259</code></pre>
</div>
</div>
<div id="application-to-the-boston-hmda-data" class="section level2">
<h2><span class="header-section-number">11.4</span> Application to the Boston HMDA Data</h2>
<p>Models <a href="rwabdv.html#eq:denyprobit2">(11.6)</a> and <a href="rwabdv.html#eq:denylogit2">(11.7)</a> indicate that denial rates are higher for African American applicants holding constant the payment-to-income ratio. Both results could be subject to omitted variable bias. In order to obtain a more trustworthy estimate of the effect of being black on the probability of a mortgage application denial we estimate a linear probability model as well as several Logit and Probit models. We thereby control for financial variables and additional applicant characteristics which are likely to influence the probability of denial and differ between black and white applicants.</p>
<p>Sample averages as shown in Table 11.1 of the book can be easily reproduced using the functions <tt>mean()</tt> (as usual for numeric variables) and <tt>prop.table()</tt> (for factor variables).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Mean P/I ratio</span>
<span class="kw">mean</span>(HMDA<span class="op">$</span>pirat)</code></pre></div>
<pre><code>## [1] 0.3308136</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># inhouse expense-to-total-income ratio</span>
<span class="kw">mean</span>(HMDA<span class="op">$</span>hirat)</code></pre></div>
<pre><code>## [1] 0.2553461</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># loan-to-value ratio</span>
<span class="kw">mean</span>(HMDA<span class="op">$</span>lvrat)</code></pre></div>
<pre><code>## [1] 0.7377759</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># consumer credit score</span>
<span class="kw">mean</span>(<span class="kw">as.numeric</span>(HMDA<span class="op">$</span>chist))</code></pre></div>
<pre><code>## [1] 2.116387</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># mortgage credit score</span>
<span class="kw">mean</span>(<span class="kw">as.numeric</span>(HMDA<span class="op">$</span>mhist))</code></pre></div>
<pre><code>## [1] 1.721008</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># public bad credit record</span>
<span class="kw">mean</span>(<span class="kw">as.numeric</span>(HMDA<span class="op">$</span>phist)<span class="op">-</span><span class="dv">1</span>)</code></pre></div>
<pre><code>## [1] 0.07352941</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># denied mortgage insurance</span>
<span class="kw">prop.table</span>(<span class="kw">table</span>(HMDA<span class="op">$</span>insurance))</code></pre></div>
<pre><code>## 
##         no        yes 
## 0.97983193 0.02016807</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># self-employed</span>
<span class="kw">prop.table</span>(<span class="kw">table</span>(HMDA<span class="op">$</span>selfemp))</code></pre></div>
<pre><code>## 
##        no       yes 
## 0.8836134 0.1163866</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># single</span>
<span class="kw">prop.table</span>(<span class="kw">table</span>(HMDA<span class="op">$</span>single))</code></pre></div>
<pre><code>## 
##        no       yes 
## 0.6067227 0.3932773</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># high school diploma</span>
<span class="kw">prop.table</span>(<span class="kw">table</span>(HMDA<span class="op">$</span>hschool))</code></pre></div>
<pre><code>## 
##         no        yes 
## 0.01638655 0.98361345</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># unemployment rate</span>
<span class="kw">mean</span>(HMDA<span class="op">$</span>unemp)</code></pre></div>
<pre><code>## [1] 3.774496</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># condominium</span>
<span class="kw">prop.table</span>(<span class="kw">table</span>(HMDA<span class="op">$</span>condomin))</code></pre></div>
<pre><code>## 
##        no       yes 
## 0.7117647 0.2882353</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># black</span>
<span class="kw">prop.table</span>(<span class="kw">table</span>(HMDA<span class="op">$</span>black))</code></pre></div>
<pre><code>## 
##       no      yes 
## 0.857563 0.142437</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># deny</span>
<span class="kw">prop.table</span>(<span class="kw">table</span>(HMDA<span class="op">$</span>deny))</code></pre></div>
<pre><code>## 
##         0         1 
## 0.8802521 0.1197479</code></pre>
<p>See Chapter 11.4 of the book or use <tt>R</tt>’s help function for more on variables contained in the <tt>HMDA</tt> dataset.</p>
<p>Before estimating the models we transform the loan-to-value ratio (<tt>lvrat</tt>) into a factor variable, where</p>
<span class="math display">\[\begin{align*}
  lvrat = 
  \begin{cases}
    \text{low} &amp; \text{if} \ \ lvrat &lt; 0.8, \\
    \text{medium} &amp; \text{if} \ \ 0.8 \leq lvrat \leq 0.95, \\
    \text{high} &amp; \text{if} \ \ lvrat &gt; 0.95
  \end{cases}
\end{align*}\]</span>
<p>and convert both credit scores to numeric variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define low, medium and high loan-to-value ratio</span>
HMDA<span class="op">$</span>lvrat &lt;-<span class="st"> </span><span class="kw">factor</span>(
  <span class="kw">ifelse</span>(HMDA<span class="op">$</span>lvrat <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.8</span>, <span class="st">&quot;low&quot;</span>,
  <span class="kw">ifelse</span>(HMDA<span class="op">$</span>lvrat <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.8</span> <span class="op">&amp;</span><span class="st"> </span>HMDA<span class="op">$</span>lvrat <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.95</span>, <span class="st">&quot;medium&quot;</span>, <span class="st">&quot;high&quot;</span>)),
  <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;low&quot;</span>, <span class="st">&quot;medium&quot;</span>, <span class="st">&quot;high&quot;</span>))

<span class="co"># convert credit scores to numeric</span>
HMDA<span class="op">$</span>mhist &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(HMDA<span class="op">$</span>mhist)
HMDA<span class="op">$</span>chist &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(HMDA<span class="op">$</span>chist)</code></pre></div>
<p>Next we reproduce the estimation results presented in Table 11.2 of the book.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate all 6 models for the denial probability</span>
lpm_HMDA &lt;-<span class="st"> </span><span class="kw">lm</span>(deny <span class="op">~</span><span class="st"> </span>black <span class="op">+</span><span class="st"> </span>pirat <span class="op">+</span><span class="st"> </span>hirat <span class="op">+</span><span class="st"> </span>lvrat <span class="op">+</span><span class="st"> </span>chist <span class="op">+</span><span class="st"> </span>mhist <span class="op">+</span><span class="st"> </span>phist 
               <span class="op">+</span><span class="st"> </span>insurance <span class="op">+</span><span class="st"> </span>selfemp, <span class="dt">data =</span> HMDA)

logit_HMDA &lt;-<span class="st"> </span><span class="kw">glm</span>(deny <span class="op">~</span><span class="st"> </span>black <span class="op">+</span><span class="st"> </span>pirat <span class="op">+</span><span class="st"> </span>hirat <span class="op">+</span><span class="st"> </span>lvrat <span class="op">+</span><span class="st"> </span>chist <span class="op">+</span><span class="st"> </span>mhist <span class="op">+</span><span class="st"> </span>phist 
                  <span class="op">+</span><span class="st"> </span>insurance <span class="op">+</span><span class="st"> </span>selfemp, 
                  <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>), 
                  <span class="dt">data =</span> HMDA)

probit_HMDA_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(deny <span class="op">~</span><span class="st"> </span>black <span class="op">+</span><span class="st"> </span>pirat <span class="op">+</span><span class="st"> </span>hirat <span class="op">+</span><span class="st"> </span>lvrat <span class="op">+</span><span class="st"> </span>chist <span class="op">+</span><span class="st"> </span>mhist <span class="op">+</span><span class="st"> </span>phist 
                     <span class="op">+</span><span class="st"> </span>insurance <span class="op">+</span><span class="st"> </span>selfemp, 
                     <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;probit&quot;</span>), 
                     <span class="dt">data =</span> HMDA)

probit_HMDA_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(deny <span class="op">~</span><span class="st"> </span>black <span class="op">+</span><span class="st"> </span>pirat <span class="op">+</span><span class="st"> </span>hirat <span class="op">+</span><span class="st"> </span>lvrat <span class="op">+</span><span class="st"> </span>chist <span class="op">+</span><span class="st"> </span>mhist <span class="op">+</span><span class="st"> </span>phist 
                     <span class="op">+</span><span class="st"> </span>insurance <span class="op">+</span><span class="st"> </span>selfemp <span class="op">+</span><span class="st"> </span>single <span class="op">+</span><span class="st"> </span>hschool <span class="op">+</span><span class="st"> </span>unemp, 
                     <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;probit&quot;</span>), 
                     <span class="dt">data =</span> HMDA)

probit_HMDA_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(deny <span class="op">~</span><span class="st"> </span>black <span class="op">+</span><span class="st"> </span>pirat <span class="op">+</span><span class="st"> </span>hirat <span class="op">+</span><span class="st"> </span>lvrat <span class="op">+</span><span class="st"> </span>chist <span class="op">+</span><span class="st"> </span>mhist 
                     <span class="op">+</span><span class="st"> </span>phist <span class="op">+</span><span class="st"> </span>insurance <span class="op">+</span><span class="st"> </span>selfemp <span class="op">+</span><span class="st"> </span>single <span class="op">+</span><span class="st"> </span>hschool <span class="op">+</span><span class="st"> </span>unemp <span class="op">+</span><span class="st"> </span>condomin 
                     <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(mhist<span class="op">==</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(mhist<span class="op">==</span><span class="dv">4</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(chist<span class="op">==</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(chist<span class="op">==</span><span class="dv">4</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(chist<span class="op">==</span><span class="dv">5</span>) 
                     <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(chist<span class="op">==</span><span class="dv">6</span>), 
                     <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;probit&quot;</span>), 
                     <span class="dt">data =</span> HMDA)

probit_HMDA_<span class="dv">4</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(deny <span class="op">~</span><span class="st"> </span>black <span class="op">*</span><span class="st"> </span>(pirat <span class="op">+</span><span class="st"> </span>hirat) <span class="op">+</span><span class="st"> </span>lvrat <span class="op">+</span><span class="st"> </span>chist <span class="op">+</span><span class="st"> </span>mhist <span class="op">+</span><span class="st"> </span>phist 
                     <span class="op">+</span><span class="st"> </span>insurance <span class="op">+</span><span class="st"> </span>selfemp <span class="op">+</span><span class="st"> </span>single <span class="op">+</span><span class="st"> </span>hschool <span class="op">+</span><span class="st"> </span>unemp, 
                     <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;probit&quot;</span>), 
                     <span class="dt">data =</span> HMDA)</code></pre></div>
<p>Just as in previous chapters, we store heteroskedasticity-robust standard errors of the coefficient estimators in a <tt>list</tt> which is then used as the argument <tt>se</tt> in <tt>stargazer()</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rob_se &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(lpm_HMDA, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(logit_HMDA, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(probit_HMDA_<span class="dv">1</span>, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(probit_HMDA_<span class="dv">2</span>, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(probit_HMDA_<span class="dv">3</span>, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))),
               <span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(probit_HMDA_<span class="dv">4</span>, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>))))

<span class="kw">stargazer</span>(lpm_HMDA, logit_HMDA, probit_HMDA_<span class="dv">1</span>, 
          probit_HMDA_<span class="dv">2</span>, probit_HMDA_<span class="dv">3</span>, probit_HMDA_<span class="dv">4</span>,  
          <span class="dt">digits =</span> <span class="dv">3</span>,
          <span class="dt">type =</span> <span class="st">&quot;latex&quot;</span>, 
          <span class="dt">header =</span> <span class="ot">FALSE</span>,
          <span class="dt">se =</span> rob_se,
          <span class="dt">model.numbers =</span> <span class="ot">FALSE</span>,
          <span class="dt">column.labels =</span> <span class="kw">c</span>(<span class="st">&quot;(1)&quot;</span>, <span class="st">&quot;(2)&quot;</span>, <span class="st">&quot;(3)&quot;</span>, <span class="st">&quot;(4)&quot;</span>, <span class="st">&quot;(5)&quot;</span>, <span class="st">&quot;(6)&quot;</span>))</code></pre></div>



<table style="text-align:center"><tr><td colspan="7" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"></td><td colspan="6">Dependent Variable: Mortgage Application Denial</td></tr>
<tr><td></td><td colspan="6" style="border-bottom: 1px solid black"></td></tr>
<tr><td style="text-align:left"></td><td colspan="6">deny</td></tr>
<tr><td style="text-align:left"></td><td><em>OLS</em></td><td><em>logistic</em></td><td colspan="4"><em>probit</em></td></tr>
<tr><td style="text-align:left"></td><td>(1)</td><td>(2)</td><td>(3)</td><td>(4)</td><td>(5)</td><td>(6)</td></tr>
<tr><td colspan="7" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">blackyes</td><td>0.084<sup>***</sup></td><td>0.688<sup>***</sup></td><td>0.389<sup>***</sup></td><td>0.371<sup>***</sup></td><td>0.363<sup>***</sup></td><td>0.246</td></tr>
<tr><td style="text-align:left"></td><td>(0.023)</td><td>(0.183)</td><td>(0.099)</td><td>(0.100)</td><td>(0.101)</td><td>(0.479)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">pirat</td><td>0.449<sup>***</sup></td><td>4.764<sup>***</sup></td><td>2.442<sup>***</sup></td><td>2.464<sup>***</sup></td><td>2.622<sup>***</sup></td><td>2.572<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.114)</td><td>(1.332)</td><td>(0.673)</td><td>(0.654)</td><td>(0.665)</td><td>(0.728)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">hirat</td><td>-0.048</td><td>-0.109</td><td>-0.185</td><td>-0.302</td><td>-0.502</td><td>-0.538</td></tr>
<tr><td style="text-align:left"></td><td>(0.110)</td><td>(1.298)</td><td>(0.689)</td><td>(0.689)</td><td>(0.715)</td><td>(0.755)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">lvratmedium</td><td>0.031<sup>**</sup></td><td>0.464<sup>***</sup></td><td>0.214<sup>***</sup></td><td>0.216<sup>***</sup></td><td>0.215<sup>**</sup></td><td>0.216<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.013)</td><td>(0.160)</td><td>(0.082)</td><td>(0.082)</td><td>(0.084)</td><td>(0.083)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">lvrathigh</td><td>0.189<sup>***</sup></td><td>1.495<sup>***</sup></td><td>0.791<sup>***</sup></td><td>0.795<sup>***</sup></td><td>0.836<sup>***</sup></td><td>0.788<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.050)</td><td>(0.325)</td><td>(0.183)</td><td>(0.184)</td><td>(0.185)</td><td>(0.185)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">chist</td><td>0.031<sup>***</sup></td><td>0.290<sup>***</sup></td><td>0.155<sup>***</sup></td><td>0.158<sup>***</sup></td><td>0.344<sup>***</sup></td><td>0.158<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.005)</td><td>(0.039)</td><td>(0.021)</td><td>(0.021)</td><td>(0.108)</td><td>(0.021)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">mhist</td><td>0.021<sup>*</sup></td><td>0.279<sup>**</sup></td><td>0.148<sup>**</sup></td><td>0.110</td><td>0.162</td><td>0.111</td></tr>
<tr><td style="text-align:left"></td><td>(0.011)</td><td>(0.138)</td><td>(0.073)</td><td>(0.076)</td><td>(0.104)</td><td>(0.077)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">phistyes</td><td>0.197<sup>***</sup></td><td>1.226<sup>***</sup></td><td>0.697<sup>***</sup></td><td>0.702<sup>***</sup></td><td>0.717<sup>***</sup></td><td>0.705<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.035)</td><td>(0.203)</td><td>(0.114)</td><td>(0.115)</td><td>(0.116)</td><td>(0.115)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">insuranceyes</td><td>0.702<sup>***</sup></td><td>4.548<sup>***</sup></td><td>2.557<sup>***</sup></td><td>2.585<sup>***</sup></td><td>2.589<sup>***</sup></td><td>2.590<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.045)</td><td>(0.576)</td><td>(0.305)</td><td>(0.299)</td><td>(0.306)</td><td>(0.299)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">selfempyes</td><td>0.060<sup>***</sup></td><td>0.666<sup>***</sup></td><td>0.359<sup>***</sup></td><td>0.346<sup>***</sup></td><td>0.342<sup>***</sup></td><td>0.348<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.021)</td><td>(0.214)</td><td>(0.113)</td><td>(0.116)</td><td>(0.116)</td><td>(0.116)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">singleyes</td><td></td><td></td><td></td><td>0.229<sup>***</sup></td><td>0.230<sup>***</sup></td><td>0.226<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td>(0.080)</td><td>(0.086)</td><td>(0.081)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">hschoolyes</td><td></td><td></td><td></td><td>-0.613<sup>***</sup></td><td>-0.604<sup>**</sup></td><td>-0.620<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td>(0.229)</td><td>(0.237)</td><td>(0.229)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">unemp</td><td></td><td></td><td></td><td>0.030<sup>*</sup></td><td>0.028</td><td>0.030</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td>(0.018)</td><td>(0.018)</td><td>(0.018)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">condominyes</td><td></td><td></td><td></td><td></td><td>-0.055</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td>(0.096)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(mhist == 3)</td><td></td><td></td><td></td><td></td><td>-0.107</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td>(0.301)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(mhist == 4)</td><td></td><td></td><td></td><td></td><td>-0.383</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td>(0.427)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(chist == 3)</td><td></td><td></td><td></td><td></td><td>-0.226</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td>(0.248)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(chist == 4)</td><td></td><td></td><td></td><td></td><td>-0.251</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td>(0.338)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(chist == 5)</td><td></td><td></td><td></td><td></td><td>-0.789<sup>*</sup></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td>(0.412)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">I(chist == 6)</td><td></td><td></td><td></td><td></td><td>-0.905<sup>*</sup></td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td>(0.515)</td><td></td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">blackyes:pirat</td><td></td><td></td><td></td><td></td><td></td><td>-0.579</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td>(1.550)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">blackyes:hirat</td><td></td><td></td><td></td><td></td><td></td><td>1.232</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td>(1.709)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">Constant</td><td>-0.183<sup>***</sup></td><td>-5.707<sup>***</sup></td><td>-3.041<sup>***</sup></td><td>-2.575<sup>***</sup></td><td>-2.896<sup>***</sup></td><td>-2.543<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.028)</td><td>(0.484)</td><td>(0.250)</td><td>(0.350)</td><td>(0.404)</td><td>(0.370)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td colspan="7" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">Observations</td><td>2,380</td><td>2,380</td><td>2,380</td><td>2,380</td><td>2,380</td><td>2,380</td></tr>
<tr><td style="text-align:left">R<sup>2</sup></td><td>0.266</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">Adjusted R<sup>2</sup></td><td>0.263</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">Log Likelihood</td><td></td><td>-635.637</td><td>-636.847</td><td>-628.614</td><td>-625.064</td><td>-628.332</td></tr>
<tr><td style="text-align:left">Akaike Inf. Crit.</td><td></td><td>1,293.273</td><td>1,295.694</td><td>1,285.227</td><td>1,292.129</td><td>1,288.664</td></tr>
<tr><td style="text-align:left">Residual Std. Error</td><td>0.279 (df = 2369)</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td style="text-align:left">F Statistic</td><td>85.974<sup>***</sup> (df = 10; 2369)</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td colspan="7" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"><em>Note:</em></td><td colspan="6" style="text-align:right"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>
</table>
<caption><p style='text-align:center'><span id="tab:hmdad">Table 11.1: </span> HMDA Data: LPM, Probit and Logit Models</p></caption>


<p>In Table <a href="rwabdv.html#tab:hmdad">11.1</a>, models (1), (2) and (3) are baseline specifications that include several financial control variables. They differ only in the way they model the denial probability. Model (1) is a linear probability model, model (2) is a Logit regression and model (3) uses the Probit approach.</p>
<p>In the linear model (1), the coefficients have direct interpretation. For example, an increase in the consumer credit score by <span class="math inline">\(1\)</span> unit is estimated to increase the probability of a loan denial by about <span class="math inline">\(0.031\)</span> percentage points. Having a high loan-to-value ratio is detriment for credit approval: the coefficient for a loan-to-value ratio higher than <span class="math inline">\(0.95\)</span> is <span class="math inline">\(0.189\)</span> so clients with this property are estimated to face an almost <span class="math inline">\(19\%\)</span> larger risk of denial than those with a low loan-to-value ratio, ceteris paribus. The estimated coefficient on the race dummy is <span class="math inline">\(0.084\)</span>, which indicates the denial probability for African Americans is <span class="math inline">\(8.4\%\)</span> larger than for white applicants with the same characteristics except for race. Apart from the housing-expense-to-income ratio and the mortgage credit score, all coefficients are significant.</p>
<p>Models (2) and (3) provide similar evidence that there is racial discrimination in the U.S. mortgage market. All coefficients except for the housing expense-to-income ratio (which is not significantly different from zero) are significant at the <span class="math inline">\(1\%\)</span> level. As discussed above, the nonlinearity makes the interpretation of the coefficient estimates more difficult than for model (1). In order to make a statement about the effect of being black, we need to compute the estimated denial probability for two individuals that differ only in race. For the comparison we consider two individuals that share mean values for all numeric regressors. For the qualitative variables we assign the property that is most representative for the data at hand. For example, consider self-employment: we have seen that about <span class="math inline">\(88\%\)</span> of all individuals in the sample are not self-employed such that we set <tt>selfemp = no</tt>. Using this approach, the estimate for the effect on the denial probability of being African American of the Logit model (2) is about <span class="math inline">\(4\%\)</span>. The next code chunk shows how to apply this approach for models (1) to (7) using <tt>R</tt>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># comppute regressor values for an average black person</span>
new &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="st">&quot;pirat&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(HMDA<span class="op">$</span>pirat),
  <span class="st">&quot;hirat&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(HMDA<span class="op">$</span>hirat),
  <span class="st">&quot;lvrat&quot;</span> =<span class="st"> &quot;low&quot;</span>,
  <span class="st">&quot;chist&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(HMDA<span class="op">$</span>chist),
  <span class="st">&quot;mhist&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(HMDA<span class="op">$</span>mhist),
  <span class="st">&quot;phist&quot;</span> =<span class="st"> &quot;no&quot;</span>,
  <span class="st">&quot;insurance&quot;</span> =<span class="st"> &quot;no&quot;</span>,
  <span class="st">&quot;selfemp&quot;</span> =<span class="st"> &quot;no&quot;</span>,
  <span class="st">&quot;black&quot;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>),
  <span class="st">&quot;single&quot;</span> =<span class="st"> &quot;no&quot;</span>,
  <span class="st">&quot;hschool&quot;</span> =<span class="st"> &quot;yes&quot;</span>,
  <span class="st">&quot;unemp&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(HMDA<span class="op">$</span>unemp),
  <span class="st">&quot;condomin&quot;</span> =<span class="st"> &quot;no&quot;</span>)

<span class="co"># differnce predicted by the LPM</span>
predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(lpm_HMDA, <span class="dt">newdata =</span> new)
<span class="kw">diff</span>(predictions)</code></pre></div>
<pre><code>##          2 
## 0.08369674</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># differnce predicted by the logit model</span>
predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(logit_HMDA, <span class="dt">newdata =</span> new, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
<span class="kw">diff</span>(predictions)</code></pre></div>
<pre><code>##          2 
## 0.04042135</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># difference predicted by probit model (3)</span>
predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(probit_HMDA_<span class="dv">1</span>, <span class="dt">newdata =</span> new, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
<span class="kw">diff</span>(predictions)</code></pre></div>
<pre><code>##          2 
## 0.05049716</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># difference predicted by probit model (4)</span>
predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(probit_HMDA_<span class="dv">2</span>, <span class="dt">newdata =</span> new, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
<span class="kw">diff</span>(predictions)</code></pre></div>
<pre><code>##          2 
## 0.03978918</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># difference predicted by probit model (5)</span>
predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(probit_HMDA_<span class="dv">3</span>, <span class="dt">newdata =</span> new, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
<span class="kw">diff</span>(predictions)</code></pre></div>
<pre><code>##          2 
## 0.04972468</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># difference predicted by probit model (6)</span>
predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(probit_HMDA_<span class="dv">4</span>, <span class="dt">newdata =</span> new, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
<span class="kw">diff</span>(predictions)</code></pre></div>
<pre><code>##          2 
## 0.03955893</code></pre>
<p>The estimates of the impact on the denial probability of being black are similar for models (2) and (3). It is interesting that the magnitude of the estimated effects is much smaller than for Probit and Logit models that do not control for financial characteristics (see section 11.2). This indicates that these simple models produce biased estimates due to omitted variables.</p>
<p>Regressions (4) to (6) use regression specifications that include different applicant characteristics and credit rating indicator variables as well as interactions. However, most of the corresponding coefficients are not significant and the estimates of the coefficient on <tt>black</tt> obtained for these models as well as the estimated difference in denial probabilities do not differ much from those obtained for the similar specifications (2) and (3).</p>
<p>An interesting question related to racial discrimination can be investigated using the Probit model (6) where the interactions <tt>blackyes:pirat</tt> and <tt>blackyes:hirat</tt> are added to model (4). If the coefficient on <tt>blackyes:pirat</tt> was different from zero, the effect of the payment-to-income ratio on the denial probability would be different for black and white applicants. Similarly, a non-zero coefficient on <tt>blackyes:hirat</tt> would indicate that loan officers weight the risk of bankruptcy associated with a high loan-to-value ratio differently for black and white mortgage applicants. We can test whether these coefficients are jointly significant at the <span class="math inline">\(5\%\)</span> level using an <span class="math inline">\(F\)</span>-Test.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">linearHypothesis</span>(probit_HMDA_<span class="dv">4</span>,
                 <span class="dt">test =</span> <span class="st">&quot;F&quot;</span>,
                 <span class="kw">c</span>(<span class="st">&quot;blackyes:pirat=0&quot;</span>, <span class="st">&quot;blackyes:hirat=0&quot;</span>),
                 <span class="dt">vcov =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## blackyes:pirat = 0
## blackyes:hirat = 0
## 
## Model 1: restricted model
## Model 2: deny ~ black * (pirat + hirat) + lvrat + chist + mhist + phist + 
##     insurance + selfemp + single + hschool + unemp
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F Pr(&gt;F)
## 1   2366                 
## 2   2364  2 0.2473 0.7809</code></pre>
<p>Since <span class="math inline">\(p\text{-value} \approx 0.77\)</span> for this test, the null cannot be rejected. Nonetheless, we can reject the hypothesis that there is no racial discrimination at all since the corresponding <span class="math inline">\(F\)</span>-test has a <span class="math inline">\(p\text{-value}\)</span> of about <span class="math inline">\(0.002\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">linearHypothesis</span>(probit_HMDA_<span class="dv">4</span>,
                 <span class="dt">test =</span> <span class="st">&quot;F&quot;</span>,
                 <span class="kw">c</span>(<span class="st">&quot;blackyes=0&quot;</span>, <span class="st">&quot;blackyes:pirat=0&quot;</span>, <span class="st">&quot;blackyes:hirat=0&quot;</span>),
                 <span class="dt">vcov =</span> vcovHC, <span class="dt">type =</span> <span class="st">&quot;HC1&quot;</span>)</code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## blackyes = 0
## blackyes:pirat = 0
## blackyes:hirat = 0
## 
## Model 1: restricted model
## Model 2: deny ~ black * (pirat + hirat) + lvrat + chist + mhist + phist + 
##     insurance + selfemp + single + hschool + unemp
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df      F   Pr(&gt;F)   
## 1   2367                      
## 2   2364  3 4.7774 0.002534 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div id="summary-3" class="section level4 unnumbered">
<h4>Summary</h4>
<p>Models (1) to (6) provide evidence that there is an effect of being African American on the probability of a mortgage application denial: in all specifications, the effect is estimated to be positive (ranging from <span class="math inline">\(4\%\)</span> to <span class="math inline">\(5\%\)</span>) and is significantly different from zero at the <span class="math inline">\(1\%\)</span> level. While the linear probability model seems to slightly overestimate this effect, it still can be used as an approximation to an intrinsically nonlinear relationship.</p>
<p>See Chapters 11.4 and 11.5 of the book for a discussion of external and internal validity of this study and some concluding remarks on regression models where the dependent variable is binary.</p>
</div>
</div>
<div id="exercises-8" class="section level2">
<h2><span class="header-section-number">11.5</span> Exercises</h2>
<div class="DCexercise">
<h4 id="titanic-survival-data" class="unnumbered">1. Titanic Survival Data</h4>
<p>Chapter <a href="rwabdv.html#palr">11.2</a> presented three approaches to model the conditional expectation function of a binary dependent variable: the linear probability model as well as Probit and Logit regression.</p>
<p>The exercises in this Chapter use data on the fate of the passengers of the ocean linear <em>Titanic</em>. We aim to explain survival, a binary variable, by socioeconomic variables using the above approaches.</p>
<p>In this exercise we start with the aggregated data set <tt>Titanic</tt>. It is part of the package <tt>datasets</tt> which is part of base <tt>R</tt>. The following quote from the <a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/Titanic.html">description</a> of the dataset motivates the attempt to predict the <em>probability</em> of survival:</p>
<p><em>The sinking of the Titanic is a famous event, and new books are still being published about it. Many well-known facts — from the proportions of first-class passengers to the ‘women and children first’ policy, and the fact that that policy was not entirely successful in saving the women and children in the third class — are reflected in the survival rates for various classes of passenger.</em></p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Assign the <tt>Titanic</tt> data to <tt>Titanic_1</tt> and get an overview.</p></li>
<li><p>Visualize the conditional survival rates for travel class (<tt>Class</tt>), gender (<tt>Sex</tt>) and age (<tt>Age</tt>) using <tt>mosaicplot()</tt>.</p></li>
</ul>
<iframe src="DCL/ex11_1.html" frameborder="0" scrolling="no" style="width:100%;height:360px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="titanic-survival-data-ctd." class="unnumbered">2. Titanic Survival Data — Ctd.</h4>
<p>The <tt>Titanic</tt> data set from Exercise 1 is not useful for regression analysis because it is highly aggregated. In this exercise you will work with <tt>titanic.csv</tt> which is available under the URL <a href="https://stanford.io/2O9RUCF" class="uri">https://stanford.io/2O9RUCF</a>.</p>
<p>The columns of <tt>titanic.csv</tt> contain the following variables:</p>
<p><tt>Survived</tt> — The survived indicator</p>
<p><tt>Pclass</tt> — passenger class</p>
<p><tt>Name</tt> — passenger’s Name</p>
<p><tt>Sex</tt> — passenger’s gender</p>
<p><tt>Age</tt> — passengers’s age</p>
<p><tt>Siblings</tt> — number of siblings aboard</p>
<p><tt>Parents.Children.Aboard</tt> — number of parents and children aboard</p>
<p><tt>fare</tt> — the fare paid in british pound</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Import the data from <tt>titanic.csv</tt> using the function <tt>read.csv2()</tt>. Save it to <tt>Titanic_2</tt>.</p></li>
<li><p>Assign the following column names to <tt>Titanic_2</tt>:</p>
<p><tt>Survived, Class, Name, Sex, Age, Siblings, Parents</tt> and <tt>Fare</tt>.</p></li>
<li><p>Get an overview over the data set. Drop the column <tt>Name</tt>.</p></li>
<li><p>Attach the packages <tt>corrplot</tt> and <tt>dplyr</tt>. Check whether there is multicollinearity in the data using <tt>corrplot()</tt>.</p></li>
</ul>
<iframe src="DCL/ex11_2.html" frameborder="0" scrolling="no" style="width:100%;height:560px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p><tt>read_csv()</tt> guesses the column specification as well as the seperators used in the <tt>.csv</tt> file. You should always check if the result is correct.</p></li>
<li><p>You may use <tt>select_if()</tt> from the <tt>dplyr</tt> package to select all numeric columns from the data set.</p></li>
</ul>
</div>
<div class="DCexercise">
<h4 id="titanic-survival-data-survival-rates" class="unnumbered">3. Titanic Survival Data — Survival Rates</h4>
<p>Contingency tables similar to those provided by the data set <tt>Titanic</tt> from Exercise 1 may shed some light on the distribution of survival conditional and possible determinants thereof, e.g., the passenger class. Contingency tables are easily created using the base <tt>R</tt> function <tt>table</tt>.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Generate a contingency table for <tt>Survived</tt> and <tt>Class</tt> using <tt>table()</tt>. Save the table to <tt>t_abs</tt>.</p></li>
<li><p><tt>t_abs</tt> reports absolute frequencies. Transform <tt>t_abs</tt> into a table which reports relative frequencies (relative to the total number of observations). Save the result to <tt>t_rel</tt>.</p></li>
<li><p>Visualize the relative frequencies in <tt>t_rel</tt> using <tt>barplot()</tt>. Use different colors for better distinquishablitly among survival and non-survival rate (it does not matter which colors you use).</p></li>
</ul>
<iframe src="DCL/ex11_4_3.html" frameborder="0" scrolling="no" style="width:100%;height:320px">
</iframe>
</div>
<div class="DCexercise">
<h4 id="titanic-survival-data-conditional-distributions-of-age" class="unnumbered">4. Titanic Survival Data — Conditional Distributions of <tt>Age</tt></h4>
<p>Contingency tables are useful for summarizing distribution of categorical variables like <tt>Survived</tt> and <tt>Class</tt> in Exercise 3. They are, however, not useful when the variable of interest takes on many different integers (and they are even impossible to generate when the variable is continuous).</p>
<p>In this exercise you are asked to generate and visualize density estimates of the distribution of <tt>Age</tt> conditional on <tt>Survived</tt> to see whether there are indications how age relates to the chance of survival (despite that the data set reports integers, we treat <tt>Age</tt> as a continuous variable here). For example, it is interesting to see if the ‘women and children first’ policy was effective.</p>
<p>The data set <tt>Titanic_2</tt> from the previous exercises is available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Obtain kernel density estimates of the distributions of <tt>Age</tt> for both the survivors and the deceased.</p></li>
<li><p>Save the results to <tt>dens_age_surv</tt> (survived) and <tt>dens_age_died</tt> (died).</p></li>
<li><p>Plot both kernel density estimates (overlay them in a single plot!). Use different colors of your choive to make the estimates distinguishable.</p></li>
</ul>
<iframe src="DCL/ex11_5_4.html" frameborder="0" scrolling="no" style="width:100%;height:330px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p>Kernel density estimates can be obtained using the functon <tt>density()</tt>.</p></li>
<li><p>Use <tt>plot()</tt> and <tt>lines()</tt> to plot the density estimates.</p></li>
</ul>
</div>
<div class="DCexercise">
<h4 id="titanic-survival-data-a-linear-probability-model-for-survival-i" class="unnumbered">5. Titanic Survival Data — A Linear Probability Model for <tt>Survival</tt> I</h4>
<p>How do socio-economic characteristics of the passengers impact the probability of survival? In particular, are there systematic differences between the three passenger classes? Do the data reflect the ‘children and women first’ policy?</p>
<p>It is natural to start the analysis by estimating a simple linear probability model like (LMP) <span class="math display">\[Survived_i = \beta_0 + \beta_1 Class2_i + \beta_2 Class3_i + u_i\]</span> with dummy variables <span class="math inline">\(Class2_i\)</span> and <span class="math inline">\(Class3_i\)</span>.</p>
<p>The data set <tt>Titanic_2</tt> from the previous exercises is available in your working environment.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Attach the <tt>AER</tt> package.</p></li>
<li><p><tt>Class</tt> is of type <tt>int</tt> (integer), Convert <tt>Class</tt> to a factor variable.</p></li>
<li><p>Estimate the linear probability model and save the result to <tt>surv_mod</tt>.</p></li>
<li><p>Obtain a robust summary of the model coefficients.</p></li>
<li><p>Use <tt>surv_mod</tt> to predict the probability of survival for the three passenger classes.</p></li>
</ul>
<iframe src="DCL/ex11_lpm.html" frameborder="0" scrolling="no" style="width:100%;height:320px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p>Linear probability models can be estimated using <tt>lm()</tt>.</p></li>
<li><p>Use <tt>predict()</tt> to obtain the predictions. Remember that a <tt>data.frame</tt> must be provided to the argument <tt>newdata</tt>.</p></li>
</ul>
</div>
<div class="DCexercise">
<h4 id="titanic-survival-data-a-linear-probability-model-for-survival-ii" class="unnumbered">6. Titanic Survival Data — A Linear Probability Model for <tt>Survival</tt> II</h4>
<p>Consider again the outcome from Exercise 5:</p>
<p><span class="math display">\[\widehat{Survived}_i = \underset{(0.03)}{0.63} - \underset{(0.05)}{0.16} Class2_i - \underset{(0.04)}{0.39} Class3_i + u_i \]</span></p>
<p>(The estimated coefficients in this model are related to the class specific sample means of <tt>Survived</tt>. You are asked to compute them below.)</p>
<p>The highly significant coefficients indicate that the probability of survival decreases with the passenger class, that is, passengers from a less luxurious class are less likely to survive.</p>
<p>This result could be affected by omitted variable bias arising from correlation of the passenger class with determinants of the probability of survival not included in the model. We therefore augment the model such that it includes all remaining variables as regressors.</p>
<p>The data set <tt>Titanic_2</tt> as well as the model <tt>surv_mod</tt> from the previous exercises are available in your working environment. The <tt>AER</tt> package is attached.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Use the model object <tt>surv_mod</tt> to obtain the class specific estimates for the probability of survival. Store them in <tt>surv_prob_c1</tt>, <tt>surv_prob_c2</tt> and <tt>surv_prob_c3</tt>.</p></li>
<li><p>Fit the augmented LMP and assign the result to the object <tt>LPM_mod</tt>.</p></li>
<li><p>Obtain a robust summary of the model coefficients.</p></li>
</ul>
<iframe src="DCL/ex11_lpm2.html" frameborder="0" scrolling="no" style="width:100%;height:320px">
</iframe>
<p><strong>Hint:</strong></p>
<ul>
<li>Remember that the formula <tt>a ~ .</tt> specifies a regression of <tt>a</tt> on all other variables in the data set provided as the argument <tt>data</tt> in <tt>glm()</tt>.</li>
</ul>
</div>
<div class="DCexercise">
<h4 id="titanic-survival-data-logistic-regression" class="unnumbered">7. Titanic Survival Data — Logistic Regression</h4>
<p>Chapter <a href="rwabdv.html#palr">11.2</a> introduces Logistic regression, also called Logit regression, which is a more suitable than the LPM for modelling the conditional probability function of a dichotomous outcome variable. Logit regression uses a nonlinear link function that restricts the fitted values to lie between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>: in Logit regression, the <em>log-odds</em> of the outcome are modeled as a linear combination of the predictors while the LPM assumes that the conditional probability function of outcome is linear.</p>
<p>The data set <tt>Titanic_2</tt> from Exercise 2 is available in your working environment. The package <tt>AER</tt> is attached.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li>Use <tt>glm()</tt> to estimate the model
<span class="math display">\[\begin{align*}
\log\left(\frac{P(survived_i = 1)}{1-P(survived_i = 1)}\right) =&amp; \, \beta_0 + \beta_1 Class2_i + \beta_2 Ckass3_i + \beta_3 Sex_i \\ +&amp; \, \beta_4 Age_i + \beta_5 Siblings_i + \beta_6 Perents_i + \beta_7 Fare_i + u_i.
\end{align*}\]</span></li>
<li><p>Obtain a robust summary of the model coefficients.</p></li>
<li><p>The data frame <tt>passengers</tt> contains data on three hypothetical male passengers that differ only in their passenger class (the other variables are set to the respective sample average). Use <tt>Logit_mod</tt> to predict the probability of survival for these passengers.</p></li>
</ul>
<iframe src="DCL/ex11_3_6.html" frameborder="0" scrolling="no" style="width:100%;height:430px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p>Remember that the formula <tt>a ~ .</tt> specifies a regression of <tt>a</tt> on all other variables in the data set provided as the argument <tt>data</tt> in <tt>glm()</tt>.</p></li>
<li><p>You need to specify the correct type of prediction in <tt>predict()</tt>.</p></li>
</ul>
</div>
<div class="DCexercise">
<h4 id="titanic-survival-data-probit-regression" class="unnumbered">8. Titanic Survival Data — Probit Regression</h4>
Repeat Exercise 7 but this time estimate the Probit model
<span class="math display">\[\begin{align*}
P(Survived_i = 1\vert Class2_i, Class3_i, \dots, Fare_i) =&amp; \, \Phi (\beta_0 + \beta_1 Class2_i + \beta_2 Class3_i + \beta_3 Sex_i \\ +&amp; \, \beta_4 Age_i + \beta_5 Siblings_i + \beta_6 Parents_i + \beta_7 Fare_i + u_i).
\end{align*}\]</span>
<p>The data set <tt>Titanic_2</tt> from the previous exercises as well as the Logit model <tt>Logit_mod</tt> are available in your working environment. The package <tt>AER</tt> is attached.</p>
<p><strong>Instructions:</strong></p>
<ul>
<li><p>Use <tt>glm()</tt> to estimate the above Probit model. Save the result to <tt>Probit_mod</tt>.</p></li>
<li><p>Obtain a robust summary of the model coefficients.</p></li>
<li><p>The data frame <tt>passengers</tt> contains data on three hypothetical male passengers that differ only in their passenger class (the other variables are set to the respective sample average). Use <tt>Probit_mod</tt> to predict the probability of survival for these passengers.</p></li>
</ul>
<iframe src="DCL/ex11_8.html" frameborder="0" scrolling="no" style="width:100%;height:430px">
</iframe>
<p><strong>Hints:</strong></p>
<ul>
<li><p>Remember that the formula <tt>a ~ .</tt> specifies a regression of <tt>a</tt> on all other variables in the data set provided as the argument <tt>data</tt> in <tt>glm()</tt>.</p></li>
<li><p>You need to specify the correct type of prediction in <tt>predict()</tt>.</p></li>
</ul>
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-AER">
<p>Kleiber, C., &amp; Zeileis, A. (2017). AER: Applied Econometrics with R (Version 1.2-5). Retrieved from <a href="https://CRAN.R-project.org/package=AER" class="uri">https://CRAN.R-project.org/package=AER</a></p>
</div>
<div id="ref-R-stargazer">
<p>Hlavac, M. (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables (Version 5.2.2). Retrieved from <a href="https://CRAN.R-project.org/package=stargazer" class="uri">https://CRAN.R-project.org/package=stargazer</a></p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="8">
<li id="fn8"><p>This is in contrast to the case of a numeric dependent variable where we use the squared errors for assessment of the quality of the prediction.<a href="rwabdv.html#fnref8">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="rwpd.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ivr.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": true,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 1
},
"edit": null,
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
