# Instrumental Variables

This chapter builds upon Chapter 12 in @Hansen2022. 


## Introduction

The concepts of **endogeneity** and **instrumental variable** are fundamental to econometrics, and mark a substantial departure from other branches of statistics. 

The ideas of endogeneity arise naturally in economics from models of simultaneous equations, most notably the classic supply/demand model of price determination.

<!-- ### Overview {-} -->

We say that there is **endogeneity** in the linear model
$$
\begin{align}
  Y_i=X_i'\beta+\varepsilon_i
\end{align}
$${#eq-IVLinMod}
if $\beta$ is the parameter of interest and if $E(\varepsilon_i|X_i)\neq 0$ and thus
$$
E(X_i\varepsilon_i)\neq 0.
$${#eq-endogen}
When @eq-endogen holds, $X$ is **endogenous** for $\beta.$

This situation constitutes a core problem in econometrics which is not so much in the focus of the statistics literature. 


Equation @eq-IVLinMod is called a **structural equation** and $\beta$ a **structural parameter** and it is important to distinguish @eq-IVLinMod from the regression/projection models we considered so far. It may be the case that a structural model coincides with a regression/projector model, but this is not necessarily the case.


Endogeneity **cannot** happen if the coefficient is defined by a linear projection. We can *define*[^1] the linear (population) projection coefficient 
$$
\beta^*=E(X_iX_i')^{-1}E(X_iY)
$$
and the corresponding linear (population) projection equation
$$
Y_i = X_i'\beta^* + \varepsilon_i^*,
$$ 
where then, by construction (projection properties), 
$$
E(X_i\varepsilon_i^*)=0
$$

[^1]: Caution, here we simply define $\beta^*$ as $\beta^*=E(X_iX_i')^{-1}E(X_iY)$ which results the population projection coefficient for the population projection of $Y$ into the space spanned by $X_i$. We did not derive the expression for $\beta^*$ using the exogeneity assumption as we did in @sec-MMEstimator; indeed, the exogeneity assumption may be violated.


The (population) projection coefficient $\beta^*$ and the structural parameter $\beta$ coincide ($\beta^*=\beta$) under exogeneity. However, under endogeneity (@eq-endogen) the projection coefficient $\beta^*$ does not equal the structural parameter
$$
\begin{align*}
\beta^* & =E(X_iX_i')^{-1}E(X_iY)\\
\beta^* & =E(X_iX_i')^{-1}E(X_i (X_i'\beta + \varepsilon_i) )\\
%\beta^* & =E(X_iX_i')^{-1}E(X_i X_i') \beta + 
%            E(X_iX_i')^{-1}E(X_i\varepsilon_i)\\
\beta^* & = \beta + E(X_iX_i')^{-1}\underbrace{E(X_i \varepsilon_i)}_{\neq 0}\neq \beta\\
\end{align*}
$$


Thus under endogeneity we cannot simply use the projection coefficient to derive an estimator since the projection coefficient does not identify the structural parameter of interest. 

That is, endogeneity implies that the least squares estimator is inconsistent for the structural parameter. Indeed, under i.i.d. sampling, least squares is consistent for the projection coefficient.
$$
\hat\beta\to_pE(X_iX_i')^{-1}E(X_iY) = \beta^* \neq \beta.
$$

But since the structural parameter $\beta$ is here the parameter of interest, endogeneity requires the development of alternative estimation methods. 


### Examples for Structural Models 


::: {#exm-MeasurementError}

## Measurement error in the regressor 

<br>

Suppose that

* $(Y_i,Z_i)$ are joint random variables 
* $E(Y_i|Z_i)=Z_i'\beta$
* $\beta$ is the structural parameter
* $Y_i  = Z_i'\beta + \varepsilon_i$ is the structural equation for which our model assumptions of @sec-LinModAssumptions can be justified.

Unfortunately, $Z_i$ is not observed (latent), instead we observe a noisy version of $Z_i$
$$
X_i=Z_i + u_i,
$$
where $u_i$ is a $(K\times 1)$ dimensional **classic measurement error**, i.e.

* $u_i$ is independent of all other stochastic quantities in the model
* $E(u_i)=0$

such that $X_i$ is a noisy but unbiased measure of $Z_i.$

It's easy to express $Y_i$ as a function of the observable $X_i$
$$
\begin{align*}
Y_i 
& = Z_i'\beta + \varepsilon_i \\ 
& = (X_i - u_i)'\beta + \varepsilon_i \\ 
& = X_i'\beta  + v,\\ 
\end{align*}
$$
where $v_i=\varepsilon_i - u_i'\beta$. 

That is, the relationship of $Y_i$ and $X_i$ is described by a linear equation
$$
Y_i = X_i'\beta  + v_i,
$$
with a stochastic error term $v_i$, but this error term is not a projection error since
$$
\begin{align*}
E(X_iv_i) 
& = E((Z_i + u_i)\,(\varepsilon_i - u_i'\beta)) \\
& = E(Z_i \varepsilon_i) - E(Z_iu_i'\beta)  + E(u_i \varepsilon_i) - E(u_iu_i')\beta \\
& = 0 - 0  + 0 - E(u_iu_i')\beta \neq 0 
\end{align*}
$$
if $\beta\neq 0$ and $E(u_iu_i')\neq 0.$ 

Note: Generally $\beta$ will not equal zero and the $(K\times K)$ variance-covariance matrix $E(u_iu_i')$ will also not equal zero since this would mean no variances in the measurement errors.


We can, however, calculate the form of the (population) projection coefficient $\beta^*$ (which can be consistently estimated): 
$$
\beta^* = \beta + E(X_iX_i')^{-1} E(X_i v_i).
$$
In the special case where $K=1$, we have
$$
\begin{align}
\beta^*_1 
& = \beta_1 + \frac{E(X_i v_i)}{E(X_i^2)}\\
& = \beta_1 - \frac{E(u_i^2)\beta_1}{E(X_i^2)}\\
& = \beta_1\left(1 - \frac{ E(u_i^2)}{E(X_i^2)}\right),
\end{align}
$${#eq-measurementBias}
where $E(X_iv_i)=-E(u_i^2)\beta_1$, and 
where the inequality follows from observing that $E(u_i^2)/E(X_i^2)<1$ since
$$
\begin{align*}
E(X_i^2) 
&= E((Z_i + u_i)^2)\\
&= E(Z_i^2) + 2E(Z_iu_i) + E(u_i^2) \\
&= E(Z_i^2) + 0 + E(u_i^2)\\  
&> E(u_i^2).
\end{align*}
$$
@eq-measurementBias shows that projection coefficient $\beta_1^*$ shrinks the structural parameter $\beta_1$ towards zero. This is called  **measurement error bias** or **attenuation bias**.
:::


::: {#exm-SupplyAndDemand}

##  Supply and Demand

<br>


The variables $Q$ and $P$ (Quantity and Price) are determined jointly by the following simultaneous equation system consisting of the demand equation
$$
Q_i = -\beta_1 P_i + \varepsilon_{1i}
$$
and of the supply equation
$$
Q_i =  \beta_2 P_i + \varepsilon_{2i},
$$
where we assume that $Q_i$ and $P_i$ are centered such that we can drop the intercepts. 

Assume that the $(2\times 1)$ dimensional joint random error $\varepsilon_i=(\varepsilon_{1i},\varepsilon_{2i})'$ satisfies $E(\varepsilon)=0$ and $E(\varepsilon\varepsilon')=I_2$ (the latter for simplicity). 

> What happens if we regress $Q_i$ on $P_i$? 

To answer this question, we need to solve $Q_i$ and $P_i$ in terms of the errors $\varepsilon_{1i}$ and $\varepsilon_{2i}$.
$$
\left(\begin{matrix}
1 & \beta_1\\
1 & -\beta_2\\
\end{matrix}
\right)
\left(\begin{matrix}
Q_i\\
P_i\\
\end{matrix}
\right) = 
\left(\begin{matrix}
e_{1i}\\
e_{2i}\\
\end{matrix}
\right)
$$
Rewriting yields (assuming invertibility)
$$
\begin{align}
\left(\begin{matrix}
Q_i\\
P_i\\
\end{matrix}
\right) 
&= 
\left(\begin{matrix}
1 & \beta_1\\
1 & -\beta_2\\
\end{matrix}
\right)^{-1}
\left(\begin{matrix}
e_{1i}\\
e_{2i}\\
\end{matrix}
\right)\\[2ex]
&= 
\frac{1}{-\beta_2 - \beta_1}
\left(\begin{matrix}
-\beta_2 &   -\beta_1\\
 - 1    &           1\\
\end{matrix}
\right)
\left(\begin{matrix}
e_{1i}\\
e_{2i}\\
\end{matrix}
\right)\\[2ex]
&= 
\left(\begin{matrix}
\beta_2 &   \beta_1\\
 1    &     -1\\
\end{matrix}\right)
\left(\begin{matrix}
e_{1i}\\
e_{2i}\\
\end{matrix}\right)
\frac{1}{\beta_1 +\beta_2}\\[2ex]
&=
\left(
\begin{matrix}
(\beta_2 e_{1i} + \beta_1  e_{2i})/(\beta_1 + \beta_2)\\
(e_{1i} - e_{2i})/(\beta_1 + \beta_2)
\end{matrix}  
\right)
\end{align}
$$

:::



## References