# Multiple Linear Regression {#sec-MLR}

In the following we focus on case of random designs $X$ (i.e. $X$ being a random variable), 
since, first, this is the more relevant case in econometrics and, second, it includes the case of 
fixed designs (i.e. $X$ being deterministic) as a special case ("degenerated random variable"). 
Caution: A random $X$ requires us to consider conditional means and variances "given $X$." 
That is, if we would be able to resample from the model, we do so by fixing (conditioning on) 
the in-principle random explanatory variable $X$. 


## Assumptions

The multiple linear regression model is defined by the following assumptions which together describe the relevant theoretical aspects of the underlying data generating process:

**Assumption 1: Model and Sampling**

**Part (a): Linear Model**

$$
\begin{align}
  Y_i=\sum_{k=1}^K\beta_k X_{ik}+\varepsilon_i, \quad i=1,\dots,n.
\end{align}
$${#eq-LinMod}
Usually, a constant (intercept) is included, in this case $X_{i1}=1$ for all $i$. 
In the following we will always assume that $X_{i1}=1$ for all $i$, unless otherwise stated.   
It is convenient to write @eq-LinMod using matrix notation
\begin{eqnarray*}
  Y_i&=&\underset{(1\times K)}{X_i'}\underset{(K\times 1)}{\beta} +\varepsilon_i, \quad i=1,\dots,n,
\end{eqnarray*}
where $X_i=(X_{i1},\dots,X_{iK})'$ and $\beta=(\beta_1,\dots,\beta_K)'$. Stacking all individual rows $i$ leads to
\begin{eqnarray*}\label{LM}
  \underset{(n\times 1)}{Y}&=&\underset{(n\times K)}{X}\underset{(K\times 1)}{\beta} + \underset{(n\times 1)}{\varepsilon},
\end{eqnarray*}
where 
\begin{equation*}
Y=\left(\begin{matrix}Y_1\\ \vdots\\Y_n\end{matrix}\right),\quad X=\left(\begin{matrix}X_{11}&\dots&X_{1K}\\\vdots&\ddots&\vdots\\ X_{n1}&\dots&X_{nK}\\\end{matrix}\right),\quad\text{and}\quad \varepsilon=\left(\begin{matrix}\varepsilon_1\\ \vdots\\ \varepsilon_n\end{matrix}\right).
\end{equation*}


**Part (b): Random Sample**

Moreover, we assume that the observed ("obs") data points 
$$
((Y_{1,obs},X_{11,obs},\dots,X_{1K,obs}),(Y_{2,obs},X_{21,obs},\dots,X_{2K,obs}),\dots,(Y_{n,obs},X_{n1,obs},\dots,X_{nK,obs}))
$$ 
are realizations of a random sample 
$$
((Y_{1},X_{11},\dots,X_{1K}),(Y_{2},X_{21},\dots,X_{2K}),\dots,(Y_{n},X_{n1},\dots,X_{nK})).
$$ 

That is, the $i$th observed $K+1$ dimensional data point 
$(Y_{i,obs},X_{i1,obs},\dots,X_{iK,obs})\in\mathbb{R}^{K+1}$ 
is a realization of a $K+1$ dimensional random variable 
$(Y_{i},X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1},$ 
where 

1. $(Y_{i},X_{i1},\dots,X_{iK})$ has the identical joint distribution as 
$(Y_{j},X_{j1},\dots,X_{jK})$ for all $i=1,\dots,n$ and all $j=1,\dots,n$, and where 
2. $(Y_{i},X_{i1},\dots,X_{iK})$ is independent of 
$(Y_{j},X_{j1},\dots,X_{jK})$ for all $i\neq j=1,\dots,n.$


Note: Due to @eq-LinMod, this i.i.d. assumption is equivalent to assuming that the multivariate 
random variables $(\varepsilon_i,X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1}$ are i.i.d. across 
$i=1,\dots,n$. 

**Remark:** Usually, we do not use a different notation for observed realizations 
$(Y_{i,obs},X_{i1,obs},\dots,X_{iK,obs})\in\mathbb{R}^{K+1}$ 
and for the corresponding random variable 
$(Y_{i},X_{i1},\dots,X_{iK})\in\mathbb{R}^{K+1}$ 
since often both interpretations (random variable and its realizations) can make sense 
in the same statement and then it depends on the considered context whether the random variables 
point if view or the realization point of view applies.


<!--  That is, the multivariate distribution of $(\varepsilon_i,X_{i1},\dots,X_{iK})$ is assumed equal 
across $i=1,\dots,n$, but the multivariate random variables $(\varepsilon_i,X_{i1},\dots,X_{iK})$ 
and $(\varepsilon_j,X_{j1},\dots,X_{jK})$ are independent for all $i\neq j$. -->




<!-- **Note.:** Of course, in principle, Assumption 1 can be violated; however, in classic econometrics one usually does not bother with a possibly violated model assumption (Assumption 1). In modern econometrics, this is a big deal.  -->

<!-- The following assumptions are the so-called *classic assumptions* and we will be often concerned about violations of these assumptions.  -->


**Assumption 2: Exogeneity**
$$E(\varepsilon_i|X_i)=0,\quad i=1,\dots,n$$
This assumption demands that the mean of the random error term $\varepsilon_i$ is zero irrespective of the realizations of $X_i$.  


This exogeneity assumption is also called "orthogonality assumption" or "mean independence assumption."

**Note:** Together with the random sample assumption (Assumption 1, Part (b)) this assumption implies even **strict** exogeneity $E(\varepsilon_i|X) = 0$ since we have independence across $i=1,\dots,n$.  Under strict exogeneity, the mean of $\varepsilon_i$ is zero irrespective of the realizations of $X_1,\dots,X_n$. 

<!-- For one example, it cannot be fulfilled when the regressors include lagged dependent variables. -->
<!-- Notice that in the presence of a constant explanatory variable, setting the expectation to zero is a normalization.  -->


**Assumption 3: Rank Condition (no perfect multicollinearity)**

$$\operatorname{rank}(X)=K\quad\text{a.s.}$$
This assumption demands that the event of one explanatory variable being linearly dependent on the others occurs with a probability equal to zero. (This is the literal translation of the "almost surely (a.s.)" concept.) The assumption implies that $n\geq K$. 

This assumption is a bit dicey and its violation belongs to one of the classic problems in applied econometrics (keywords: dummy variable trap, multicollinearity, variance inflation).  The violation of this assumption harms any economic interpretation as we cannot disentangle the explanatory variables' individual effects on $Y$. Therefore, this assumption is also often called an "identification assumption."


**Assumption 4: Error distribution** 

Depending on the context (i.e., parameter estimation vs. hypothesis testing and small $n$ vs. large $n$) there are different more or less restrictive assumptions.  Some of the most common ones are the following (roughly order from least to most restrictive):

- **Conditional Distribution:** $\varepsilon_i|X_i \sim f_{\varepsilon|X}$ for all $i=1,\dots,n$ and for any distribution $f_{\varepsilon|X}$ such that $\varepsilon_i|X_i$ has two (or more) finite moments.
- **Conditional Normal Distribution:** $\varepsilon_i|X_i \sim \mathcal{N}(0,\sigma^2(X_i))$ for all $i=1,\dots,n$.
- **Independence between error and predictors:** $\varepsilon_i\sim f_\varepsilon$ such that $f_\varepsilon=f_{\varepsilon|X}$ and such that $f_\varepsilon$ has two (or more) finite moments.  
- **Independence between error and predictors and normal:** As above, but with $f_\varepsilon=\mathcal{N}(0,\sigma^2)$. 
- **Spherical errors ("Gauss-Markov assumptions"):** The conditional distributions of  $\varepsilon_i|X_i$ may generally depend on $X_i$, but such that
\begin{align*}
E(\varepsilon_i^2|X_i)         &=\sigma^2>0\quad\text{for all }i=1,\dots,n\\
E(\varepsilon_i\varepsilon_j|X)&=0\quad\text{for all }i\neq j\quad\text{with}\quad i=1,\dots,n\quad\text{and}\quad j=1,\dots,n.
\end{align*}
Thus, here one assumes that, for a given realization of $X_i$, the error process is uncorrelated (i.e. $Cov(\varepsilon_i,\varepsilon_j|X)=E(\varepsilon_i\varepsilon_j|X)=0$ for all $i\neq j$) and homoskedastic (i.e. $Var(\varepsilon_i|X)=\sigma^2$ for all $i$).



<!-- **Technical Note:** When we write that $Var(\varepsilon_i|X)=\sigma^2$ or $Var(\varepsilon_i|X_i)=\sigma^2_i,$ we implicitly assume that these second moments exists and that they are finite.  -->


#### Homoskedastic versus Heteroskedastic Error Terms {-}


The i.i.d. assumption is not as restrictive as it may seem on first sight. It allows for dependence between $\varepsilon_i$ and $(X_{i1},\dots,X_{iK})\in\mathbb{R}^K$. That is, the error term $\varepsilon_i$ can have a conditional distribution which depends on $(X_{i1},\dots,X_{iK})$; see @sec-condDistr. 

<!-- However, we need to rule out one certain 
dependency between $\varepsilon_i$ and $X_i$; namely the conditional mean of $\varepsilon_i$ is not 
allowed to depend on $X_i$ (see Assumption 2: Exogeneity). -->

<!-- Example: $\varepsilon|X_i\sim U[-0.5|X_{i2}+X_{i3}|, 0.5|X_{i2}+X_{i3}|]$, 
with $X_{i2}\sim U[-4,4]$. This error term is mean independent of $X_i$ since 
$E(\varepsilon_i|X_i)=0$, but it has heteroskedastic conditional variance 
$Var(\varepsilon_i|X_i)=\frac{1}{12}X_i^2$. -->


The exogeneity assumption (Assumption 2: Exogeneity) requires that the conditional mean of $\varepsilon_i$ is independent of $X_i$. Besides this, dependencies between $\varepsilon_i$ and $X_{i1},\dots,X_{iK}$ are allowed. For instance, the variance of $\varepsilon_i$ can be a function of $X_{i1},\dots,X_{iK}$. If this is the case, $\varepsilon_i$ is said to be "heteroskedastic." 

- **Heteroskedastic error terms:** The conditional variances $Var(\varepsilon_i|X_i=x_i)=\sigma^2(x_i)$ are equal to a non-constant variance-function $\sigma^2(x_i)>0$ which is a function of the realization $X_i=x_i.$ 

**Example:** $\varepsilon_i|X_i\sim U[-0.5|X_{i2}|, 0.5|X_{i2}|],$ with $X_{i2}\sim U[-4,4]$. This error term is mean independent of $X_i$ since $E(\varepsilon_i|X_i)=0$, but it has a heteroskedastic conditional variance since $Var(\varepsilon_i|X_i)=\frac{1}{12}X_i^2$ depends on $X_i$. 



Sometimes, we need to be more restrictive by assuming that also the variances of the error terms $\varepsilon_i$ are independent from $X_i$. (Higher moments may still depend on $X_i$.) This assumption leads to "homoskedastic" error terms. 


- **Homoskedastic error terms:** The conditional variances $Var(\varepsilon_i|X_i=x_i)=\sigma^2$ are equal to some constant $\sigma^2>0$ for every possible realization $X_i=x_i.$ 

<!-- Sometimes, we need to be even more restrictive by assuming that the error terms $\varepsilon_i$  -->
<!-- are themselves **i.i.d.** across $i=1,\dots,n$. This is more restrictive than the assumption that the  -->
<!-- multivariate random variables $(\varepsilon_i,X_{i1},\dots,X_{iK})$ are i.i.d. across $i=1,\dots,n$  -->
<!-- since it implies that the whole distribution (not only the first two moments) of $\varepsilon_i$  -->
<!-- does not depend on $X_i$. This assumption also implies  **homoskedastic** error terms since when  -->
<!-- the whole distribution of $\varepsilon_i$ does not depend on $X_i\in\mathbb{R}^K$ also its variance  -->
<!-- doesn't depend on $X_i$.  -->

**Example:** For doing small sample inference (see @sec-ssinf), we need to assume that the error terms $\varepsilon_i$ are i.i.d. across $i=1,\dots,n$ plus the normality assumption, i.e., $\varepsilon_i\stackrel{\textrm{i.i.d.}}{\sim}{\mathcal N} (0, \sigma^2)$ for all $i=1,\dots,n$ which leads to homoskedastic variances $Var(\varepsilon_i|X_i)=\sigma^2$ for every possible realization of $X_i$. 



### Some Implications of the Exogeneity Assumption 

::: {#thm-a}

## Unconditional Mean

If $E(\varepsilon_i|X_i)=0$ for all $i=1,\dots,n$, then the also the unconditional mean of the error term is zero, i.e.
$$
E(\varepsilon_i)=0,\quad i=1,\dots,n.
$$

:::


::: {.proof}
Using the *Law of Total Expectations* (i.e., $E[E(Z|X)]=E(Z)$) we can rewrite $E(\varepsilon_i)$ as
$$
E(\varepsilon_i)=E[E(\varepsilon_i|X_i)]
$$
for all $i=1,\dots,n.$ But the exogeneity assumption yields
$$
E[E(\varepsilon_i|X_i)]=E[0]=0
$$
for all $i=1,\dots,n,$ which completes the proof. $\square$
:::


Generally, two random variables $X$ and $Y$ are said to be **orthogonal** if their cross moment is zero, i.e. if $E(XY)=0$. Exogeneity is sometimes also called "orthogonality," due to the following result.  

::: {#thm-b}

## Orthogonality

Under exogeneity, i.e. if $E(\varepsilon_i|X_{i})=0$, the regressors and the error term are orthogonal to each other, i.e,
$$
E(X_{ik}\varepsilon_i)=0
$$
for all $i=1,\dots,n$ and $k=1,\dots,K$.
:::

::: {.proof}
\begin{align*}
E(X_{ik}\varepsilon_i)
&=E(E(X_{ik}\varepsilon_i|X_{ik}))\quad{\small\text{(By the Law of Total Expectations)}}\\
&=E(X_{ik}E(\varepsilon_i|X_{ik}))\quad{\small\text{(By the linearity of cond.~expectations)}}
\end{align*}
Now, to show that $E(X_{ik}\varepsilon_i)=0$, we need to show that $E(\varepsilon_i|X_{ik})=0,$ which is done in the following:

Since $X_{ik}$ is an element of $X_i,$ a slightly more sophisticated use of the *Law of Total Expectations* (i.e., $E(Y|X)=E(E(Y|X,Z)|X)$) implies that
$$
E(\varepsilon_i|X_{ik})=E(E(\varepsilon_i|X_i)|X_{ik}).
$$
So, the exogeneity assumption, $E(\varepsilon_i|X_i)=0$ yields
$$
E(\varepsilon_i|X_{ik})=E(\underbrace{E(\varepsilon_i|X_i)}_{=0}|X_{ik})=E(0|X_{ik})=0.
$$
I.e., we have that $E(\varepsilon_i|X_{ik})=0$ which allows us to conclude that
$$
E(X_{ik}\varepsilon_i)=E(X_{ik}E(\varepsilon_i|X_{ik}))=E(X_{ik}0)=0
$$
which completes the proof. $\square$
:::


Because the mean of the error term is zero ($E(\varepsilon_i)=0$ for all $i$ (see @thm-a), it follows that the orthogonality property, $E(X_{ik}\varepsilon_i)=0,$ is equivalent to a zero correlation property. 

::: {#thm-c}

## No Correlation

If $E(\varepsilon_i|X_{i})=0$, then 
\begin{eqnarray*}
  Cov(\varepsilon_i,X_{ik})&=&0\quad\text{for all}\quad i=1,\dots,n\quad\text{and}\quad k=1,\dots,K.
\end{eqnarray*}
:::

::: {.proof}
\begin{eqnarray*}
  Cov(\varepsilon_i,X_{ik})&=&E(X_{ik}\varepsilon_i)-E(X_{ik})\,E(\varepsilon_i)\quad{\small\text{(Def. of Cov)}}\\
  &=&E(X_{ik}\varepsilon_i)\quad{\small\text{(By point (a): $E(\varepsilon_i)=0$)}}\\
  &=&0\quad{\small\text{(By orthogonality result in point (b))}}\quad\square
\end{eqnarray*}
:::

<!-- ## The Algebra of Least Squares -->

<!-- In this section, we take the point of view where $(Y^{obs},X^{obs})$ is an observed data-set, i.e., a given realization of the random sample $((Y_1,X_1),\dots,(Y_n,X_n))$. That is, we are not yet interested in the statistical properties of estimators, but are only in doing some linear algebra.  -->




## Deriving the Expression of the OLS Estimator

We derive the expression for the OLS estimator $\hat\beta=(\hat\beta_1,\dots,\hat\beta_K)'\in\mathbb{R}^K$ as the vector-valued minimizing argument of the sum of squared residuals, $S_n(b)$ with $b\in\mathbb{R}^K$, for a given sample $((Y_1,X_1),\dots,(Y_n,X_n))$.  In matrix terms this is
\begin{align*}
S_n(b)&=(Y-X b)^{\prime}(Y-X b)=Y^{\prime}Y-2 Y^{\prime} X b+b^{\prime} X^{\prime} X b.
\end{align*}
To find the minimizing argument $$\hat\beta=\arg\min_{b\in\mathbb{R}^K}S_n(b)$$ we compute all partial derivatives
$$
\begin{aligned}
\underset{(K\times 1)}{\frac{\partial S(b)}{\partial b}} &=-2\left(X^{\prime}Y -X^{\prime} Xb\right).
\end{aligned}
$$
and set them equal to zero which leads to $K$ linear equations (the "normal equations") in $K$ unknowns. This system of equations defines the OLS estimates, $\hat{\beta}$, for a given data-set:
$$
\begin{aligned}
-2\left(X^{\prime}Y -X^{\prime} X\hat{\beta}\right)=\underset{(K\times 1)}{0}.
\end{aligned}
$$
From our rank assumption (Assumption 3) it follows that $X^{\prime}X$ is an invertible matrix which allows us to solve the equation system by 
$$
\begin{aligned}
\underset{(K\times 1)}{\hat{\beta}} &=\left(X^{\prime} X\right)^{-1} X^{\prime} Y
\end{aligned}
$$


The following codes computes the estimate $\hat{\beta}$ for a given realization $(Y,X)$ of the random sample $(Y,X)$. 

```{r, fig.align="center"}
# Some given data
X_1 <- c(1.9,0.8,1.1,0.1,-0.1,4.4,4.6,1.6,5.5,3.4)
X_2 <- c(66, 62, 64, 61, 63, 70, 68, 62, 68, 66)
Y   <- c(0.7,-1.0,-0.2,-1.2,-0.1,3.4,0.0,0.8,3.7,2.0)
dataset <-  cbind.data.frame(X_1,X_2,Y)
## Compute the OLS estimation
my.lm <- lm(Y ~ X_1 + X_2, data = dataset)
## Plot sample regression surface
library("scatterplot3d") # library for 3d plots
plot3d <- scatterplot3d(x = X_1, y = X_2, z = Y,
          angle=33, scale.y=0.8, pch=16,
          color ="red", main ="OLS Regression Surface")
plot3d$plane3d(my.lm, lty.box = "solid", col=gray(.5), 
          draw_polygon=TRUE)
```




## Some Quantities of Interest

**Predicted values and residuals.**

- The (OLS) **predicted values**: $\hat{Y}_i=X_i'\hat\beta$.  
In matrix notation: $\hat Y=X\underbrace{(X'X)^{-1}X'Y}_{\hat\beta}=P_X Y$
- The (OLS) **residuals**: $\hat\varepsilon_i=Y_i-\hat{Y}_i$. 
In matrix notation: $\hat\varepsilon=Y-\hat{Y}=\left(I_n-X(X'X)^{-1}X'\right)Y=M_X Y$


**Projection matrices.** 

The matrix 
$$
P_X=X(X'X)^{-1}X'
$$ 
is the $(n\times n)$ **projection matrix** that projects any vector from $\mathbb{R}^n$ into the column space spanned by the column vectors of $X$ and 
$$
M_X=I_n-X(X'X)^{-1}X'=I_n-P_X
$$ 
is the associated $(n\times n)$ **orthogonal projection matrix** that projects any vector from $\mathbb{R}^n$ into the vector space that is orthogonal to that spanned by $X$. 


The projection matrices $P_X$ and $M_X$ have some nice properties:

(a) $P_X$ and $M_X$ are **symmetric**, i.e. $P_X=P_X'$ and $M_X=M_X'$.
(b) $P_X$ and $M_X$ are **idempotent**, i.e. $P_XP_X=P_X$ and $M_X M_X=M_X$.
(c) Moreover, we have that $X'P_X=X'$, $P_XX=X$, $X'M_X=0$, $M_XX=0$, and $P_XM_X=0$.

All of these properties follow directly from the definitions of $P_X$ and $M_X$ (check it out).  Using these properties one can show that the residual vector $\hat\varepsilon=(\hat\varepsilon_1,\dots,\hat\varepsilon_n)'$ is orthogonal to each of the column vectors in $X$, i.e 
\begin{eqnarray}
X'\hat\varepsilon&=&X'M_XY\quad\text{\small(By Def.~of $M_X$)}\\
\Leftrightarrow X'\hat\varepsilon&=&\underset{(K\times n)}{0}\underset{(n\times 1)}{Y}\quad\text{\small(since $X'M_X=0$)}\\
\Leftrightarrow X'\hat\varepsilon&=&\underset{(K\times 1)}{0}
\end{eqnarray}
<!-- \begin{align} -->
<!-- X'\hat\varepsilon=\underset{(K\times 1)}{0}.\label{eq:orthXe} -->
<!-- \end{align} -->
Note that, in the case with intercept, the result $X'\hat\varepsilon=0$ implies that $\sum_{i=1}^n\hat\varepsilon_i=0$. Moreover, the equation $X'\hat\varepsilon=0$ implies also that the residual vector $\hat{\varepsilon}$ is orthogonal to the predicted values vector, since
\begin{align*}
X'\hat\varepsilon&=0\\
\Rightarrow\;\hat\beta'X'\hat\varepsilon&=\hat\beta'0\\
\Leftrightarrow\;\hat Y'\hat\varepsilon&=0.
\end{align*}

Another insight from equation $X'\hat\varepsilon=0$ is that the vector $\hat\varepsilon$ has to satisfy $K$ linear restrictions which means it looses $K$ degrees of freedom.[^1] Consequently, the vector of residuals $\hat\varepsilon$ has only $n-K$ so-called *degrees of freedom*.  This loss of $K$ degrees of freedom also appears in the definition of the *unbiased* variance estimator
\begin{align}
  s_{UB}^2&=\frac{1}{n-K}\sum_{i=1}^n\hat\varepsilon_i^2\label{EqVarEstim}.
\end{align}

[^1]: The $K$ linear restrictions follow from the fact that $X'\hat\varepsilon=0$ are $K$ equations $\sum_{i=1}^nX_{ik}\hat\varepsilon_i=0$ for $k=1,\dots,K$.  

**Variance decomposition:** A further useful result that can be shown using the properties of $P_X$ and $M_X$ is that $Y'Y=\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon$, i.e.
\begin{eqnarray*}
Y'Y&=&(\hat Y+\hat\varepsilon)'(\hat Y+\hat\varepsilon)\notag\\
  &=&(P_XY+M_XY)'(P_XY+M_XY)\notag\\
  &=&(Y'P_X'+Y'M_X')(P_XY+M_XY)\notag\\
  &=&Y'P_X'P_XY+Y'M_X'M_XY+0\notag\\
  &=&\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon
\end{eqnarray*}
The decomposition 
$$
\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon
$$
is the basis for the well-known variance decomposition result for OLS regressions. 

::: {#thm-vardecomp}
For the linear regression model with intercept (@eq-LinMod), the total sample variance of the dependent variable $Y_1,\dots,Y_n$ can be decomposed as following:
\begin{eqnarray}
\underset{\text{total sample variance}}{\frac{1}{n}\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}&=&\underset{\text{explained sample variance}}{\frac{1}{n}\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}+\underset{\text{unexplained sample variance}}{\frac{1}{n}\sum_{i=1}^n\hat\varepsilon_i^2,}\label{VarDecomp}
\end{eqnarray}
where $\bar{Y}=\frac{1}{n}\sum_{i=1}^nY_i$ and $\bar{\hat{Y}}=\frac{1}{n}\sum_{i=1}^n\hat{Y}_i$. 
:::

::: {.proof}
From equation $X'\hat\varepsilon=0$ we have for regressions with intercept that $\sum_{i=1}^n\hat\varepsilon_i=0$. Hence, from $Y_i=\hat{Y}_i+\hat\varepsilon_i$ it follows that
\begin{eqnarray*}
  \frac{1}{n}\sum_{i=1}^n Y_i&=&\frac{1}{n}\sum_{i=1}^n \hat{Y}_i+\frac{1}{n}\sum_{i=1}^n \hat\varepsilon_i\\
  \bar{Y}&=&\bar{\hat{Y}}+0
\end{eqnarray*}

Using the decomposition $Y'Y=\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon$, we can now derive the result:
\begin{eqnarray*}
   Y'Y&=&\hat{Y}'\hat{Y}+\hat\varepsilon'\hat\varepsilon\\
   Y'Y-n\bar{Y}^2&=&\hat{Y}'\hat{Y}-n\bar{Y}^2+\hat\varepsilon'\hat\varepsilon\\
   Y'Y-n\bar{Y}^2&=&\hat{Y}'\hat{Y}-n\bar{\hat{Y}}^2+\hat\varepsilon'\hat\varepsilon\quad\text{(by $\bar{Y}=\bar{\hat{Y}}$)}\\
   \sum_{i=1}^nY_i^2-n\bar{Y}^2&=&\sum_{i=1}^n\hat{Y}_i^2-n\bar{\hat{Y}}^2+\sum_{i=1}^n\hat\varepsilon_i^2\\
   \sum_{i=1}^n(Y_i-\bar{Y})^2&=&\sum_{i=1}^n(\hat{Y}_i-\bar{\hat{Y}})^2+\sum_{i=1}^n\hat\varepsilon_i^2\quad\square\\
\end{eqnarray*}
:::


#### Coefficients of determination: $R^2$ and $\overline{R}^2$ {-}

The larger the proportion of the explained variance, the better is the fit of the model. This motivates the definition of the so-called $R^2$ coefficient of determination:
\begin{eqnarray*}
R^2=\frac{\sum_{i=1}^n\left(\hat{Y}_i-\bar{\hat{Y}}\right)^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\;=\;1-\frac{\sum_{i=1}^n\hat{\varepsilon}_i^2}{\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}
\end{eqnarray*}
Obviously, we have that $0\leq R^2\leq 1$. The closer $R^2$ lies to $1$, the better is the fit of the model to the observed data. However, a high/low $R^2$ does not mean a validation/falsification of the estimated model. Any relation (i.e., model assumption) needs a plausible explanation from relevant economic theory.  The most often criticized disadvantage of the $R^2$ is that additional regressors (relevant or not) will always increase the $R^2$. Here is an example of the problem.
```{r}
set.seed(123)
n     <- 100                  # Sample size
X     <- runif(n, 0, 10)      # Relevant X variable
X_ir  <- runif(n, 5, 20)      # Irrelevant X variable
error <- rt(n, df = 10)*10    # True error
Y     <- 1 + 5 * X + error    # Y variable
lm1   <- summary(lm(Y~X))     # Correct OLS regression 
lm2   <- summary(lm(Y~X+X_ir))# OLS regression with X_ir 
lm1$r.squared < lm2$r.squared
```
So, $R^2$ increases here even though `X_ir` is a completely irrelevant explanatory variable.  Because of this, the $R^2$ cannot be used as a criterion for model selection. Possible solutions are given by penalized criterions such as the so-called **adjusted** $R^2$, $\overline{R}^2,$ defined as
\begin{eqnarray*}
  \overline{R}^2&=&1-\frac{\frac{1}{n-K}\sum_{i=1}^n\hat{\varepsilon}^2_i}{\frac{1}{n-1}\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2}\leq R^2%\\
  %=\dots=
  %&=&1-\frac{n-1}{n-K}\left(1-R^2\right)\quad{\small\text{(since $1-R^2=(\sum_i\hat\varepsilon_i^2)/(\sum_i(Y_i-\bar{Y}))$)}}\\
  %&=&1-\frac{n-1}{n-K}+\frac{n-1}{n-K}R^2\quad+\frac{K-1}{n-K}R^2-\frac{K-1}{n-K}R^2\\
  %&=&1-\frac{n-1}{n-K}+R^2\quad+\frac{K-1}{n-K}R^2\\
  %&=&-\frac{K-1}{n-K}+R^2\quad+\frac{K-1}{n-K}R^2\\
  %&=&R^2-\underbrace{\frac{K-1}{n-K}\left(1-R^2\right)}_{\geq 0\;\text{and}\;\leq(K-1)/(n-K)}\;\leq\;R^2
\end{eqnarray*}
The adjustment is in terms of the degrees of freedom $n-K$.


## Method of Moments Estimator

Remember that the exogeneity assumption (Assumption 2), $E(\varepsilon_i|X_i)=0,$ implies that $E(X_{ik}\varepsilon_i)=0$ for all $k=1,\dots,K$. Thus, the exogeneity assumption gives us a system of $K$ linear equations:
$$
\left.
  \begin{array}{c}
  E(\varepsilon_i)=0\\
  E(X_{i2}\varepsilon_i)=0\\
  \vdots\\
  E(X_{iK}\varepsilon_i)=0
  \end{array}
\right\}\Leftrightarrow \underset{(K\times 1)}{E(X_i\varepsilon_i)}=\underset{(K\times 1)}{0}
$$

The linear equation system
$$
\underset{(K\times 1)}{E(X_i\varepsilon_i)}=\underset{(K\times 1)}{0}
$$ 
allows us to identify the unknown parameter vector $\beta\in\mathbb{R}^K$ in terms of population moments:
$$
\begin{align*}
E(X_i(\overbrace{Y_i - X_i'\beta}^{=\varepsilon_i})) &= 0\\
%\Leftrightarrow \hspace{1.5cm}
E(X_iY_i) - E(X_iX_i')\beta & = 0\\
E(X_iX_i')\beta & =  E(X_iY_i)  \\
\beta & = \left(E(X_iX_i')\right)^{-1} E(X_iY_i)  \\
\end{align*}
$$
The fundamental idea behind "method of moments estimation" is to define an estimator by substituting population moments by sample moment analogues (sample means):
$$
\begin{align*}
\hat\beta_{mm} 
& = \left(\frac{1}{n}\sum_{i=1}^n X_iX_i'\right)^{-1} \frac{1}{n}\sum_{i=1}^n X_iY_i,
\end{align*}
$$
where $\hat\beta_{mm}$ can be simplified as following:
$$
\begin{align*}
\hat\beta_{mm} 
& = \left(\frac{1}{n}\sum_{i=1}^n X_iX_i'\right)^{-1} \frac{1}{n}\sum_{i=1}^n X_iY_i  \\
& = \left(\sum_{i=1}^n X_iX_i'\right)^{-1} \sum_{i=1}^n X_iY_i\\ 
& = \left(X'X\right)^{-1} X'Y.\\ 
\end{align*}
$$

Thus the method of moments estimator, $\hat\beta_{mm},$ coincides with the OLS estimator.





## Practice


### Factor Variables and the Dummy-Variable Trap

In the following, we consider simple linear regression model that aims to predict wages in the year 2008 using gender as the only predictor. We use data provided in the accompanying materials of Stock and Watson's *Introduction to Econometrics* textbook [@stock2015]. You can download the data stored as an xlsx-file `cps_ch3.xlsx` [HERE](https://github.com/lidom/Script-Econometrics-MSc/blob/main/85467d963958861b842aa28b9eb262b1ea250101/data/cps92_12.xlsx). 


Let us first prepare the dataset:
```{r}
## load the 'tidyverse' package
suppressPackageStartupMessages(library("tidyverse"))
## load the 'readxl' package
library("readxl")

## import the data into R
cps <- read_excel(path = "data/cps_ch3.xlsx")

# names(cps)
# range(cps$year)
# range(cps$a_sex) # 1 = male, 2 = female

cps_2008 <- cps %>% 
  mutate(
    wage   = ahe08,
    gender = fct_recode(as_factor(a_sex), "male" = "1", "female" = "2") 
  ) %>%  
  filter(year == 2008) %>%
  select(wage, gender) 
```

The first six lines of the dataset look as following:
```{r, echo=FALSE}
suppressPackageStartupMessages(library("kableExtra"))
head(cps_2008) %>% kbl() %>%  kable_styling()

```

Computing the estimation results:
```{r, echo=TRUE, eval=FALSE}
lm_obj <- lm(wage ~ gender, data = cps_2008)

coef(lm_obj)
```

```{r, echo=FALSE, eval=TRUE}
lm_obj <- lm(wage ~ gender, data = cps_2008)
```

gives

* $\hat\beta_1=$ `r round(coef(lm_obj)[1], 2)`
* $\hat\beta_2=$ `r round(coef(lm_obj)[2], 2)`

To compute these estimation results, one needs to assign a numeric 0/1 coding to the factor levels `male` and `female` of the factor variable `gender`. To see the numeric values used by `R` one can take a look at `model.matrix(lm_obj)`:
```{r}
X <- model.matrix(lm_obj) # this is the internally used X-matrix
X[1:6,]

cps_2008$gender[1:6]
```

Thus `R` internally codes `female` subjects by a 1 and `male` subjects by a 0, such that
$$
\hat\beta_1  + \hat\beta_2 X_{i,gender}=\left\{\begin{array}{ll}\hat\beta_1&\text{ if }X_{i,gender}=\text{male}\\\hat\beta_1 + \hat\beta_2&\text{ if }X_{i,gender}=\text{female}\end{array} \right.
$$

Interpretation: 

* The average wage of male workers in 2008 was $\hat{\beta}_1=$ `r round(coef(lm_obj)[1], 2)` (USD/Hour).
* The average wage of female workers in 2008 was $\hat{\beta}_1 + \hat{\beta}_2=$ `r round(sum(coef(lm_obj)), 2)` (USD/Hour).
* The difference in the earnings between male and female works in 2008 is $\hat{\beta}_2=$ `r round(coef(lm_obj)[2], 2)` (USD/Hour).


### Dummy-Variable Trap {-}

Above, we used `R`'s internal handling of factor variables which you should always do. If you construct a dummy variable for each of the levels of a factor yourself, however, you may blundered into the dummy variable trap:
```{r}
## Intercept variable
X_1      <- rep(1, times = nrow(cps_2008))

## 1. Dummy variable for 'female'
X_female <- ifelse(cps_2008$gender == "female", 1, 0)

## 2. Dummy variable for 'male'
X_male   <- ifelse(cps_2008$gender == "male", 1, 0)

## Construct the model matrix 'X'
X        <- cbind(X_1, X_female, X_male)
```

```{r, echo = FALSE, eval = TRUE}
## Dependent variable
Y        <- cps_2008$wage
```

Computing the estimation result for $\beta$ "by hand" yields

```{r, echo=TRUE, eval=FALSE}
## Dependent variable
Y        <- cps_2008$wage

## Computing the estimator
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% Y
```

```{r, echo=TRUE, eval=FALSE}
Error in solve.default(X %*% t(X)) : 
  Lapack routine dgesv: system is exactly singular: U[3,3] = 0
Calls: .main ... eval_with_user_handlers -> eval -> eval -> solve -> solve.default
Execution halted
```

<font color="#FF0000">An error message!</font> We blundered into the dummy variable trap 🤬 

The estimation result is not computable since $(X'X)$ is not invertible due to the perfect multicollinearity between the intercept and the two dummy variables `X_female` and `X_male` 
<center>
`X_1` = `X_female` $+$ `X_male` 
</center>
which violates Assumption 3 (no perfect multicollinearity). 


**Solution:** Use one dummy-variable less than factor levels. I.e., in this example you can either drop `X_female` or `X_male`. 

```{r}
## New model matrix after dropping X_male:
X        <- cbind(X_1, X_female)

## Computing the estimator
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% Y
beta_hat
```

This give the same result as computed by `R`'s `lm()` function using the factor variable `gender`.


### Detecting Heteroskedasticity

A very simple, but highly effective way to check for heteroskedastic error terms is to plot the residuals. If we have the correct model (hopefully we have), then (and only then) the residuals are good approximations to the realizations of the error terms $\varepsilon_i$. Thus a plot of the residuals can be used to check for heteroskedasticity. 

You can use `R`'s internal diagnostic plots that can be called using the `plot()` method for `lm`-objects:

```{r}
# install.packages("devtools")
# library(devtools)
# install_git("https://github.com/ccolonescu/PoEdata")

library(PoEdata) #for PoE4 datasets)

data("food")

lm_object <- lm(food_exp ~ income, data = food)

plot(lm_object, which = 1)
```

Interpretation: 

* The diagnostic plot indicates that the variance increases with income.

The plot shows the residuals $\hat{\varepsilon}_i$ plotted against the fitted values $\hat{Y}_i$. Plotting against the fitted values is actually a quite smart idea since this also works for multiple predictors $X_{i1},\dots,X_{iK}$ with $K\geq 3$. 



### Checking (Non-)Linearity of the Regression Line

To check whether there are non-linear relationships between the outcome and the predictors, on should take a look at the data, using a pairs-plot function that procures scatter plots for all pairs of variables in a dataset. 

::: {.panel-tabset}

## Base `R`

```{r}
car_data <- read.csv(file = "https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv")

my_car_df <- data.frame(
    "MPG"   = car_data$MPG.city,
    "HP"    = car_data$Horsepower
    )

pairs(my_car_df)
```


## Tidyverse `R`

```{r}
## install.packages("tidyverse")
## install.packages("GGally")
suppressPackageStartupMessages(library("tidyverse"))
suppressPackageStartupMessages(library("GGally")) # nice pairs plot 

car_data <- readr::read_csv(
  file = "https://cdn.rawgit.com/lidom/Teaching_Repo/bc692b56/autodata.csv",
  show_col_types = FALSE)

my_car_df <- car_data %>% 
 dplyr::mutate(
  "MPG"   = MPG.city, 
  "HP"    = Horsepower) %>%
 select("MPG", "HP")

ggpairs(my_car_df) + theme_bw()
```
:::

The plots indicate a positive, but non-linear relationship between the outcome variable `MPG` (gasoline consumption in milles per gallon) and the predictor `HP` (power of the mashing in horsepower). 

If we do not take this into account, we get bad model fits as indicated by diagnostic plots:

::: {.panel-tabset}

## Base `R`

```{r, fig.dim = c(5, 10)}
lm_obj_1 <- lm(MPG ~ HP, data = my_car_df)

par(mfrow=c(2,1))
plot(y=my_car_df$MPG, x=my_car_df$HP, 
     main = "Simple Linear Regression",
     xlab = "Horse Power", 
     ylab = "Miles per Gallon")
abline(lm_obj_1)
##
plot(lm_obj_1, which=1, 
       main = "Simple Linear Regression")
```


## Tidyverse `R`

```{r}
## install.packages("tidyverse")
## install.packages("ggfortify")
suppressPackageStartupMessages(library("tidyverse"))
library("ggfortify") # tidy diagnostic plots

lm_obj_1 <- lm(MPG ~ HP, data = my_car_df)

## Plot: Simple Linear Regression
my_car_df %>%
ggplot(aes(x=HP, y=MPG)) + 
    geom_point(alpha=0.8, size=3)+
    stat_smooth(method='lm', formula = y~x) +
    theme_classic() + 
    labs(x = "Horse Power", y = "Miles per Gallon", 
         title = "Simple Linear Regression")

## Diagnostic Plot: Simple Linear Regression 
autoplot(lm_obj_1, which = 1)
```
:::


A polynomial regression model with polynomial degree 2 improves the model fit.


::: {.panel-tabset}

## Base `R`

```{r, fig.dim = c(5, 10)}
## Adding new variable HP squared:
my_car_df$HP_sq <- car_data$Horsepower^2

lm_obj_2 <- lm(MPG ~ HP + HP_sq, data = my_car_df)

par(mfrow=c(2,1))
plot(y=my_car_df$MPG, x=my_car_df$HP,
     main = "Polynomial Regression (Degree: 2)",
     xlab = "Horse Power", 
     ylab = "Miles per Gallon")
X_seq <- seq(from = min(my_car_df$HP), 
             to   = max(my_car_df$HP), len = 25)
lines(y = predict(lm_obj_2, 
                  newdata=data.frame("HP"    = X_seq, 
                                     "HP_sq" = X_seq^2)), 
      x = X_seq)
plot(lm_obj_2, which=1, 
       main = "Polynomial Regression (Degree: 2)")
```


## Tidyverse `R`

```{r}
## install.packages("tidyverse")
## install.packages("ggfortify")
suppressPackageStartupMessages(library("tidyverse"))
library("ggfortify") # tidy diagnostic plots

## Adding new variable HP squared:
my_car_df <- my_car_df %>% 
 dplyr::mutate(
  "HP_sq" = HP^2
)

## Polynomial Regression

lm_obj_2 <- lm(MPG ~ HP + HP_sq, data = my_car_df)

## Plot: Polynomial Regression
my_car_df %>%
ggplot(aes(x=HP, y=MPG)) + 
    geom_point(alpha=0.8, size=3)+
    stat_smooth(method='lm', formula = y~poly(x, 2, raw = TRUE)) +
    theme_classic() + 
    labs(x = "Horse Power", y = "Miles per Gallon", 
         title = "Polynomial Regression", 
         subtitle = "Polynomial Degree: 2")

## Diagnostic Plot: Polynomial Regression
autoplot(lm_obj_2, which = 1)     
```

:::


### Behavior of the OLS Estimates for Resampled Data 

Usually, we only observe *one* estimate $\hat{\beta}$ of $\beta$ computed based on *one* given dataset (one realization of the random sample). However, in order to understand the statistical properties of the estimators $\hat{\beta}$ we need to view them as random variables which yield different realizations in repeated realizations of the random sample from Model (@eq-LinMod). This allows us then to think about questions like: 

* Is the estimator able to estimate the unknown parameter-value correctly on average?  
* Are the estimation results more precise if we have more data?

A first idea about the statistical properties of the estimator $\hat{\beta}$ can be gained using Monte Carlo simulations as following.
```{r}
## Sample sizes
n_small      <-  30 # smallish sample size
n_large      <- 100 # largish sample size

## True parameter values
beta0 <- 1
beta1 <- 1

## Monte-Carlo (MC) Simulation 
## 1. Generate data
## 2. Compute and store estimates
## Repeat steps 1. and 2. many times
set.seed(3)
## Number of Monte Carlo repetitions
## How many samples to draw from the models
B          <- 1000

## Containers to store the estimation results
beta0_estimates_n_small <- numeric(B)
beta1_estimates_n_small <- numeric(B)
beta0_estimates_n_large <- numeric(B)
beta1_estimates_n_large <- numeric(B)

for(b in 1:B){
## Generate artificial samples (n_small)
error_n_small     <- rnorm(n_small, mean = 0, sd = 5)
X_n_small         <- runif(n_small, min = 1, max = 10)
Y_n_small         <- beta0 + beta1 * X_n_small + error_n_small
lm_obj            <- lm(Y_n_small ~ X_n_small) 
## Save estimation results 
beta0_estimates_n_small[b] <- lm_obj$coefficients[1]
beta1_estimates_n_small[b] <- lm_obj$coefficients[2]

## Generate artificial samples (n_large)
error_n_large     <- rnorm(n_large, mean = 0, sd = 5)
X_n_large         <- runif(n_large, min = 1, max = 10)
Y_n_large         <- beta0 + beta1 * X_n_large + error_n_large
lm_obj            <- lm(Y_n_large ~ X_n_large)
## Save estimation results 
beta0_estimates_n_large[b] <- lm_obj$coefficients[1]
beta1_estimates_n_large[b] <- lm_obj$coefficients[2] 
}
```

Now, we have produced `B=1000` realizations of the estimators $\hat\beta_0$ and $\hat\beta_1$ and saved these realizations in the vectors 

* `beta0_estimates_n_small`
* `beta1_estimates_n_small`
* `beta0_estimates_n_large` 
* `beta1_estimates_n_large` 


These artificially generated realizations allow us to visualize the behavior of the OLS estimates for the repeatedly sampled data. 

```{r, fig.width=6, fig.height=4.5, fig.align='center', echo=FALSE}
## Plotting the results
suppressPackageStartupMessages(library("scales")) # alpha() produces transparent colors

## Define a common y-axis range
y_range <- range(beta0_estimates_n_small,
                 beta1_estimates_n_small)*1.1

## Generate the plot
par(family = "serif") # Serif fonts
## Layout of plotting area
layout(matrix(c(1:6), 2, 3, byrow = TRUE), widths = c(3,1,1))
## Plot 1
plot(x=0, y=0, axes=FALSE, xlab="X", ylab="Y", type="n",
     xlim=c(1,10), ylim=c(-5,35), main="Small Sample (n=30)")
axis(1, tick = FALSE); axis(2, tick = FALSE, las = 2)
for(b in 1:B){
abline(a=beta0_estimates_n_small[b], 
       b=beta1_estimates_n_small[b], lty=2, lwd = 1.3, col="darkorange")
}
abline(a = beta0, b = beta1, lwd=1.3, col="darkblue")
legend("topleft", col=c("darkorange", "darkblue"), legend=c(
"Sample regression lines from repeated samples", 
                  "Population regression line"), 
       lwd=1.3, lty=c(2,1), bty="n")
## Plot 2
plot(x=rep(0,B), y=beta0_estimates_n_small, axes=FALSE, 
     xlab="", ylab="", pch=19, cex=1.2, ylim=y_range,
  main=expression(hat(beta)[0]), col=alpha("red",0.2))
points(x = 0, y=beta0, pch="-", cex = 1.2, col="black")
text(x=0, y=beta0, labels = expression(beta[0]), pos = 4)
## Plot 3
plot(x=rep(0,B), y=beta1_estimates_n_small, axes=FALSE, 
     xlab="", ylab="", pch=19, cex=1.2, ylim=y_range,
  main=expression(hat(beta)[1]), col=alpha("red",0.2))
points(x = 0, y=beta1, pch="-", cex = 1.2, col="black")
text(x=0, y=beta1, labels = expression(beta[1]), pos = 4)
## Plot 4
plot(x=0, y=0, axes=FALSE, xlab="X", ylab="Y", type="n",
     xlim=c(1,10), ylim=c(-5,35), main="Large Sample (n=100)")
axis(1, tick = FALSE); axis(2, tick = FALSE, las = 2)
for(b in 1:B){
abline(a=beta0_estimates_n_large[b], 
       b=beta1_estimates_n_large[b], lty=2, lwd = 1.3, col="darkorange")
}
abline(a = beta0, b = beta1, lwd=1.3, col="darkblue")
## Plot 5
plot(x=rep(0, B), y=beta0_estimates_n_large, axes=FALSE, 
     xlab="", ylab="", pch=19, cex=1.2, ylim=y_range,
  main=expression(hat(beta)[0]), col=alpha("red",0.2))
points(x = 0, y=beta0, pch="-", cex = 1.2, col="black")
text(x=0, y=beta0, labels = expression(beta[0]), pos = 4)
## Plot 6
plot(x=rep(0, B), y=beta1_estimates_n_large, axes=FALSE, 
     xlab="", ylab="", pch=19, cex=1.2, ylim=y_range,
  main=expression(hat(beta)[1]), col=alpha("red",0.2))
points(x=0, y=beta1, pch="-", cex = 1.2, col="black")
text(x=0, y=beta1, labels = expression(beta[1]), pos = 4)
```

This are promising plots: 

* The realizations of $\hat\beta_0$ and $\hat\beta_1$ are scattered around the true (unknown) parameter values $\beta_0$ and $\beta_1$. This indicates unbiasdness of the estimators. 
* In the case of a larger sample size, the realizations of $\hat\beta_0$ and $\hat\beta_1$ concentrate stronger around the true (unknown) parameter values $\beta_0$ and $\beta_1$. This indicates consistency of the estimators.  

However, this was only a simulation for one specific data generating process. Such a Monte Carlo simulation does not allow us to generalize these properties. In the following chapters, we use theoretical arguments to investigate under which assumptions we can make *general* statements about the distributional properties of the estimator $\hat\beta.$ 


## References {-}